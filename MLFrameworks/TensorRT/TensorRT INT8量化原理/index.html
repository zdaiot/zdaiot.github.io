<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇文章主要参考了TensorRT(5)-INT8校准原理，并添加了一些自己的见解。 Low Precision Inference现有的深度学习框架，如Pytorch、Tensorflow在训练一个深度神经网络时，往往都会使用 float 32（Full Precise ，简称FP32）的数据精度来表示，权值、偏置、激活值等。若一个网络很深的话，比如像VGG，ResNet这种，网络参数是极其多的"><meta name="keywords" content="TensorRT,INT8"><meta property="og:type" content="article"><meta property="og:title" content="TensorRT INT8量化原理"><meta property="og:url" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT INT8量化原理/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="本篇文章主要参考了TensorRT(5)-INT8校准原理，并添加了一些自己的见解。 Low Precision Inference现有的深度学习框架，如Pytorch、Tensorflow在训练一个深度神经网络时，往往都会使用 float 32（Full Precise ，简称FP32）的数据精度来表示，权值、偏置、激活值等。若一个网络很深的话，比如像VGG，ResNet这种，网络参数是极其多的"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/no_satuation_int8_quantization.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/distribution-of-different-layers.png.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/satuation_int8_quantization.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/Calibration-process.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/satuation-before-and-after.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/Calibration-result1.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/Calibration-result2.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/DP4A.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/performance-auccary.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/performance-speed.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/performance-titanx.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/performance-dgpu.png"><meta property="og:updated_time" content="2020-07-07T01:02:31.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="TensorRT INT8量化原理"><meta name="twitter:description" content="本篇文章主要参考了TensorRT(5)-INT8校准原理，并添加了一些自己的见解。 Low Precision Inference现有的深度学习框架，如Pytorch、Tensorflow在训练一个深度神经网络时，往往都会使用 float 32（Full Precise ，简称FP32）的数据精度来表示，权值、偏置、激活值等。若一个网络很深的话，比如像VGG，ResNet这种，网络参数是极其多的"><meta name="twitter:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT%20INT8量化原理/no_satuation_int8_quantization.png"><link rel="canonical" href="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT INT8量化原理/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>TensorRT INT8量化原理 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT INT8量化原理/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> TensorRT INT8量化原理<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MLFrameworks/TensorRT/TensorRT INT8量化原理.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-07-07 09:02:31" itemprop="dateCreated datePublished" datetime="2020-07-07T09:02:31+08:00">2020-07-07</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/" itemprop="url" rel="index"><span itemprop="name">MLFrameworks</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>10k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>9 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇文章主要参考了<a href="https://arleyzhang.github.io/articles/923e2c40/" target="_blank" rel="noopener">TensorRT(5)-INT8校准原理</a>，并添加了一些自己的见解。</p><h2 id="Low-Precision-Inference"><a href="#Low-Precision-Inference" class="headerlink" title="Low Precision Inference"></a>Low Precision Inference</h2><p>现有的深度学习框架，如Pytorch、Tensorflow在训练一个深度神经网络时，往往都会使用 <strong>float 32（Full Precise ，简称FP32）</strong>的数据精度来表示，权值、偏置、激活值等。若一个网络很深的话，比如像VGG，ResNet这种，网络参数是极其多的，计算量就更多了（比如VGG 19.6 billion FLOPS, ResNet-152 11.3 billion FLOPS）。如果多的计算量，如果都采用FP32进行推理，对于嵌入式设备来说计算量是不能接受的。解决此问题主要有两种方案：</p><ol><li>模型压缩、剪枝。在这里不做讨论。</li><li>在部署推理时（inference）使用低精度数据，比如INT8。当然训练的时候仍然采取FP32的精度。</li></ol><p><strong>下面从经验上分析低精度推理的可行性。</strong></p><p>实际上有些人认为，即便在推理时使用低精度的数据（比如INT8），在提升速度的同时，也并不会造成太大的精度损失，比如 <a href="https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/" target="_blank" rel="noopener">Why are Eight Bits Enough for Deep Neural Networks?</a> 以及<a href="https://towardsdatascience.com/low-precision-inference-with-tensorrt-6eb3cda0730b" target="_blank" rel="noopener">Low Precision Inference with TensorRT</a> 这两篇博文。</p><p>文章的作者认为网络在训练的过程中学习到了数据样本的模式可分性，同时由于数据中存在的噪声，使得网络具有较强的鲁棒性，也就是说在输入样本中做轻微的变动并不会过多的影响性能。与图像上目标间的位置、姿态、角度等变化相比，这些噪声引进的变动只是很少的一部分。但实际上这些噪声引进的变动同样会使各个层的激活值输出发生变动，然而却对结果影响不大，也就是说<strong>训练好的网络对这些噪声具有一定的容忍度（tolerance ）。</strong></p><p>正是由于在训练过程中使用高精度（FP32）的数值表示，才使得网络具有一定的容忍度。训练时使用高精度的数值表示，可以使得网络在每一步训练都会对参数进行少量的修正，这在网络最后收敛的时候是很重要的，因为收敛的时候要求修正量很小很小（一般训练初始 阶段学习率稍大，越往后学习率越小）。</p><p>那么<strong>如果使用低精度的数据来表示网络参数以及中间值的话，势必会存在误差，这个误差某种程度上可以认为是一种噪声</strong>。那也就是说，<strong>使用低精度数据引进的差异是在网络的容忍度之内的</strong>，所以对结果不会产生太大影响。</p><p>以上分析都是基于经验的，理论上的分析比较少，不过文章提到了两篇 paper，如下：</p><ul><li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/37631.pdf" target="_blank" rel="noopener">Improving the speed of neural networks on CPUs</a></li><li><a href="https://arxiv.org/abs/1412.7024" target="_blank" rel="noopener">Training deep neural networks with low precision multiplications</a></li></ul><p>感兴趣的同学可以自行研究。</p><p>TensorRT 的INT8模式只支持计算能力为6.1的GPU（Compute Capability 6.1 ），比如： GP102 (Tesla P40 and NVIDIA Titan X), GP104 (<a href="https://devblogs.nvidia.com/parallelforall/new-pascal-gpus-accelerate-inference-in-the-data-center/" target="_blank" rel="noopener">Tesla P4</a>), and GP106 GPUs，主要根源是这些<strong>GPU支持 DP4A硬件指令</strong>。DP4A下面会稍微介绍一下。</p><h2 id="TensorRT-INT8-Inference"><a href="#TensorRT-INT8-Inference" class="headerlink" title="TensorRT INT8 Inference"></a>TensorRT INT8 Inference</h2><p>首先看一下不同精度的动态范围：</p><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">动态范围</th><th style="text-align:left">最小正数</th></tr></thead><tbody><tr><td style="text-align:left">FP32</td><td style="text-align:left">$−3.4×10^{38} +3.4×10^{38}$</td><td style="text-align:left">$1.4×10^{−45}$</td></tr><tr><td style="text-align:left">FP16</td><td style="text-align:left">$−65504 +65504$</td><td style="text-align:left">$5.96×10^{−8}$</td></tr><tr><td style="text-align:left">INT8</td><td style="text-align:left">$−128 +127$</td><td style="text-align:left">1</td></tr></tbody></table></div><p>实际上将FP32的精度降为INT8还是比较具有挑战性的。</p><h3 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a>Quantization</h3><p>将FP32降为INT8的过程相当于<strong>信息再编码</strong>（re-encoding information ），就是原来使用32bit来表示一个tensor，现在使用8bit来表示一个tensor，还要求精度不能下降太多。将FP32转换为 INT8的操作需要针对<strong>每一层的输入张量（tensor）和 网络学习到的参数（learned parameters）</strong>进行。</p><p>首先能想到的最简单的映射方式就是线性映射（或称线性量化，linear quantization）, 就是说映射前后的关系满足下式：</p><script type="math/tex;mode=display">
\text{FP32 Tensor (T) = scale_factor(sf) * 8-bit Tensor(t) + FP32_bias (b)}</script><p>试验证明，偏置实际上是不需要的，因此去掉偏置，也就是</p><script type="math/tex;mode=display">
T = sf * t</script><p>$sf$是<strong>每一层上每一个tensor</strong>的换算系数或称比例因子（scaling factor），因此现在的问题就变成了如何确定比例因子。然后最简单的方法是下图这样的：</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/no_satuation_int8_quantization.png" alt="no_satuation_int8_quantization"></p><ul><li>简单的将一个tensor 中的 $-|max|$ 和 $|max|$ FP32 value 映射为 -127 和 127 ，中间值按照线性关系进行映射。</li><li>称这种映射关系为不饱和的（No saturation，$max$很大实际模型的激活值可能取不到），对称的。</li></ul><p>但是试验结果显示这样做会导致比较大的精度损失。</p><p>下面这张图展示的是不同网络结构的不同layer的激活值分布，有卷积层，有池化层，他们之间的分布很不一样，因此<strong>合理的量化方式应该适用于不同的激活值分布，并且减小信息损失。</strong>因为从FP32到INT8其实就是一种信息再编码的过程。</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/distribution-of-different-layers.png.png" alt="1535654439553"></p><p>上图的解释有思考如下：</p><ul><li>上图是一些网络模型中间层的激活值统计，横坐标是激活值，纵坐标是统计数量的归一化表示，这里是归一化表示，不是绝对数值统计；</li><li>这个激活值统计针对的是<strong>一批图片</strong>，不同的图片输出的激活值不完全相同。所以图上并不是一条曲线而是多条曲线（一张图片对应一条曲线，或者称为散点图更好一点），只不过前面一部分重复在一块了（红色虚线圈起来的部分），说明对于<strong>不同图片生成的大部分激活值其分布是相似的</strong>；但是在激活值比较大时（红色实线圈起来的部分），曲线不重复了，一个激活值对应多个不同的统计量，这时的激活值分布就比较乱了。</li><li>后面这一部分在整个层中是占少数的（占比很小，比如10^-9, 10^-7, 10^-3），因此<strong>后面这一段完全可以不考虑到映射关系中去，保留激活值分布的主要部分</strong>。开始以为网络之所以能把不同类别的图片分开是由于后面实线部分的差异导致的，后来想了一下：这个并不包含Tensor空间位置的分布，只是数值上的分布，所以后面的应该对结果影响不大。</li></ul><p>因此TensorRT的做法是：</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/satuation_int8_quantization.png" alt="satuation_int8_quantization"></p><ul><li>这种做法不是将 $±|max|$ 映射为 $±127$，而是存在一个 阈值 $|T|$ ，将 $±|T|$ 映射为±127，显然这里 $|T|&lt;|max|$。</li><li>超出 阈值 $±|T|$ 外的直接映射为阈值 $±127$。比如上图中的三个红色点，直接映射为$-127$。</li><li>称这种映射关系为<strong>饱和的（Saturate ），不对称的。</strong></li><li><strong>只要阈值选取得当，就能将分布散乱的较大的激活值舍弃掉，也就有可能使精度损失不至于降低太多。</strong></li></ul><p>网络的前向计算涉及到两部分数值：权值和激活值（weights 和activation，二者要做乘法运算，可以将激活值看作是某层的输出要和下一层的权重做乘法运算），Szymon Migacz也提到他们曾经做过实验，说对weights 做saturation变换没有什么变化，因此<strong>对于weights的INT8量化就使用的是不饱和的方式</strong>（饱和方式需要确定$|T|$，比较麻烦）；而对activation做saturation变换就有比较显著的性能提升，因此<strong>对activation使用的是饱和的量化方式</strong>。</p><p>那现在的问题是如何确定$|T|$？假设有一个FP32的tensor，FP32肯定是能够表达这个tensor的最佳分布。现在我们要用一个不同的分布（INT8）来表达这个tensor，这个INT8 分布不是一个最佳的分布。<strong>饱和的INT8分布由于阈值|T|的取值会有很多种情况（$128 \sim |max|$），其中肯定有一种情况是相对其他最接近FP32的，我们就是要把这种情况找出来。</strong></p><p>那么就<strong>需要一个衡量指标来衡量不同的INT8 分布与原来的FP3F2分布之间的差异程度。</strong>这个衡量指标就是<strong>相对熵</strong>（relative entropy），又称为<strong>KL散度</strong>（<strong>Kullback–Leibler divergence</strong>，简称<strong>KLD</strong>），信息散度（information divergence），信息增益（information gain）。叫法实在太多了，最常见的就是相对熵，跟交叉熵也是有关系的。</p><ul><li><p>假设我们要给一个信息进行完美编码，那么最短平均编码长度就是信息熵。</p></li><li><p>如果编码方案不一定完美（由于对概率分布的估计不一定正确），这时的平均编码长度就是交叉熵。</p><p>平均编码长度 = 最短平均编码长度 + 一个增量</p><p>交叉熵在深度学习中广泛使用，衡量了测试集标签分布和模型预测分布之间的差异程度。</p></li><li><p><strong>编码方法不一定完美时，平均编码长度相对于最小值的增加量（即上面那个增量）是相对熵。</strong></p></li></ul><p>即 <strong>交叉熵=信息熵+相对熵</strong></p><p>通俗的理解 信息熵，交叉熵，相对熵，参考：<a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">知乎：如何通俗的解释交叉熵与相对熵?</a></p><p>如何理解信息熵用来表示最短平均编码长度，参考： <a href="http://blog.csdn.net/hearthougan/article/details/77774948" target="_blank" rel="noopener">如何理解用信息熵来表示最短的平均编码长度</a></p><p>在这里，FP32的tensor就是要表达的信息量，FP32也是最佳分布（可以认为最短编码长度32bit），现在要做的是使用INT8 来编码FP32的信息，同时要求INT8编码后差异尽可能最小。考虑两个分布 P（FP32）、Q（INT8）KL散度计算如下：</p><script type="math/tex;mode=display">
\text{KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)}</script><p>$P，Q$分别称为reference_distribution、quantize _distribution，$i$表示第$i$层。实际上这里也说明了<strong>每一层的tensor 的$|T|$值都是不一样的。确定每一层的$|T|$值的过程称为校准（Calibration ）。</strong></p><h3 id="Calibration"><a href="#Calibration" class="headerlink" title="Calibration"></a>Calibration</h3><p>上面已经说了 <strong>KL散度越小代表INT8编码后的信息损失越少</strong>。接下来来看看如何根据KL散度寻找最佳INT8分布。其实前面也已经提到了，如果要让最后的精度损失不大，是要考虑一些先验知识的，这个先验知识就是每一层在 FP32精度下的激活值分布，只有根据这个才能找到更加合理的阈值$|T|$。也就是说首先得有一个以FP32精度训练好的模型。基本上现有的深度学习框架都是默认FP32精度的，有些模型还支持FP16精度训练。所以基本上只要没有特别设定，训练出来的模型肯定是 FP32 的。</p><p>那激活值分布如何得到？这里的做法是<strong>从验证集</strong>选取一个子集作为校准集（Calibration Dataset），校准集应该具有代表性，多样性，最好是验证集的一个子集，不应该只是分类类别的一小部分。激活值分布就是从校准集中得到的。</p><p>按照<a href="http://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#calibrationset" target="_blank" rel="noopener">NVIDIA 官方的说法</a>：</p><blockquote><p>Note: The calibration set must be representative of the input provided to TensorRT at runtime; for example, for image classification networks, it should not consist of images from just a small subset of categories. For ImageNet networks, around 500 calibration images is adequate.</p></blockquote><p>对于ImageNet 数据集来说 校准集大小一般500张图片就够了（Szymon Migacz的演讲说用1000张），这里有点怀疑也有点震惊，没想到 ImageNet 1000个分类，100多万张图片，500张就够了，不过从2.5节的图表中的结果可以看出500张确实够了。</p><p>然后要做的是：</p><ul><li>首先在校准集上进行 FP32 inference 推理；</li><li>对于<strong>网络的每一层</strong>（遍历）：<ul><li>收集这一层的激活值，并做<strong>直方图</strong>（histograms ），分成几个组别（bins）（官方给的一个说明使用的是2048组），分组是为了下面遍历|T| 时，减少遍历次数；</li><li>对于不同的阈值|T|进行遍历，因为这里|T|的取值肯定在第128-2047组之间，所以就选取每组的中间值进行遍历；<ul><li>选取使得 KL_divergence(ref_distr, quant_distr) 取得最小值的 |T|。</li></ul></li></ul></li><li>返回一系列|T|值，每一层都有一个|T|。创建 <strong>CalibrationTable</strong> 。</li></ul><p>解释一下：假设最后使得KL散度最小的|T|值是第200组的中间值，那么就把原来第0-200组的数值线性映射到0-128之间，超出范围的直接映射到128。</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/Calibration-process.png" alt="1516636471289"></p><p>校准的过程可以参考一下这个：<a href="https://www.jianshu.com/p/43318a3dc715，" target="_blank" rel="noopener">https://www.jianshu.com/p/43318a3dc715，</a> 这篇文章提供了一个详细的根据KL散度来将原始信息进行编码的例子，包括直方图的使用。跟这里的校准过程极为相像。</p><p>下面是一个官方 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="noopener">GTC2017 PPT</a> 中给的校准的伪代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先分成 2048个组，每组包含多个数值（基本都是小数）</span></span><br><span class="line">Input: FP32 histogram H with <span class="number">2048</span> bins: bin[ <span class="number">0</span> ], …, bin[ <span class="number">2047</span> ] </span><br><span class="line">  </span><br><span class="line">For i in range( 128 , 2048 ): // |T|的取值肯定在 第128-2047 组之间,取每组的中点</span><br><span class="line">	reference_distribution_P = [ bin[ <span class="number">0</span> ] , ..., bin[ i<span class="number">-1</span> ] ] <span class="comment">// 选取前 i 组构成P，i&gt;=128</span></span><br><span class="line">	outliers_count = sum( bin[ i ] , bin[ i+<span class="number">1</span> ] , … , bin[ <span class="number">2047</span> ] ) <span class="comment">//边界外的组</span></span><br><span class="line">	reference_distribution_P[ i<span class="number">-1</span> ] += outliers_count <span class="comment">//边界外的组加到边界P[i-1]上，没有直接丢掉</span></span><br><span class="line">	P /= sum(P) <span class="comment">// 归一化</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 将前面的P（包含i个组，i&gt;=128），映射到 0-128 上，映射后的称为Q，Q包含128个组，</span></span><br><span class="line">    <span class="comment">// 一个整数是一组</span></span><br><span class="line">	candidate_distribution_Q = quantize [ bin[ <span class="number">0</span> ], …, bin[ i<span class="number">-1</span> ] ] into <span class="number">128</span> levels</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//这时的P（包含i个组，i&gt;=128）和Q向量（包含128个组）的大小是不一样的，无法直接计算二者的KL散度</span></span><br><span class="line">	<span class="comment">//因此需要将Q扩展为 i 个组，以保证跟P大小一样</span></span><br><span class="line">	expand candidate_distribution_Q to ‘ i ’ bins </span><br><span class="line">	</span><br><span class="line">	Q /= sum(Q) <span class="comment">// 归一化</span></span><br><span class="line">	<span class="comment">//计算P和Q的KL散度</span></span><br><span class="line">	divergence[ i ] = KL_divergence( reference_distribution_P, candidate_distribution_Q)</span><br><span class="line">End For</span><br><span class="line"><span class="comment">//找出 divergence[ i ] 最小的数值，假设 divergence[m] 最小，</span></span><br><span class="line"><span class="comment">//那么|T|=( m + 0.5 ) * ( width of a bin )</span></span><br><span class="line">Find index ‘m’ <span class="keyword">for</span> which divergence[ m ] is minimal</span><br><span class="line">threshold = ( m + <span class="number">0.5</span> ) * ( width of a bin )</span><br></pre></td></tr></table></figure><p>解释一下第16行：</p><ul><li>计算KL散度 KL_divergence(P, Q) 时，要求序列P和Q的长度一致，即 len(P) == len(Q)；</li><li>Candidate_distribution_Q 是将 P 线性映射到 128个bins得到的，长度为128。而reference_distribution_P 包含 i （i&gt;=128）个 bins （bin[0] - bin[i-1] ），二者长度不等；</li><li>需要将 candidate_distribution_Q 扩展回 i 个bins 然后才能与 i个bins 的 reference_distribution_P计算KL散度。</li></ul><p>举个简单的例子：</p><ul><li><p>假设reference_distribution_P 包含 8 个bins（这里一个bin就只包含一个数据）:</p><p>P = [ 1, 0, 2, 3, 5, 3, 1, 7]</p></li><li><p>我们想把它映射为 2 个bins，于是 4个一组合并：</p><p>[1 + 0 + 2 + 3 , 5 + 3 + 1 + 7] = [6, 16]</p></li><li><p>然后要成比例的 扩展回到 8个组，保留原来是0的组：</p><p>Q = [ 6/3, 0, 6/3, 6/3, 16/4, 16/4, 16/4, 16/4] = [ 2, 0, 2, 2, 4, 4, 4, 4]</p></li><li><p>然后对 P和Q进行标准化：</p><p>P /= sum(P) 、Q /= sum(Q)</p></li><li><p>最后计算散度：</p><p>result = KL_divergence(P, Q)</p></li></ul><p>我们来看看 ResNet-152中 res4b30层校准前后的结果对比，图中那个白线就是|T|的取值（可能小于128，因为分成2048组是最前面的128组其值不一定大于128）：</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/satuation-before-and-after.png" alt="1516638878836"></p><p>再看看其他几种网络的校准情况：</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/Calibration-result1.jpg" alt="result_1"></p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/Calibration-result2.jpg" alt="result_2"></p><h3 id="DP4A-Dot-Product-of-4-8-bits-Accumulated-to-a-32-bit"><a href="#DP4A-Dot-Product-of-4-8-bits-Accumulated-to-a-32-bit" class="headerlink" title="DP4A(Dot Product of 4 8-bits Accumulated to a 32-bit)"></a>DP4A(<strong>D</strong>ot <strong>P</strong>roduct of <strong>4</strong> 8-bits <strong>A</strong>ccumulated to a 32-bit)</h3><p>TensorRT 进行优化的方式是 DP4A (<strong>D</strong>ot <strong>P</strong>roduct of <strong>4</strong> 8-bits <strong>A</strong>ccumulated to a 32-bit)，如下图：</p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/DP4A.png" alt="1516642345023"></p><p>这是PASCAL 系列GPU的硬件指令，INT8卷积就是使用这种方式进行的卷积计算。这个没搞太明白是怎么回事，参考这篇博客获取详细信息<a href="https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/" target="_blank" rel="noopener">Mixed-Precision Programming with CUDA 8</a>。下面是 官方 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="noopener">GTC2017 PPT</a> 中给的INT8卷积计算的伪代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// I8 input tensors: I8_input, I8_weights, INT8输入tensor</span></span><br><span class="line"><span class="comment">// I8 output tensors: I8_output， INT8输出tensor</span></span><br><span class="line"><span class="comment">// F32 bias (original bias from the F32 model),FP32的偏置</span></span><br><span class="line"><span class="comment">// F32 scaling factors: input_scale, output_scale, weights_scale[K], 这个是前面说的缩放因子sf</span></span><br><span class="line">I32_gemm_out = I8_input * I8_weights <span class="comment">// Compute INT8 GEMM (DP4A)，卷积计算，INT32输出</span></span><br><span class="line">F32_gemm_out = (<span class="keyword">float</span>)I32_gemm_out <span class="comment">// Cast I32 GEMM output to F32 float，强制转换为FP32</span></span><br><span class="line"><span class="comment">//前面计算I8_input * I8_weights时，总的缩放系数为 input_scale * weights_scale[K]</span></span><br><span class="line"><span class="comment">//但是输出的缩放系数为output_scale，所以为了保证缩放程度匹配，要将F32_gemm_out乘以 </span></span><br><span class="line"><span class="comment">//output_scale / (input_scale * weights_scale[ i ] )</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">// At this point we have F32_gemm_out which is scaled by ( input_scale * weights_scale[K] ),</span></span><br><span class="line"><span class="comment">// but to store the final result in int8 we need to have scale equal to "output_scale", so we have to rescale:</span></span><br><span class="line"><span class="comment">// (this multiplication is done in F32, *_gemm_out arrays are in NCHW format)</span></span><br><span class="line">For i in <span class="number">0</span>, ... K<span class="number">-1</span>:</span><br><span class="line">rescaled_F32_gemm_out[ :, i, :, :] = F32_gemm_out[ :, i, :, :] * [ output_scale /(input_scale * weights_scale[ i ] ) ]</span><br><span class="line">  </span><br><span class="line"><span class="comment">//将FP32精度的偏置 乘上缩放因子，加到前面的计算结果中</span></span><br><span class="line"><span class="comment">// Add bias, to perform addition we have to rescale original F32 bias so that it's scaled with "output_scale"</span></span><br><span class="line">rescaled_F32_gemm_out _with_bias = rescaled_F32_gemm_out + output_scale * bias</span><br><span class="line"><span class="comment">//ReLU 激活</span></span><br><span class="line"><span class="comment">// Perform ReLU (in F32)</span></span><br><span class="line">F32_result = ReLU(rescaled_F32_gemm_out _with_bias)</span><br><span class="line"><span class="comment">//重新转换为 INT8</span></span><br><span class="line"><span class="comment">// Convert to INT8 and save to global</span></span><br><span class="line">I8_output = Saturate( Round_to_nearest_integer( F32_result ) )</span><br></pre></td></tr></table></figure><p>它这个INT8卷积的计算是这样的，虽然输入的tensor已经降为 INT8，但是在卷积计算的时候用了DP4A的计算模式，卷积计算完之后是INT32的，然后又要转成 FP32，然后激活，最后再将FP32的转为INT8.</p><p>只知道这么计算会快很多，但不知道为什么，详情还是看<a href="https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/" target="_blank" rel="noopener">Mixed-Precision Programming with CUDA 8</a> 这个吧，我看的也是糊里糊涂的。</p><p>不过这个对于tensorRT的使用没啥影响，这个是很底层的东西，涉及到硬件优化。</p><h3 id="Typical-workflow-in-TensorRT"><a href="#Typical-workflow-in-TensorRT" class="headerlink" title="Typical workflow in TensorRT"></a>Typical workflow in TensorRT</h3><p>典型的工作流还是直接使用 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="noopener">GTC2017 PPT</a> 原文说法吧：</p><ul><li>You will need:<ul><li>Model trained in FP32.</li><li>Calibration dataset.</li></ul></li><li>TensorRT will:<ul><li>Run inference in FP32 on calibration dataset.</li><li>Collect required statistics.</li><li>Run calibration algorithm → optimal scaling factors.</li><li>Quantize FP32 weights → INT8.</li><li>Generate “CalibrationTable” and INT8 execution engine.</li></ul></li></ul><h3 id="Results-Accuracy-amp-Performance"><a href="#Results-Accuracy-amp-Performance" class="headerlink" title="Results - Accuracy &amp; Performance"></a>Results - Accuracy &amp; Performance</h3><p><strong>精度并没有损失太多</strong></p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/performance-auccary.png" alt="1516641328187"></p><p><strong>速度提升还蛮多的，尤其是当 batch_size 大于1时，提升更明显</strong></p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/performance-speed.png" alt="1516641398718"></p><p><strong>TITAN X GPU优化效果</strong></p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/performance-titanx.png" alt="1516642501791"></p><p><strong>DRIVE PX 2, dGPU 优化效果</strong></p><p><img src="/MLFrameworks/TensorRT/TensorRT INT8量化原理/performance-dgpu.png" alt="1516642517725"></p><h3 id="Open-challenges-improvements"><a href="#Open-challenges-improvements" class="headerlink" title="Open challenges / improvements"></a>Open challenges / improvements</h3><p>一些开放式的提升和挑战：</p><ul><li>Unsigned int8 for activations after ReLU. 无符号 INT8 的映射。</li><li>RNNs → open research problem. TensorRT 3.0开始已经支持RNN了。</li><li>Fine tuning of saturation thresholds. 对阈值 |T|的 微调方法。</li><li>Expose API for accepting custom, user provided scale factors. 开放API，使用户可以自定义 换算系数（比例因子）</li></ul><p>这几个开放问题还是很值得研究的。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>介绍了一种自动化，无参数的 FP32 到 INT8 的转换方法；</li><li>对称的，不饱和的线性量化，会导致精度损失较大；</li><li>通过最小化 KL散度来选择 饱和量化中的 阈值 |T|;</li><li>FP32完全可以降低为INT8推理，精度几乎持平，速度有很大提升。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arleyzhang.github.io/articles/923e2c40/" target="_blank" rel="noopener">TensorRT(5)-INT8校准原理</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT INT8量化原理/" title="TensorRT INT8量化原理">https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT INT8量化原理/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/TensorRT/" rel="tag"><i class="fa fa-tag"></i> TensorRT</a><a href="/tags/INT8/" rel="tag"><i class="fa fa-tag"></i> INT8</a></div><div class="post-nav"><div class="post-nav-item"><a href="/DeepLearningApplications/人脸检测/RetinaFace：Single-stage Dense Face Localisation in the Wild/" rel="prev" title="RetinaFace：Single-stage Dense Face Localisation in the Wild"><i class="fa fa-chevron-left"></i> RetinaFace：Single-stage Dense Face Localisation in the Wild</a></div><div class="post-nav-item"> <a href="/DeepLearningApplications/目标检测/目标检测评价指标/" rel="next" title="目标检测评价指标">目标检测评价指标<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Low-Precision-Inference"><span class="nav-number">1.</span> <span class="nav-text">Low Precision Inference</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT-INT8-Inference"><span class="nav-number">2.</span> <span class="nav-text">TensorRT INT8 Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantization"><span class="nav-number">2.1.</span> <span class="nav-text">Quantization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Calibration"><span class="nav-number">2.2.</span> <span class="nav-text">Calibration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DP4A-Dot-Product-of-4-8-bits-Accumulated-to-a-32-bit"><span class="nav-number">2.3.</span> <span class="nav-text">DP4A(Dot Product of 4 8-bits Accumulated to a 32-bit)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Typical-workflow-in-TensorRT"><span class="nav-number">2.4.</span> <span class="nav-text">Typical workflow in TensorRT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Results-Accuracy-amp-Performance"><span class="nav-number">2.5.</span> <span class="nav-text">Results - Accuracy &amp; Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Open-challenges-improvements"><span class="nav-number">2.6.</span> <span class="nav-text">Open challenges / improvements</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">3.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">324</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">56</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">381</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">34:13</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '2a8d733379a68137df8bd4d286e80d5c',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>