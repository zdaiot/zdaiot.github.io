<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="最近稍微学习了一下TensorRT，这里参考这很多博客，主要参考了如何使用TensorRT对训练好的PyTorch模型进行加速?。然后加上自己的一些注释。 现在训练深度学习模型主流的框架有TensorFlow，Pytorch，mxnet，caffe等。这个贴子只涉及Pytorch，对于TensorFlow的话，可以参考TensorRT部署深度学习模型，这个帖子是C++如何部署TensorRT。其实"><meta name="keywords" content="Pytorch,TensorRT,onnx"><meta property="og:type" content="article"><meta property="og:title" content="使用TensorRT对Pytorch模型加速"><meta property="og:url" content="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="最近稍微学习了一下TensorRT，这里参考这很多博客，主要参考了如何使用TensorRT对训练好的PyTorch模型进行加速?。然后加上自己的一些注释。 现在训练深度学习模型主流的框架有TensorFlow，Pytorch，mxnet，caffe等。这个贴子只涉及Pytorch，对于TensorFlow的话，可以参考TensorRT部署深度学习模型，这个帖子是C++如何部署TensorRT。其实"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-db9df98635059fa8d8db009b2347600e_720w.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-c0444b35f607445bf62fd4c7cc30be8d_720w.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-877f488bcd1c4cb278a050c1647dcaa8_720w.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-d1e0cab46990702c9a71d312c28532ce_720w.jpg"><meta property="og:updated_time" content="2021-08-20T08:38:38.414Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="使用TensorRT对Pytorch模型加速"><meta name="twitter:description" content="最近稍微学习了一下TensorRT，这里参考这很多博客，主要参考了如何使用TensorRT对训练好的PyTorch模型进行加速?。然后加上自己的一些注释。 现在训练深度学习模型主流的框架有TensorFlow，Pytorch，mxnet，caffe等。这个贴子只涉及Pytorch，对于TensorFlow的话，可以参考TensorRT部署深度学习模型，这个帖子是C++如何部署TensorRT。其实"><meta name="twitter:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-db9df98635059fa8d8db009b2347600e_720w.jpg"><link rel="canonical" href="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>使用TensorRT对Pytorch模型加速 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 使用TensorRT对Pytorch模型加速<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-06-14 15:40:33" itemprop="dateCreated datePublished" datetime="2020-06-14T15:40:33+08:00">2020-06-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-08-20 16:38:38" itemprop="dateModified" datetime="2021-08-20T16:38:38+08:00">2021-08-20</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/" itemprop="url" rel="index"><span itemprop="name">MLFrameworks</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>24k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>22 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>最近稍微学习了一下TensorRT，这里参考这很多博客，主要参考了<a href="https://zhuanlan.zhihu.com/p/88318324" target="_blank" rel="noopener">如何使用TensorRT对训练好的PyTorch模型进行加速?</a>。然后加上自己的一些注释。</p><p>现在训练深度学习模型主流的框架有TensorFlow，Pytorch，mxnet，caffe等。这个贴子只涉及Pytorch，对于TensorFlow的话，可以参考<a href="https://zhuanlan.zhihu.com/p/84125533" target="_blank" rel="noopener">TensorRT部署深度学习模型</a>，这个帖子是C++如何部署TensorRT。其实原理都是一样的，对于TensorFlow模型，需要把pb模型转化为uff模型；对于Pytorch模型，需要把pth模型转化为onnx模型；对于caffe模型，则不需要转化，因为tensorRT是可以直接读取caffe模型的；mxnet模型也是需要转化为onnx的。</p><p>对于TensorRT的安装，这里就不赘述了，之前我的博客有介绍过。</p><h2 id="Python环境下Pytorch模型转化为TensorRT"><a href="#Python环境下Pytorch模型转化为TensorRT" class="headerlink" title="Python环境下Pytorch模型转化为TensorRT"></a>Python环境下Pytorch模型转化为TensorRT</h2><p>Python环境下Pytorch模型转化为TensorRT有<strong>两种路径</strong>，一种是先把Pytorch的pt模型转化为onnx，然后再转化为TensorRT；另一种是直接把pytorch的pt模型转成TensorRT。</p><h3 id="Pytorch-gt-Onnx-gt-TensorRT"><a href="#Pytorch-gt-Onnx-gt-TensorRT" class="headerlink" title="Pytorch-&gt;Onnx-&gt;TensorRT"></a>Pytorch-&gt;Onnx-&gt;TensorRT</h3><p>首先，先把pt模型转化为onnx模型，需要安装onnx，直接<code>pip install onnx</code>即可。以ResNet50为例，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line">print(torch.__version__)</span><br><span class="line"></span><br><span class="line">input_name = [<span class="string">'input'</span>]</span><br><span class="line">output_name = [<span class="string">'output'</span>]</span><br><span class="line">input = Variable(torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)).cuda()</span><br><span class="line">model = torchvision.models.resnet50(pretrained=<span class="literal">True</span>).cuda()</span><br><span class="line">torch.onnx.export(model, input, <span class="string">'resnet50.onnx'</span>, input_names=input_name, output_names=output_name, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查一下生成的onnx</span></span><br><span class="line">test = onnx.load(<span class="string">'resnet50.onnx'</span>)</span><br><span class="line">onnx.checker.check_model(test)</span><br><span class="line">print(<span class="string">"==&gt; Passed"</span>)</span><br></pre></td></tr></table></figure><p>以上代码使用torchvision里面预训练的resnet50模型为基础，将resnet50的pt模型转化成res50.onnx，其中<strong>规定onnx的输入名是’input’，输出名是’output’</strong>，输入图像的大小是3通道224x224。其中batch size是1，其实这个batch size你可以取3、4、5等。运行这个代码就可以生成一个名为resnet50.onnx文件。</p><p>比较Pytorch和TensorRT的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> cuda</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'test.jpg'</span></span><br><span class="line">max_batch_size = <span class="number">1</span></span><br><span class="line">onnx_model_path = <span class="string">'resnet50.onnx'</span></span><br><span class="line"></span><br><span class="line">TRT_LOGGER = trt.Logger()  <span class="comment"># This logger is required to build an engine</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_img_np_nchw</span><span class="params">(filename)</span>:</span></span><br><span class="line">    image = cv2.imread(filename)</span><br><span class="line">    image_cv = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    image_cv = cv2.resize(image_cv, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    miu = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    img_np = np.array(image_cv, dtype=float) / <span class="number">255.</span></span><br><span class="line">    r = (img_np[:, :, <span class="number">0</span>] - miu[<span class="number">0</span>]) / std[<span class="number">0</span>]</span><br><span class="line">    g = (img_np[:, :, <span class="number">1</span>] - miu[<span class="number">1</span>]) / std[<span class="number">1</span>]</span><br><span class="line">    b = (img_np[:, :, <span class="number">2</span>] - miu[<span class="number">2</span>]) / std[<span class="number">2</span>]</span><br><span class="line">    img_np_t = np.array([r, g, b])</span><br><span class="line">    img_np_nchw = np.expand_dims(img_np_t, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> img_np_nchw</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HostDeviceMem</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host_mem, device_mem)</span>:</span></span><br><span class="line">        <span class="string">"""Within this context, host_mom means the cpu memory and device means the GPU memory</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.host = host_mem</span><br><span class="line">        self.device = device_mem</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Host:\n"</span> + str(self.host) + <span class="string">"\nDevice:\n"</span> + str(self.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__str__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allocate_buffers</span><span class="params">(engine)</span>:</span></span><br><span class="line">    inputs = []</span><br><span class="line">    outputs = []</span><br><span class="line">    bindings = []</span><br><span class="line">    stream = cuda.Stream()</span><br><span class="line">    <span class="keyword">for</span> binding <span class="keyword">in</span> engine:</span><br><span class="line">        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size</span><br><span class="line">        dtype = trt.nptype(engine.get_binding_dtype(binding))</span><br><span class="line">        <span class="comment"># Allocate host and device buffers</span></span><br><span class="line">        host_mem = cuda.pagelocked_empty(size, dtype)</span><br><span class="line">        device_mem = cuda.mem_alloc(host_mem.nbytes)</span><br><span class="line">        <span class="comment"># Append the device buffer to device bindings.</span></span><br><span class="line">        bindings.append(int(device_mem))</span><br><span class="line">        <span class="comment"># Append to the appropriate list.</span></span><br><span class="line">        <span class="keyword">if</span> engine.binding_is_input(binding):</span><br><span class="line">            inputs.append(HostDeviceMem(host_mem, device_mem))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs.append(HostDeviceMem(host_mem, device_mem))</span><br><span class="line">    <span class="keyword">return</span> inputs, outputs, bindings, stream</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_engine</span><span class="params">(max_batch_size=<span class="number">1</span>, onnx_file_path=<span class="string">""</span>, engine_file_path=<span class="string">""</span>, \</span></span></span><br><span class="line"><span class="function"><span class="params">               fp16_mode=False, int8_mode=False, save_engine=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               )</span>:</span></span><br><span class="line">    <span class="string">"""Attempts to load a serialized engine if available, otherwise builds a new TensorRT engine and saves it."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_engine</span><span class="params">(max_batch_size, save_engine)</span>:</span></span><br><span class="line">        <span class="string">"""Takes an ONNX file and creates a TensorRT engine to run inference with"""</span></span><br><span class="line">        <span class="keyword">with</span> trt.Builder(TRT_LOGGER) <span class="keyword">as</span> builder, \</span><br><span class="line">                builder.create_network() <span class="keyword">as</span> network, \</span><br><span class="line">                trt.OnnxParser(network, TRT_LOGGER) <span class="keyword">as</span> parser:</span><br><span class="line"></span><br><span class="line">            builder.max_workspace_size = <span class="number">1</span> &lt;&lt; <span class="number">30</span>  <span class="comment"># Your workspace size</span></span><br><span class="line">            builder.max_batch_size = max_batch_size</span><br><span class="line">            <span class="comment"># pdb.set_trace()</span></span><br><span class="line">            builder.fp16_mode = fp16_mode  <span class="comment"># Default: False</span></span><br><span class="line">            builder.int8_mode = int8_mode  <span class="comment"># Default: False</span></span><br><span class="line">            <span class="keyword">if</span> int8_mode:</span><br><span class="line">                <span class="comment"># To be updated</span></span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Parse model file</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(onnx_file_path):</span><br><span class="line">                quit(<span class="string">'ONNX file &#123;&#125; not found'</span>.format(onnx_file_path))</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'Loading ONNX file from path &#123;&#125;...'</span>.format(onnx_file_path))</span><br><span class="line">            <span class="keyword">with</span> open(onnx_file_path, <span class="string">'rb'</span>) <span class="keyword">as</span> model:</span><br><span class="line">                print(<span class="string">'Beginning ONNX file parsing'</span>)</span><br><span class="line">                parser.parse(model.read())</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'Completed parsing of ONNX file'</span>)</span><br><span class="line">            print(<span class="string">'Building an engine from file &#123;&#125;; this may take a while...'</span>.format(onnx_file_path))</span><br><span class="line"></span><br><span class="line">            engine = builder.build_cuda_engine(network)</span><br><span class="line">            print(<span class="string">"Completed creating Engine"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> save_engine:</span><br><span class="line">                <span class="keyword">with</span> open(engine_file_path, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(engine.serialize())</span><br><span class="line">            <span class="keyword">return</span> engine</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(engine_file_path):</span><br><span class="line">        <span class="comment"># If a serialized engine exists, load it instead of building a new one.</span></span><br><span class="line">        print(<span class="string">"Reading engine from file &#123;&#125;"</span>.format(engine_file_path))</span><br><span class="line">        <span class="keyword">with</span> open(engine_file_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f, trt.Runtime(TRT_LOGGER) <span class="keyword">as</span> runtime:</span><br><span class="line">            <span class="keyword">return</span> runtime.deserialize_cuda_engine(f.read())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> build_engine(max_batch_size, save_engine)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_inference</span><span class="params">(context, bindings, inputs, outputs, stream, batch_size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Transfer data from CPU to the GPU.</span></span><br><span class="line">    [cuda.memcpy_htod_async(inp.device, inp.host, stream) <span class="keyword">for</span> inp <span class="keyword">in</span> inputs]</span><br><span class="line">    <span class="comment"># Run inference.</span></span><br><span class="line">    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)</span><br><span class="line">    <span class="comment"># Transfer predictions back from the GPU.</span></span><br><span class="line">    [cuda.memcpy_dtoh_async(out.host, out.device, stream) <span class="keyword">for</span> out <span class="keyword">in</span> outputs]</span><br><span class="line">    <span class="comment"># Synchronize the stream</span></span><br><span class="line">    stream.synchronize()</span><br><span class="line">    <span class="comment"># Return only the host outputs.</span></span><br><span class="line">    <span class="keyword">return</span> [out.host <span class="keyword">for</span> out <span class="keyword">in</span> outputs]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess_the_outputs</span><span class="params">(h_outputs, shape_of_output)</span>:</span></span><br><span class="line">    h_outputs = h_outputs.reshape(*shape_of_output)</span><br><span class="line">    <span class="keyword">return</span> h_outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_np_nchw = get_img_np_nchw(filename)</span><br><span class="line">img_np_nchw = img_np_nchw.astype(dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># These two modes are dependent on hardwares</span></span><br><span class="line">fp16_mode = <span class="literal">False</span></span><br><span class="line">int8_mode = <span class="literal">False</span></span><br><span class="line">trt_engine_path = <span class="string">'./model_fp16_&#123;&#125;_int8_&#123;&#125;.trt'</span>.format(fp16_mode, int8_mode)</span><br><span class="line"><span class="comment"># Build an engine</span></span><br><span class="line">engine = get_engine(max_batch_size, onnx_model_path, trt_engine_path, fp16_mode, int8_mode)</span><br><span class="line"><span class="comment"># Create the context for this engine</span></span><br><span class="line">context = engine.create_execution_context()</span><br><span class="line"><span class="comment"># Allocate buffers for input and output</span></span><br><span class="line">inputs, outputs, bindings, stream = allocate_buffers(engine)  <span class="comment"># input, output: host # bindings</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Do inference</span></span><br><span class="line">shape_of_output = (max_batch_size, <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># Load data to the buffer</span></span><br><span class="line">inputs[<span class="number">0</span>].host = img_np_nchw.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs[1].host = ... for multiple input</span></span><br><span class="line">t1 = time.time()</span><br><span class="line">trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)  <span class="comment"># numpy data</span></span><br><span class="line">t2 = time.time()</span><br><span class="line">feat = postprocess_the_outputs(trt_outputs[<span class="number">0</span>], shape_of_output)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'TensorRT ok'</span>)</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet50(pretrained=<span class="literal">True</span>).cuda()</span><br><span class="line">resnet_model = model.eval()</span><br><span class="line"></span><br><span class="line">input_for_torch = torch.from_numpy(img_np_nchw).cuda()</span><br><span class="line">t3 = time.time()</span><br><span class="line">feat_2 = resnet_model(input_for_torch)</span><br><span class="line">t4 = time.time()</span><br><span class="line">feat_2 = feat_2.cpu().data.numpy()</span><br><span class="line">print(<span class="string">'Pytorch ok!'</span>)</span><br><span class="line"></span><br><span class="line">mse = np.mean((feat - feat_2) ** <span class="number">2</span>)</span><br><span class="line">print(<span class="string">"Inference time with the TensorRT engine: &#123;&#125;"</span>.format(t2 - t1))</span><br><span class="line">print(<span class="string">"Inference time with the PyTorch model: &#123;&#125;"</span>.format(t4 - t3))</span><br><span class="line">print(<span class="string">'MSE Error = &#123;&#125;'</span>.format(mse))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'All completed!'</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TensorRT ok</span><br><span class="line">Pytorch ok!</span><br><span class="line">Inference time with the TensorRT engine: 0.0037250518798828125</span><br><span class="line">Inference time with the PyTorch model: 0.3574800491333008</span><br><span class="line">MSE Error = 3.297184357139993e-12</span><br></pre></td></tr></table></figure><blockquote><p>这里测得时间有点不准，第一次运行的时间是比较长的，但是我暂时没有GPU实验，所以先不讨论这个结果了。</p></blockquote><p>代码来源于<a href="https://github.com/RizhaoCai/PyTorch_ONNX_TensorRT" target="_blank" rel="noopener">PyTorch_ONNX_TensorRT</a>。</p><h3 id="Pytorch-gt-TensorRT"><a href="#Pytorch-gt-TensorRT" class="headerlink" title="Pytorch-&gt;TensorRT"></a>Pytorch-&gt;TensorRT</h3><p>接下来介绍Python环境下，直接把Pytorch模型转化为TensorRT，参考的代码来源于<a href="https://github.com/NVIDIA-AI-IOT/torch2trt" target="_blank" rel="noopener">NVIDIA-AI-IOT/torch2trt</a>这个工程比较简单易懂，质量很高，安装也不难，原文作者（即下面的第一个参考链接）运行的结果如下：</p><p><img src="/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-db9df98635059fa8d8db009b2347600e_720w.jpg" alt="img"></p><p>对于你自己的Pytorch模型，只需要把代码的model进行替换即可。注意在运行过程中经常会出现”output tensor has no attribute _trt”，这是因为你模型当中有一些操作还没有实现，需要自己实现。</p><h2 id="C-环境下Pytorch模型转化为TensorRT"><a href="#C-环境下Pytorch模型转化为TensorRT" class="headerlink" title="C++环境下Pytorch模型转化为TensorRT"></a>C++环境下Pytorch模型转化为TensorRT</h2><p>C++环境下Pytorch模型转化为TensorRT有<strong>两种路径</strong>，一种是先把Pytorch的<code>pth</code>模型转化为<code>onnx</code>，然后使用TensorRT进行解析从而构造TensorRT引擎，这里和Python环境下的第一种方法大同小异；另一种是先把Pytorch的<code>pth</code>模型转化为<code>onnx</code>模型，然后使用<a href="https://github.com/onnx/onnx-tensorrt" target="_blank" rel="noopener">onnx-tensorrt</a>转换为TensorRT的<code>trt</code>文件，然后在C++环境下的使用TensorRT直接加载<code>trt</code>文件，从而构建engine。</p><h3 id="Pytorch-gt-Onnx-gt-TensorRT解析"><a href="#Pytorch-gt-Onnx-gt-TensorRT解析" class="headerlink" title="Pytorch-&gt;Onnx-&gt;TensorRT解析"></a>Pytorch-&gt;Onnx-&gt;TensorRT解析</h3><p>c++环境下，以TensorRT5.1.5.0的sampleOnnxMNIST为例子，用opencv读取一张图片，然后让TensorRT进行doInference输出(1,1000)的特征。代码如下所示</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"NvInfer.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"NvOnnxParser.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"argsParser.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"logger.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"common.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"image.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> DebugP(x) std::cout &lt;&lt; <span class="meta-string">"Line"</span> &lt;&lt; __LINE__ &lt;&lt; <span class="meta-string">"  "</span> &lt;&lt; #x &lt;&lt; <span class="meta-string">"="</span> &lt;&lt; x &lt;&lt; std::endl</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> INPUT_H = <span class="number">224</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> INPUT_W = <span class="number">224</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> INPUT_C = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> OUTPUT_SIZE = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//指定输入输出名称</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* INPUT_BLOB_NAME = <span class="string">"input"</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* OUTPUT_BLOB_NAME = <span class="string">"output"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> gSampleName = <span class="string">"TensorRT.sample_onnx_image"</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">samplesCommon::Args gArgs;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">onnxToTRTModel</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; modelFile, <span class="comment">// name of the onnx model</span></span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">unsigned</span> <span class="keyword">int</span> maxBatchSize,    <span class="comment">// batch size - NB must be at least as large as the batch we want to run with</span></span></span></span><br><span class="line"><span class="function"><span class="params">                    IHostMemory*&amp; trtModelStream)</span> <span class="comment">// output buffer for the TensorRT model</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 1. create the builder</span></span><br><span class="line">    <span class="comment">//创建一个 IBuilder，传进gLogger参数是为了方便打印信息，gLogger是一个日志类，在common.h文件中定义</span></span><br><span class="line">    <span class="comment">//builder 这个地方感觉像是使用了建造者模式。</span></span><br><span class="line">    IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());</span><br><span class="line">    assert(builder != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="comment">//创建一个 network对象，但是这个network对象只是一个空架子，里面的属性还没有具体的数值。</span></span><br><span class="line">    nvinfer1::INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建一个onnx模型解析对象parser</span></span><br><span class="line">    <span class="keyword">auto</span> parser = nvonnxparser::createParser(*network, gLogger.getTRTLogger());</span><br><span class="line">    <span class="comment">//Optional - uncomment below lines to view network layer information</span></span><br><span class="line">    <span class="comment">//config-&gt;setPrintLayerInfo(true);</span></span><br><span class="line">    <span class="comment">//parser-&gt;reportParsingInfo();</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//调用解析函数parseFromFile填充network对象，将onnx中的tensor解析为tensorRT中的tensor，此处使用到了模型文件，还用到了 common.cpp 文件中的辅助函数：locateFile( ) </span></span><br><span class="line">    <span class="keyword">if</span> ( !parser-&gt;parseFromFile( locateFile(modelFile, gArgs.dataDirs).c_str(), <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(gLogger.getReportableSeverity()) ) )</span><br><span class="line">    &#123;</span><br><span class="line">        gLogError &lt;&lt; <span class="string">"Failure while parsing ONNX file"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Build the engine</span></span><br><span class="line">    <span class="comment">// 设置batch size；实际推理时不能大于该batch size</span></span><br><span class="line">    builder-&gt;setMaxBatchSize(maxBatchSize);</span><br><span class="line">    <span class="comment">//builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 20);</span></span><br><span class="line">    <span class="comment">// 设置工作空间 size，Layer algorithms often require temporary workspace.</span></span><br><span class="line">    builder-&gt;setMaxWorkspaceSize(<span class="number">10</span> &lt;&lt; <span class="number">20</span>);</span><br><span class="line">    <span class="comment">// 设置推理方式</span></span><br><span class="line">    builder-&gt;setFp16Mode(gArgs.runInFp16);</span><br><span class="line">    builder-&gt;setInt8Mode(gArgs.runInInt8);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (gArgs.runInInt8)</span><br><span class="line">    &#123;</span><br><span class="line">        samplesCommon::setAllTensorScales(network, <span class="number">127.0f</span>, <span class="number">127.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    samplesCommon::enableDLA(builder, gArgs.useDLACore);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. Build engine</span></span><br><span class="line">    <span class="comment">//使用network创建 CudaEngine，优化方法在这里执行。</span></span><br><span class="line">    <span class="comment">//至此，Onnx模型已转换为tensorRT object。</span></span><br><span class="line">    ICudaEngine* engine = builder-&gt;buildCudaEngine(*network);</span><br><span class="line">    assert(engine);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we can destroy the parser</span></span><br><span class="line">    parser-&gt;destroy();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// serialize the engine, then close everything down</span></span><br><span class="line">    <span class="comment">//将转换好的tensorRT object序列化到内存中，trtModelStream是一块内存空间。</span></span><br><span class="line">    <span class="comment">//这里也可以序列化到磁盘中。</span></span><br><span class="line">    trtModelStream = engine-&gt;serialize();</span><br><span class="line">    engine-&gt;destroy();</span><br><span class="line">    network-&gt;destroy();</span><br><span class="line">    builder-&gt;destroy();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doInference</span><span class="params">(IExecutionContext&amp; context, <span class="keyword">float</span>* input, <span class="keyword">float</span>* output, <span class="keyword">int</span> batchSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 使用传进来的context恢复engine。</span></span><br><span class="line">    <span class="keyword">const</span> ICudaEngine&amp; engine = context.getEngine();</span><br><span class="line">    <span class="comment">//engine.getNbBindings()是为了获取与这个engine相关的输入输出tensor的数量。</span></span><br><span class="line">    <span class="comment">//这个地方，输入+输出 总共就2个，所以做个验证。</span></span><br><span class="line">    <span class="comment">// input and output buffer pointers that we pass to the engine - the engine requires exactly IEngine::getNbBindings(),</span></span><br><span class="line">    <span class="comment">// of these, but in this case we know that there is exactly one input and one output.</span></span><br><span class="line">    assert(engine.getNbBindings() == <span class="number">2</span>);</span><br><span class="line">    <span class="comment">//void* 型数组，主要用于下面GPU开辟内存。</span></span><br><span class="line">    <span class="keyword">void</span>* buffers[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In order to bind the buffers, we need to know the names of the input and output tensors.</span></span><br><span class="line">    <span class="comment">// note that indices are guaranteed to be less than IEngine::getNbBindings()</span></span><br><span class="line">    <span class="comment">//获取与这个engine相关的输入输出tensor的索引。</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);</span><br><span class="line">    </span><br><span class="line">    DebugP(inputIndex); DebugP(outputIndex);</span><br><span class="line">    <span class="comment">//为输入输出tensor开辟显存。</span></span><br><span class="line">    <span class="comment">// create GPU buffers and a stream</span></span><br><span class="line">    CHECK(cudaMalloc(&amp;buffers[inputIndex], batchSize * INPUT_C * INPUT_H * INPUT_W * <span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">    CHECK(cudaMalloc(&amp;buffers[outputIndex], batchSize * OUTPUT_SIZE * <span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//创建cuda流，用于管理数据复制，存取，和计算的并发操作</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    CHECK(cudaStreamCreate(&amp;stream));</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//从内存到显存，从CPU到GPU，将输入数据拷贝到显存中</span></span><br><span class="line">    <span class="comment">//input是读入内存中的数据；buffers[inputIndex]是显存上的存储区域，用于存放输入数据</span></span><br><span class="line">    <span class="comment">// DMA the input to the GPU,  execute the batch asynchronously, and DMA it back:</span></span><br><span class="line">    CHECK(cudaMemcpyAsync(buffers[inputIndex], input, batchSize * INPUT_C * INPUT_H * INPUT_W * <span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">//启动cuda核，异步执行推理计算</span></span><br><span class="line">    context.enqueue(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="comment">//从显存到内存，将计算结果拷贝回内存中</span></span><br><span class="line">    <span class="comment">//output是内存中的存储区域;buffers[outputIndex]是显存中的存储区域，存放模型输出.</span></span><br><span class="line">    CHECK(cudaMemcpyAsync(output, buffers[outputIndex], batchSize * OUTPUT_SIZE * <span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyDeviceToHost, stream));</span><br><span class="line">     <span class="comment">//这个是为了同步不同的cuda流。</span></span><br><span class="line">    cudaStreamSynchronize(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//销毁流对象和释放显存</span></span><br><span class="line">    <span class="comment">// release the stream and the buffers</span></span><br><span class="line">    cudaStreamDestroy(stream);</span><br><span class="line">    CHECK(cudaFree(buffers[inputIndex]));</span><br><span class="line">    CHECK(cudaFree(buffers[outputIndex]));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief This function prints the help information for running this sample</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printHelpInfo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Usage: ./sample_onnx_mnist [-h or --help] [-d or --datadir=&lt;path to data directory&gt;] [--useDLACore=&lt;int&gt;]\n"</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"--help          Display help information\n"</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"--datadir       Specify path to a data directory, overriding the default. This option can be used multiple times to add multiple directories. If no data directories are given, the default is to use (data/samples/mnist/, data/mnist/)"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"--useDLACore=N  Specify a DLA engine for layers that support DLA. Value can range from 0 to n-1, where n is the number of DLA engines on the platform."</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"--int8          Run in Int8 mode.\n"</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"--fp16          Run in FP16 mode."</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> argsOK = samplesCommon::parseArgs(gArgs, argc, argv);</span><br><span class="line">    <span class="keyword">if</span> (gArgs.help)</span><br><span class="line">    &#123;</span><br><span class="line">        printHelpInfo();</span><br><span class="line">        <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!argsOK)</span><br><span class="line">    &#123;</span><br><span class="line">        gLogError &lt;&lt; <span class="string">"Invalid arguments"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        printHelpInfo();</span><br><span class="line">        <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (gArgs.dataDirs.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        gArgs.dataDirs = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;&#123;<span class="string">"data/samples/mnist/"</span>, <span class="string">"data/mnist/"</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> sampleTest = gLogger.defineTest(gSampleName, argc, <span class="keyword">const_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">char</span>**&gt;(argv));</span><br><span class="line"></span><br><span class="line">    gLogger.reportTestStart(sampleTest);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create a TensorRT model from the onnx model and serialize it to a stream</span></span><br><span class="line">    IHostMemory* trtModelStream&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 执行onnxToTRTModel</span></span><br><span class="line">    <span class="keyword">if</span> (!onnxToTRTModel(<span class="string">"resnet50.onnx"</span>, <span class="number">1</span>, trtModelStream))</span><br><span class="line">        gLogger.reportFail(sampleTest);</span><br><span class="line"></span><br><span class="line">    assert(trtModelStream != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Successfully parsed ONNX file!!!!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Start reading the input image!!!!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    cv::Mat image = cv::imread(locateFile(<span class="string">"test.jpg"</span>, gArgs.dataDirs), cv::IMREAD_COLOR);</span><br><span class="line">    <span class="keyword">if</span> (image.empty()) &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"The input image is empty!!! Please check....."</span>&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    DebugP(image.size());</span><br><span class="line">    cv::cvtColor(image, image, cv::COLOR_BGR2RGB);</span><br><span class="line"></span><br><span class="line">    cv::Mat dst = cv::Mat::zeros(INPUT_H, INPUT_W, CV_32FC3);</span><br><span class="line">    cv::resize(image, dst, dst.size());</span><br><span class="line">    DebugP(dst.size());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span>* data = normal(dst); </span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 创建运行时环境 IRuntime对象，传入 gLogger 用于打印信息</span></span><br><span class="line">    <span class="comment">// deserialize the engine</span></span><br><span class="line">    IRuntime* runtime = createInferRuntime(gLogger);</span><br><span class="line">    assert(runtime != <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="keyword">if</span> (gArgs.useDLACore &gt;= <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        runtime-&gt;setDLACore(gArgs.useDLACore);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(trtModelStream-&gt;data(), trtModelStream-&gt;size(), <span class="literal">nullptr</span>);</span><br><span class="line">    assert(engine != <span class="literal">nullptr</span>);</span><br><span class="line">    trtModelStream-&gt;destroy();</span><br><span class="line">    <span class="comment">// 创建上下文环境，主要用于inference 函数中启动cuda核</span></span><br><span class="line">    IExecutionContext* context = engine-&gt;createExecutionContext();</span><br><span class="line">    assert(context != <span class="literal">nullptr</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">float</span> prob[OUTPUT_SIZE];</span><br><span class="line">    <span class="keyword">typedef</span> <span class="built_in">std</span>::chrono::high_resolution_clock Time;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="built_in">std</span>::chrono::duration&lt;<span class="keyword">double</span>, <span class="built_in">std</span>::ratio&lt;<span class="number">1</span>, <span class="number">1000</span>&gt;&gt; ms;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="built_in">std</span>::chrono::duration&lt;<span class="keyword">float</span>&gt; fsec;</span><br><span class="line">    <span class="keyword">double</span> total = <span class="number">0.0</span>;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// deploy 阶段：调用 inference 函数，进行推理过程</span></span><br><span class="line">    <span class="comment">// run inference and cout time</span></span><br><span class="line">    <span class="keyword">auto</span> t0 = Time::now();</span><br><span class="line">    doInference(*context, data, prob, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">auto</span> t1 = Time::now();</span><br><span class="line">    fsec fs = t1 - t0;</span><br><span class="line">    ms d = <span class="built_in">std</span>::chrono::duration_cast&lt;ms&gt;(fs);</span><br><span class="line">    total += d.count();</span><br><span class="line">    <span class="comment">// 销毁无用对象</span></span><br><span class="line">    <span class="comment">// destroy the engine</span></span><br><span class="line">    context-&gt;destroy();</span><br><span class="line">    engine-&gt;destroy();</span><br><span class="line">    runtime-&gt;destroy();</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span> &lt;&lt; <span class="string">"Running time of one image is:"</span> &lt;&lt; total &lt;&lt; <span class="string">"ms"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">   </span><br><span class="line">    gLogInfo &lt;&lt; <span class="string">"Output:\n"</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; OUTPUT_SIZE; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        gLogInfo &lt;&lt; prob[i] &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    gLogInfo &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gLogger.reportTest(sampleTest, <span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>image.cpp</code>的代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"image.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">float</span> kMean[<span class="number">3</span>] = &#123; <span class="number">0.485f</span>, <span class="number">0.456f</span>, <span class="number">0.406f</span> &#125;;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">float</span> kStdDev[<span class="number">3</span>] = &#123; <span class="number">0.229f</span>, <span class="number">0.224f</span>, <span class="number">0.225f</span> &#125;;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> map_[<span class="number">7</span>][<span class="number">3</span>] = &#123; &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125; ,</span><br><span class="line">				&#123;<span class="number">128</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">				&#123;<span class="number">0</span>,<span class="number">128</span>,<span class="number">0</span>&#125;,</span><br><span class="line">				&#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">128</span>&#125;,</span><br><span class="line">				&#123;<span class="number">128</span>,<span class="number">128</span>,<span class="number">0</span>&#125;,</span><br><span class="line">				&#123;<span class="number">128</span>,<span class="number">0</span>,<span class="number">128</span>&#125;,</span><br><span class="line">				&#123;<span class="number">0</span>,<span class="number">128</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span>* <span class="title">normal</span><span class="params">(cv::Mat img)</span> </span>&#123;</span><br><span class="line">	<span class="comment">//cv::Mat image(img.rows, img.cols, CV_32FC3);</span></span><br><span class="line">	<span class="keyword">float</span> * data;</span><br><span class="line">	data = (<span class="keyword">float</span>*)<span class="built_in">calloc</span>(img.rows*img.cols * <span class="number">3</span>, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; <span class="number">3</span>; ++c)</span><br><span class="line">	&#123;</span><br><span class="line">        </span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; img.rows; ++i)</span><br><span class="line">		&#123; <span class="comment">//获取第i行首像素指针 </span></span><br><span class="line">			cv::Vec3b *p1 = img.ptr&lt;cv::Vec3b&gt;(i);</span><br><span class="line">			<span class="comment">//cv::Vec3b *p2 = image.ptr&lt;cv::Vec3b&gt;(i);</span></span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; img.cols; ++j)</span><br><span class="line">			&#123;</span><br><span class="line">				data[c * img.cols * img.rows + i * img.cols + j] = (p1[j][c] / <span class="number">255.0f</span> - kMean[c]) / kStdDev[c];</span><br><span class="line">       </span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">        </span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> data;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>image.hpp的内容为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">	<span class="keyword">int</span> w;</span><br><span class="line">	<span class="keyword">int</span> h;</span><br><span class="line">	<span class="keyword">int</span> c;</span><br><span class="line">	<span class="keyword">float</span> *data;</span><br><span class="line">&#125; image;</span><br><span class="line"><span class="function"><span class="keyword">float</span>* <span class="title">normal</span><span class="params">(cv::Mat img)</span></span>;</span><br></pre></td></tr></table></figure><p>运行结果为：</p><p><img src="/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-c0444b35f607445bf62fd4c7cc30be8d_720w.jpg" alt="img"></p><p>同样的<code>test.jpg</code>在python环境下的运行结果为：</p><p><img src="/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-877f488bcd1c4cb278a050c1647dcaa8_720w.png" alt="img"></p><p>可以发现，c++环境下resnet50输出的(1,1000)的特征与Python环境下feat1(TensorRT)和feat2(pytorch)的结果差距很小。</p><p><strong>总结：</strong></p><ol><li><code>onnxToTRTModel</code>的过程为：创建一个builder和network，使用parser解析Onnx模型填充network；使用builder类的方法设置batch size、workspace、推理方法（fp32、fp16、int8）。使用<code>builder-&gt;buildCudaEngine(*network)</code>执行优化方法创建engine。因为一个模型从导入到生成Engine是需要花费一些时间的，所以<strong>可以将engine序列化到内存或者存储文件中</strong>，便于使用（需要注意的是，若将序列化文件存储到了文件中，该文件和GPU平台高度相关，换一个平台需要重新生成该文件）。最后是销毁一些中间变量<code>parser</code>、<code>engine</code>、<code>network</code>、<code>builder</code>。</li><li><code>mian</code>函数主要过程为：执行<code>onnxToTRTModel</code>函数得到序列化engine——内存中的<code>trtModelStream</code>。创建运行时环境 IRuntime对象<code>runtime</code>，然后使用该对象反序列化<code>trtModelStream</code>得到<code>engine</code>；使用<code>engine</code>创建上下文环境<code>context</code>，主要用于inference 函数中启动cuda核。执行<code>doInference</code>函数过程。然后销毁<code>context</code>、<code>engine</code>、<code>runtime</code>。</li><li><code>doInference</code>函数的主要过程为：从上下文环境<code>context</code>恢复<code>engine</code>。为输入输出tensor开辟显存，到<code>void* buffers</code>中。创建cuda流，用于管理数据复制，存取，和计算的并发操作。从内存到显存，从CPU到GPU，将输入数据拷贝到显存中。启动cuda核，异步执行推理计算。从显存到内存，将计算结果拷贝回内存中。同步不同的cuda流。销毁流对象<code>stream</code>和释放显存<code>buffers</code>。</li></ol><h3 id="Pytorch-gt-Onnx-gt-TensorRT模型"><a href="#Pytorch-gt-Onnx-gt-TensorRT模型" class="headerlink" title="Pytorch-&gt;Onnx-&gt;TensorRT模型"></a>Pytorch-&gt;Onnx-&gt;TensorRT模型</h3><p>上面的是将pytorch首先转化为onnx，然后让TensorRT解析onnx从而构建TensorRT引擎。那么我们如何让TensorRT直接加载引擎文件呢，也就是说，我们先把onnx转化为TensorRT的trt文件，然后让c++环境下的TensorRT直接加载trt文件，从而构建engine。</p><p>在这里我们首先使用onnx-tensorrt这个项目来使resnet50.onnx转化为resnet50.trt。采用的项目是<a href="https://github.com/onnx/onnx-tensorrt" target="_blank" rel="noopener">onnx-tensorrt</a>这个项目的安装也不难，这个也在我之前的博客里面有介绍，所以不展开了。</p><p>运行如下命令，就可以获得rensnet50.trt这个引擎文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onnx2trt resnet50.onnx -o resnet50.trt</span><br></pre></td></tr></table></figure><p>需要注意的是，<strong>onnx-tensort这个项目在编译的时有一个指定GPU计算能力的选项</strong>，如下图所示：</p><p><img src="/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/v2-d1e0cab46990702c9a71d312c28532ce_720w.jpg" alt="img"></p><p><strong>另外在onnx2trt命令有个<code>-b</code>操作，是指定生成的trt文件的batch size的。在实际test过程中，你的batch size是多少，这个就设置成多少。按照下面参考链接1作者的说法：</strong></p><blockquote><p> 记得我当时trt文件的batch size是1，但是我实际的batch size是8，运行后，只有一张图片有结果，其他7张图片都是0。</p></blockquote><p>还有，导出onnx模型时，网络的数据输入要和TensorRt数据输入大小保持一致（b,c,w,h）。对于retinaface，若不这样做，否则可能因为特征图大小不一样，导致预测的anchor偏置数量与预设anchor数量不一致。在进行后处理时导致访问非法内存。</p><p>如果能顺利生成trt文件的话，在代码中可以直接添加以下函数，来生成engine, 其他就不需要改变。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">read_TRT_File</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; engineFile, IHostMemory*&amp; trtModelStream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="built_in">std</span>::fstream file;</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"loading filename from:"</span> &lt;&lt; engineFile &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">     nvinfer1::IRuntime* trtRuntime;</span><br><span class="line">     <span class="comment">//nvonnxparser::IPluginFactory* onnxPlugin = createPluginFactory(gLogger.getTRTLogger());</span></span><br><span class="line">     file.open(engineFile, <span class="built_in">std</span>::ios::binary | <span class="built_in">std</span>::ios::in);</span><br><span class="line">     file.seekg(<span class="number">0</span>, <span class="built_in">std</span>::ios::end);</span><br><span class="line">     <span class="keyword">int</span> length = file.tellg();</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"length:"</span> &lt;&lt; length &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">     file.seekg(<span class="number">0</span>, <span class="built_in">std</span>::ios::beg);</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;<span class="keyword">char</span>[]&gt; data(<span class="keyword">new</span> <span class="keyword">char</span>[length]);</span><br><span class="line">     file.read(data.get(), length);</span><br><span class="line">     file.close();</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"load engine done"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"deserializing"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">     <span class="comment">// 创建运行时环境 IRuntime对象，传入 gLogger 用于打印信息</span></span><br><span class="line">     trtRuntime = createInferRuntime(gLogger.getTRTLogger());</span><br><span class="line">     <span class="comment">//ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, onnxPlugin);</span></span><br><span class="line">     <span class="comment">// 反序列化前向引擎</span></span><br><span class="line">     ICudaEngine* engine = trtRuntime-&gt;deserializeCudaEngine(data.get(), length, <span class="literal">nullptr</span>);</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"deserialize done"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">     assert(engine != <span class="literal">nullptr</span>);</span><br><span class="line">     <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"The engine in TensorRT.cpp is not nullptr"</span> &lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">     <span class="comment">//将转换好的tensorRT object序列化到内存中，trtModelStream是一块内存空间。</span></span><br><span class="line">     <span class="comment">//这里也可以序列化到磁盘中。</span></span><br><span class="line">     <span class="comment">// Serialize engine and destroy it</span></span><br><span class="line">     trtModelStream = engine-&gt;serialize();</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>如果想保存引擎文件的话，可以在自己的代码中添加这几句话，就可以生成trt文件，然后下次直接调用trt文件。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::IHostMemory* data = engine-&gt;serialize();</span><br><span class="line"><span class="built_in">std</span>::ofstream file;</span><br><span class="line">file.open(filename, <span class="built_in">std</span>::ios::binary | <span class="built_in">std</span>::ios::out);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"writing engine file..."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">file.write((<span class="keyword">const</span> <span class="keyword">char</span>*)data-&gt;data(), data-&gt;size());</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"save engine file done"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">file.close();</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>按照目前我的观察，这里只说C++接口，按照第一种方法可以在导出Onnx模型后，使用C++代码控制使用fp32、fp16或者int8进行推理，但是若按照第二种方法，无法使用C++代码控制使用fp32、fp16或者int8进行推理，虽然onnx2trt有一个<code>-d</code>参数指明是float32还是float16，但即便是这样，按照<a href="https://github.com/onnx/onnx-tensorrt/issues/32" target="_blank" rel="noopener">How to use FP16 ot INT8? #32</a>，好像也有很多坑待填。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/88318324" target="_blank" rel="noopener">如何使用TensorRT对训练好的PyTorch模型进行加速?</a><br><a href="https://arleyzhang.github.io/articles/7f4b25ce/" target="_blank" rel="noopener">TensorRT(1)-介绍-使用-安装</a><br><a href="https://arleyzhang.github.io/articles/c17471cb/" target="_blank" rel="noopener">TensorRT(2)-基本使用：mnist手写体识别</a><br><a href="https://blog.csdn.net/may0324/article/details/90083988" target="_blank" rel="noopener">caffe模型TensorRT部署实践（一）</a><br><a href="https://github.com/NVIDIA/TensorRT/issues/322" target="_blank" rel="noopener">The TensorRT support multi-gpus inference? #322</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/" title="使用TensorRT对Pytorch模型加速">https://www.zdaiot.com/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Pytorch/" rel="tag"><i class="fa fa-tag"></i> Pytorch</a><a href="/tags/TensorRT/" rel="tag"><i class="fa fa-tag"></i> TensorRT</a><a href="/tags/onnx/" rel="tag"><i class="fa fa-tag"></i> onnx</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MLFrameworks/TensorRT/onnx-tensorrt安装/" rel="prev" title="onnx-tensorrt安装"><i class="fa fa-chevron-left"></i> onnx-tensorrt安装</a></div><div class="post-nav-item"> <a href="/MLFrameworks/TensorRT/TensorRT基本概念/" rel="next" title="TensorRT基本概念">TensorRT基本概念<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Python环境下Pytorch模型转化为TensorRT"><span class="nav-number">1.</span> <span class="nav-text">Python环境下Pytorch模型转化为TensorRT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch-gt-Onnx-gt-TensorRT"><span class="nav-number">1.1.</span> <span class="nav-text">Pytorch-&gt;Onnx-&gt;TensorRT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch-gt-TensorRT"><span class="nav-number">1.2.</span> <span class="nav-text">Pytorch-&gt;TensorRT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-环境下Pytorch模型转化为TensorRT"><span class="nav-number">2.</span> <span class="nav-text">C++环境下Pytorch模型转化为TensorRT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch-gt-Onnx-gt-TensorRT解析"><span class="nav-number">2.1.</span> <span class="nav-text">Pytorch-&gt;Onnx-&gt;TensorRT解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch-gt-Onnx-gt-TensorRT模型"><span class="nav-number">2.2.</span> <span class="nav-text">Pytorch-&gt;Onnx-&gt;TensorRT模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">327</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">55</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">383</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">35:21</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : 'd8ce3d526cd7d883aa76738aad79c5a1',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>