<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本篇文章主要参考了TensorRT(1)-介绍-使用-安装，并加上了一些自己的理解。 简介TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT 之前称为GIE。  由上图可以很清楚的看出，训练（training）和 推理（inference）的区别："><meta name="keywords" content="TensorRT,onnx"><meta property="og:type" content="article"><meta property="og:title" content="TensorRT基本概念"><meta property="og:url" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="本篇文章主要参考了TensorRT(1)-介绍-使用-安装，并加上了一些自己的理解。 简介TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT 之前称为GIE。  由上图可以很清楚的看出，训练（training）和 推理（inference）的区别："><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/Training-vs-Inference-1.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-model-import1.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-plugin.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-optimize-method-1592132734926.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-layer-fusion.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/v2-1eb534671cfada04b5d027dbaf352f5e_720w.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-workflow-1.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-workflow-2.png"><meta property="og:updated_time" content="2020-06-14T10:57:36.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="TensorRT基本概念"><meta name="twitter:description" content="本篇文章主要参考了TensorRT(1)-介绍-使用-安装，并加上了一些自己的理解。 简介TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT 之前称为GIE。  由上图可以很清楚的看出，训练（training）和 推理（inference）的区别："><meta name="twitter:image" content="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/Training-vs-Inference-1.jpg"><link rel="canonical" href="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>TensorRT基本概念 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> TensorRT基本概念<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MLFrameworks/TensorRT/TensorRT基本概念.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-06-14 18:57:36" itemprop="dateCreated datePublished" datetime="2020-06-14T18:57:36+08:00">2020-06-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/" itemprop="url" rel="index"><span itemprop="name">MLFrameworks</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>9k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>8 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本篇文章主要参考了<a href="https://arleyzhang.github.io/articles/7f4b25ce/" target="_blank" rel="noopener">TensorRT(1)-介绍-使用-安装</a>，并加上了一些自己的理解。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT 之前称为GIE。</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/Training-vs-Inference-1.jpg" alt="Training-vs-Inference-Graphic"></p><p>由上图可以很清楚的看出，训练（training）和 推理（inference）的区别：</p><ul><li>训练（training）包含了前向传播和后向传播两个阶段，针对的是训练集。训练时通过误差反向传播来不断修改网络权值（weights）。</li><li>推理（inference）只包含前向传播一个阶段，针对的是除了训练集之外的新数据。可以是测试集，但不完全是，更多的是整个数据集之外的数据。其实就是针对新数据进行预测，预测时，速度是一个很重要的因素。</li></ul><p>为了提高部署推理的速度，出现了很多轻量级神经网络，比如squeezenet，mobilenet，shufflenet等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。</p><p><strong>而tensorRT则是对训练好的模型进行优化，tensorRT只是推理优化器。</strong>当你的网络训练完之后，可以将训练模型文件直接丢进tensorRT中，而不再需要依赖深度学习框架（Caffe，TensorFlow等），如下：</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-model-import1.png" alt="1517972547350"></p><p>可以认为tensorRT是一个只有前向传播的深度学习框架，这个框架可以<strong>将 Caffe，TensorFlow等网络模型解析，然后与tensorRT中对应的层进行一一映射</strong>，把其他框架的模型统一全部转换到tensorRT中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略，并进行部署加速。</p><p>TensorRT4.0几乎可以支持所有常用的深度学习框架，对于caffe和tensorflow来说，tensorRT可以直接解析他们的网络模型；对于caffe2，pytorch，mxnet，chainer，CNTK等框架则是首先要将模型转为 ONNX 的通用深度学习模型，然后对ONNX模型做解析。而tensorflow和MATLAB已经将TensorRT集成到框架中去了。</p><p>基本上比较经典的层比如，卷积，反卷积，全连接，RNN，softmax等，在tensorRT中都是有对应的实现方式的，tensorRT是可以直接解析的。</p><p>对于自定义层，<strong>tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层。</strong> 如下图：</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-plugin.png" alt="pasted-image-0-1-625x519"></p><h2 id="优化方式"><a href="#优化方式" class="headerlink" title="优化方式"></a>优化方式</h2><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-optimize-method-1592132734926.png" alt="1535459793419"></p><p>TensorRT优化方法主要有以下几种方式，最主要的是前面两种。</p><h3 id="层间融合或张量融合（Layer-amp-Tensor-Fusion）"><a href="#层间融合或张量融合（Layer-amp-Tensor-Fusion）" class="headerlink" title="层间融合或张量融合（Layer &amp; Tensor Fusion）"></a>层间融合或张量融合（Layer &amp; Tensor Fusion）</h3><p>如下图左侧是GoogLeNet Inception模块的计算图。这个结构中有很多层，在部署模型推理时，这每一层的运算操作都是由GPU完成的，但实际上是GPU通过启动不同的CUDA（Compute unified device architecture）核心来完成计算的，CUDA核心计算张量的速度是很快的，但是<strong>往往大量的时间是浪费在CUDA核心的启动和对每一层输入/输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费</strong>。TensorRT通过对<strong>层间的横向或纵向合并</strong>（合并后的结构称为<strong>CBR</strong>，意指 convolution, bias, and ReLU layers are fused to form a single layer），使得层的数量大大减少。</p><ul><li>横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。而在绝大部分框架中，比如卷积、偏置和激活层这三层是需要调用三次cuDNN对应的API</li><li>纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心。</li></ul><p>合并之后的计算图（下图右侧）的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效。对于concat这一层，比如说这边计算出来一个1×3×24×24，另一边计算出来1×5×24×24，concat到一起，变成一个1×8×24×24的矩阵，这个叫concat这层这其实是完全没有必要的，因为TensorRT完全可以实现直接接到需要的地方，不用专门做concat的操作，所以这一层也可以取消掉。</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-layer-fusion.png" alt="1517973035996"></p><p>另外还可以做并发（Concurrency），如下图左半部分（max pool和1×1 CBR）与右半部分（大的1×1 CBR，3×3 CBR和5×5 CBR）彼此之间是相互独立的两条路径，本质上是不相关的，可以在GPU上通过并发来做，来达到的优化的目标。</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/v2-1eb534671cfada04b5d027dbaf352f5e_720w.jpg" alt="img"></p><h3 id="数据精度校准（Weight-amp-Activation-Precision-Calibration）"><a href="#数据精度校准（Weight-amp-Activation-Precision-Calibration）" class="headerlink" title="数据精度校准（Weight &amp;Activation Precision Calibration）"></a>数据精度校准（Weight &amp;Activation Precision Calibration）</h3><p><strong>大部分深度学习框架在训练神经网络时网络中的张量（Tensor）都是32位浮点数的精度（Full 32-bit precision，FP32）</strong>，一旦网络训练完成，在<strong>部署推理的过程中由于不需要反向传播，完全可以适当降低数据精度</strong>，比如降为FP16或INT8的精度。更低的数据精度将会使得内存占用和延迟更低，模型体积更小。</p><p>如下表为不同精度的动态范围：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Precision</th><th style="text-align:center">Dynamic Range</th></tr></thead><tbody><tr><td style="text-align:center">FP32</td><td style="text-align:center">−3.4×1038 +3.4×1038−3.4×1038 +3.4×1038</td></tr><tr><td style="text-align:center">FP16</td><td style="text-align:center">−65504 +65504−65504 +65504</td></tr><tr><td style="text-align:center">INT8</td><td style="text-align:center">−128 +127</td></tr></tbody></table></div><p>INT8只有256个不同的数值，使用INT8来表示 FP32精度的数值，肯定会丢失信息，造成性能下降。不过TensorRT会提供完全自动化的校准（Calibration ）过程，会以最好的匹配性能将FP32精度的数据降低为INT8精度，最小化性能损失。关于校准过程，后面会专门做一个探究。可以见<a href="https://arleyzhang.github.io/articles/923e2c40/" target="_blank" rel="noopener">TensorRT(5)-INT8校准原理</a></p><h3 id="Kernel-Auto-Tuning"><a href="#Kernel-Auto-Tuning" class="headerlink" title="Kernel Auto-Tuning"></a>Kernel Auto-Tuning</h3><p>网络模型在推理计算时，是调用GPU的CUDA核进行计算的。TensorRT可以针对不同的算法，不同的网络模型，不同的GPU平台，进行 CUDA核的调整（怎么调整的还不清楚），以保证当前模型在特定平台上以最优性能计算。</p><p>TensorRT will pick the implementation from a library of kernels that delivers the best performance for the target GPU, input data size, filter size, tensor layout, batch size and other parameters.</p><h3 id="Dynamic-Tensor-Memory"><a href="#Dynamic-Tensor-Memory" class="headerlink" title="Dynamic Tensor Memory"></a>Dynamic Tensor Memory</h3><p>在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。</p><h3 id="Multi-Stream-Execution"><a href="#Multi-Stream-Execution" class="headerlink" title="Multi-Stream Execution"></a>Multi-Stream Execution</h3><p>Scalable design to process multiple input streams in parallel，这个应该就是GPU底层的优化了。</p><h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><p>TensorRT中有三个Parser用于模型的导入：</p><ul><li>Caffe Parser: 支持Caffe框架模型的导入</li><li>UFF Parser：通用框架格式(UFF)是描述DNN的执行图的数据格式</li><li>ONNX Parser：通用模型交换格式(ONNX)是一种开放式的文件格式，用于存储训练好的模型</li></ul><p>需要清楚的是，各种框架间模型的转换，需要的仅仅是模型的定义及权值。通过将模型保存为以上三个Parser可以解析的格式，则基本上就可以将模型导入到TensorRT中。</p><p>事实上，一个模型从导入到执行，会经过下面三个阶段：</p><ul><li>Network Definition: 这一阶段在TensorRT中定义网络模型，可以使用TensorRT提供的Parser导入已有模型进行定义，也可以使用TensorRT中提供的网络层来编程定义(这一步应该也需要准备好相关的权值)</li><li>Builder：前面提到过，TensorRT会对模型进行优化，这一步就是配置各项优化参数，并能生成可执行Inference的Engine</li><li>Engine：Engine可理解为一个Builder的实例，是我们导入的模型经过Builder的配置所生成的一个优化过的Inference执行器，所有的Inference可直接调用Engine来执行</li></ul><p><strong>一个模型从导入到生成Engine是需要花费一些时间的，因此TensorRT提供了Engine的序列化和反序列化操作，一旦我们确定了一个Engine，可以对其进行序列化操作，下次执行Inference时直接反序列化该Engine即可。</strong></p><p>其余琐碎的东西：</p><ul><li>TensorRT提供了C++接口和Python接口，官方建议使用C++接口</li><li><strong>一个Engine的建立是根据特定GPU和CUDA版本来的，所以在一个机器上序列化的Engine到另一个机器上不一定能使用，因此在使用Builder生成Engine前，要注意自己的环境配置</strong></li><li>TensorRT可结合DALI(加速数据读取)和DLA(加速某些层的运算)一起使用</li><li>对于TensorRT中不支持的层，需要自己编写相应的文件，TensorRT提供了相关支持</li></ul><h2 id="TensorRT使用流程"><a href="#TensorRT使用流程" class="headerlink" title="TensorRT使用流程"></a>TensorRT使用流程</h2><p>在使用tensorRT的过程中需要提供以下文件（以caffe为例）：</p><ol><li>A network architecture file (deploy.prototxt), 模型文件</li><li>Trained weights (net.caffemodel), 权值文件</li><li>A label file to provide a name for each output class. 标签文件</li></ol><p>前两个是为了解析模型时使用，最后一个是推理输出时将数字映射为有意义的文字标签。</p><p>tensorRT的使用包括两个阶段， build and deployment：</p><h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><p>build：Import and optimize trained models to generate inference engines。build阶段主要完成模型转换（从caffe或TensorFlow到TensorRT），<strong>在模型转换时会完成前述优化过程中的层间融合，精度校准</strong>。这一步的输出是一个<strong>针对特定GPU平台和网络模型的优化过的TensorRT模型</strong>，这个TensorRT模型可以<strong>序列化</strong>存储到磁盘或内存中。<strong>存储到磁盘中的文件称之为 plan file</strong>。</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-workflow-1.png" alt="pasted-image-0-5-625x140"></p><p>下面代码是一个简单的build过程：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个builder</span></span><br><span class="line">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class="line"><span class="comment">// parse the caffe model to populate the network, then set the outputs</span></span><br><span class="line"><span class="comment">// 创建一个network对象，不过这时network对象只是一个空架子</span></span><br><span class="line">INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class="line"><span class="comment">//tensorRT提供一个高级别的API：CaffeParser，用于解析Caffe模型</span></span><br><span class="line"><span class="comment">//parser.parse函数接受的参数就是上面提到的文件，和network对象</span></span><br><span class="line"><span class="comment">//这一步之后network对象里面的参数才被填充，才具有实际的意义</span></span><br><span class="line">CaffeParser parser;</span><br><span class="line"><span class="keyword">auto</span> blob_name_to_tensor = parser.parse(“deploy.prototxt”,</span><br><span class="line">                                        trained_file.c_str(),</span><br><span class="line">                                        *network,</span><br><span class="line">                                        DataType::kFLOAT);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 标记输出 tensors</span></span><br><span class="line"><span class="comment">// specify which tensors are outputs</span></span><br><span class="line">network-&gt;markOutput(*blob_name_to_tensor-&gt;find(<span class="string">"prob"</span>));</span><br><span class="line"><span class="comment">// Build the engine</span></span><br><span class="line"><span class="comment">// 设置batchsize和工作空间，然后创建inference engine</span></span><br><span class="line">builder-&gt;setMaxBatchSize(<span class="number">1</span>);</span><br><span class="line">builder-&gt;setMaxWorkspaceSize(<span class="number">1</span> &lt;&lt; <span class="number">30</span>); </span><br><span class="line"><span class="comment">//调用buildCudaEngine时才会进行前述的层间融合或精度校准优化方式</span></span><br><span class="line">ICudaEngine* engine = builder-&gt;buildCudaEngine(*network);</span><br></pre></td></tr></table></figure><p>上面的过程使用了一个高级别的API：CaffeParser，直接读取 caffe的模型文件，就可以解析，也就是填充network对象。解析的过程也可以直接使用一些低级别的C++API，比如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ITensor* in = network-&gt;addInput(“input”, DataType::kFloat, Dims3&#123;…&#125;);</span><br><span class="line">IPoolingLayer* pool = network-&gt;addPooling(in, PoolingType::kMAX, …);</span><br></pre></td></tr></table></figure><p><strong>解析caffe模型之后，必须要指定输出tensor，设置batch size，和设置工作空间。</strong></p><ol><li>设置batch size就跟使用caffe测试是一样的。关于<code>builder-&gt;setMaxBatchSize(1);</code>官方解释为：The maximum batch size specifies the batch size for which TensorRT will optimize. At runtime, a smaller batch size may be chosen.</li><li>设置工作空间是进行前述层间融合和张量融合的必要措施。层间融合和张量融合的过程是在调用<code>builder-&gt;buildCudaEngine</code>时才进行的。关于<code>builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 30);</code>官方解释为：Layer algorithms often require temporary workspace. This parameter limits the maximum size that any layer in the network can use. If an insufficient scratch is provided, it is possible that TensorRT may not be able to find an implementation for a given layer.</li></ol><p>另外，官方文档问题：Q: How do I choose the optimal workspace size?</p><p>A: Some TensorRT algorithms require additional workspace on the GPU. The method IBuilderConfig::setMaxWorkspaceSize() controls the maximum amount of workspace that may be allocated, and will prevent algorithms that require more workspace from being considered by the builder. At runtime, the space is allocated automatically when creating an IExecutionContext. The amount allocated will be no more than is required, even if the amount set in IBuilderConfig::setMaxWorkspaceSize() is much higher. Applications should therefore allow the TensorRT builder as much workspace as they can afford; at runtime TensorRT will allocate no more than this, and typically less.</p><h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><p>deploy：Generate runtime inference engine for inference。deploy阶段主要完成推理过程，<strong>Kernel Auto-Tuning 和 Dynamic Tensor Memory 应该是在这里完成的</strong>。将上面一个步骤中的plan文件<strong>首先反序列化，并创建一个 runtime engine，然后就可以输入数据</strong>（比如测试集或数据集之外的图片），然后输出分类向量结果或检测结果。</p><p><img src="/MLFrameworks/TensorRT/TensorRT基本概念/TensorRT-workflow-2.png" alt="pasted-image-0-6-625x129"></p><p>以下是一个简单的deploy代码：<strong>这里面没有包含反序列化过程和测试时的batch流获取</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The execution context is responsible for launching the </span></span><br><span class="line"><span class="comment">// compute kernels 创建上下文环境 context，用于启动kernel</span></span><br><span class="line">IExecutionContext *context = engine-&gt;createExecutionContext();</span><br><span class="line"><span class="comment">// In order to bind the buffers, we need to know the names of the </span></span><br><span class="line"><span class="comment">// input and output tensors. //获取输入，输出tensor索引</span></span><br><span class="line"><span class="keyword">int</span> inputIndex = engine-&gt;getBindingIndex(INPUT_LAYER_NAME),</span><br><span class="line"><span class="keyword">int</span> outputIndex = engine-&gt;getBindingIndex(OUTPUT_LAYER_NAME);</span><br><span class="line"><span class="comment">//申请GPU显存</span></span><br><span class="line"><span class="comment">// Allocate GPU memory for Input / Output data</span></span><br><span class="line"><span class="keyword">void</span>* buffers = <span class="built_in">malloc</span>(engine-&gt;getNbBindings() * <span class="keyword">sizeof</span>(<span class="keyword">void</span>*));</span><br><span class="line">cudaMalloc(&amp;buffers[inputIndex], batchSize * size_of_single_input);</span><br><span class="line">cudaMalloc(&amp;buffers[outputIndex], batchSize * size_of_single_output);</span><br><span class="line"><span class="comment">//使用cuda 流来管理并行计算</span></span><br><span class="line"><span class="comment">// Use CUDA streams to manage the concurrency of copying and executing</span></span><br><span class="line">cudaStream_t stream;</span><br><span class="line">cudaStreamCreate(&amp;stream);</span><br><span class="line"><span class="comment">//从内存到显存，input是读入内存中的数据；buffers[inputIndex]是显存上的存储区域，用于存放输入数据</span></span><br><span class="line"><span class="comment">// Copy Input Data to the GPU</span></span><br><span class="line">cudaMemcpyAsync(buffers[inputIndex], input, </span><br><span class="line">                batchSize * size_of_single_input, </span><br><span class="line">                cudaMemcpyHostToDevice, stream);</span><br><span class="line"><span class="comment">//启动cuda核计算</span></span><br><span class="line"><span class="comment">// Launch an instance of the GIE compute kernel</span></span><br><span class="line">context.enqueue(batchSize, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line"><span class="comment">//从显存到内存，buffers[outputIndex]是显存中的存储区，存放模型输出；output是内存中的数据</span></span><br><span class="line"><span class="comment">// Copy Output Data to the Host</span></span><br><span class="line">cudaMemcpyAsync(output, buffers[outputIndex], </span><br><span class="line">                batchSize * size_of_single_output, </span><br><span class="line">                cudaMemcpyDeviceToHost, stream));</span><br><span class="line"><span class="comment">//如果使用了多个cuda流，需要同步</span></span><br><span class="line"><span class="comment">// It is possible to have multiple instances of the code above</span></span><br><span class="line"><span class="comment">// in flight on the GPU in different streams.</span></span><br><span class="line"><span class="comment">// The host can then sync on a given stream and use the results</span></span><br><span class="line">cudaStreamSynchronize(stream);</span><br></pre></td></tr></table></figure><p>在执行的时候创建<strong>context，主要是分配预先的资源</strong>，engine加context就可以做推断（Inference）。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arleyzhang.github.io/articles/7f4b25ce/" target="_blank" rel="noopener">TensorRT(1)-介绍-使用-安装</a><br><a href="https://www.cnblogs.com/vh-pg/p/11677137.html" target="_blank" rel="noopener">《一》TensorRT之基本概念</a><br><a href="https://murphypei.github.io/blog/2019/09/trt-useage" target="_blank" rel="noopener">TensorRT 实战教程</a><br><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html" target="_blank" rel="noopener">tensorrt/developer-guide</a><br><a href="https://blog.csdn.net/ahxieqi/article/details/93628439" target="_blank" rel="noopener">TensorRT - Custom Layer</a><br><a href="https://zhuanlan.zhihu.com/p/35657027" target="_blank" rel="noopener">高性能深度学习支持引擎实战——TensorRT</a><br><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1592725491&amp;ver=2413&amp;signature=jTi63A64A9cB*DVMLC6B*FcZvqHk559SHhZwh5k87zM7ckAbRwt0ycJy9hizKGyEHjhvGb39a5LJfnn0x1u8iiZOtvfGHwuUzTVwPVa-sIL5YXMiIjTwXRVVQooBzwTX&amp;new=1" target="_blank" rel="noopener">深度学习算法优化系列二十二 | 利用TensorRT部署YOLOV3-Tiny INT8量化模型</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/" title="TensorRT基本概念">https://www.zdaiot.com/MLFrameworks/TensorRT/TensorRT基本概念/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/TensorRT/" rel="tag"><i class="fa fa-tag"></i> TensorRT</a><a href="/tags/onnx/" rel="tag"><i class="fa fa-tag"></i> onnx</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MLFrameworks/TensorRT/使用TensorRT对Pytorch模型加速/" rel="prev" title="使用TensorRT对Pytorch模型加速"><i class="fa fa-chevron-left"></i> 使用TensorRT对Pytorch模型加速</a></div><div class="post-nav-item"> <a href="/MLFrameworks/TensorRT/编译DALI库/" rel="next" title="编译DALI库">编译DALI库<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化方式"><span class="nav-number">2.</span> <span class="nav-text">优化方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#层间融合或张量融合（Layer-amp-Tensor-Fusion）"><span class="nav-number">2.1.</span> <span class="nav-text">层间融合或张量融合（Layer &amp; Tensor Fusion）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据精度校准（Weight-amp-Activation-Precision-Calibration）"><span class="nav-number">2.2.</span> <span class="nav-text">数据精度校准（Weight &amp;Activation Precision Calibration）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernel-Auto-Tuning"><span class="nav-number">2.3.</span> <span class="nav-text">Kernel Auto-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-Tensor-Memory"><span class="nav-number">2.4.</span> <span class="nav-text">Dynamic Tensor Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Stream-Execution"><span class="nav-number">2.5.</span> <span class="nav-text">Multi-Stream Execution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#名词解释"><span class="nav-number">3.</span> <span class="nav-text">名词解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT使用流程"><span class="nav-number">4.</span> <span class="nav-text">TensorRT使用流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#build"><span class="nav-number">4.1.</span> <span class="nav-text">build</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deploy"><span class="nav-number">4.2.</span> <span class="nav-text">deploy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">324</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">56</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">381</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">34:13</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '2f24f43d0dea5cc2ea332561899ce35a',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>