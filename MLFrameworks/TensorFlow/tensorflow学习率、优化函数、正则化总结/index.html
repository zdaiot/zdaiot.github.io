<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="学习率自适应学习率在模型的初期的时候，往往设置为较大的学习速率比较好，因为距离极值点比较远，较大的学习速率可以快速靠近极值点；而，后期，由于已经靠近极值点，模型快收敛了，此时，采用较小的学习速率较好，较大的学习速率，容易导致在真实极值点附近来回波动，就是无法抵达极值点。 在tensorflow中，提供了一个较为友好的API, tf.train.exponential_decay(learning_"><meta name="keywords" content="Lr,TensorFlow,优化算法,正则化"><meta property="og:type" content="article"><meta property="og:title" content="tensorflow学习率、优化函数、正则化总结"><meta property="og:url" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="学习率自适应学习率在模型的初期的时候，往往设置为较大的学习速率比较好，因为距离极值点比较远，较大的学习速率可以快速靠近极值点；而，后期，由于已经靠近极值点，模型快收敛了，此时，采用较小的学习速率较好，较大的学习速率，容易导致在真实极值点附近来回波动，就是无法抵达极值点。 在tensorflow中，提供了一个较为友好的API, tf.train.exponential_decay(learning_"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191352718.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191740163.png"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191908959.png"><meta property="og:updated_time" content="2019-09-15T03:44:33.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="tensorflow学习率、优化函数、正则化总结"><meta name="twitter:description" content="学习率自适应学习率在模型的初期的时候，往往设置为较大的学习速率比较好，因为距离极值点比较远，较大的学习速率可以快速靠近极值点；而，后期，由于已经靠近极值点，模型快收敛了，此时，采用较小的学习速率较好，较大的学习速率，容易导致在真实极值点附近来回波动，就是无法抵达极值点。 在tensorflow中，提供了一个较为友好的API, tf.train.exponential_decay(learning_"><meta name="twitter:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191352718.png"><link rel="canonical" href="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>tensorflow学习率、优化函数、正则化总结 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> tensorflow学习率、优化函数、正则化总结<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-04-01 22:18:24" itemprop="dateCreated datePublished" datetime="2019-04-01T22:18:24+08:00">2019-04-01</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-09-15 11:44:33" itemprop="dateModified" datetime="2019-09-15T11:44:33+08:00">2019-09-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/" itemprop="url" rel="index"><span itemprop="name">MLFrameworks</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/TensorFlow/" itemprop="url" rel="index"><span itemprop="name">TensorFlow</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>12k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>11 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><h3 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h3><p>在模型的初期的时候，往往设置为较大的学习速率比较好，因为距离极值点比较远，较大的学习速率可以快速靠近极值点；而，后期，由于已经靠近极值点，模型快收敛了，此时，采用较小的学习速率较好，较大的学习速率，容易导致在真实极值点附近来回波动，就是无法抵达极值点。</p><p>在tensorflow中，提供了一个较为友好的API, <code>tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)</code>，其数学表达式是这样的：</p><script type="math/tex;mode=display">decayed\_learning_rate=learning\_rate×decay\_rate^{global\_step/decay\_steps}</script><p>先解释API中的参数的意思，第一个参数learning_rate即初始学习速率，第二个参数，是用来计算步骤的，每调用一次优化器，即自增1，第三个参数decay_steps通常设为一个常数，如数学公式中所示，与第五个参数配合使用效果较好，第五个参数staircase如果设置为True，那么指数部分就会采用整除策略，表示每decay_step，学习速率变为原来的decay_rate，至于第四个参数decay_rate表示的是学习速率的下降倍率。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line">starter_learning_rate = 0.1</span><br><span class="line">learning_rate = tf.exponential_decay(starter_learning_rate, global_step, 100000, 0.96, staircase=True)</span><br><span class="line">optimizer = tf.GradientDescent(learning_rate)</span><br><span class="line">optimizer.minimize(...my loss..., global_step=global_step)</span><br></pre></td></tr></table></figure><p>意思就是，初始的学习速率是0.1，每经过10万轮次训练后，学习速率变为原来的0.96。</p><h3 id="不同层设置不同学习率"><a href="#不同层设置不同学习率" class="headerlink" title="不同层设置不同学习率"></a>不同层设置不同学习率</h3><p><strong>先输出层的参数变量（要先进行参数初始化）</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">variables_names = [v.name for v in tf.trainable_variables()]</span><br><span class="line">values = sess.run(variables_names)</span><br><span class="line">for k, v in zip(variables_names, values):</span><br><span class="line">    print &quot;Variable: &quot;, k</span><br><span class="line">    print &quot;Shape: &quot;, v.shape</span><br><span class="line">    print v</span><br></pre></td></tr></table></figure><p></p><p><strong>设置前后层的学习率</strong></p><p>比如要对前20层的参数使用较低的学习率微调（20层大概有40种参数，20个weight，20个bia）<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">var1 = tf.trainable_variables()[0:40]</span><br><span class="line">var2 = tf.trainable_variables()[40:]</span><br><span class="line">train_op1 = GradientDescentOptimizer(0.00001).minimize(loss, var_list=var1)</span><br><span class="line">train_op2 = GradientDescentOptimizer(0.0001).minimize(loss, var_list=var2)</span><br><span class="line">train_op = tf.group(train_op1, train_op2)</span><br></pre></td></tr></table></figure><p></p><p><strong>另外一种高效的方法：,可以简单的理解为minimize方法包含了gradients和apply_gradients两个步骤，而第一种方法中gradients方法运行了两次</strong><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var_list1 = [variables from first 5 layers]</span><br><span class="line">var_list2 = [the rest of variables]</span><br><span class="line">opt1 = tf.train.GradientDescentOptimizer(0.00001)</span><br><span class="line">opt2 = tf.train.GradientDescentOptimizer(0.0001)</span><br><span class="line">grads = tf.gradients(loss, var_list1 + var_list2)</span><br><span class="line">grads1 = grads[:len(var_list1)]</span><br><span class="line">grads2 = grads[len(var_list1):]</span><br><span class="line">tran_op1 = opt1.apply_gradients(zip(grads1, var_list1))</span><br><span class="line">train_op2 = opt2.apply_gradients(zip(grads2, var_list2))</span><br><span class="line">train_op = tf.group(train_op1, train_op2)</span><br></pre></td></tr></table></figure><p></p><p><strong>注意，</strong>若需要根据<code>global_step</code>动态调整学习率，只需要在<code>minimize</code>或<code>opt1.apply_gradients</code>中传入一次即可，若传入两次，<code>global_step</code>会自加两次。也就是说：损失函数优化器的minimize()或者apply_gradients()中global_step=global_steps能够提供global_step自动加一的操作。</p><p>例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line"># Last layer has a 10 times learning rate</span><br><span class="line">self.lr = tf.train.exponential_decay(self.learning_rate, global_step, self.decay_step, self.learning_rate_decay_factor, staircase=True) # 学习率随着global_step而衰减的</span><br><span class="line"># Last layer has a 10 times learning rate</span><br><span class="line">self.lr_last = tf.train.exponential_decay(self.learning_rate*10, global_step, self.decay_step, self.learning_rate_decay_factor, staircase=True)</span><br><span class="line"></span><br><span class="line">var_list1 = tf.trainable_variables()[0:28]</span><br><span class="line">var_list2 = tf.trainable_variables()[28:30]</span><br><span class="line">opt1 = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=0.9)</span><br><span class="line">opt2 = tf.train.MomentumOptimizer(learning_rate=self.lr_last, momentum=0.9)</span><br><span class="line"></span><br><span class="line"># for BatchNorm</span><br><span class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">with tf.control_dependencies(update_ops):</span><br><span class="line">    grads = tf.gradients(self.loss, var_list1 + var_list2)</span><br><span class="line">    grads1 = grads[:len(var_list1)]</span><br><span class="line">    grads2 = grads[len(var_list1):]</span><br><span class="line"></span><br><span class="line">    # 在这个地方，global_step自加１，但是只需要加一个global_step即可，否则会自加1两次</span><br><span class="line">    train_op1 = opt1.apply_gradients(zip(grads1, var_list1), global_step=global_step)</span><br><span class="line">    train_op2 = opt2.apply_gradients(zip(grads2, var_list2))</span><br><span class="line">train_op = tf.group(train_op1, train_op2)</span><br></pre></td></tr></table></figure><p></p><h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><p>目前TensorFlow支持11种不同的经典优化器（参考<a href="https://www.tensorflow.org/api_docs/python/tf/train#Optimizers" target="_blank" rel="noopener">TensorFlow API tf.train文档</a>）<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.train.Optimizer</span><br><span class="line">tf.train.GradientDescentOptimizer</span><br><span class="line">tf.train.AdadeltaOptimizer</span><br><span class="line">tf.train.AdagtadOptimizer</span><br><span class="line">tf.train.AdagradDAOptimizer</span><br><span class="line">tf.train.MomentumOptimizer</span><br><span class="line">tf.train.AdamOptimizer</span><br><span class="line">tf.train.FtrlOptimizer</span><br><span class="line">tf.train.ProximalGradientDescentOptimizer</span><br><span class="line">tf.train.ProximalAdagradOptimizer</span><br><span class="line">tf.train.RMSProOptimizer</span><br></pre></td></tr></table></figure><p></p><h3 id="tf-train-GradientDescentOptimizer"><a href="#tf-train-GradientDescentOptimizer" class="headerlink" title="tf.train.GradientDescentOptimizer"></a>tf.train.GradientDescentOptimizer</h3><p>这个优化器主要实现的是 梯度下降算法<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    learning_rate,</span><br><span class="line">    use_locking=False,</span><br><span class="line">    name=&apos;GradientDescent&apos;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><ul><li>learning_rate: （学习率）张量或者浮点数</li><li>use_locking: 为True时锁定更新</li><li>name: 梯度下降名称，默认为”GradientDescent”.</li></ul><h3 id="tf-train-MomentumOptimizer"><a href="#tf-train-MomentumOptimizer" class="headerlink" title="tf.train.MomentumOptimizer"></a>tf.train.MomentumOptimizer</h3><p>实现 动量梯度下降算法 ，可参考<a href="https://blog.csdn.net/yinruiyang94/article/details/77944338" target="_blank" rel="noopener">简述动量Momentum梯度下降</a></p><p>其中， 即momentum，表示要在多大程度上保留原来的更新方向，这个值在0-1之间，在训练开始时，由于梯度可能会很大，所以初始值一般选为0.5；当梯度不那么大时，改为0.9。 是学习率，即当前batch的梯度多大程度上影响最终更新方向，跟普通的SGD含义相同。与之和不一定为1。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    learning_rate,</span><br><span class="line">    momentum,</span><br><span class="line">    use_locking=False,</span><br><span class="line">    name=&apos;Momentum&apos;,</span><br><span class="line">    use_nesterov=False</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><ul><li>learning_rate: （学习率）张量或者浮点数</li><li>momentum: （动量）张量或者浮点数</li><li>use_locking: 为True时锁定更新</li><li>name: 梯度下降名称，默认为 “Momentum”.</li><li>use_nesterov: 为True时，使用<a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">Nesterov Momentum.</a></li></ul><h3 id="tf-train-AdamOptimizer"><a href="#tf-train-AdamOptimizer" class="headerlink" title="tf.train.AdamOptimizer"></a>tf.train.AdamOptimizer</h3><p>实现 Adam优化算法（Adam 这个名字来源于 adaptive moment estimation，自适应矩估计。）</p><p>可参考博客梯度优化算法<a href="https://blog.csdn.net/zb1165048017/article/details/78392623" target="_blank" rel="noopener">Adam</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    learning_rate=0.001,</span><br><span class="line">    beta1=0.9,</span><br><span class="line">    beta2=0.999,</span><br><span class="line">    epsilon=1e-08,</span><br><span class="line">    use_locking=False,</span><br><span class="line">    name=&apos;Adam&apos;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><ul><li>learning_rate: （学习率）张量或者浮点数</li><li>beta1: 浮点数或者常量张量 ，表示 The exponential decay rate for the 1st moment estimates.</li><li>beta2: 浮点数或者常量张量 ，表示 The exponential decay rate for the 2nd moment estimates.</li><li>epsilon: A small constant for numerical stability. This epsilon is “epsilon hat” in the Kingma and Ba paper (in the formula just before Section 2.1), not the - epsilon in Algorithm 1 of the paper.</li><li>use_locking: 为True时锁定更新</li><li>name: 梯度下降名称，默认为 “Adam”.</li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>所谓的过拟合问题指的是当一个模型很复杂时，它可以很好的“记忆”每一个训练数据中的随机噪音的部分而忘记了要去“学习”训练数据中通用的趋势。</p><p>为了避免过拟合问题，一个非常常用的方法就是正则化。也就是在损失函数中加入刻画模型复杂程度的指标。假设用于损失函数的为$J(\theta)$，那此时不直接优化$J(\theta)$，而是优化$J(\theta)+\lambda R(w)$。其中$R(w)$刻画的是模型的复杂度，而$\lambda$表示的是模型复杂损失在总损失中的比例。这里的$\theta$表示的是一个神经网络中所有的参数，它包括边上的权重$w$和偏置项$b$。一般来说模型复杂度只由权重表示。常用的刻画模型复杂度的函数$R(w)$有两种，一种是L1正则化，另一种是L2正则化。这两种都是通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声。这两种正则化的区别是首先L1正则化会让参数变得稀疏（指会有更多参数变为0），而L2正则化则不会（因为参数的平方后会让小的参数变得更小，大的参数变得更大，同样起到了特征选取的功能，而不会让参数变为0）。其次是L1正则化计算不可导，而L2的正则化损失函数可导。</p><p>以上是一些简单的正则花基础，下面是神经网络的搭建：</p><p>当网络复杂的时候定义网络的结构部分和计算损失函数的部分可能不在一个函数中，这样通过简单的变量这种计算损失函数就不方便了。此时可以使用Tensorflow中提供的集合，它可以在一个计算图（tf.Graph）中保存一组实体（比如张量）。以下是通过集合计算一个5层的神经网络带L2正则化的损失函数的计算过程。</p><h3 id="tensorflow代码"><a href="#tensorflow代码" class="headerlink" title="tensorflow代码"></a>tensorflow代码</h3><p>生成模拟数据集。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">data = []</span><br><span class="line">label = []</span><br><span class="line">np.random.seed(0) # 设置随机数生成时所用算法开始的整数值</span><br><span class="line"></span><br><span class="line"># 以原点为圆心，半径为1的圆把散点划分成红蓝两部分，并加入随机噪音。</span><br><span class="line">for i in range(150):</span><br><span class="line">    x1 = np.random.uniform(-1,1)  # 随机生成下一个实数，它在 [-1，1) 范围内。</span><br><span class="line">    x2 = np.random.uniform(0,2)</span><br><span class="line">    if x1**2 + x2**2 &lt;= 1:</span><br><span class="line">        data.append([np.random.normal(x1, 0.1),np.random.normal(x2,0.1)])</span><br><span class="line">        label.append(0)</span><br><span class="line">    else:</span><br><span class="line">        data.append([np.random.normal(x1, 0.1), np.random.normal(x2, 0.1)])</span><br><span class="line">        label.append(1)</span><br><span class="line"></span><br><span class="line">data = np.hstack(data).reshape(-1,2) # 把数据转换成n行2列</span><br><span class="line">label = np.hstack(label).reshape(-1, 1)  # 把数据转换为n行1列</span><br><span class="line">plt.scatter(data[:,0], data[:,1], c=label,cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2, edgecolor=&quot;white&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><p>结果：<br><img src="/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191352718.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#2. 定义一个获取权重，并自动加入正则项到损失的函数。</span><br><span class="line">def get_weight(shape, lambda1):</span><br><span class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32) # 生成一个变量</span><br><span class="line">    tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(lambda1)(var)) # add_to_collection()函数将新生成变量的L2正则化损失加入集合losses</span><br><span class="line">    return var # 返回生成的变量</span><br><span class="line"></span><br><span class="line">#3. 定义神经网络。</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">sample_size = len(data)</span><br><span class="line"></span><br><span class="line"># 每层节点的个数</span><br><span class="line">layer_dimension = [2,10,5,3,1]</span><br><span class="line"># 神经网络的层数</span><br><span class="line">n_layers = len(layer_dimension)</span><br><span class="line"># 这个变量维护前向传播时最深层的节点，开始的时候就是输入层</span><br><span class="line">cur_layer = x</span><br><span class="line"># 当前层的节点个数</span><br><span class="line">in_dimension = layer_dimension[0]</span><br><span class="line"></span><br><span class="line"># 循环生成网络结构</span><br><span class="line">for i in range(1, n_layers):</span><br><span class="line">    out_dimension = layer_dimension[i] # layer_dimension[i]为下一层的节点个数</span><br><span class="line">    # 生成当前层中权重的变量，并将这个变量的L2正则化损失加入计算图上的集合</span><br><span class="line">    weight = get_weight([in_dimension, out_dimension], 0.003)</span><br><span class="line">    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension])) # 偏置</span><br><span class="line">    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias) # 使用Relu激活函数</span><br><span class="line">    in_dimension = layer_dimension[i]  # 进入下一层之前将下一层的节点个数更新为当前节点个数</span><br><span class="line"></span><br><span class="line">y= cur_layer</span><br><span class="line"></span><br><span class="line"># 在定义神经网络前向传播的同时已经将所有的L2正则化损失加入了图上的集合，这里是损失函数的定义。</span><br><span class="line">mse_loss = tf.reduce_sum(tf.pow(y_ - y, 2)) / sample_size # 也可以写成：tf.reduce_mean(tf.square(y_ - y`))</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, mse_loss) # 将均方误差损失函数加入损失集合</span><br><span class="line"># get_collection()返回一个列表，这个列表是所有这个集合中的元素，在本样例中这些元素就是损失函数的不同部分，将他们加起来就是最终的损失函数</span><br><span class="line">loss = tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line"># 4. 训练不带正则项的损失函数mse_loss。</span><br><span class="line"># 定义训练的目标函数mse_loss，训练次数及训练模型</span><br><span class="line">train_op = tf.train.AdamOptimizer(0.001).minimize(mse_loss)</span><br><span class="line">TRAINING_STEPS = 40000</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run() # 初始化所有的变量</span><br><span class="line">    for i in range(TRAINING_STEPS):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: data, y_: label&#125;)</span><br><span class="line">        if i % 2000 == 0:</span><br><span class="line">            print(&quot;After %d steps, mse_loss: %f&quot; % (i,sess.run(mse_loss, feed_dict=&#123;x: data, y_: label&#125;)))</span><br><span class="line"></span><br><span class="line">    # 画出训练后的分割曲线       </span><br><span class="line">    xx, yy = np.mgrid[-1.2:1.2:.01, -0.2:2.2:.01]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.scatter(data[:,0], data[:,1], c=label, cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2, edgecolor=&quot;white&quot;)</span><br><span class="line">plt.contour(xx, yy, probs, levels=[.5], cmap=&quot;Greys&quot;, vmin=0, vmax=.1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">After 0 steps, mse_loss: 0.588501</span><br><span class="line">After 2000 steps, mse_loss: 0.039796</span><br><span class="line">After 4000 steps, mse_loss: 0.018524</span><br><span class="line">After 6000 steps, mse_loss: 0.018494</span><br><span class="line">After 8000 steps, mse_loss: 0.018374</span><br><span class="line">After 10000 steps, mse_loss: 0.018358</span><br><span class="line">After 12000 steps, mse_loss: 0.018356</span><br><span class="line">After 14000 steps, mse_loss: 0.018355</span><br><span class="line">After 16000 steps, mse_loss: 0.016440</span><br><span class="line">After 18000 steps, mse_loss: 0.013988</span><br><span class="line">After 20000 steps, mse_loss: 0.013142</span><br><span class="line">After 22000 steps, mse_loss: 0.012886</span><br><span class="line">After 24000 steps, mse_loss: 0.012700</span><br><span class="line">After 26000 steps, mse_loss: 0.012550</span><br><span class="line">After 28000 steps, mse_loss: 0.006441</span><br><span class="line">After 30000 steps, mse_loss: 0.006439</span><br><span class="line">After 32000 steps, mse_loss: 0.006438</span><br><span class="line">After 34000 steps, mse_loss: 0.006438</span><br><span class="line">After 36000 steps, mse_loss: 0.006445</span><br><span class="line">After 38000 steps, mse_loss: 0.006438</span><br></pre></td></tr></table></figure><p></p><p><img src="/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191740163.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#5. 训练带正则项的损失函数loss。</span><br><span class="line"># 定义训练的目标函数loss，训练次数及训练模型</span><br><span class="line">train_op = tf.train.AdamOptimizer(0.001).minimize(loss)</span><br><span class="line">TRAINING_STEPS = 40000</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    for i in range(TRAINING_STEPS):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: data, y_: label&#125;)</span><br><span class="line">        if i % 2000 == 0:</span><br><span class="line">            print(&quot;After %d steps, loss: %f&quot; % (i, sess.run(loss, feed_dict=&#123;x: data, y_: label&#125;)))</span><br><span class="line"></span><br><span class="line">    # 画出训练后的分割曲线       </span><br><span class="line">    xx, yy = np.mgrid[-1:1:.01, 0:2:.01]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.scatter(data[:,0], data[:,1], c=label,cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2, edgecolor=&quot;white&quot;)</span><br><span class="line">plt.contour(xx, yy, probs, levels=[.5], cmap=&quot;Greys&quot;, vmin=0, vmax=.1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">After 0 steps, loss: 0.705000</span><br><span class="line">After 2000 steps, loss: 0.056949</span><br><span class="line">After 4000 steps, loss: 0.045995</span><br><span class="line">After 6000 steps, loss: 0.041472</span><br><span class="line">After 8000 steps, loss: 0.040165</span><br><span class="line">After 10000 steps, loss: 0.039961</span><br><span class="line">After 12000 steps, loss: 0.039916</span><br><span class="line">After 14000 steps, loss: 0.039912</span><br><span class="line">After 16000 steps, loss: 0.039912</span><br><span class="line">After 18000 steps, loss: 0.038334</span><br><span class="line">After 20000 steps, loss: 0.038128</span><br><span class="line">After 22000 steps, loss: 0.037962</span><br><span class="line">After 24000 steps, loss: 0.037932</span><br><span class="line">After 26000 steps, loss: 0.037921</span><br><span class="line">After 28000 steps, loss: 0.037918</span><br><span class="line">After 30000 steps, loss: 0.037910</span><br><span class="line">After 32000 steps, loss: 0.037908</span><br><span class="line">After 34000 steps, loss: 0.037910</span><br><span class="line">After 36000 steps, loss: 0.037907</span><br><span class="line">After 38000 steps, loss: 0.037905</span><br></pre></td></tr></table></figure><p></p><p><img src="/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/20180123191908959.png" alt></p><h4 id="tf-add-to-collection"><a href="#tf-add-to-collection" class="headerlink" title="tf.add_to_collection"></a>tf.add_to_collection</h4><ul><li>tf.add_to_collection(‘list_name’, element)：将元素element添加到列表list_name中</li><li>tf.get_collection(‘list_name’)：返回名称为list_name的列表</li><li>tf.add_n(list)：将列表元素相加并返回</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, tf.constant(2.2))</span><br><span class="line">tf.add_to_collection(&apos;losses&apos;, tf.constant(3.))</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(tf.get_collection(&apos;losses&apos;)))</span><br><span class="line">    print(sess.run(tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">[2.2, 3.0] </span><br><span class="line">5.2</span><br><span class="line">注意： </span><br><span class="line">使用tf.add_n对列表元素进行相加时，列表内元素类型必须一致，否则会报错。</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/crackpotisback/p/7105748.html" target="_blank" rel="noopener">Tensorflow 自适应学习速率</a><br><a href="https://blog.csdn.net/perom/article/details/80540470" target="_blank" rel="noopener">TensorFlow三种常用的优化器</a><br><a href="https://blog.csdn.net/shwan_ma/article/details/78881961" target="_blank" rel="noopener">【tensorflow】在不同层上设置不同的学习率，fine-tuning</a><br><a href="https://stackoverflow.com/questions/34945554/how-to-set-layer-wise-learning-rate-in-tensorflow" target="_blank" rel="noopener">how-to-set-layer-wise-learning-rate-in-tensorflow</a><br><a href="https://blog.csdn.net/leviopku/article/details/78508951" target="_blank" rel="noopener">TensorFlow中global_step的简单分析</a><br><a href="https://blog.csdn.net/lilong117194/article/details/79130032" target="_blank" rel="noopener">tensorflow中的正则化解决过拟合问题</a><br><a href="https://www.jianshu.com/p/6612f368e8f4" target="_blank" rel="noopener">tf.add_to_collection</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/" title="tensorflow学习率、优化函数、正则化总结">https://www.zdaiot.com/MLFrameworks/TensorFlow/tensorflow学习率、优化函数、正则化总结/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Lr/" rel="tag"><i class="fa fa-tag"></i> Lr</a><a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a><a href="/tags/优化算法/" rel="tag"><i class="fa fa-tag"></i> 优化算法</a><a href="/tags/正则化/" rel="tag"><i class="fa fa-tag"></i> 正则化</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MLFrameworks/TensorFlow/caffe模型转tensorflow模型/" rel="prev" title="caffe模型转tensorflow模型"><i class="fa fa-chevron-left"></i> caffe模型转tensorflow模型</a></div><div class="post-nav-item"> <a href="/MachineLearning/机器学习/Cross-Validation（交叉验证）详解/" rel="next" title="Cross-Validation（交叉验证）详解">Cross-Validation（交叉验证）详解<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率"><span class="nav-number">1.</span> <span class="nav-text">学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#自适应学习率"><span class="nav-number">1.1.</span> <span class="nav-text">自适应学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不同层设置不同学习率"><span class="nav-number">1.2.</span> <span class="nav-text">不同层设置不同学习率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化函数"><span class="nav-number">2.</span> <span class="nav-text">优化函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-train-GradientDescentOptimizer"><span class="nav-number">2.1.</span> <span class="nav-text">tf.train.GradientDescentOptimizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-train-MomentumOptimizer"><span class="nav-number">2.2.</span> <span class="nav-text">tf.train.MomentumOptimizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-train-AdamOptimizer"><span class="nav-number">2.3.</span> <span class="nav-text">tf.train.AdamOptimizer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-number">3.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介"><span class="nav-number">3.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorflow代码"><span class="nav-number">3.2.</span> <span class="nav-text">tensorflow代码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-add-to-collection"><span class="nav-number">3.2.1.</span> <span class="nav-text">tf.add_to_collection</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">324</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">56</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">381</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.2m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">34:04</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '581d2db173e57818b2143d6ce8a69cd9',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>