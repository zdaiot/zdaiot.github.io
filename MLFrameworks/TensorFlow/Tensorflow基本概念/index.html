<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="深度学习（神经网络）之所以具备智能，就是因为它具有反馈机制。深度学习具有一套对输出所做的评价函数（损失函数），损失函数在对神经网络做出评价后，会通过某种方式（梯度下降法）更新网络的组成参数，以期望系统得到更好的输出数据。  由此可见，神经网络的系统主要由以下几个方面组成：  输入 系统本身（神经网络结构），以及涉及到系统本身构建的问题：如网络构建方式、网络执行方式、变量维护、模型存储和恢复等等问题"><meta name="keywords" content="TensorFlow"><meta property="og:type" content="article"><meta property="og:title" content="Tensorflow基本概念"><meta property="og:url" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="深度学习（神经网络）之所以具备智能，就是因为它具有反馈机制。深度学习具有一套对输出所做的评价函数（损失函数），损失函数在对神经网络做出评价后，会通过某种方式（梯度下降法）更新网络的组成参数，以期望系统得到更好的输出数据。  由此可见，神经网络的系统主要由以下几个方面组成：  输入 系统本身（神经网络结构），以及涉及到系统本身构建的问题：如网络构建方式、网络执行方式、变量维护、模型存储和恢复等等问题"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/250.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/248.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/v2-b4607eca9cc0750b5375bba9d6b58474_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/tensors_flowing.gif"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/f5b848b0-48f9-4421-8dc8-f142890bbd35.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/251.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/252.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/253.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/254.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/255.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/256.jpg"><meta property="og:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/20190806161204991.png"><meta property="og:updated_time" content="2019-09-14T03:46:26.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Tensorflow基本概念"><meta name="twitter:description" content="深度学习（神经网络）之所以具备智能，就是因为它具有反馈机制。深度学习具有一套对输出所做的评价函数（损失函数），损失函数在对神经网络做出评价后，会通过某种方式（梯度下降法）更新网络的组成参数，以期望系统得到更好的输出数据。  由此可见，神经网络的系统主要由以下几个方面组成：  输入 系统本身（神经网络结构），以及涉及到系统本身构建的问题：如网络构建方式、网络执行方式、变量维护、模型存储和恢复等等问题"><meta name="twitter:image" content="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/250.jpg"><link rel="canonical" href="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Tensorflow基本概念 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">达达And木槿</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="达达And木槿"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> Tensorflow基本概念<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MLFrameworks/TensorFlow/Tensorflow基本概念.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-03-12 23:20:21" itemprop="dateCreated datePublished" datetime="2019-03-12T23:20:21+08:00">2019-03-12</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-09-14 11:46:26" itemprop="dateModified" datetime="2019-09-14T11:46:26+08:00">2019-09-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/" itemprop="url" rel="index"><span itemprop="name">MLFrameworks</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MLFrameworks/TensorFlow/" itemprop="url" rel="index"><span itemprop="name">TensorFlow</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>38k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>35 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>深度学习（神经网络）之所以具备智能，就是因为它具有反馈机制。深度学习具有一套对输出所做的评价函数（损失函数），损失函数在对神经网络做出评价后，会通过某种方式（梯度下降法）更新网络的组成参数，以期望系统得到更好的输出数据。</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/250.jpg" alt></p><p>由此可见，神经网络的系统主要由以下几个方面组成：</p><ul><li>输入</li><li>系统本身（神经网络结构），以及涉及到系统本身构建的问题：如网络构建方式、网络执行方式、变量维护、模型存储和恢复等等问题</li><li>损失函数</li><li>反馈方式：训练方式</li></ul><p>定义好以上的组成部分，我们就可以用流程化的方式将其组合起来，让系统对输入进行学习，调整参数。因为该系统的反馈机制，所以，组成的方式肯定需要循环。</p><p>而对于Tensorflow来说，其设计理念肯定离不开神经网络本身。所以，学习Tensorflow之前，对神经网络有一个整体、深刻的理解也是必须的。如下图：<strong>Tensorflow的执行示意</strong>。</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/248.jpg" alt></p><p>那么对于以上所列的几点，什么才是最重要的呢？我想肯定是有关系统本身所涉及到的问题。即如何构建、执行一个神经网络？ 在Tensorflow中，<strong>用计算图来构建网络，用会话来具体执行网络</strong>。深入理解了这两点，我想，对于Tensorflow的设计思路，以及运行机制，也就略知一二了。</p><ul><li>图（tf.Graph）：计算图，主要用于构建网络，本身不进行任何实际的计算。计算图的设计启发是高等数学里面的链式求导法则的图。可以将计算图理解为是一个计算模板或者计划书。</li><li>会话（tf.session）：会话，主要用于执行网络。所有关于神经网络的计算都在这里进行，它执行的依据是计算图或者计算图的一部分，同时，会话也会负责分配计算资源和变量存放，以及维护执行过程中的变量。</li></ul><p>另外，我们需要对<strong>tensorflow内部API有一个整体了解</strong>，tensorflow API 结构一览：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/v2-b4607eca9cc0750b5375bba9d6b58474_hd.jpg" alt></p><p>在上图中， 左边的部分是高层API， 高层API在牺牲部分灵活性的情况下可以帮助你快速的搭建模型； 右边的部分属于底层API， 灵活性较高，但模型的实现较为复杂，一个简单模型往往需要上千行代码。</p><p>接下来，我们主要从计算图开始，看一看Tensorflow是如何构建、执行网络的。</p><h2 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h2><p>tensorflow 定义的数据流图如下所示：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/tensors_flowing.gif" alt></p><p>从上面可以看出，tensorflow的Graph明确定义为：<strong>由节点和有向边描述数学运算的有向无环图。</strong>例如</p><ul><li>调用 <code>tf.constant(42.0)</code> 可创建单个 <code>tf.Operation</code>，该操作可以生成值 42.0，将该值添加到默认图中，并返回表示常量值的 <code>tf.Tensor</code>。</li><li>调用 <code>tf.matmul(x, y)</code> 可创建单个 <code>tf.Operation</code>，该操作会将 <code>tf.Tensor</code> 对象 x 和 y 的值相乘，将其添加到默认图中，并返回表示乘法运算结果的 <code>tf.Tensor</code>。</li><li>执行 <code>v = tf.Variable(0)</code> 可向图添加一个 <code>tf.Operation</code>，该操作可以存储一个可写入的张量值，该值在多个 <code>tf.Session.run</code> 调用之间保持恒定。<code>tf.Variable</code> 对象会封装此操作，并可以像张量一样使用，即读取已存储值的当前值。<code>tf.Variable</code> 对象也具有 <code>assign</code> 和 <code>assign_add</code> 等方法，这些方法可创建 <code>tf.Operation</code>对象，这些对象在执行时将更新已存储的值。（请参阅变量了解关于变量的更多信息。）</li><li>调用 <code>tf.train.Optimizer.minimize</code> 可将操作和张量添加到计算梯度的默认图中，并返回一个 <code>tf.Operation</code>，该操作在运行时会将这些梯度应用到一组变量上。</li></ul><p><strong>前向图中的节点</strong>分为三类：</p><ul><li>Operation，主要为数学函数和表达式。比如图中的MatMul，BiasAdd和Softmax，绝大多数节点都属于此类。</li><li>存储模型参数的张量（Variable）：比如图中的$W_{h1}$和$b$。</li><li>占位符(placeholder)：比如图中的Input和Class Labels，通常用来描述输入、输出数据的类型和形状。</li></ul><p><strong>后向图中的节点</strong>同样分为三类</p><ul><li>梯度值：经过前向图计算出的模型参数的梯度，比如图中的<code>Gradients</code></li><li>更新模型参数的操作：比如图中$Update W$和$Update b$，它们定义了如何将梯度值更新到对应的模型参数。</li><li>更新后的模型参数，比如图中<code>SGD Trainer</code>内的<code>W</code>和<code>b</code>，与前向图中的模型参数一一对应，但参数值得到了更新，用于模型的下一轮训练。</li></ul><p><strong>有向边：</strong>定义了节点之间的关系。分为两类：一类用于传输数据，绝大部分<strong>流动着张量的边都是此类</strong>，在图中使用实线表示，简称数据边。还有一种特殊边，一般画为虚线边，称为控制依赖，可以用于控制操作的运算，这被用来确保happens-before关系，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成执行。例如<code>tf.global_variables_initializer()</code>形成的边。</p><h3 id="模型载体：操作（节点）"><a href="#模型载体：操作（节点）" class="headerlink" title="模型载体：操作（节点）"></a>模型载体：操作（节点）</h3><p>数据流图中的节点按照功能的不同可以分为下面三种：</p><ul><li>计算节点（Operation）：对应的是无状态的计算或控制操作，主要负责算法逻辑表达或流程控制</li><li>存储节点（Variable）：对应的是有状态的变量操作，通常用来存储模型参数</li><li>数据节点（Placeholder）：对应的是特殊的占位符操作，用来描述待输入数据的属性</li></ul><blockquote><p>对于无状态节点，其输出由输入张量和节点操作共同确定，没有内部状态，不长期保存任何值。对于有状态节点，如存储节点，其输出还受到节点内部保存的状态值影响。</p></blockquote><p>节点之间连接的边流动着的是张量。对于计算节点，<strong>Operation绝大部分的输入，输出都为张量。对于<code>tf.global_variables_initializer()</code>这种类型为<code>NoOp</code>(无输入输出)的除外。</strong>对于存储节点，需要使用tensor进行初始化，输出是变量，但是可以像张量那样使用。Just like any <code>Tensor</code>, variables created with <code>Variable()</code> can be used as inputs for other Ops in the graph。对于数据节点，可以在<code>session.run()</code>的时候使用numpy进行初始化，但是输出仍然是张量。</p><p>先看几个函数定义<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.constant(value, dtype=<span class="literal">None</span>, shape=<span class="literal">None</span>, name=<span class="string">'Const'</span>, verify_shape=<span class="literal">False</span>) <span class="comment"># 一种Operation</span></span><br><span class="line">tf.Variable(initializer, name)</span><br><span class="line">tf.placeholder(dtype, shape=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p></p><p>由上面的定义可以看出，tensorflow中所有的节点都有<code>name</code>的参数，用于指定该节点的名字，从节点得到的tensor也跟该节点名称相关。tensor的名字全局唯一。</p><h4 id="计算节点：Operation类"><a href="#计算节点：Operation类" class="headerlink" title="计算节点：Operation类"></a>计算节点：Operation类</h4><p>tensorflow是一个符号式编程的框架，首先要定义一个graph，然后用一个session来运行这个graph得到结果。graph就是由一系列Op构成的。上面的<code>tf.constant()，tf.add()，tf.mul()</code>都是op，执行这些函数的时候，tensorflow内部会自动构造相应的Operation实例。凡是Op，都需要通过session运行之后，才能得到结果。<strong>Operation的输入，输出都为张量。但是<code>NoOp</code>除外</strong></p><h5 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h5><ul><li>name：Operation在数据流图中的名称</li><li>type：Operation的类型名称</li><li>inputs：输入张量列表</li><li>control_inputs：输入控制依赖列表</li><li>outputs：输出张量列表</li><li>device：操作执行时使用的设备</li><li>graph：Operation所属的数据流图</li><li>traceback：Operation实例化时的调用栈</li></ul><h5 id="constant"><a href="#constant" class="headerlink" title="constant"></a>constant</h5><p>constant是一种Op，下面对<code>constant</code>在graph中的结点表示进行说明。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_graph</span><span class="params">(g, filename)</span>:</span></span><br><span class="line">    print(filename)</span><br><span class="line">    print(g.as_graph_def())</span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">var = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">dump_graph(g, <span class="string">'after_var_creation.graph'</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">dump_graph(g, <span class="string">'after_initializer_creation.graph'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    dump_graph(g, <span class="string">'after_initializer_run.graph'</span>)</span><br></pre></td></tr></table></figure><p>在执行完<code>var = tf.constant([1, 2, 3, 4, 5, 6, 7])</code>后，图中生成了以下结点：</p><ul><li>Const：用来保存var常量；</li></ul><p>在执行完<code>tf.global_variables_initializer()</code>后，图中结点为：</p><ul><li>Const</li><li>init</li></ul><p>下面我只展示，<code>dump_graph(g, &#39;after_initializer_run.graph&#39;)</code>的输出。</p><p>由输出可以看到，我们可以看到关于常量的类型，形状、具体的值都已经在一个node中包含了。虽然函数<code>global_variables_initializer()</code>的执行在图中添加了一个init的结点，但是没有任何操作。可见，<strong>常量在会话中是不需要进行所谓初始化的</strong>。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">node &#123;</span><br><span class="line">  name: "Const"</span><br><span class="line">  op: "Const"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "value"</span><br><span class="line">    value &#123;</span><br><span class="line">      tensor &#123;</span><br><span class="line">        dtype: DT_INT32</span><br><span class="line">        tensor_shape &#123;</span><br><span class="line">          dim &#123;</span><br><span class="line">            size: 7</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        tensor_content: "\001\000\000\000\002\000\000\000\003\000\000\000\004\000\000\000\005\000\000\000\006\000\000\000\007\000\000\000"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "init"</span><br><span class="line">  op: "NoOp"</span><br><span class="line">&#125;</span><br><span class="line">versions &#123;</span><br><span class="line">  producer: 22</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="存储节点：Variables"><a href="#存储节点：Variables" class="headerlink" title="存储节点：Variables"></a>存储节点：Variables</h4><p><strong>变量也就是参数，例如CNN中的权重、卷积核、偏置等，当训练网络的时候，变量会自动更新。</strong></p><p>很多人会以为<code>tf.Variable()</code>也是op，其实不是的。tensorflow里，<strong>首字母大写的类，首字母小写的才是op</strong>。<code>tf.Variable()</code>就是一个类，不过它包含了各种op，比如你定义了<code>x = tf.Variable([2, 3], name = &#39;vector&#39;)</code>，那么x就具有如下op：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.initializer <span class="comment"># 对x做初始化，即赋值为初始值[2, 3]</span></span><br><span class="line">x.value() <span class="comment"># 获取x的值</span></span><br><span class="line">x.assign(...) <span class="comment"># 赋值操作</span></span><br><span class="line">x.assign_add(...) <span class="comment"># 加法操作</span></span><br></pre></td></tr></table></figure><p></p><p><code>tf.Variable()</code>必须先初始化，再做运算，否则会报错。下面的写法就不是很安全，容易导致错误：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.truncated_normal([<span class="number">700</span>, <span class="number">10</span>]))</span><br><span class="line">U = tf.Variable(<span class="number">2</span> * W)</span><br></pre></td></tr></table></figure><p></p><p>要把W赋值给U，必须现把W初始化。但很多人往往忘记初始化，从而出错。保险起见，应该按照下面这样写：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.truncated_normal([<span class="number">700</span>, <span class="number">10</span>]))</span><br><span class="line">U = tf.Variable(<span class="number">2</span> * W.intialized_value())</span><br></pre></td></tr></table></figure><p></p><h5 id="变量属性"><a href="#变量属性" class="headerlink" title="变量属性"></a>变量属性</h5><ul><li>name：变量在数据流图中的名称</li><li>dtype：变量的数据类型</li><li>shape：变量的形状</li><li>initial_value：变量的初始值</li><li>initializer：计算前为变量赋值的初始化操作</li><li>device：存储变量的设备</li><li>graph：变量所属的数据流图</li><li>op：变量操作</li></ul><p>这里有两种方法建立变量。下面分别进行介绍。</p><h5 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable()"></a>tf.Variable()</h5><p>tf.Variable()类的初始化函数参数<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initial_value=<span class="literal">None</span>, trainable=<span class="literal">True</span>, collections=<span class="literal">None</span>, validate_shape=<span class="literal">True</span>, caching_device=<span class="literal">None</span>, name=<span class="literal">None</span>, variable_def=<span class="literal">None</span>, dtype=<span class="literal">None</span>, expected_shape=<span class="literal">None</span>, import_scope=<span class="literal">None</span>, constraint=<span class="literal">None</span></span><br></pre></td></tr></table></figure><p></p><p>其中，<code>trainable</code>参数为False的时候，不训练该参数。而其中的<code>name</code>为可选项。</p><p>tensorflow中随机数生成函数主要有：</p><ul><li>tf.random_normal: 从正态分布中输出随机值</li><li>tf.random_uniform: 从均匀分布中返回随机值</li><li>tf.truncated_normal: 截断的正态分布函数。生成的值遵循一个正态分布，但不会大于平均值2个标准差。</li><li>tf.random_shuffle: 沿着要被洗牌的张量的第一个维度，随机打乱。</li></ul><p><code>initial_value</code>参数可以嵌套使用tensorflow的随机数生成函数。</p><p>例如<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">var = tf.Variable(<span class="number">1</span>)</span><br><span class="line">var1 = tf.Variable(<span class="number">2</span>)</span><br><span class="line">var2 = tf.Variable(<span class="number">3</span>, name=<span class="string">'foo'</span>)</span><br><span class="line">var3 = tf.Variable(<span class="number">4</span>, name=<span class="string">'foo'</span>)</span><br><span class="line"><span class="comment"># seed 设计随机种子，保证每次运行结果相同</span></span><br><span class="line">var4 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p></p><p>可以使用相同的<code>name</code>定义不同的变量，tensorflow会自动检测，若变量名相同，会被重命名。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var <span class="keyword">is</span> named <span class="keyword">as</span> <span class="string">"Variable"</span>, var1 <span class="keyword">is</span> named <span class="keyword">as</span> <span class="string">"Variable_1"</span>,var2 <span class="keyword">is</span> named <span class="keyword">as</span> <span class="string">"foo"</span>,var3 <span class="keyword">is</span> named <span class="keyword">as</span> <span class="string">"foo_1"</span> <span class="keyword">in</span> Tensorflow.</span><br></pre></td></tr></table></figure><h5 id="tf-get-variable"><a href="#tf-get-variable" class="headerlink" title="tf.get_variable()"></a>tf.get_variable()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name,</span><br><span class="line">    shape=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    initializer=<span class="literal">None</span>,</span><br><span class="line">    regularizer=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">None</span>,</span><br><span class="line">    collections=<span class="literal">None</span>,</span><br><span class="line">    caching_device=<span class="literal">None</span>,</span><br><span class="line">    partitioner=<span class="literal">None</span>,</span><br><span class="line">    validate_shape=<span class="literal">True</span>,</span><br><span class="line">    use_resource=<span class="literal">None</span>,</span><br><span class="line">    custom_getter=<span class="literal">None</span>,</span><br><span class="line">    constraint=<span class="literal">None</span>,</span><br><span class="line">    synchronization=tf.VariableSynchronization.AUTO,</span><br><span class="line">    aggregation=tf.VariableAggregation.NONE</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>在<code>tf.Variable()</code>中，<code>name</code>参数为可选项，而在<code>tf.get_variable()</code>中，<code>name</code>函数为必填项，因为<code>tf.get_variable()</code>根据这个名字<strong>创建或者获取</strong>变量，避免无意识的变量复用而造成的错误。</p><p><code>initializer</code>几个常用函数如下：</p><ul><li>tf.constant_initializer()：初始化为常数，这个非常有用，通常偏置项就是用它初始化的。</li><li>tf.truncated_normal_initializer()：生成截断正态分布的随机数</li><li>tf.random_normal_initializer()：生成标准正态分布的随机数</li><li>tf.RandomUniform()：生成均匀分布的随机数</li></ul><p>例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">var = tf.get_variable(<span class="string">'var'</span>, [<span class="number">1</span>]) <span class="comment"># 第二个参数为shape</span></span><br><span class="line">var1 = tf.get_variable(<span class="string">'var1'</span>, [<span class="number">1</span>]，initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br></pre></td></tr></table></figure><p></p><h5 id="tf-name-scope-和tf-variable-scope"><a href="#tf-name-scope-和tf-variable-scope" class="headerlink" title="tf.name_scope()和tf.variable_scope()"></a>tf.name_scope()和tf.variable_scope()</h5><p>首先，看一下tf.variable_scope()的定义<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.variable_scope(name_or_scope,default_name=<span class="literal">None</span>,values=<span class="literal">None</span>,initializer=<span class="literal">None</span>,regularizer=<span class="literal">None</span>,caching_device=<span class="literal">None</span>,partitioner=<span class="literal">None</span>,custom_getter=<span class="literal">None</span>,reuse=<span class="literal">None</span>,dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p></p><p>将参数设置<code>reuse=True</code>，这样<code>tf.get_variable()</code>就可以直接获取已经声明的变量（而且只能获得已经声明的变量，若变量未声明，则不会产生变量，进而报错）。而<code>reuse=False</code>，则<code>tf.get_variable()</code>将创建新的变量，若name属性相同的变量已经存在，会报错。对于嵌套的<code>tf.variable_scope()</code>，<strong>若不指定reuse参数，则默认与外面最近的一层保持一致</strong>。</p><p>对于<code>tf.name_scope()</code>定义为<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.name_scope(</span><br><span class="line">    name,</span><br><span class="line">    default_name=<span class="literal">None</span>,</span><br><span class="line">    values=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p>可以看出来，<code>tf.name_scope()</code>没有<code>reuse</code>参数，而且这个<strong>函数主要使用tensorboard可视化计算图的时候</strong></p><p><code>tf.get_variable()</code> is a high level version of<code>tf.Varible()</code>.区别主要为：</p><p><strong>在scope中，<code>tf.name_scope()</code>不会影响到<code>tf.get_variable()</code>，<code>tf.variable_scope()</code>会影响到所有Op和变量。</strong></p><p>例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scoping</span><span class="params">(fn, scope1, scope2, vals)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> fn(scope1):</span><br><span class="line">        a = tf.Variable(vals[<span class="number">0</span>], name=<span class="string">'a'</span>)</span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, initializer=vals[<span class="number">1</span>])</span><br><span class="line">        c = tf.constant(vals[<span class="number">2</span>], name=<span class="string">'c'</span>)</span><br><span class="line">        <span class="keyword">with</span> fn(scope2):</span><br><span class="line">            d = tf.add(a * b, c, name=<span class="string">'res'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'\n  '</span>.join([scope1, a.name, b.name, c.name, d.name]), <span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">d1 = scoping(tf.variable_scope, <span class="string">'scope_vars'</span>, <span class="string">'res'</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">d2 = scoping(tf.name_scope,     <span class="string">'scope_name'</span>, <span class="string">'res'</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">'logs'</span>, sess.graph)</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">print</span> sess.run([d1, d2])</span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure><p></p><p>We define a function named scoping() and input different paramaters to see output:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scope_vars</span><br><span class="line">  scope_vars/a:<span class="number">0</span></span><br><span class="line">  scope_vars/b:<span class="number">0</span></span><br><span class="line">  scope_vars/c:<span class="number">0</span></span><br><span class="line">  scope_vars/res/res:<span class="number">0</span></span><br><span class="line"></span><br><span class="line">scope_name</span><br><span class="line">  scope_name/a:<span class="number">0</span></span><br><span class="line">  b:<span class="number">0</span></span><br><span class="line">  scope_name/c:<span class="number">0</span></span><br><span class="line">  scope_name/res/res:<span class="number">0</span></span><br></pre></td></tr></table></figure><p></p><p>另外，关于两者的区别</p><ul><li>We should use tf.get_variable()(but not tf.Variable()) to create variables that we reuse in next lines.</li><li>If we create a variable by tf.variable(),we can not reuse it by tf.get_variable().</li><li>We should usetf.variable_scope() to manage variables that we reuse in next lines.</li></ul><p>？？？值得注意的是，使用<code>tf.get_variable()</code>的时候，trainable的属性也是可以继承的，比如说：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'test'</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    a = tf.get_variable(<span class="string">'weights'</span>, [<span class="number">1</span>], trainable=<span class="literal">False</span>)</span><br><span class="line">    b = tf.get_variable(<span class="string">'weights'</span>, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">variable_names = [v.name <span class="keyword">for</span> v <span class="keyword">in</span> tf.trainable_variables()]</span><br><span class="line">values = sess.run(variable_names)</span><br></pre></td></tr></table></figure><p></p><p>则<code>tf.trainable_variables()</code>为空。</p><h5 id="Variables在图中的结构"><a href="#Variables在图中的结构" class="headerlink" title="Variables在图中的结构"></a>Variables在图中的结构</h5><p>作为存储节点的变量不是一个简单的节点，而是一幅由多个子节点组成的子图。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_graph</span><span class="params">(g, filename)</span>:</span></span><br><span class="line">    print(filename)</span><br><span class="line">    print(g.as_graph_def())</span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">var = tf.Variable(<span class="number">2</span>) <span class="comment"># 初始值</span></span><br><span class="line">dump_graph(g, <span class="string">'after_var_creation.graph'</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">dump_graph(g, <span class="string">'after_initializer_creation.graph'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    dump_graph(g, <span class="string">'after_initializer_run.graph'</span>)</span><br></pre></td></tr></table></figure><p>在执行完tf.Variable(2)以后，图中生成了以下几个结点：</p><ul><li><strong>Variable/initial_value（变量初始值）</strong>：the stateful TensorFlow op that owns the memory for the variable. Every time you run that op, it emits the buffer(as a “ref tensor”) so that other ops can read or write it.</li><li><strong>Variable/Assign（更新变量值的操作）</strong>： the initializer operation that writes the initial value into the variable’s memory. It is typically run once, when you do sess.run(tf.global_variables_initializer()) in your program.</li><li><strong>Variable/read（读取变量值的操作）</strong>：an operation that “dereferences” the Variable op’s “ref tensor” output.</li><li><strong>Variable/（变量操作）</strong>：the tensor that you provided as the initial_value argument of the tf.Variable constructor.</li></ul><p>在上述节点中，前三种节点对应的是无状态操作，而变量操作节点对应的是有状态操作。在tensorboard中，在渲染数据流图时，<strong>为有状态操作添加了一对括号</strong>。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">after_initializer_creation.graph</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable/initial_value"</span><br><span class="line">  op: "Const"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "value"</span><br><span class="line">    value &#123;</span><br><span class="line">      tensor &#123;</span><br><span class="line">        dtype: DT_INT32</span><br><span class="line">        tensor_shape &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        int_val: 2</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable"</span><br><span class="line">  op: "VariableV2"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "container"</span><br><span class="line">    value &#123;</span><br><span class="line">      s: ""</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "shape"</span><br><span class="line">    value &#123;</span><br><span class="line">      shape &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "shared_name"</span><br><span class="line">    value &#123;</span><br><span class="line">      s: ""</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable/Assign"</span><br><span class="line">  op: "Assign"</span><br><span class="line">  input: "Variable"</span><br><span class="line">  input: "Variable/initial_value"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "T"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "_class"</span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        s: "loc:@Variable"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "use_locking"</span><br><span class="line">    value &#123;</span><br><span class="line">      b: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "validate_shape"</span><br><span class="line">    value &#123;</span><br><span class="line">      b: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable/read"</span><br><span class="line">  op: "Identity"</span><br><span class="line">  input: "Variable"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "T"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "_class"</span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        s: "loc:@Variable"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">versions &#123;</span><br><span class="line">  producer: 26</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着执行完<code>tf.global_variables_initializer()</code>后，图中结点变为：</p><ul><li>Variable/initial_value</li><li>Variable</li><li>Variable/Assign</li><li>Variable/read</li><li>init ： 图中变量初始化的作用</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">after_initializer_run.graph</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable/initial_value"</span><br><span class="line">  op: "Const"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "value"</span><br><span class="line">    value &#123;</span><br><span class="line">      tensor &#123;</span><br><span class="line">        dtype: DT_INT32</span><br><span class="line">        tensor_shape &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        int_val: 2</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable"</span><br><span class="line">  op: "VariableV2"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "container"</span><br><span class="line">    value &#123;</span><br><span class="line">      s: ""</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "shape"</span><br><span class="line">    value &#123;</span><br><span class="line">      shape &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "shared_name"</span><br><span class="line">    value &#123;</span><br><span class="line">      s: ""</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable/Assign"</span><br><span class="line">  op: "Assign"</span><br><span class="line">  input: "Variable"</span><br><span class="line">  input: "Variable/initial_value"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "T"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "_class"</span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        s: "loc:@Variable"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "use_locking"</span><br><span class="line">    value &#123;</span><br><span class="line">      b: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "validate_shape"</span><br><span class="line">    value &#123;</span><br><span class="line">      b: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Variable/read"</span><br><span class="line">  op: "Identity"</span><br><span class="line">  input: "Variable"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "T"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT32</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "_class"</span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        s: "loc:@Variable"</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "init"</span><br><span class="line">  op: "NoOp"</span><br><span class="line">  input: "^Variable/Assign"</span><br><span class="line">&#125;</span><br><span class="line">versions &#123;</span><br><span class="line">  producer: 26</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意<code>after_initializer_run</code>与<code>after_initializer_creation</code>输出是一样的。也就是说，执行到<code>init = tf.global_variables_initializer()</code>的时候，图的结构已经固定了。<strong>但是如果要使用变量，必须放到回话中，执行<code>sess.run(init)</code>。此时，<code>inti</code>节点会将<code>init_value</code>传入<code>Assign</code>子节点。否则数据没有流动起来。</strong></p><h5 id="打印所有需要训练的变量"><a href="#打印所有需要训练的变量" class="headerlink" title="打印所有需要训练的变量"></a>打印所有需要训练的变量</h5><p>一般来说，打印tensorflow变量的函数有两个：tf.trainable_variables () 和 tf.all_variables()</p><p>不同的是：tf.trainable_variables () 指的是需要训练的变量。tf.all_variables() 指的是所有变量</p><p>一般而言，我们更关注需要训练的训练变量：</p><p><strong>值得注意的是，在输出变量名时，要对整个graph进行初始化</strong></p><p><strong>打印需要训练的变量名称</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">variable_names = [v.name <span class="keyword">for</span> v <span class="keyword">in</span> tf.trainable_variables()]</span><br><span class="line">print(variable_names)</span><br></pre></td></tr></table></figure><p></p><p><strong>打印需要训练的变量名称和变量值</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">variable_names = [v.name <span class="keyword">for</span> v <span class="keyword">in</span> tf.trainable_variables()]</span><br><span class="line">values = sess.run(variable_names)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> zip(variable_names, values):</span><br><span class="line">    print(<span class="string">"Variable: "</span>, k)</span><br><span class="line">    print(<span class="string">"Shape: "</span>, v.shape)</span><br><span class="line">    print(v)</span><br></pre></td></tr></table></figure><p></p><p><strong>这里提供一个函数，打印变量名称，shape及其变量数目</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_num_of_total_parameters</span><span class="params">(output_detail=False, output_to_logging=False)</span>:</span></span><br><span class="line">    total_parameters = <span class="number">0</span></span><br><span class="line">    parameters_string = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> variable <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line"></span><br><span class="line">        shape = variable.get_shape()</span><br><span class="line">        variable_parameters = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> dim <span class="keyword">in</span> shape:</span><br><span class="line">            variable_parameters *= dim.value</span><br><span class="line">        total_parameters += variable_parameters</span><br><span class="line">        <span class="keyword">if</span> len(shape) == <span class="number">1</span>:</span><br><span class="line">            parameters_string += (<span class="string">"%s %d, "</span> % (variable.name, variable_parameters))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            parameters_string += (<span class="string">"%s %s=%d, "</span> % (variable.name, str(shape), variable_parameters))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> output_to_logging:</span><br><span class="line">        <span class="keyword">if</span> output_detail:</span><br><span class="line">            logging.info(parameters_string)</span><br><span class="line">        logging.info(<span class="string">"Total %d variables, %s params"</span> % (len(tf.trainable_variables()), <span class="string">"&#123;:,&#125;"</span>.format(total_parameters)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> output_detail:</span><br><span class="line">            print(parameters_string)</span><br><span class="line">        print(<span class="string">"Total %d variables, %s params"</span> % (len(tf.trainable_variables()), <span class="string">"&#123;:,&#125;"</span>.format(total_parameters)))</span><br></pre></td></tr></table></figure><p></p><h5 id="变量按照下标索引更新"><a href="#变量按照下标索引更新" class="headerlink" title="变量按照下标索引更新"></a>变量按照下标索引更新</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    a = tf.Variable(initial_value=[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">    b = tf.scatter_update(a, [<span class="number">0</span>, <span class="number">1</span>], [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>]])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(a))</span><br><span class="line">    print(sess.run(b))</span><br></pre></td></tr></table></figure><p>输出:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">[[<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">4</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure><p></p><p>同样的，若想只更新某些维度<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    a = tf.Variable(initial_value=[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">    b = tf.scatter_update(a, [<span class="number">1</span>], [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>]])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(a))</span><br><span class="line">    print(sess.run(b))</span><br></pre></td></tr></table></figure><p></p><p>运行结果<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">4</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure><p></p><h5 id="变量操作"><a href="#变量操作" class="headerlink" title="变量操作"></a>变量操作</h5><p>在上面我们已经介绍了变量由四个子节点组成，那么变量操作子节点和变量有啥区别呢，这里做个简单的介绍。</p><p>变量操作是TensorFlow中一类有状态操作，用于存储变量的值。变量操作对应的操作函数是<code>tensorflow/python/ops/state_ops.py</code>文件中定义的<code>variable_op_v2</code>。构造变量操作的时候，需要给定其存储变量的形状与数据类型。<strong>每个变量对应的变量操作对象在变量初始化时构造。</strong></p><p>例如<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(<span class="number">1.0</span>, name = <span class="string">'a'</span>)</span><br></pre></td></tr></table></figure><p></p><p>在tensorboard中变量操作节点的名字为<code>Variable/(a)</code>，实际上这里的(a)节点是变量 <code>a</code> 的私有成员 <code>_variable</code>，即变量 <code>a</code>的变量操作。<strong>（a）节点内部存储变量a的值，当用户想要读取或更改a的值时，均需要经过read或Assign节点。</strong></p><p>其中，<code>Assign</code>为为更改变量的节点。由此可得<code>tf.assign(ref, value, validate_shape = None, use_locking = None, name = None)</code>通过将 “value” 赋给 “ref” 来更新 “ref”。</p><ul><li>ref：一个可变的张量.应该来自变量节点.节点可能未初始化.</li><li>value：张量.必须具有与 ref 相同的类型.是要分配给变量的值.</li><li>validate_shape：当为False的时候，ref和value的维度可以不相同。</li></ul><h5 id="变量恢复与存储"><a href="#变量恢复与存储" class="headerlink" title="变量恢复与存储"></a>变量恢复与存储</h5><p><strong>Saving A Model:</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.get_variable(<span class="string">'a'</span>, [])</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>, [])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line">saver.save(sess, <span class="string">'./tftcp.model'</span>)</span><br></pre></td></tr></table></figure><p></p><p>产生如下文件：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint</span><br><span class="line">tftcp.model.data<span class="number">-00000</span>-of<span class="number">-00001</span></span><br><span class="line">tftcp.model.index</span><br><span class="line">tftcp.model.meta</span><br></pre></td></tr></table></figure><p></p><ul><li>tftcp.model.data-00000-of-00001：存储网络模型</li><li>tftcp.model.meta：网络的图结构，存储了重建图所需的所有信息。</li><li>tftcp.model.index：连接上面的两个文件，从模型中读取到图节点对应的值。（is an indexing structure linking the first two things. It says “where in the data file do I find the parameters corresponding to this node?”）</li><li>checkpoint：重建图的时候并不需要，只是在训练过程中存储了网络的多个模型，并跟踪最近的模型。</li></ul><p><strong>Loading a Model：</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.get_variable(<span class="string">'a'</span>, [])</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>, [])</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">saver.restore(sess, <span class="string">'./tftcp.model'</span>)</span><br><span class="line">sess.run([a,b])</span><br></pre></td></tr></table></figure><p></p><h4 id="数据节点：placeholder"><a href="#数据节点：placeholder" class="headerlink" title="数据节点：placeholder"></a>数据节点：placeholder</h4><p>placeholder，翻译过来就是占位符。其实它类似于函数里的自变量。比如z = x + y，那么x和y就可以定义成占位符。占位符，顾名思义，就这是占一个位子，平时不用关心它们的值，当你做运算的时候，你再把你的数据灌进去就行了。是不是和自变量很像？看下面的代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.placeholder(tf.float32, shape=[<span class="number">3</span>]) <span class="comment"># a是一个3维向量</span></span><br><span class="line">b = tf.constant([<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], tf.float32)</span><br><span class="line">c = a + b</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> sess.run(c, feed_dict = &#123;a: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;) <span class="comment"># 把[1, 2, 3]灌到a里去</span></span><br></pre></td></tr></table></figure><p></p><p>输出结果是<code>[6, 7, 8]</code>。上面代码中出现了feed_dict的概念，其实就是用<code>[1, 2, 3]</code>代替<code>a</code>的意思。相当于在本轮计算中，自变量a的取值为<code>[1, 2, 3]</code>。其实不仅仅是<code>tf.placeholder</code>才可以用feed_dict，很多op都可以。只要<code>tf.Graph.is_feedable(tensor)</code>返回值是True，那么这个tensor就可用用feed_dict来灌入数据。</p><p><code>tf.constant()</code>是直接定义在graph里的，它是graph的一部分，会随着graph一起加载。如果通过<code>tf.constant()</code>定义了一个维度很高的张量，那么graph占用的内存就会变大，加载也会变慢。而<code>tf.placeholder</code>就没有这个问题，所以如果数据维度很高的话，定义成tf.placeholder是更好的选择。</p><h5 id="placeholder属性"><a href="#placeholder属性" class="headerlink" title="placeholder属性"></a>placeholder属性</h5><ul><li>name：占位符操作在数据流图中的名称</li><li>dtype：填充数据的类型</li><li>shape：填充数据的形状</li></ul><h5 id="placeholder在图中的结构"><a href="#placeholder在图中的结构" class="headerlink" title="placeholder在图中的结构"></a>placeholder在图中的结构</h5><p>下面对<code>placeholder</code>在graph中的结点表示进行说明。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_graph</span><span class="params">(g, filename)</span>:</span></span><br><span class="line">    print(filename)</span><br><span class="line">    print(g.as_graph_def())</span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">var = tf.placeholder(tf.float32, <span class="literal">None</span>)</span><br><span class="line">dump_graph(g, <span class="string">'after_var_creation.graph'</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">dump_graph(g, <span class="string">'after_initializer_creation.graph'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    dump_graph(g, <span class="string">'after_initializer_run.graph'</span>)</span><br></pre></td></tr></table></figure><p></p><p>在执行完placeholder(tf.float32, None)后，图中生成了一个结点：</p><ul><li>Placeholder</li></ul><p>输出如下：可见关于Placeholder的所有信息都在其结点中表示出来了。虽然函数<code>global_variables_initializer()</code>的执行在图中添加了一个init的结点，但是没有任何操作。可见，<strong>Placeholder在会话中是不需要进行所谓初始化的</strong>。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">after_var_creation.graph</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Placeholder"</span><br><span class="line">  op: "Placeholder"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_FLOAT</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "shape"</span><br><span class="line">    value &#123;</span><br><span class="line">      shape &#123;</span><br><span class="line">        unknown_rank: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">versions &#123;</span><br><span class="line">  producer: 26</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">after_initializer_creation.graph</span><br><span class="line">node &#123;</span><br><span class="line">  name: "Placeholder"</span><br><span class="line">  op: "Placeholder"</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "dtype"</span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_FLOAT</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: "shape"</span><br><span class="line">    value &#123;</span><br><span class="line">      shape &#123;</span><br><span class="line">        unknown_rank: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">node &#123;</span><br><span class="line">  name: "init"</span><br><span class="line">  op: "NoOp"</span><br><span class="line">&#125;</span><br><span class="line">versions &#123;</span><br><span class="line">  producer: 26</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>变量需要在session中执行初始化语句才能使用，而constant和placeholder不需要。</p><p>例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(<span class="string">'tf.global_variables_initializer():'</span>, tf.global_variables_initializer().name, tf.global_variables_initializer().type)</span><br><span class="line"></span><br><span class="line">a = tf.global_variables_initializer()</span><br><span class="line">print(<span class="string">'a:'</span>, a.name, a.type)</span><br><span class="line"></span><br><span class="line">b = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">2.0</span>, name  =  <span class="string">'c'</span>)</span><br><span class="line">d = tf.add(b, c, name  =  <span class="string">'d'</span>)</span><br><span class="line">e = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line">f = tf.placeholder(tf.float32, shape=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'b:'</span>, b, b.name) <span class="comment"># b,type AttributeError: 'Tensor' object has no attribute 'type'</span></span><br><span class="line">print(<span class="string">'c:'</span>, c, c.name)</span><br><span class="line">print(<span class="string">'d:'</span>, d, d.name, d.shape)</span><br><span class="line">print(<span class="string">'e:'</span>, e, e.name, e.shape)</span><br><span class="line">print(<span class="string">'f:'</span>, f, f.name, f.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'tf.constant(1.0):'</span>, tf.constant(<span class="number">1.0</span>), tf.constant(<span class="number">1.0</span>).name) <span class="comment"># tf.constant(1.0).type</span></span><br><span class="line">print(<span class="string">'tf.add(b,c):'</span>, tf.add(b,c), tf.add(b,c, name=<span class="string">"d"</span>).name) <span class="comment"># tf.add(b,c).type</span></span><br></pre></td></tr></table></figure><p></p><p>输出结果<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.global_variables_initializer(): init NoOp</span><br><span class="line">a: init_2 NoOp</span><br><span class="line">b: Tensor(<span class="string">"Const:0"</span>, shape=(), dtype=float32) Const:<span class="number">0</span></span><br><span class="line">c: Tensor(<span class="string">"c:0"</span>, shape=(), dtype=float32) c:<span class="number">0</span></span><br><span class="line">d: Tensor(<span class="string">"d:0"</span>, shape=(), dtype=float32) d:<span class="number">0</span> ()</span><br><span class="line">e: &lt;tf.Variable <span class="string">'Variable:0'</span> shape=() dtype=float32_ref&gt; Variable:<span class="number">0</span> ()</span><br><span class="line">f: Tensor(<span class="string">"Placeholder:0"</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32) Placeholder:<span class="number">0</span> (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">tf.constant(<span class="number">1.0</span>): Tensor(<span class="string">"Const_1:0"</span>, shape=(), dtype=float32) Const_2:<span class="number">0</span></span><br><span class="line">tf.add(b,c): Tensor(<span class="string">"Add:0"</span>, shape=(), dtype=float32) d_1:<span class="number">0</span></span><br></pre></td></tr></table></figure><p></p><p><code>tf.global_variables_initializer()</code>返回的是一个Op，所以有type属性，而<code>tf.constant</code>和<code>tf.add</code>返回的都是tensor，所以没有type属性。如果想查看<code>constant</code>和<code>add</code>等Operation的属性，需要使用tensorboard。不能直接打印出来。</p><p>然后，可以看出，<code>tf.global_variables_initializer()</code>返回的Op类型为<code>NoOp</code>,即不存在输入，也不存在输出。所有变量的初始化器通过控制依赖边与该NoOp相连，保证所有的全局变量被初始化。如下所示：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/f5b848b0-48f9-4421-8dc8-f142890bbd35.jpg" alt></p><p>另外，从结果可以看出，无论是否有一个左值接受<code>tf.global_variables_initializer()</code>和<code>tf.add()</code>的结果，都不影响打印出来的结果。</p><p>从结果可以看出，对于计算节点，<strong>Operation的输入，输出都为张量。</strong>对于存储节点，需要使用tensor进行初始化，输出是变量，但是可以像张量那样使用。Just like any <code>Tensor</code>, variables created with <code>Variable()</code> can be used as inputs for other Ops in the graph。对于数据节点，可以在<code>session.run()</code>的时候使用numpy进行初始化，但是输出仍然是张量。</p><p>最后，可以看出如果对于Operation没有手动命名，tensorflow会自动命名，并且以<code>Const:0</code>和<code>Const_1:0</code>区分不同的变量，而手动命名若重复，也会以相同的规则重命名。</p><p><strong>张量和计算图上的每一个节点所有代表的结果是对应的。张量的命名：<code>node:src_output</code>。其中<code>node</code>为节点的名称，<code>src_output</code>表示当前张量来自节点的第几个输出。</strong>即上述<code>Const:0</code>即该张量来自名字为<code>Const</code>节点的第0个输出。</p><p>对于<code>tf.constant</code>为操作（节点），在tensorflow的图中为一个节点。而由<code>tf.constant</code>得到的变量为<code>tensor</code>，在tensorflow的图中体现为边上的数据。</p><h3 id="数据载体：张量（数据边）"><a href="#数据载体：张量（数据边）" class="headerlink" title="数据载体：张量（数据边）"></a>数据载体：张量（数据边）</h3><p>对于tensorflow的数据边，绝大部分流动着张量。</p><blockquote><p>控制边这里不讨论</p></blockquote><p>张量是数据留图上的数据载体，但在物理实现上是一个句柄，他存储张量的元信息以及指向张量数据的内存缓冲区指针。</p><p>tensorflow张量除了支持常用的浮点数、整数、字符串、布尔型，还支持复数和量化整数类型。</p><h4 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h4><ul><li>dtype：张量传输数据的类型</li><li>name：张量在数据流图中的名称</li><li>graph：张量所属的数据流图</li><li>op：生成该张量的前置操作</li><li>shape：张量传输数据的形状</li><li>value_index：张量在该前置操作所有输出值中的索引</li></ul><p>其中，name属性</p><ul><li>张量的唯一标识符；</li><li>给出了张量是如何计算出来的。</li></ul><p>计算图中的每一个节点都表示一个运算，而张量则将节点运算结果的属性保存下来。张量和计算图上的每一个节点所有代表的结果是对应的。张量的命名：<code>node:src_output</code>。其中<code>node</code>为节点的名称，<code>src_output</code>表示当前张量来自节点的第几个输出。如<code>Const:0</code>即该张量来自名字为<code>Const</code>节点的第0个输出。</p><h4 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h4><p>可以使用Tensor类构造方法，不过一般情况下不需要使用Tensor类，而是通过操作间接创建张量，典型的张量创建包括<strong>常量定义操作和代数计算操作</strong>。</p><p>例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">c = tf.add(a,b)</span><br><span class="line"></span><br><span class="line">print([a,b,c])</span><br></pre></td></tr></table></figure><p></p><p>输出为：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Tensor <span class="string">'Const:0'</span> shape=() dtype=float32&gt;,</span><br><span class="line">&lt;tf.Tensor <span class="string">'Const_1:0'</span> shape=() dtype=float32&gt;,</span><br><span class="line">&lt;tf.Tensor <span class="string">'Add:0'</span> shape=() dtype=float32&gt;]</span><br></pre></td></tr></table></figure><p></p><h4 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h4><p>需要在会话中执行张量的eval方法或者会话的run方法。若不在会话中执行eval()，会报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(c.eval())</span><br><span class="line">    print(sess.run([a,b,c]))</span><br></pre></td></tr></table></figure><p>输出结果为:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.0</span></span><br><span class="line">[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br></pre></td></tr></table></figure><p></p><h4 id="成员方法"><a href="#成员方法" class="headerlink" title="成员方法"></a>成员方法</h4><ul><li>eval：取出张量值</li><li>get_shape：获取张量的形状</li><li>set_shape：修改张量的形状</li><li>consumers：获取张量的后面操作</li></ul><h3 id="变量与张量的关系"><a href="#变量与张量的关系" class="headerlink" title="变量与张量的关系"></a>变量与张量的关系</h3><p>变量可以通过<code>read</code>子节点得到张量，也可以使用<code>assign</code>节点将一个变量的张量赋值给另外一个节点的张量。</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">  img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>以上代码中定义了一个计算图，在该计算图中定义了一个常量。<strong>Tensorflow默认会创建一张计算图。所以上面代码中的前两行，可以省略。默认情况下，计算图是空的</strong>。</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/251.jpg" alt></p><p>在执行完<code>img = tf.constant(1.0, shape=[1,5,5,3])</code>以后，计算图中生成了一个node，<strong>一个node结点由name, op, input, attrs组成，即结点名称、操作、输入以及一系列的属性（类型、形状、值）等组成</strong>，计算图就是由这样一个个的node组成的。对于<code>tf.constant()</code>函数，只会生成一个node，但对于有的函数，如<code>tf.Variable(initializer, name)</code>（注意其第一个参数是初始化器）就会生成多个node结点（后面会讲到）。</p><p>那么执行完<code>img = tf.constant(1.0, shape=[1,5,5,3])</code>后，计算图中就多一个node结点。（<strong>因为每个node的属性很多，我只表示name，op，input属性</strong>）</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/252.jpg" alt></p><p>继续添加代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">k = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p></p><p>代码执行后的计算图如下：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/253.jpg" alt></p><p>需要注意的是，<strong>如果没有对结点进行命名，Tensorflow自动会将其命名为：Const、Const_1、const_2</strong>……。其他类型的结点类同。</p><p>现在，我们添加一个变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">k = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">kernel = tf.Variable(k)</span><br></pre></td></tr></table></figure><p>该变量用<strong>一个常量作为初始化器</strong>。我们先看一下计算图：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/254.jpg" alt></p><p>如图所示：<br>执行完tf.Variable()函数后，一共产生了三个结点：</p><ul><li>Variable：变量维护（不存放实际的值）</li><li>Variable/Assign：变量分配</li><li>Variable/read：变量使用</li></ul><p>图中只是完成了操作的定义，但并没有执行操作（如Variable/Assign结点的Assign操作，所以，<strong>此时候变量依然不可以使用，这就是为什么要在会话中初始化的原因</strong>）。</p><p>我们继续添加代码：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">k = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">kernel = tf.Variable(k)</span><br><span class="line">y = tf.nn.conv2d(img, kernel, strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br></pre></td></tr></table></figure><p></p><p>得到的计算图如下：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/255.jpg" alt></p><p>可以看出，<strong>变量读取是通过<code>Variable/read</code>来进行的。</strong></p><p>如果在这里我们直接开启会话，并执行计算图中的卷积操作，系统就会报错。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">k = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">kernel = tf.Variable(k)</span><br><span class="line">y2 = tf.nn.conv2d(img, kernel, strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(y2)</span><br></pre></td></tr></table></figure><p></p><p>这段代码错误的原因在于，变量并没有初始化就被使用，而从图中清晰的可以看到，直接执行卷积，是回溯不到变量的值（Const_1）的（箭头方向）。</p><p>所以，在执行之前，要进行初始化，代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">k = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">kernel = tf.Variable(k)</span><br><span class="line">y2 = tf.nn.conv2d(img, kernel, strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><p></p><p>执行完<code>tf.global_variables_initializer()</code>函数以后，计算图如下：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/256.jpg" alt></p><p><code>tf.global_variables_initializer()</code>产生了一个名为init的node，<strong>该结点将所有的Variable/Assign结点作为输入，将initial_value传入Assign子节点，以达到对整张计算图中的变量进行初始化。</strong></p><p>所以，在开启会话后，执行的第一步操作，就是变量初始化(当然变量初始化的方式有很多种，我们也可以显示调用tf.assign()来完成对单个结点的初始化)。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line">k = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">kernel = tf.Variable(k)</span><br><span class="line">y2 = tf.nn.conv2d(img, kernel, strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># do someting....</span></span><br></pre></td></tr></table></figure><h3 id="tf-reset-default-graph"><a href="#tf-reset-default-graph" class="headerlink" title="tf.reset_default_graph()"></a>tf.reset_default_graph()</h3><p>如下是官网对tf.reset_default_graph()函数描述的翻译：tf.reset_default_graph函数用于清除默认图形堆栈并重置全局默认图形。</p><p>注意：默认图形是当前线程的一个属性。该tf.reset_default_graph函数只适用于当前线程。当一个tf.Session或者tf.InteractiveSession激活时调用这个函数会导致未定义的行为。调用此函数后使用任何以前创建的tf.Operation或tf.Tensor对象将导致未定义的行为。</p><p>demo1，无tf.reset_default_graph()函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Signature: tf.name_scope(*args, **kwds)</span></span><br><span class="line"><span class="string">Docstring:</span></span><br><span class="line"><span class="string">Returns a context manager for use when defining a Python op.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 也就是说，它的主要目的是为了更加方便地管理参数命名。</span></span><br><span class="line"><span class="comment"># 与 tf.Variable() 结合使用。简化了命名</span></span><br><span class="line"><span class="comment"># 注意，这里的 with 和 python 中其他的 with 是不一样的</span></span><br><span class="line"><span class="comment"># 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行此代码，就会再生成其他命名空间</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights1 = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">'weights'</span>)</span><br><span class="line">    bias1 = tf.Variable([<span class="number">0.3</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 下面是在另外一个命名空间来定义变量的</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights2 = tf.Variable([<span class="number">4.0</span>, <span class="number">2.0</span>], name=<span class="string">'weights'</span>)</span><br><span class="line">    bias2 = tf.Variable([<span class="number">0.33</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突</span></span><br><span class="line">print(weights1.name)</span><br><span class="line">print(weights2.name)</span><br><span class="line">print(bias1.name)</span><br><span class="line">print(bias2.name)</span><br></pre></td></tr></table></figure><p>执行结果：</p><p><img src="/MLFrameworks/TensorFlow/Tensorflow基本概念/20190806161204991.png" alt="img"></p><p>从上述结果可以看出，每次运行jupyter notebook时都会在上一次执行的基础生成新的张量。即：每在jupyter notebook上运行一次上述程序，就会在图上新增一个节点。</p><p>demo2，有<code>tf.reset_default_graph()</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 利用这个可清空default graph以及nodes</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Signature: tf.name_scope(*args, **kwds)</span></span><br><span class="line"><span class="string">Docstring:</span></span><br><span class="line"><span class="string">Returns a context manager for use when defining a Python op.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 也就是说，它的主要目的是为了更加方便地管理参数命名。</span></span><br><span class="line"><span class="comment"># 与 tf.Variable() 结合使用。简化了命名</span></span><br><span class="line"><span class="comment"># 注意，这里的 with 和 python 中其他的 with 是不一样的</span></span><br><span class="line"><span class="comment"># 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行此代码，就会再生成其他命名空间</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights1 = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">'weights'</span>)</span><br><span class="line">    bias1 = tf.Variable([<span class="number">0.3</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 下面是在另外一个命名空间来定义变量的</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights2 = tf.Variable([<span class="number">4.0</span>, <span class="number">2.0</span>], name=<span class="string">'weights'</span>)</span><br><span class="line">    bias2 = tf.Variable([<span class="number">0.33</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突</span></span><br><span class="line">print(weights1.name)</span><br><span class="line">print(weights2.name)</span><br><span class="line">print(bias1.name)</span><br><span class="line">print(bias2.name)</span><br></pre></td></tr></table></figure><p>执行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 第一次执行结果</span><br><span class="line">conv1/weights:0</span><br><span class="line">conv2/weights:0</span><br><span class="line">conv1/bias:0</span><br><span class="line">conv2/bias:0</span><br><span class="line"></span><br><span class="line"># 第二次执行结果</span><br><span class="line">conv1/weights:0</span><br><span class="line">conv2/weights:0</span><br><span class="line">conv1/bias:0</span><br><span class="line">conv2/bias:0</span><br><span class="line"></span><br><span class="line"># 第三次执行结果</span><br><span class="line">conv1/weights:0</span><br><span class="line">conv2/weights:0</span><br><span class="line">conv1/bias:0</span><br><span class="line">conv2/bias:0</span><br></pre></td></tr></table></figure><p>无论执行多少次生成的张量始终不变。换句话说就是：tf.reset_default_graph函数用于清除默认图形堆栈并重置全局默认图形。</p><h2 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p><code>tf.Session</code>接受三个可选参数：</p><ul><li>target。 如果将此参数留空（默认设置），会话将仅使用本地机器中的设备。但是，您也可以指定 grpc:// 网址，以便指定 TensorFlow 服务器的地址，这使得会话可以访问该服务器控制的机器上的所有设备。请参阅 tf.train.Server 以详细了解如何创建 TensorFlow 服务器。例如，在常见的图间复制配置中，tf.Session 连接到 tf.train.Server 的流程与客户端相同。分布式 TensorFlow 部署指南介绍了其他常见情形。</li><li>graph。 默认情况下，新的 tf.Session 将绑定到当前的默认图，并且仅能够在当前的默认图中运行操作。如果您在程序中使用了多个图（更多详情请参阅使用多个图进行编程），则可以在构建会话时指定明确的 tf.Graph。</li><li>config。 此参数允许您指定一个控制会话行为的 tf.ConfigProto。例如，部分配置选项包括：<ul><li>allow_soft_placement。将此参数设置为 True 可启用“软”设备放置算法，该算法会忽略尝试将仅限 CPU 的操作分配到 GPU 设备上的 tf.device 注解，并将这些操作放置到 CPU 上。</li><li>cluster_def。使用分布式 TensorFlow 时，此选项允许您指定要在计算中使用的机器，并提供作业名称、任务索引和网络地址之间的映射。详情请参阅 tf.train.ClusterSpec.as_cluster_def。</li><li>graph_options.optimizer_options。在执行图之前使您能够控制 TensorFlow 对图实施的优化。</li><li>gpu_options.allow_growth。将此参数设置为 True 可更改 GPU 内存分配器，使该分配器逐渐增加分配的内存量，而不是在启动时分配掉大多数内存。</li></ul></li></ul><h3 id="run"><a href="#run" class="headerlink" title="run"></a>run</h3><h4 id="run简介"><a href="#run简介" class="headerlink" title="run简介"></a>run简介</h4><p>会话构造时已经绑定了数据流图，在<code>run()</code>中传入待求解的张量和待填充的数据即可。</p><p><code>tf.Session().run()</code>函数的定义：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">run(</span><br><span class="line">    fetches,</span><br><span class="line">    feed_dict=<span class="literal">None</span>,</span><br><span class="line">    options=<span class="literal">None</span>,</span><br><span class="line">    run_metadata=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p><code>tf.Session().run()</code>函数的功能为：执行fetches参数所提供的operation操作或计算其所提供的Tensor。</p><p><code>run()</code>函数每执行一步，<strong>都会执行与fetches有关的图中的所有结点的计算</strong>，以完成fetches中的任务。其中，feed_dict提供了部分数据输入的功能。（和tf.Placeholder()搭配使用，很舒服）</p><p>参数说明：</p><ul><li>fetches：可以是张量，此时候返回值与fetches格式一致；该参数还可以是一个操作，因为操作的输出本质也是张量。</li><li>feed_dict：字典格式。给模型输入其计算过程中所需要的值。</li></ul><p>如上说明，<strong>一个session()中包含了Operation被执行，以及Tensor被evaluated的环境。</strong>即可以在会话中执行Op，也可以计算Tensor。您可以将一个或多个 tf.Operation 或 tf.Tensor 对象传递到 tf.Session.run，TensorFlow 将执行计算结果所需的操作。</p><p>例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">c = tf.add(a,b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.add(a,b))) <span class="comment"># 执行Operation</span></span><br><span class="line">    print(sess.run([a,b,c])) <span class="comment">#计算张量</span></span><br></pre></td></tr></table></figure><p></p><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.0</span></span><br><span class="line">[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br></pre></td></tr></table></figure><p><code>tf.Session.run</code>要求您指定一组 <strong>fetch</strong>，这些 fetch 可确定返回值，并且可能是 <code>tf.Operation、tf.Tensor或类张量类型，例如 tf.Variable</code>。这些 fetch 决定了必须执行哪些子图（属于整体 tf.Graph）以生成结果：该子图包含 fetch 列表中指定的所有操作，以及其输出用于计算 fetch 值的所有操作。例如，以下代码段说明了 tf.Session.run 的不同参数如何导致执行不同的子图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">output = tf.nn.softmax(y)</span><br><span class="line">init_op = w.initializer</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Run the initializer on `w`.</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `output`. `sess.run(output)` will return a NumPy array containing</span></span><br><span class="line">  <span class="comment"># the result of the computation.</span></span><br><span class="line">  print(sess.run(output))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `y` and `output`. Note that `y` will only be computed once, and its</span></span><br><span class="line">  <span class="comment"># result used both to return `y_val` and as an input to the `tf.nn.softmax()`</span></span><br><span class="line">  <span class="comment"># op. Both `y_val` and `output_val` will be NumPy arrays.</span></span><br><span class="line">  y_val, output_val = sess.run([y, output])</span><br></pre></td></tr></table></figure><p>tf.Session.run 也可以选择接受 feed 字典，该字典是从 tf.Tensor 对象（通常是 tf.placeholder 张量）到在执行时会替换这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a placeholder that expects a vector of three floating-point values,</span></span><br><span class="line"><span class="comment"># and a computation that depends on it.</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="number">3</span>])</span><br><span class="line">y = tf.square(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Feeding a value changes the result that is returned when you evaluate `y`.</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]&#125;))  <span class="comment"># =&gt; "[1.0, 4.0, 9.0]"</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">5.0</span>]&#125;))  <span class="comment"># =&gt; "[0.0, 0.0, 25.0]"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises &lt;a href="../api_docs/python/tf/errors/InvalidArgumentError"&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt;, because you must feed a value for</span></span><br><span class="line">  <span class="comment"># a `tf.placeholder()` when evaluating a tensor that depends on it.</span></span><br><span class="line">  sess.run(y)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises `ValueError`, because the shape of `37.0` does not match the shape</span></span><br><span class="line">  <span class="comment"># of placeholder `x`.</span></span><br><span class="line">  sess.run(y, &#123;x: <span class="number">37.0</span>&#125;)</span><br></pre></td></tr></table></figure><h4 id="run深入"><a href="#run深入" class="headerlink" title="run深入"></a>run深入</h4><p>当我们编写tensorflow代码时， 总是定义好整个计算图，然后才调用sess.run()去执行整个定义好的计算图， 那么有几个问题：</p><ul><li>什么会添加到tensorflow的图中。</li><li>当执行sess.sun()的时候， 程序是否执行了计算图上的所有节点呢？</li><li>sees.run()中的fetch, 为了取回（Fetch）操作的输出内容, 我们在sess.run()里面传入tensor， 那这些tensor的先后顺序会影响最后的结果嘛?比如有些tensor是有先后执行关系的，如果置于后面，会重复计算嘛？</li><li>在同一个sess中，使用sess.run()运行同一个节点，得到的结果相同么？</li></ul><p><strong>对于第一个问题：</strong>若程序中没有声明图，tensorflow使用默认图。那么所有使用<code>tf</code>的节点均会添加到tensorflow图中，而numpy函数、print函数(Python内置类型的加减乘法除外)都不会被添加到图中，也就不会在<code>sess.run()</code>函数中执行。</p><p>实验一：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable(tf.zeros([<span class="number">50</span>,<span class="number">12</span>]), trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">a = np.reshape(a, (<span class="number">12</span>,<span class="number">50</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(a))</span><br></pre></td></tr></table></figure><p></p><p>报错：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: cannot reshape array of size <span class="number">1</span> into shape (<span class="number">12</span>,<span class="number">50</span>)</span><br></pre></td></tr></table></figure><p></p><p>可以看到，虽然在sess.run()中确实运行了<code>numpy</code>方法（尴尬，与理论不符合啊）。但是报错了，我们简单粗暴的理解为numpy没有在图中，不能进行就行了(手动滑稽)。</p><p>对于Python内置类型，可以直接使用。</p><p>实验二：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable(tf.ones([<span class="number">2</span>,<span class="number">2</span>]), trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#a = np.reshape(a, (12,50))</span></span><br><span class="line">b = a * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run([a,b]))</span><br></pre></td></tr></table></figure><p></p><p>运行结果<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[array([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>]], dtype=float32), array([[<span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">2.</span>, <span class="number">2.</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure><p></p><p><strong>总结：</strong>在tensorflow图中，尽量全部使用tensorflow的函数（如果可能也尽量不用Python的内置类型做运算），不要使用numpy等Python库的函数（可以解释print函数为什么在sess.run()的时候不能运行）。若在运行过程中需要更改的，使用tensorflow变量（根据需要声明是否需要可训练），对于输入量，使用占位符。</p><p><strong>对于第二个问题</strong>，只有fetch里的图元素, 才会被执行, 不在fetch中的图节点是不会执行的</p><p>实验三：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0.0</span>,dtype=tf.float32)</span><br><span class="line">one = tf.constant(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line">new_val = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_val) <span class="comment">#返回tensor， 值为new_val</span></span><br><span class="line">update2 = tf.assign(state, <span class="number">10000</span>) <span class="comment">#没有fetch，便没有执行</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        print(sess.run([update, state, new_val]))</span><br><span class="line">        print(<span class="string">"--"</span>)</span><br></pre></td></tr></table></figure><p></p><p>对于windows平台上tensorflow1.8程序输出的结果<strong>（可能）</strong>为：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">3.0</span>, <span class="number">3.0</span>, <span class="number">3.0</span>]</span><br><span class="line">--</span><br></pre></td></tr></table></figure><p></p><p>但是，对于Ubuntu平台tensorflow1.8程序输出的结果<strong>（可能）</strong>为<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">3.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">--</span><br></pre></td></tr></table></figure><p></p><p>若将<code>sess.run(init)</code>放到了for循环里面</p><p>windows平台上tensorflow1.8程序运行结果<strong>（可能）</strong>如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br></pre></td></tr></table></figure><p></p><p>但是，对于Ubuntu平台tensorflow1.8程序输出的结果<strong>（可能）</strong>为<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br><span class="line">[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">--</span><br></pre></td></tr></table></figure><p></p><p><strong>分析：</strong>出现不同结果的原因我们将会在探索第三个问题的时候给出。但是对于我们这里的第二个问题，可以看到不管在哪个平台上<code>update2</code>节点都不会运行。</p><p><strong>总结：</strong>对于sess.sun(fetch), 只有fetch里的图元素, 才会被执行, 不在fetch中的图节点是不会执行的。若在图中使用了numpy等Python库函数（如print），在sess.run()是不会执行的。</p><p><strong>对于第三个问题</strong>，当sess.run() 里面的fetch是一个list时， 无论是update在前, 还是state在前, 不会执行update之后看到state在update后面就再执行一次state, 都是在这个<strong>list中的节点在流程图中全部执行完之后在取值的</strong>。 当我们将sess.run()里面的fetch列表中的节点打乱时， 取出来的值依然是一次流程图计算出来的结果。</p><p>实验四：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0.0</span>,dtype=tf.float32)</span><br><span class="line">one = tf.constant(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line">new_val = tf.add(state, one) <span class="comment"># 1</span></span><br><span class="line">update = tf.assign(state, new_val) <span class="comment">#update 1 # state 1</span></span><br><span class="line">update2 = tf.assign(state, <span class="number">10000</span>)  <span class="comment"># update2 10000 # state 10000</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">        print(sess.run([state, new_val, update2, update]))</span><br><span class="line">        print(<span class="string">"--"</span>)</span><br></pre></td></tr></table></figure><p></p><p>结果<strong>可能</strong>为：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10001.0</span>, <span class="number">10001.0</span>, <span class="number">10001.0</span>, <span class="number">10001.0</span>]</span><br><span class="line">--</span><br></pre></td></tr></table></figure><p></p><p><strong>分析：</strong>在不同平台上，相同版本的tensorflow运行就结果可能不同。甚至同一个环境运行多次的结果也不近相同，出现了这个问题的原因主要在于：TensorFlow是属于符号式编程的，它不会直接运行定义了的操作，而是在计算图中创造一个相关的节点，这个节点可以用Session.run()进行执行。这个使得TF可以在优化过程中(do optimization)决定优化的顺序(the optimal order)，并且在运算中剔除一些不需要使用的节点，而这一切都发生在运行中(run time)。<strong>如果你只是在计算图中使用tf.Tensors，你就不需要担心依赖问题（dependencies），但是你更可能会使用tf.Variable()，这个操作使得问题变得更加困难。</strong>你应该也注意到了，你在代码中定义操作(ops)的顺序是不会影响到在TF运行时的执行顺序的，唯一会影响到执行顺序的是控制依赖。控制依赖对于张量来说是直接的。每一次你在操作中使用一个张量时，操作将会定义一个对于这个张量来说的隐式的依赖。但是如果你同时也使用了变量，事情就变得更糟糕了，因为变量可以取很多值。当处理这些变量时，你可能需要显式地去通过使用tf.control_dependencies()去控制依赖。</p><p><strong>对于第四个问题：</strong>每次sess.run()都会执行一次图，所以在训练过程尽量一次在sess.run()取出所有要得到的值，否则运算量很大。而且，也有可能出现一些意想不到的错误（例如，tensorflow训练过程中若执行了两次sess.run()可能执行了两次取batchsize数据的操作，使得一次训练过程中数据加载两次）。</p><h4 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a>tf.control_dependencies</h4><p>该函数保证其辖域中的操作<strong>必须要在该函数所传递的参数中的操作完成后再进行</strong>。请看下面一个例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a_1 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">b_1 = tf.Variable(<span class="number">2</span>)</span><br><span class="line">update_op = tf.assign(a_1, <span class="number">10</span>)</span><br><span class="line">add = tf.add(a_1, b_1)</span><br><span class="line"></span><br><span class="line">a_2 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">b_2 = tf.Variable(<span class="number">2</span>)</span><br><span class="line">update_op = tf.assign(a_2, <span class="number">10</span>) <span class="comment"># assign赋值给update_op操作</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([update_op]):</span><br><span class="line">    add_with_dependencies = tf.add(a_2, b_2) <span class="comment">#　节点</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    ans_1, ans_2 = sess.run([add, add_with_dependencies])</span><br><span class="line">    print(<span class="string">"Add: "</span>, ans_1)</span><br><span class="line">    print(<span class="string">"Add_with_dependency: "</span>, ans_2)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Add:  <span class="number">3</span></span><br><span class="line">Add_with_dependency:  <span class="number">12</span></span><br></pre></td></tr></table></figure><p><strong>解释：</strong>对于<code>add</code>节点，在<code>sess.run()</code>的时候并没有执行<code>update_op</code>节点。而在执行<code>add_with_dependencies</code>节点的时候，使用了<code>with tf.control_dependencies</code>函数使得先执行<code>update_op</code>节点更新<code>a_2</code>节点，然后执行<code>add_with_dependencies</code>节点。</p><p>可以传入<code>None</code> 来消除依赖：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></span><br><span class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="literal">None</span>):</span><br><span class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting for either `a` or `b`.</span></span><br></pre></td></tr></table></figure><p></p><p><strong>注意</strong>：控制依赖只对那些在上下文环境中<strong>建立</strong>的操作有效，仅仅在context中<strong>使用</strong>一个操作或张量是没用的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># WRONG</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></span><br><span class="line">  t = tf.matmul(tensor, tensor)</span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">    <span class="comment"># The matmul op is created outside the context, so no control dependency will be added.</span></span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># RIGHT</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency will be added.</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure><p><strong>总结：</strong><code>tf.control_dependencies</code>函数有两个常用的场景：</p><ul><li>使用<code>tf.assign</code>更新了变量，而该变量要在之后使用到。</li><li>tensorflow的<code>batch_norm</code>函数中。</li></ul><h3 id="关闭会话"><a href="#关闭会话" class="headerlink" title="关闭会话"></a>关闭会话</h3><ul><li>使用sess.close()显式关闭会话</li><li>使用with语句定义sess时候，会隐式关闭会话<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run()</span><br><span class="line">    <span class="comment"># Session.__exit__ 被隐式调用。进而调用Session.close</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>tensorflow的图由节点和边组成，而图只是模型的整体框架，类似于人的各个结构。而session则是负责将tensorflow图的边流动起来，类似于人的血夜。而tensorflow的边分为控制边和数据边，对于控制边，需要在session.run()中传入节点（例如变量需要在<code>session.run</code>中执行<code>tf.global_variables_initializer()</code>初始化后才能使用），而对于数据边，需要在<code>session.run()</code>中传入tensor。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://looop.cn/?p=3353" target="_blank" rel="noopener">Tensorflow中constant、Varibles、placeholder在graph中的结点表示</a><br><a href="http://looop.cn/?p=3365" target="_blank" rel="noopener">Tensorflow中的图（tf.Graph）和会话（tf.Session）</a><br><a href="http://looop.cn/?p=3383" target="_blank" rel="noopener">Variables in Tensorflow</a><br><a href="https://www.cnblogs.com/jiaxblog/p/9054051.html" target="_blank" rel="noopener">TensorFlow学习笔记1：graph、session和op</a><br><a href="https://www.tensorflow.org/guide/#low_level_apis" target="_blank" rel="noopener">TensorFlow 指南</a><br><a href="https://www.tensorflow.org/guide/low_level_intro#next_steps" target="_blank" rel="noopener">简介</a><br><a href="https://www.tensorflow.org/guide/variables" target="_blank" rel="noopener">变量</a><br><a href="https://www.tensorflow.org/guide/tensors" target="_blank" rel="noopener">张量</a><br><a href="https://www.tensorflow.org/guide/graphs?hl=zh-CN#what_is_a_tfgraph" target="_blank" rel="noopener">图和会话</a><br><a href="https://zhuanlan.zhihu.com/p/55400862" target="_blank" rel="noopener">一文滤清 TensorFlow</a><br><a href="https://www.cnblogs.com/IUNI/p/9415864.html" target="_blank" rel="noopener">tensorflow学习—sess.run()</a><br><a href="https://blog.csdn.net/PKU_Jade/article/details/73498753" target="_blank" rel="noopener">tf.control_dependencies()作用及用法</a><br><a href="https://blog.csdn.net/huitailangyz/article/details/85015611" target="_blank" rel="noopener">tensorflow中的batch_norm以及tf.control_dependencies和tf.GraphKeys.UPDATE_OPS的探究</a><br><a href="https://blog.csdn.net/shwan_ma/article/details/78879620" target="_blank" rel="noopener">【tensorflow】打印Tensorflow graph中的所有需要训练的变量—tf.trainable_variables()</a><br><a href="https://blog.csdn.net/LoseInVain/article/details/78780020" target="_blank" rel="noopener">Effective TensorFlow Chapter 7: TensorFlow中的执行顺序和控制依赖</a><br><a href="https://blog.csdn.net/duanlianvip/article/details/98626111" target="_blank" rel="noopener">【TensorFlow】tf.reset_default_graph()函数</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/" title="Tensorflow基本概念">https://www.zdaiot.com/MLFrameworks/TensorFlow/Tensorflow基本概念/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Linux/软件/ubuntu安装ftp/" rel="prev" title="ubuntu安装ftp"><i class="fa fa-chevron-left"></i> ubuntu安装ftp</a></div><div class="post-nav-item"> <a href="/Python/常用第三方包/Matplotlib教程/" rel="next" title="Matplotlib教程">Matplotlib教程<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据流图"><span class="nav-number">1.</span> <span class="nav-text">数据流图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型载体：操作（节点）"><span class="nav-number">1.1.</span> <span class="nav-text">模型载体：操作（节点）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#计算节点：Operation类"><span class="nav-number">1.1.1.</span> <span class="nav-text">计算节点：Operation类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#属性"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">属性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#constant"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">constant</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#存储节点：Variables"><span class="nav-number">1.1.2.</span> <span class="nav-text">存储节点：Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#变量属性"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">变量属性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tf-Variable"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">tf.Variable()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tf-get-variable"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">tf.get_variable()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tf-name-scope-和tf-variable-scope"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">tf.name_scope()和tf.variable_scope()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Variables在图中的结构"><span class="nav-number">1.1.2.5.</span> <span class="nav-text">Variables在图中的结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#打印所有需要训练的变量"><span class="nav-number">1.1.2.6.</span> <span class="nav-text">打印所有需要训练的变量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#变量按照下标索引更新"><span class="nav-number">1.1.2.7.</span> <span class="nav-text">变量按照下标索引更新</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#变量操作"><span class="nav-number">1.1.2.8.</span> <span class="nav-text">变量操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#变量恢复与存储"><span class="nav-number">1.1.2.9.</span> <span class="nav-text">变量恢复与存储</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据节点：placeholder"><span class="nav-number">1.1.3.</span> <span class="nav-text">数据节点：placeholder</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#placeholder属性"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">placeholder属性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#placeholder在图中的结构"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">placeholder在图中的结构</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">1.1.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据载体：张量（数据边）"><span class="nav-number">1.2.</span> <span class="nav-text">数据载体：张量（数据边）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#张量的属性"><span class="nav-number">1.2.1.</span> <span class="nav-text">张量的属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建"><span class="nav-number">1.2.2.</span> <span class="nav-text">创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#求解"><span class="nav-number">1.2.3.</span> <span class="nav-text">求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#成员方法"><span class="nav-number">1.2.4.</span> <span class="nav-text">成员方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量与张量的关系"><span class="nav-number">1.3.</span> <span class="nav-text">变量与张量的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实例"><span class="nav-number">1.4.</span> <span class="nav-text">实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-reset-default-graph"><span class="nav-number">1.5.</span> <span class="nav-text">tf.reset_default_graph()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#会话"><span class="nav-number">2.</span> <span class="nav-text">会话</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化"><span class="nav-number">2.1.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#run"><span class="nav-number">2.2.</span> <span class="nav-text">run</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#run简介"><span class="nav-number">2.2.1.</span> <span class="nav-text">run简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#run深入"><span class="nav-number">2.2.2.</span> <span class="nav-text">run深入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-control-dependencies"><span class="nav-number">2.2.3.</span> <span class="nav-text">tf.control_dependencies</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关闭会话"><span class="nav-number">2.3.</span> <span class="nav-text">关闭会话</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-1"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">达达And木槿</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">305</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">52</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">364</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">30:09</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '2c7ce5913eb978a7a22602f1df3a90a5',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>