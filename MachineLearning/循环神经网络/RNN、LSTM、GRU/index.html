<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CNN和RNN的不同在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。循环神经网络(Recurrent Neural Networks ，以下简称RNN)的输出和模型间有反馈，它广泛的用于自然语言处理中的语音识别，手写书别以及机器翻译等领域。 在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类"><meta name="keywords" content="GRU,LSTM,RNN"><meta property="og:type" content="article"><meta property="og:title" content="RNN、LSTM、GRU"><meta property="og:url" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="CNN和RNN的不同在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。循环神经网络(Recurrent Neural Networks ，以下简称RNN)的输出和模型间有反馈，它广泛的用于自然语言处理中的语音识别，手写书别以及机器翻译等领域。 在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-f16c8acc01d2d4691521681465504.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-d6749df8fb93b0b0.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/ff3e05fc-2b9b-495f-b3a4-75e27571521681508982.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/v2-b45f69904d546edde41d9539e4c5548c_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-feb16ca499c4b96a.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-becb05d96b1e4af7_3.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-9ac355076444b66f.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-b9a16a53d58ca2b9_3.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-ea943b818b8e18d01521687961830.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-ac1eb618f37a9dea_21521688662083.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-7169541c790efd13.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-96b387f711d1d12c1521689087175.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-7fa07e640593f930.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-d88caa3c4faf5353.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-4c9186bf786063d6.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/63d50d37-59be-4765-a075-7a41f24da4da.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/f1e3edce-3c41-4b58-8cce-4a826b81521705295002.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/0a791f1d-b1a3-4a3d-94b9-f11c1081521705957295.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-dd3d241fa44a71c0.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz.qpic.cn1521728365332.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz1.qpic.cn.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz3.qpic.cn.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz4.qpic.cn.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-0f80ad5540ea27f9.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-bd2f1feaea22630e.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/1630478-20190429094829407-235125842.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/1630478-20190429140253550-673920122.png"><meta property="og:updated_time" content="2019-09-14T03:23:46.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="RNN、LSTM、GRU"><meta name="twitter:description" content="CNN和RNN的不同在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。循环神经网络(Recurrent Neural Networks ，以下简称RNN)的输出和模型间有反馈，它广泛的用于自然语言处理中的语音识别，手写书别以及机器翻译等领域。 在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类"><meta name="twitter:image" content="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-f16c8acc01d2d4691521681465504.png"><link rel="canonical" href="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>RNN、LSTM、GRU | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> RNN、LSTM、GRU<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MachineLearning/循环神经网络/RNN、LSTM、GRU.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-03-21 19:41:39" itemprop="dateCreated datePublished" datetime="2018-03-21T19:41:39+08:00">2018-03-21</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-09-14 11:23:46" itemprop="dateModified" datetime="2019-09-14T11:23:46+08:00">2019-09-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/循环神经网络/" itemprop="url" rel="index"><span itemprop="name">循环神经网络</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>8.1k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CNN和RNN的不同"><a href="#CNN和RNN的不同" class="headerlink" title="CNN和RNN的不同"></a>CNN和RNN的不同</h2><p>在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。循环神经网络(Recurrent Neural Networks ，以下简称RNN)的输出和模型间有反馈，它广泛的用于自然语言处理中的语音识别，手写书别以及机器翻译等领域。</p><p>在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类问题DNN和CNN不好解决，就是训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。</p><p>而对于这类问题，RNN则比较的擅长。那么RNN是怎么做到的呢？RNN假设我们的样本是基于序列的。比如是从序列索引1到序列索引$\tau$的。对于这其中的任意序列索引号$t$,它对应的输入是对应的样本序列中的$x^{(t)}$。而模型在序列索引号$t$位置的隐藏状态$h^{(t)}$，则由$x^{(t)}$和在$t-1$位置的隐藏状态$h^{(t-1)}$共同决定。在任意序列索引号$t$，我们也有对应的模型预测输出$o^{(t)}$。通过预测输出$o^{(t)}$和训练序列真实输出$y^{(t)}$,以及损失函数$L^{(t)}$，我们就可以用DNN类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出。</p><h2 id="Recurrent-Neural-Networks-RNN"><a href="#Recurrent-Neural-Networks-RNN" class="headerlink" title="Recurrent Neural Networks(RNN)"></a>Recurrent Neural Networks(RNN)</h2><h3 id="RNN结构"><a href="#RNN结构" class="headerlink" title="RNN结构"></a>RNN结构</h3><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-f16c8acc01d2d4691521681465504.png" alt></p><p>在上面的示例图中，神经网络的模块A，正在读取某个输入$x_i$，并输出一个值$h_i$。循环可以使得信息可以从当前步传递到下一步。RNN 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-d6749df8fb93b0b0.png" alt></p><h3 id="RNN结构详细解析"><a href="#RNN结构详细解析" class="headerlink" title="RNN结构详细解析"></a>RNN结构详细解析</h3><p>对该结构进行详细解析，如下图所示：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/ff3e05fc-2b9b-495f-b3a4-75e27571521681508982.jpg" alt></p><ul><li>x代表当前状态下数据的输入，h表示接收到上一节点的输入</li><li>y为当前节点状态下的输出，而h’为传递到下一个节点的输出</li><li>可以看出输出h’与x和h的值都有关</li><li>y则常常使用h’投入到一个线性层(主要是维度映射)，然后使用softmax进行分类得到需要的数据。而一般情况下，如何从h’得到y常常要看具体模型的使用方式。</li></ul><h3 id="timesteps和output-dim"><a href="#timesteps和output-dim" class="headerlink" title="timesteps和output_dim"></a>timesteps和output_dim</h3><p>以keras深度学习框架为例，调用RNN层的API为<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.recurrent.SimpleRNN(output_dim, init=&apos;glorot_uniform&apos;, inner_init=&apos;orthogonal&apos;, activation=&apos;tanh&apos;, W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)</span><br></pre></td></tr></table></figure><p></p><p>输入数据的维度为<code>(batch_size, timesteps, input_dim)</code>；如果<code>return_sequences=True</code>，则输出数据的维度为<code>(batch_size, timesteps, output_dim)</code>，若<code>return_sequences=False</code>，则输出数据的维度为<code>(batch_size, output_dim)</code>。那么怎么理解输入数据的timesteps和输出维度。</p><p>那么怎么理解这里的<code>timesteps和output_dim</code>参数呢？其实，我们之前的图画的不够明确，里面有很多细节都隐藏了，那么真实的图长什么样呢？RecurrentNNs的结构应该这样画，在理解上才会更清晰些，对比MLP，也一目了然。(自己画的为了简约，只画了4个timesteps ）</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/v2-b45f69904d546edde41d9539e4c5548c_r.jpg" alt></p><p>看图。每个时序$t$的输入为$I^{t}$，也就是说一个<code>time_step</code>对应一个输入张量，隐状态$h_{i}^{t}$也就代表了一张MLP的hidden layer的一个cell。输出$o_{i}^{t}$理解无异。以一个具体例子进行说明。</p><p>例如有一个样本为<code>hello</code>，而<code>timesteps=4</code>，那么输入为<code>hell</code>的编码，训练目标为<code>llow</code>的编码。对应在图上，就是<code>T=1</code>的时候输入<code>h</code>的编码，目标为<code>l</code>的编码；<code>T=2</code>的时候输入<code>e</code>的编码，目标为<code>l</code>的编码；…；<code>T=4</code>的时候输入<code>l</code>的编码，目标为<code>w</code>的编码。现在固定一个时刻看，以<code>T=1</code>为例，可以看出这就是一个MLP模型，隐状态即为其中的隐层单元，而隐层单元在经过一层网络即可得到输出，而这里所谓的<code>output_dim</code>就是指MLP输出层神经元的数目。不过值得注意的是，从<code>T=1</code>到<code>T=4</code>，共用一个MLP模型，也就是说参数是共享的，这也对应了循环神经网络中<strong>时间与循环</strong>的概念。</p><blockquote><p>有些时候为了简化处理，会取$o^t=h^t$，即MLP模型只有一层。</p></blockquote><h3 id="长期依赖问题"><a href="#长期依赖问题" class="headerlink" title="长期依赖问题"></a>长期依赖问题</h3><p>RNN的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。然而RNN要做到这个，还有很多依赖因素。</p><p>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN可以学会使用先前的信息。</p><p>如下图的不太长的相关信息和位置间隔：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-feb16ca499c4b96a.png" alt></p><p>但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p><p>如下图的相当长的相关信息和位置间隔：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-becb05d96b1e4af7_3.png" alt></p><p>理论表明，RNN在训练长序列的时候，会出现梯度消失或梯度爆炸的现象。</p><h2 id="Long-Short-Term-LSTM"><a href="#Long-Short-Term-LSTM" class="headerlink" title="Long Short Term(LSTM)"></a>Long Short Term(LSTM)</h2><p>LSTM是一种特殊的RNN，可以解决RNN出现的长期依赖问题。</p><h3 id="LSTM结构"><a href="#LSTM结构" class="headerlink" title="LSTM结构"></a>LSTM结构</h3><p>LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！</p><p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-9ac355076444b66f.png" alt></p><p>LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-b9a16a53d58ca2b9_3.png" alt></p><p>上述框图中使用的各种元素的图标含义如下：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-ea943b818b8e18d01521687961830.png" alt></p><p>在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p><h3 id="LSTM结构详细解析"><a href="#LSTM结构详细解析" class="headerlink" title="LSTM结构详细解析"></a>LSTM结构详细解析</h3><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。</p><p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-ac1eb618f37a9dea_21521688662083.png" alt></p><h4 id="门结构"><a href="#门结构" class="headerlink" title="门结构"></a>门结构</h4><p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-7169541c790efd13.png" alt></p><p>Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p><p>LSTM 拥有三个门，来保护和控制细胞状态。</p><h4 id="忘记阶段"><a href="#忘记阶段" class="headerlink" title="忘记阶段"></a>忘记阶段</h4><p>在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为<strong>忘记门层</strong>完成。该门会读取$h_{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C_{t-1}$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。</p><p>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前<strong>主语</strong>的性别，因此正确的<strong>代词</strong>可以被选择出来。当我们看到新的<strong>主语</strong>，我们希望忘记旧的<strong>主语</strong>。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-96b387f711d1d12c1521689087175.png" alt></p><h4 id="选择记忆阶段"><a href="#选择记忆阶段" class="headerlink" title="选择记忆阶段"></a>选择记忆阶段</h4><p>该阶段确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量$\tilde{C}_t$,会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。</p><p>在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-7fa07e640593f930.png" alt></p><p>现在是更新旧细胞状态的时间了，$C_{t-1}$更新为$C_t$。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。</p><p>我们把旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \tilde{C}_t$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。</p><p>在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-d88caa3c4faf5353.png" alt></p><h4 id="输出阶段"><a href="#输出阶段" class="headerlink" title="输出阶段"></a>输出阶段</h4><p>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p><p>在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-4c9186bf786063d6.png" alt></p><h3 id="LSTM的另一种理解方法"><a href="#LSTM的另一种理解方法" class="headerlink" title="LSTM的另一种理解方法"></a>LSTM的另一种理解方法</h3><p>LSTM结构（图右）和普通RNN的主要输入输出区别如下所示。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/63d50d37-59be-4765-a075-7a41f24da4da.jpg" alt></p><ul><li>RNN只有一个传递状态$h_t$，而LSTM有两个传递状态$c_t$(cell state)和$h_t$(hidden state)，注意：<strong>RNN中的$h_t$对于LSTM的$c_t$</strong></li><li>对于传递下去的$c_t$改变的很慢，通常$c_t$是上一个状态传来的$c_{t-1}$加上一些数值。</li><li>$h_t$在不同节点下往往有很大的区别</li></ul><p>首先使用LSTM的当前输入$x_t$和上一个状态传递下来的$h_{t-1}$拼接训练得到四个状态。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/f1e3edce-3c41-4b58-8cce-4a826b81521705295002.jpg" alt></p><p>其中，$z^f,z^i,z^o$是由拼接向量乘以权重矩阵之后，再通过一个$sigmoid$激活函数换成0到1之间的数值，来作为一种门控状态。而$z$则是将结果通过一个$tanh$激活函数转换成-1到1之间的值(这里使用$tanh$是因为这里是将其作为输入数据，而不是门控信号)。</p><p>下面进一步介绍这四个状态在LSTM内部的使用</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/0a791f1d-b1a3-4a3d-94b9-f11c1081521705957295.jpg" alt></p><p>LSTM内部主要有三个阶段：</p><ul><li>忘记阶段。该阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会“忘记不重要的，记住重要的”。具体来说，就是通过计算得到$z^f$(f表示forget)来表示忘记门控，来控制上一个状态的$c^{t-1}$哪些需要留哪些需要遗忘。</li><li>选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行”记忆”。主要是会对输入$x^t$进行选择记忆。哪些重要则着重记录下来，那些不重要，则少记一些。当前的输入内容由前面计算得到的$z$表示。而选择门控信号则是有$z^i$(i表示information)来进行控制。</li></ul><blockquote><p>将上面两步结果相加，即可得到传输给下一个状态的$c_t$。也就是上图中的第一个公式</p></blockquote><ul><li>输出阶段。这个阶段将决定哪些将会被当前状态的输出。主要是通过$z^o$进行控制的。并且还对上一阶段得到的$c^o$进行了放缩(通过一个$tanh$激活函数进行变化)。</li></ul><p>与普通RNN类似，输出$y^t$往往最终也是通过$h^t$变化得到。</p><h2 id="Gate-Recurrent-Unit-GRU"><a href="#Gate-Recurrent-Unit-GRU" class="headerlink" title="Gate Recurrent Unit(GRU)"></a>Gate Recurrent Unit(GRU)</h2><p>GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。</p><p>相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p><h3 id="GRU网络结构"><a href="#GRU网络结构" class="headerlink" title="GRU网络结构"></a>GRU网络结构</h3><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-dd3d241fa44a71c0.png" alt></p><p>输入输出结构如下：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz.qpic.cn1521728365332.jpg" alt></p><p>GRU的输入输出结构与普通的RNN是一样的。有一个当前的输入$x^t$，和上一个节点传下来的隐状态$h^{t-1}$，这个隐状态包含了之前隐节点的相关关系。</p><p>结合$x^t$和$h^{t-1}$，GRU会得到当前隐藏节点的输出$y^t$和传递给下一个节点的隐状态$h^t$。</p><h3 id="GRU结构详细解析"><a href="#GRU结构详细解析" class="headerlink" title="GRU结构详细解析"></a>GRU结构详细解析</h3><p>首先，我们先通过上一个传输下来的状态$h^{t-1}$和当前节点的输入$x^t$来获取两个门控状态。如下图所示。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz1.qpic.cn.jpg" alt></p><p>其中$r$控制重置的门控(reset gate)，$z$为控制更新的门控(update gate)。</p><blockquote><p>$\sigma$为sigmoid函数，通过这个函数可以将数据变换为0-1范围内的数值，从而来充当门控信号。</p></blockquote><p>得到门控信号之后，首先使用重置门控来得到”重置”之后的数据$h^{t-1’}=h^{t-1}\bigodot r$，再将$h^{t-1’}$与输入$x^t$进行拼接，再通过一个tanh激活函数来将数据放缩到-1~1的范围内。即得到下图的$h’$</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz3.qpic.cn.jpg" alt></p><p>这里的$h’$主要是包含了当前输入的$x^t$数据。有针对性的对$h’$添加到当前的隐藏状态，相当于“记忆了当前时刻的状态”，类似于LSTM的选择记忆阶段。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/mmbiz4.qpic.cn.jpg" alt></p><blockquote><p>上图中的$\bigodot$是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。$\bigoplus$则代表进行矩阵的加法操作。</p></blockquote><p>最后，介绍GRU最关键的一个步骤，称为”更新记忆”阶段。</p><p>更新表达式为：$h^t=z\bigodot h^{t-1}+(1-z)\bigodot h’$</p><p>首先再次强制一下，门控信号(这里的z)的范围为0~1。门控信号越接近1，代表”记忆”下来的数据越多；而越接近0则代表”遗忘”的越多。</p><p>GRU很聪明的一点在于，使用同一个门控$z$就同时可以进行遗忘和选择记忆(LSTM则要使用多个门控)。</p><ul><li>$z\bigodot h^{t-1}$：表示对原本隐藏状态的选择性”遗忘”。这里的z可以想象是遗忘门(forget gate)，忘记$h^t$维度中一些不重要的信息。</li><li>$(1-z)\bigodot h’$：表示对包含当前节点信息的$h’$进行选择性”记忆”。与上面类似，这里的$(1-z)$同理会忘记$h’$维度中一些不重要的信息。或者，这里我们更应当看做是对$h’$维度中的某些信息进行选择。</li><li>$h^t=z\bigodot h^{t-1}+(1-z)\bigodot h’$：综上所述，这一步的操作就是忘记传递下来的$h^{t-1}$中的某些维度信息，并加入当前节点输入的某些维度信息。</li></ul><blockquote><p>可以看到，这里的遗忘$z$和选择$(1-z)$是联动的。也就是传递进来的维度信息，我们对进行选择性遗忘，则遗忘了多少权重$z$，我们就会使用包含当前输入的$h’$中所对应的权重进行弥补$1-z$。以保持一种”恒定”状态。</p></blockquote><h3 id="LSTM和GRU的关系"><a href="#LSTM和GRU的关系" class="headerlink" title="LSTM和GRU的关系"></a>LSTM和GRU的关系</h3><p>r（reset gate）实际与名字不符。我们这里仅仅使用它来获得了$h’$。</p><p>那么，$h’$实际上可以看成对应于LSTM中的hidden state；上一个节点传下来的$h^{t-1}$则对应于LSTM中的cell state。z对应的则是LSTM中的$z^f$ forget gate，那么$(1-z)$似乎是选择门$z^i$了。</p><h2 id="LSTM的变体"><a href="#LSTM的变体" class="headerlink" title="LSTM的变体"></a>LSTM的变体</h2><h3 id="peephole-connection"><a href="#peephole-connection" class="headerlink" title="peephole connection"></a>peephole connection</h3><p>其中一个流形的 LSTM 变体，就是由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a> 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-0f80ad5540ea27f9.png" alt></p><p>上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。</p><h3 id="coupled-忘记和输入门"><a href="#coupled-忘记和输入门" class="headerlink" title="coupled 忘记和输入门"></a>coupled 忘记和输入门</h3><p>另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/42741-bd2f1feaea22630e.png" alt></p><h2 id="堆叠RNN"><a href="#堆叠RNN" class="headerlink" title="堆叠RNN"></a>堆叠RNN</h2><p>堆叠RNN的结构如下所示：</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/1630478-20190429094829407-235125842.png" alt="img" style="zoom:80%"></p><p>以上是按时间展开的堆叠循环神经网络，横坐标表示不同的时刻，纵坐标表示不同的层。一般的，我们定义 $h_t^{(l)}$为在时刻$t$时第$l$层的隐状态，则它是由时刻$t-1$第$l$层的隐状态与时刻$t$第$l-1$层的隐状态共同决定：</p><script type="math/tex;mode=display">
\mathbf{h}_{t}^{(l)}=f\left(U^{(l)} \mathbf{h}_{t-1}^{(l)}+W^{(l)} \mathbf{h}_{t}^{(l-1)}+\mathbf{b}^{(l)}\right)</script><p>其中$U^{(l)}、W^{(l)}$是权重矩阵，$b^{(l)}$是偏置，$h_t^{(0)} = x_t$。</p><h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p>双向LSTM（Bidirectional Long-Short Term Memorry，Bi-LSTM）不仅能利用到过去的信息，还能捕捉到后续的信息，比如在词性标注问题中，一个词的词性由上下文的词所决定，那么用双向LSTM就可以利用好上下文的信息。双向LSTM由两个信息传递相反的LSTM循环层构成，其中第一层按时间顺序传递信息，第二层按时间逆序传递信息。</p><p>为了研究方便，这里以双向RNN结构为例进行介绍，如下图所示。</p><p><img src="/MachineLearning/循环神经网络/RNN、LSTM、GRU/1630478-20190429140253550-673920122.png" alt="img"></p><p>下面来看下隐状态该如何计算，可以看到$t$时刻第一层（顺时间循环层）的隐状态$h_t^{(1)}$取决于前一时刻的隐状态$h_{t-1}^{(1)}$和输入值$x_t$。$t$时刻第二层（逆时间循环层）的隐状态$h_t^{(2)}$取决于下一时刻的隐状态$h_{t+1}^{(2)}$和输入值$x_t$。这与堆叠RNN不同，堆叠RNN除第一层外，其余层的隐状态不由输入值$x_t$直接输入得到，而是取决于前一时刻该层的隐状态和当前时刻前一层的隐状态。</p><script type="math/tex;mode=display">
\begin{aligned} \mathbf{h}_{t}^{(1)} &=f\left(U^{(1)} \mathbf{h}_{t-1}^{(1)}+W^{(1)} \mathbf{x}_{t}+\mathbf{b}^{(1)}\right) \\ \mathbf{h}_{t}^{(2)} &=f\left(U^{(2)} \mathbf{h}_{t+1}^{(2)}+W^{(2)} \mathbf{x}_{t}+\mathbf{b}^{(2)}\right) \\ \mathbf{h}_{t} &=\mathbf{h}_{t}^{(1)} \oplus \mathbf{h}_{t}^{(2)} \end{aligned}</script><p>因为模型在训练的时候会将$T$时间序列的数据一起输入到网络中，计算损失并更新参数，所以双向RNN中逆时间循环层可以实现。</p><p>双向RNN和堆叠RNN可以结合使用，在顺时间循环层和逆时间循环层可以构造堆叠RNN。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.zhihu.com/question/41949741/answer/318771336" target="_blank" rel="noopener">LSTM神经网络输入输出究竟是怎样的？ - Scofield的回答 - 知乎</a><br><a href="https://www.cnblogs.com/Luv-GEM/p/10788849.html" target="_blank" rel="noopener">TensorFlow之RNN：堆叠RNN、LSTM、GRU及双向LSTM</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/" title="RNN、LSTM、GRU">https://www.zdaiot.com/MachineLearning/循环神经网络/RNN、LSTM、GRU/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/GRU/" rel="tag"><i class="fa fa-tag"></i> GRU</a><a href="/tags/LSTM/" rel="tag"><i class="fa fa-tag"></i> LSTM</a><a href="/tags/RNN/" rel="tag"><i class="fa fa-tag"></i> RNN</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Linux/维护/Ubuntu修改默认启动项/" rel="prev" title="Ubuntu修改默认启动项"><i class="fa fa-chevron-left"></i> Ubuntu修改默认启动项</a></div><div class="post-nav-item"> <a href="/MachineLearning/卷积神经网络/卷积神经网络模型结构/" rel="next" title="卷积神经网络模型结构">卷积神经网络模型结构<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN和RNN的不同"><span class="nav-number">1.</span> <span class="nav-text">CNN和RNN的不同</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Networks-RNN"><span class="nav-number">2.</span> <span class="nav-text">Recurrent Neural Networks(RNN)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN结构"><span class="nav-number">2.1.</span> <span class="nav-text">RNN结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN结构详细解析"><span class="nav-number">2.2.</span> <span class="nav-text">RNN结构详细解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#timesteps和output-dim"><span class="nav-number">2.3.</span> <span class="nav-text">timesteps和output_dim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#长期依赖问题"><span class="nav-number">2.4.</span> <span class="nav-text">长期依赖问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Long-Short-Term-LSTM"><span class="nav-number">3.</span> <span class="nav-text">Long Short Term(LSTM)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM结构"><span class="nav-number">3.1.</span> <span class="nav-text">LSTM结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM结构详细解析"><span class="nav-number">3.2.</span> <span class="nav-text">LSTM结构详细解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#核心思想"><span class="nav-number">3.2.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#门结构"><span class="nav-number">3.2.2.</span> <span class="nav-text">门结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#忘记阶段"><span class="nav-number">3.2.3.</span> <span class="nav-text">忘记阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#选择记忆阶段"><span class="nav-number">3.2.4.</span> <span class="nav-text">选择记忆阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#输出阶段"><span class="nav-number">3.2.5.</span> <span class="nav-text">输出阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM的另一种理解方法"><span class="nav-number">3.3.</span> <span class="nav-text">LSTM的另一种理解方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gate-Recurrent-Unit-GRU"><span class="nav-number">4.</span> <span class="nav-text">Gate Recurrent Unit(GRU)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU网络结构"><span class="nav-number">4.1.</span> <span class="nav-text">GRU网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU结构详细解析"><span class="nav-number">4.2.</span> <span class="nav-text">GRU结构详细解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM和GRU的关系"><span class="nav-number">4.3.</span> <span class="nav-text">LSTM和GRU的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM的变体"><span class="nav-number">5.</span> <span class="nav-text">LSTM的变体</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#peephole-connection"><span class="nav-number">5.1.</span> <span class="nav-text">peephole connection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coupled-忘记和输入门"><span class="nav-number">5.2.</span> <span class="nav-text">coupled 忘记和输入门</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#堆叠RNN"><span class="nav-number">6.</span> <span class="nav-text">堆叠RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#双向RNN"><span class="nav-number">7.</span> <span class="nav-text">双向RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">8.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">327</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">55</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">382</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">35:09</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : 'a2195c75b3f238f5a284cf275e8fb522',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>