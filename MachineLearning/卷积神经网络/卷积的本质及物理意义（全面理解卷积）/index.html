<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="信号的定义通常，人们将这些具有某种内容的语言文字、声讯图像以及统计数据等称为消息，如果消息是人们所需要的，那么就称为信息，而信号是携带消息的随时间变化的物理量。 从定义可以看出，图像也看作是信号的一种。那么信号与系统中所述的卷积、傅里叶级数、傅里叶变换和图像中的卷积乃至深度学习中的卷积有何区别和联系呢？本文带着这个问题进行探讨。 卷积卷积来源"><meta name="keywords" content="CNN,傅里叶变换,卷积"><meta property="og:type" content="article"><meta property="og:title" content="卷积的本质及物理意义（全面理解卷积）"><meta property="og:url" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="信号的定义通常，人们将这些具有某种内容的语言文字、声讯图像以及统计数据等称为消息，如果消息是人们所需要的，那么就称为信息，而信号是携带消息的随时间变化的物理量。 从定义可以看出，图像也看作是信号的一种。那么信号与系统中所述的卷积、傅里叶级数、傅里叶变换和图像中的卷积乃至深度学习中的卷积有何区别和联系呢？本文带着这个问题进行探讨。 卷积卷积来源"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/2019-05-17-22-30-181558108099937.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/2ca39677363c65a305207a5491b75825_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/8e1fce9d7607d97cebf73e1f36f03f06_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/d4fa1de0327eb491a6941ac84a56e432_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/1ca366b593d877a16c8a49773774b5b9_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/055bf33bb84555a952804c5dbeb75dd9_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/563deb4a6599d052b3ba108661872c57_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/81ca9447d6c45c162c2d76df75a6690a_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/Fourier_series_square_wave_circles_animation.gif"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/Fourier_series_sawtooth_wave_circles_animation.gif"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/e2e3c0af3bdbcba721c5415a4c65da9e_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/19679c871bd33d94e2fc8b174f0d14ab_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/40cf849e55ed95732a60b52d4019d609_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/Fourier_series_and_transform.gif"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/07199fc0250791d768771b50c098e26a_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/e1985fe86283a7b14d1fc7e11d322fcb_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/4695ce06197677bab880cd55b6846f12_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/419cd0b2e965aca25d5f8a5a6362d728_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/a185be412974fd73a7925cf1f1cc5372_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/ece53f825c6de629befba3de12f929a7_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/42e1f6dc43e8868b4962f5ba389a5df4_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/3e88e9463e4667e50ebdda51dee88358_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/974efc6a99e06dcd623193e960ccbe93_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/c2d7bfc819ebcbea8d6f2c8271d4791d_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/f116ae26859bdc80b28ea0f8f894ccc0_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/0fdfa0a9b6eea036703ab2499381080c_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/097c9051af221c171730d4bc8f436a72_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-c76d2c7ce11d38d76addf847face35d2_hd.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-4729710146abe4cbeeba1b315bbf620d_hd.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-3fcaa905013995eb3c4f23b6538840fe_hd.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-387fe282ad5d70113e69679b81499d62_hd.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-a30d021f90d53c4089bf472a6ed517e6_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-b00f391d3ba69f1c8bd6807e28ca79a4_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-1cc14436b43f795bb5bcb91ca8ac52ec_hd.png"><meta property="og:updated_time" content="2020-02-29T03:06:07.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="卷积的本质及物理意义（全面理解卷积）"><meta name="twitter:description" content="信号的定义通常，人们将这些具有某种内容的语言文字、声讯图像以及统计数据等称为消息，如果消息是人们所需要的，那么就称为信息，而信号是携带消息的随时间变化的物理量。 从定义可以看出，图像也看作是信号的一种。那么信号与系统中所述的卷积、傅里叶级数、傅里叶变换和图像中的卷积乃至深度学习中的卷积有何区别和联系呢？本文带着这个问题进行探讨。 卷积卷积来源"><meta name="twitter:image" content="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/2019-05-17-22-30-181558108099937.png"><link rel="canonical" href="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>卷积的本质及物理意义（全面理解卷积） | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 卷积的本质及物理意义（全面理解卷积）<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-10 21:17:02" itemprop="dateCreated datePublished" datetime="2019-05-10T21:17:02+08:00">2019-05-10</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-29 11:06:07" itemprop="dateModified" datetime="2020-02-29T11:06:07+08:00">2020-02-29</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/卷积神经网络/" itemprop="url" rel="index"><span itemprop="name">卷积神经网络</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>34k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>31 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="信号的定义"><a href="#信号的定义" class="headerlink" title="信号的定义"></a>信号的定义</h2><p>通常，人们将这些具有某种内容的语言文字、声讯图像以及统计数据等称为消息，如果消息是人们所需要的，那么就称为信息，而信号是携带消息的随时间变化的物理量。</p><p>从定义可以看出，<strong>图像也看作是信号的一种</strong>。那么信号与系统中所述的卷积、傅里叶级数、傅里叶变换和图像中的卷积乃至深度学习中的卷积有何区别和联系呢？本文带着这个问题进行探讨。</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h3 id="卷积来源"><a href="#卷积来源" class="headerlink" title="卷积来源"></a>卷积来源</h3><p>卷积其实就是为<strong>冲击函数</strong>诞生的。“冲击函数”是狄拉克为了解决一些瞬间作用的物理现象而提出的符号。古人曰：“说一堆大道理不如举一个好例子”，冲量这一物理现象很能说明“冲击函数”。在t时间内对一物体作用F的力，倘若作用时间t很小，作用力F很大，但让Ft的乘积不变，即冲量不变。于是在用t做横坐标、F做纵坐标的坐标系中，就如同一个面积不变的长方形，底边被挤的窄窄的，高度被挤的高高的，在数学中它可以被挤到无限高，但即使它无限瘦、无限高、但它仍然保持面积不变（它没有被挤没！），为了证实它的存在，可以对它进行积分，积分就是求面积嘛！于是“卷积”这个数学怪物就这样诞生了。</p><p>卷积是“信号与系统”中论述系统对输入信号的响应而提出的。</p><h3 id="卷积意义"><a href="#卷积意义" class="headerlink" title="卷积意义"></a>卷积意义</h3><h4 id="信号处理角度"><a href="#信号处理角度" class="headerlink" title="信号处理角度"></a>信号处理角度</h4><p>信号处理是将一个信号空间映射到另外一个信号空间，通常就是时域到频域（还有z域，s域），信号的能量就是函数的范数（信号与函数等同的概念），大家都知道有个<strong>Paserval定理就是说映射前后范数不变，在数学中就叫保范映射</strong>，实际上信号处理中的变换基本都是保范映射，只要Paserval定理成立就是保范映射（就是能量不变的映射）。</p><p>信号处理中如何出现卷积的。假设$B$是一个<strong>因果系统</strong>，其$t$时刻的输入为<strong>因果信号</strong>$x(t)$，输出为$y(t)$，系统的响应函数为$h(t)$，按理说，输出与输入的关系应该为</p><script type="math/tex;mode=display">
\begin{eqnarray}y(t)=x(t)h(t)\end{eqnarray}</script><blockquote><p>系统响应函数：在信号与系统或电路理论等学科中，冲激响应(或叫脉冲响应)一般是指系统在输入为单位冲激函数时的输出（响应）。对于连续时间系统来说，冲激响应一般用函数h(t)来表示。在频域可以用H(ω)来描述，在复数域可以用H(s) 来描述。三者的关系也是一一对应的。</p></blockquote><p>然而，实际的情况是，系统的输出不仅与系统在$t$时刻的响应有关，还与它在$t$时刻之前的响应有关，不过系统有个衰减过程，所以$\tau(小于t)$时刻的输入对$t$时刻输出的影响通常可以表示为$x(\tau)h(t-\tau)$，这个过程可能是离散的，也可能是连续的，<strong>所以$t$时刻的输出应该为$t$时刻之前系统响应函数在各个时刻响应的叠加，这就是卷积。从数学的角度来看，所谓卷积就是两个函数乘积的积分(连续函数)或者求和(离散函数)。从运算来看，所谓卷积，其实就是把一个函数卷(翻)过来，然后与另一个函数求内积。</strong>在连续情况下，用数学公式表示就是积分了。</p><script type="math/tex;mode=display">
\begin{eqnarray}y(t)=x(t)*h(t)=\int_0^t x(\tau)h(t-\tau)d\tau\end{eqnarray}</script><p>若为离散情况，那么就是级数了：</p><script type="math/tex;mode=display">
\begin{eqnarray}y\left[ n \right] =x(n)*h(n) = \sum\limits_{k = 0}^{n} {x\left[ k \right]h\left[ {n - k} \right]}\end{eqnarray}</script><p>上述公式是在因果系统并且在因果信号下推导出来的，所以积分范围为$(0,t)$。对该公式进行推广。得到下式：</p><p><strong>连续情况下：</strong></p><script type="math/tex;mode=display">
\begin{eqnarray}y(t)=x(t)*h(t)=\int_{-\infty}^\infty x(\tau)h(t-\tau)d\tau\end{eqnarray}</script><p><strong>离散情况下：</strong></p><script type="math/tex;mode=display">
\begin{eqnarray}y\left[ n \right] =x(n)*h(n)= \sum\limits_{k = - \infty }^{ + \infty } {x\left[ k \right]h\left[ {n - k} \right]}\end{eqnarray}</script><p>式$({4})$和$({5})$分别为连续和离散情况下的卷积。</p><h4 id="幽默笑话——谈卷积的物理意义"><a href="#幽默笑话——谈卷积的物理意义" class="headerlink" title="幽默笑话——谈卷积的物理意义"></a>幽默笑话——谈卷积的物理意义</h4><p>有一个七品县令，喜欢用打板子来惩戒那些市井无赖，而且有个惯例：如果没犯大罪，只打一板，释放回家，以示爱民如子。</p><p>有一个无赖，想出人头地却没啥指望，心想：既然扬不了善名，出恶名也成啊。怎么出恶名？炒作呗！怎么炒作？找名人呀！他自然想到了他的行政长官——县令。</p><p>无赖于是光天化日之下，站在县衙门前撒了一泡尿，后果是可想而知地，自然被请进大堂挨了一板子，然后昂首挺胸回家，躺了一天，嘿！身上啥事也没有！第二天如法炮制，全然不顾行政长管的仁慈和衙门的体面，第三天、第四天……每天去县衙门领一个板子回来，还喜气洋洋地，坚持一个月之久！这无赖的名气已经和衙门口的臭气一样，传遍八方了！</p><p>县令大人噤着鼻子，呆呆地盯着案子上的惊堂木，拧着眉头思考一个问题：这三十个大板子怎么不好使捏？……想当初，本老爷金榜题名时，数学可是得了满分，今天好歹要解决这个问题：</p><p>——人（系统！）挨板子（脉冲！）以后，会有什么表现（输出！）？</p><p>——费话，疼呗！</p><p>——我问的是：会有什么表现？</p><p>——看疼到啥程度。像这无赖的体格，每天挨一个板子啥事都不会有，连哼一下都不可能，你也看到他那得意洋洋的嘴脸了（输出0）；如果一次连揍他十个板子，他可能会皱皱眉头，咬咬牙，硬挺着不哼（输出1）；揍到二十个板子，他会疼得脸部扭曲，象猪似地哼哼（输出3）；揍到三十个板子，他可能会象驴似地嚎叫，一把鼻涕一把泪地求你饶他一命（输出5）；揍到四十个板子，他会大小便失禁，勉强哼出声来（输出1）；揍到五十个板子，他连哼一下都不可能（输出0）——死啦！</p><p>县令铺开坐标纸，以打板子的个数作为X轴，以哼哼的程度（输出）为Y轴，绘制了一条曲线：</p><p>——呜呼呀！这曲线像一座高山，弄不懂。为啥那个无赖连挨了三十天大板却不喊绕命呀？</p><p>—— 呵呵，你打一次的时间间隔（$Δτ$=24小时）太长了，所以那个无赖承受的痛苦程度一天一利索，没有叠加，始终是一个常数；如果缩短打板子的时间间隔（建议$Δτ$=0.5秒），那他的痛苦程度可就迅速叠加了；等到这无赖挨三十个大板（$t=30$）时，痛苦程度达到了他能喊叫的极限，会收到最好的惩戒效果，再多打就显示不出您的仁慈了。</p><p>——还是不太明白，时间间隔小，为什么痛苦程度会叠加呢？</p><p>——这与人（线性时不变系统）对板子（脉冲、输入、激励）的响应有关。什么是响应？人挨一个板子后，疼痛的感觉会在一天（假设的，因人而异）内慢慢消失（衰减），而不可能突然消失。这样一来，只要打板子的时间间隔很小，每一个板子引起的疼痛都来不及完全衰减，都会对最终的痛苦程度有不同的贡献：</p><script type="math/tex;mode=display">
\begin{eqnarray}t个大板子造成的痛苦程度=Σ(第τ个大板子引起的痛苦*衰减系数)\end{eqnarray}</script><p>衰减系数是$（t-τ）$的函数，仔细品味</p><p>数学表达为：$y(t)=∫T(τ)H(t-τ)dτ$</p><p>——拿人的痛苦来说卷积的事，太残忍了。除了人以外，其他事物也符合这条规律吗？</p><p>——呵呵，县令大人毕竟仁慈。其实除人之外，很多事情也遵循此道。好好想一想，铁丝为什么弯曲一次不折，快速弯曲多次却会轻易折掉呢？</p><p>——恩，一时还弄不清，容本官慢慢想来——但有一点是明确地——来人啊，将撒尿的那个无赖抓来，狠打40大板！</p><p>也可以这样理解：$T(τ)$即第$τ$个板子，$H(t-τ)$就是第$τ$个板子引起的痛苦到$t$时刻的痛苦程度，所有板子加起来就是$∫T(τ)H(t-τ)dτ$</p><h3 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h3><p>卷积是一种积分运算，它可以用来描述线性时不变系统的输入和输出的关系：即输出可以通过输入和一个表征系统特性的函数（冲激响应函数）进行卷积运算得到。</p><h4 id="连续"><a href="#连续" class="headerlink" title="连续"></a>连续</h4><h5 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h5><script type="math/tex;mode=display">
\begin{eqnarray}y(t)=x(t)*h(t)=\int x(\tau)h(t-\tau)d\tau\end{eqnarray}</script><p>先把函数$x(t),h(t)$中的$t$变为$\tau$，<strong>向右</strong>移动距离$t$，然后相对于原点对称，然后两个函数相乘再积分，就得到了在$t$处的输出。对每个$t$值重复上述过程，就得到了输出曲线。</p><h5 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h5><script type="math/tex;mode=display">
\begin{eqnarray}h(x,y)=f(x,y)*g(x,y)=\int \int f(u,v)g(x-u,y-v)dudv\end{eqnarray}</script><p>先将$f(x,y),g(x,y)$中的$x,y$替换为$u,v$，将$g(u,v)$沿着$u$轴<strong>向上</strong>平移$x$，沿着$v$轴<strong>向上</strong>平移$y$，然后将$g(u,v)$绕其<strong>原点旋转</strong>180度。然后两个函数相乘后求积分，得到一个点$(x,y)$处的输出。</p><h4 id="离散"><a href="#离散" class="headerlink" title="离散"></a>离散</h4><h5 id="一维卷积-1"><a href="#一维卷积-1" class="headerlink" title="一维卷积"></a>一维卷积</h5><script type="math/tex;mode=display">
\begin{eqnarray}y\left[ n \right] =x\left[n\right]*h\left[n\right]= \sum\limits_{k = - \infty }^{ + \infty } {x\left[ k \right]h\left[ {n - k} \right]}\end{eqnarray}</script><p>先把函数$x\left[ n\right],h\left[ n\right]$中的$n$变为$k$，<strong>向右</strong>移动距离$n$，然后相对于原点对称，然后两个函数相乘再求和，就得到了在$n$处的输出。对每个$n$值重复上述过程，就得到了输出曲线。</p><h5 id="二维卷积-1"><a href="#二维卷积-1" class="headerlink" title="二维卷积"></a>二维卷积</h5><script type="math/tex;mode=display">
\begin{eqnarray}y\left[m,n \right] =x\left[ m,n\right]*h\left[ m,n\right]= \sum\limits_{j,k = - \infty }^{ + \infty } {x\left[j,k \right]h\left[m-j,n - k\right]}\end{eqnarray}</script><p>先将$x\left[m,n\right],h\left[m,n\right]$中的$m,n$替换为$j,k$，将$h\left[j,k\right]$沿着$j$轴<strong>向上</strong>平移$m$，沿着$k$轴<strong>向上</strong>平移$n$，然后将$g\left[j,k\right]$绕其<strong>原点旋转180度</strong>。然后两个函数相乘后求和，得到一个点处的输出。</p><h3 id="卷积在具体学科中的应用"><a href="#卷积在具体学科中的应用" class="headerlink" title="卷积在具体学科中的应用"></a>卷积在具体学科中的应用</h3><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><p>用一个模板和一幅图像进行卷积，对于图像上的一个点，让模板的原点和该点重合，然后模板上的点和图像上对应的点相乘，然后各点的积相加，就得到了该点的卷积值。对图像上的每个点都这样处理。<strong>由于大多数模板都是对称的，所以模板不旋转。卷积是一种积分运算，用来求两个曲线重叠区域面积。可以看作加权求和，可以用来消除噪声、特征增强。</strong></p><p>把一个点的像素值用它周围的点的像素值的加权平均代替。</p><p>卷积是一种线性运算，图像处理中常见的mask运算都是卷积，广泛应用于图像滤波。</p><p>卷积在数据处理中用来平滑，卷积有平滑效应和展宽效应.</p><h4 id="电磁学"><a href="#电磁学" class="headerlink" title="电磁学"></a>电磁学</h4><p>卷积法的原理是根据线性定常电路的性质(齐次性、叠加性、时不变性、积分性等),借助电路的单位冲激响应$h(t)$,求解系统响应的工具，系统的激励一般都可以表示为冲击函数和激励的函数的卷积，而卷积为高等数学中的积分概念。概念中冲击函数的幅度是由每个矩形微元的面积决定的。</p><p><strong>卷积关系最重要的一种情况，就是在信号与线性系统或数字信号处理中的卷积定理。利用该定理，可以将时间域或空间域中的卷积运算等价为频率域的相乘运算，从而利用FFT等快速算法，实现有效的计算，节省运算代价。</strong></p><h4 id="信号处理"><a href="#信号处理" class="headerlink" title="信号处理"></a>信号处理</h4><p>1）卷积实质上是对信号进行滤波(可以从频域的角度上考虑)；<br>2）卷积就是用冲击函数表示激励函数，然后根据冲击响应求解系统的零状态响应。卷积是求和（积分）。对于线性时不变的系统，输入可以分解成很多强度不同的冲激的和的形式（对于时域就是积分），那么输出也就是这些冲激分别作用到系统产生的响应的和（或者积分）。所以卷积的物理意义就是表达了时域中输入，系统冲激响应，以及输出之间的关系。</p><h4 id="多项式"><a href="#多项式" class="headerlink" title="多项式"></a>多项式</h4><p>信号处理中的一个重要运算是卷积.初学卷积的时候,往往是在连续的情形,两个函数$f(x),g(x)$的卷积,是$∫f(u)g(x-u)du$。当然，证明卷积的一些性质并不困难，比如交换，结合等等，但是对于卷积运算的来处，初学者就不甚了了。</p><p>其实，从离散的情形看卷积，或许更加清楚，对于两个序列$f[n],g[n]$,一般可以将其卷积定义为$s[x]= ∑_{k=1}^nf[k]g[x-k]$。</p><p>卷积的一个典型例子,其实就是初中就学过的多项式相乘的运算。</p><p>比如$(x<em>x+3</em>x+2)(2*x+5)$一般计算顺序如下：</p><script type="math/tex;mode=display">
(x*x+3*x+2)(2*x+5)</script><script type="math/tex;mode=display">
= (x*x+3*x+2)*2*x+(x*x+3*x+2)*5</script><script type="math/tex;mode=display">
= 2*x*x*x+3*2*x*x+2*2*x+ 5*x*x+3*5*x+10</script><p>然后合并同类项的系数,</p><script type="math/tex;mode=display">
2x*x*x</script><script type="math/tex;mode=display">
(3*2+1*5)x*x</script><script type="math/tex;mode=display">
(2*2+3*5)x</script><script type="math/tex;mode=display">
2*5</script><p>最终结果为：</p><script type="math/tex;mode=display">
2*x*x*x+11*x*x+19*x+10</script><p>实际上，从线性代数可以知道，多项式构成一个向量空间，其<strong>基底</strong>可选为${1,x,x<em>x,x</em>x<em>x,…}$如此，则任何多项式均可与无穷维空间中的一个坐标向量相对应，如$(x</em>x+3<em>x+2)$对应于$(1,3,2)$，$(2</em>x+5)$对应于$(2,5)$。线性空间中没有定义两个向量间的卷积运算，而只有加法、数乘两种运算。而实际上，多项式的乘法，就无法在线性空间中说明，可见线性空间的理论多么局限了。但如果按照我们上面对向量卷积的定义来处理坐标向量，则有$(1,3,2)*(2,5)=(2,11,19,10)$。</p><p>回到多项式的表示上来，$(x<em>x+3</em>x+2)(2<em>x+5)=2</em>x<em>x</em>x+11<em>x</em>x+19<em>x+10$，结果跟我们用传统办法得到的是完全一样的。换句话，<strong>多项式相乘，相当于系数向量的卷积。</strong> 其实道理也很简单，卷积运算实际上是分别求 $x</em>x<em>x ,x</em>x,x,1$的系数，也就是说，他把加法和乘积杂合在一起做了。(传统的办法是先做乘法，然后在合并同类项的时候才作加法)以$x<em>x$的系数为例，得到$x</em>x$，或者是用$x*x$乘5，或者是用$3x$乘$2x$,也就是<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 3 2</span><br><span class="line">5 2 _</span><br><span class="line">5 + 6 = 11</span><br></pre></td></tr></table></figure><p></p><p>其实，这正是向量的内积。如此则，卷积运算，可以看作是一串内积运算。既然是一串内积运算，则我们可以试图用矩阵表示上述过程。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ 2 3 1 0 0 0]</span><br><span class="line">[ 0 2 3 1 0 0]==A</span><br><span class="line">[ 0 0 2 3 1 0]</span><br><span class="line">[ 0 0 0 2 3 1]</span><br><span class="line"></span><br><span class="line">[0 0 2 5 0 0]&apos; == x</span><br><span class="line"></span><br><span class="line">b= Ax=[ 2 11 19 10]&apos;</span><br></pre></td></tr></table></figure><p></p><p>采用行的观点看$Ax$，则b的每行都是一个内积。A的每一行都是序列<code>[2 3 1]</code>的一个移动位置。显然,在这个特定的背景下，我们知道，<strong>卷积满足交换、结合</strong>等定律。因为,众所周知的<strong>多项式的乘法满足交换律,结合律</strong>。在一般情形下，其实也成立。</p><p>在这里，我们发现多项式除了构成特定的线性空间外，基与基之间还存在某种特殊的联系，正是这种联系，给予多项式空间以特殊的性质。</p><p>在学向量的时候，一般都会举这个例子，甲有三个苹果，5个橘子，乙有5个苹果，三个橘子，则共有几个苹果，橘子。老师反复告诫，橘子就是橘子，苹果就是苹果，可不能混在一起。所以有(3,5)+(5,3)=(8,8).是的，橘子和苹果无论怎么加，都不会出什么问题的，但是，如果考虑橘子乘橘子，或者橘子乘苹果，这问题就不大容易说清了。</p><p>又如复数，如果仅仅定义复数为数对$(a,b)$，仅仅在线性空间的层面看待C2，那就未免太简单了。实际上，只要加上一条$(a,b)<em>(c,d)=(ac-bd,ad+bc)$。则情况马上改观，复变函数的内容多么丰富多彩，是众所周知的。另外，回想信号处理里面的一条基本定理：<em>*频率域的乘积,相当于时域或空域信号的卷积</em></em>。恰好和这里的情形完全对等。这后面存在什么样的隐态联系,需要继续参详.</p><p>从这里看,高等的卷积运算其实不过是一种初等的运算的抽象而已。中学学过的数学里面，其实还蕴涵着许多高深的内容（比如交换代数）。温故而知新,斯言不谬。其实这道理一点也不复杂，人类繁衍了多少万年了，但过去n多年，人们只知道男女媾精，乃能繁衍后代。精子，卵子的发现，生殖机制的研究，也就是最近多少年的事情。</p><p>孔子说，道在人伦日用中，看来我们应该多用审视的眼光看待周围，乃至自身，才能知其然，而知其所以然。</p><h2 id="从基变换角度理解傅里叶变换"><a href="#从基变换角度理解傅里叶变换" class="headerlink" title="从基变换角度理解傅里叶变换"></a>从基变换角度理解傅里叶变换</h2><p>之前学的时候，学的不知道所以然，虽然最后信号与系统考的也不低。但是因为不了解本质，没有举一反三，所以所学的东西考试完全部还给老师了。后来看到了这篇文章<a href="https://zhuanlan.zhihu.com/p/19763358" target="_blank" rel="noopener">傅里叶分析之掐死教程（完整版）更新于2014.06.06</a>，感觉三观被颠覆了。下面，我们首先给出傅里叶分析涉及到的公式，然后开始作者的表演。</p><p>在开始作者的表演之前，我们先介绍一下理论知识与推导，一来已经是研究生了，生动形象的理解之后还要知道公式理论。二来自己学的时候也懵逼，当做做了一个笔记吧。</p><p>通常我们解释傅里叶变换是从“时域”到“频域”变换的角度理解的，但是如果单纯这么理解，则拉普拉斯变换的“频域”的物理意义很难解释清楚。为了更好的理解“各种”变换，我们利用数学工具从<strong>基变换或者分解</strong>的角度去解释傅里叶变换，顺便证明傅里叶级数和傅里叶变换。</p><h3 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h3><h4 id="域"><a href="#域" class="headerlink" title="域"></a>域</h4><p>域是代数中一个非常重要的概念，是一种特殊的环（交换除环）。介绍域是为介绍线性空间做准备，而域的准确概念在本文中并不重要。因此为了方便，大家可以把域理解为一个<strong>可以做加减乘除的集合</strong>。也就是说，域由一个集合和四则运算构成，这个集合内的元素两两间可以做这四种运算（除了除以 0），结果依然在这个集合里。</p><p>常见的域有有理数域$\mathbb{Q}$、实数域$\mathbb{R}$和复数域$\mathbb{C}$ ，它们分别是全体有理数、实数、复数关于数的加减乘除构成的。</p><h4 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h4><p>线性空间的概念是建立在域的基础上的。假设$F$是一个域，而$V$是一个集合。如果$V$内的元素之间可以做<strong>加法</strong>（也就是说两个元素做加法的结果仍然在集合内），$V$和$F$可以做<strong>数乘</strong>（和域内的乘法不同，数乘是说$V$中的一个元素和$F$中的一个元素做数乘结果仍在$V$内。数乘可以理解为一个二元函数，它把$F$内的一个元素和$V$内的一个元素映射到$V$内的一个元素），这个$V$就叫做<strong>域$F$)上</strong>的<strong>一个线性空间</strong>。（$V$中的元素可以称为<strong>向量</strong>）</p><p>当然了，上边所说的加法和乘法还要满足几个性质，比如加法要构成阿贝尔群等等。但为了方便，这里不详细说明，只是举几个线性空间的例子。</p><p>比如对$\forall n \in \mathbb{Z},\mathbb{R}^n$（n 维欧式空间）是实数域$\mathbb{R}$上的线性空间。因为空间中任意两个向量做加法（按照一般向量加法）或用一个实数乘一个向量，结果仍然是一个 n 维欧式空间里的向量。</p><p>而定义在闭区间$[a,b]$上的连续函数也构成一个$\mathbb{R}$上的线性空间。因为任意两个连续函数的和仍然是原来区间上的连续函数，一个连续函数乘一个实数也是连续函数。</p><h4 id="基与维数"><a href="#基与维数" class="headerlink" title="基与维数"></a>基与维数</h4><p>我们在$V$中找到了一些线性无关的向量，这些向量可以通过线性组合构成无数其他向量。细心的你可能已经发现了：这些向量的所有线性组合构成的向量就构成了一个线性空间。这个空间中的元素一定属于$V$，因此它叫做$V$的一个子空间。</p><p>如果构成的线性空间恰好是$V$，我们称这些向量是$V$的<strong>一组基</strong>，而向量的个数叫做$V$的<strong>维数</strong>。如果$V$中有无限多个线性无关的向量，它就是无限维的。</p><p>在这里我们不加证明地给出：<strong>任何非零线性空间均有基。</strong></p><h4 id="内积"><a href="#内积" class="headerlink" title="内积"></a>内积</h4><p>之前我们介绍线性空间包括了两种运算：第一种是空间内向量的加法，第二种是空间内向量与域中元素的数乘。而内积则是空间内两向量的运算，或者我们可以理解为把空间中两个元素映射为域中一个元素的一个二元函数。我们规定这个函数应该满足一些性质。</p><p>首先我们规定，内积是双线性的。在介绍双线性之前，我们先讲讲线性。线性就是说一个一元函数把线性组合映射为线性组合，即$f(\lambda_1\alpha_1+\cdots+\lambda_r\alpha_r)=\lambda_1f(\alpha_1)+\cdots+\lambda_rf(\alpha_r)$恒成立.</p><p>而双线性是针对二元函数的，它是说固定其中任何一个变量后，这个一元函数都是线性的。</p><p>除了双线性，内积还必须是<strong>正定的</strong>。即一个向量和它自己的内积必须是非负的，而非零向量和它自己的内积必须是正的。</p><p>容易验证，对于欧式空间中的向量点乘是满足这个性质的。而我们前边提到的闭区间上的连续函数，可以定义其内积为$\left<f ,g\right>=\int_a^b f(x)g(x)\mathrm{d}x$。</f></p><p>最后，我们定义两个向量<strong>正交</strong>是指它们的内积为零。</p><p>注意：<strong>一组线性无关的基不一定正交</strong>，例如在二维平面上$\left( \begin{array} {ccc}0\\1\end{array} \right)$和$\left( \begin{array} {ccc}1\\1\end{array} \right)$组成一对基（二维平面上所有的向量均可由这对基线性组合得到），但是并不正交。而$\left( \begin{array} {ccc}0\\1\end{array} \right)$和$\left( \begin{array} {ccc}1\\0\end{array} \right)$组成一对正交基（二维平面所有的向量均可以由这对基线性组合得到）。而标准正交基不仅要求基与基之间相互正交，还要求基内的每一个向量均是单位向量。</p><h3 id="傅里叶基和傅里叶级数"><a href="#傅里叶基和傅里叶级数" class="headerlink" title="傅里叶基和傅里叶级数"></a>傅里叶基和傅里叶级数</h3><h4 id="矢量表示为正交矢量集"><a href="#矢量表示为正交矢量集" class="headerlink" title="矢量表示为正交矢量集"></a>矢量表示为正交矢量集</h4><p>二维欧式空间$\mathbb{R}^2$中，我取一组<strong>正交基</strong>$\vec{V_1}=(1,0),\vec{V_2}=(0,2)$，那么向量$\vec{V}=(3,4)$写成这组基的线性组合时系数分别是多少呢？</p><p>将这个问题抽象成一个数学问题：</p><script type="math/tex;mode=display">
\begin{eqnarray}\vec{V}=C_1\vec{V_1}+C_2\vec{V_2}\end{eqnarray}</script><p>两边同时右点乘矢量$\vec{V_1}$得到</p><script type="math/tex;mode=display">
\begin{eqnarray}\vec{V}\cdot \vec{V_1}=\left[ C_1\vec{V_1} + C_2 \vec{V_2} \right] \cdot \vec{V_1} = C_1\vec{V_1}\cdot \vec{V_1} + C_2\vec{V_2}\cdot \vec{V_1} = C_1 |\vec{V_1}|^2 = C_1 V_1^2\end{eqnarray}</script><p>得到标量系数</p><script type="math/tex;mode=display">
\begin{eqnarray}C_1=\frac{\vec{V}\cdot \vec{V_1}}{V_1^2}\end{eqnarray}</script><p>其中，$\cdot$为内积</p><blockquote><p>正是因为$\vec{V_1}$和$\vec{V_2}$正交所以才能这样算。从这也可以看出正交的好处：一可以简化计算，二可以直接计算出向量在正交基组成的空间中的系数。这些好处对于傅里叶级数和傅里叶变换特别重要。</p></blockquote><p>同理可得</p><script type="math/tex;mode=display">
\begin{eqnarray}C_2=\frac{\vec{V}\cdot \vec{V_2}}{V_2^2}\end{eqnarray}</script><p>代入可得答案：</p><script type="math/tex;mode=display">
\begin{eqnarray}\frac{\left<(3,4),(1,0)\right>}{\left<(1,0),(1,0)\right>}=3\end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}\frac{\left<(3,4),(0,2)\right>}{\left<(0,2),(0,2)\right>}=\frac{8}{4}=2\end{eqnarray}</script><p><strong>总结：</strong>将原空间中的元素表示成目标空间完备正交基的线性组合时(可以理解为换基也可以理解为分解)，可以使用内积计算正交基的系数。计算方法为：将原空间中的元素和目标空间一个正交基做内积，然后除以目标空间该正交基自身的内积(相当于模)，得到该正交基的系数。另外，从计算方法可以看出，广义内积可以理解为一种广义意义上的投影。</p><h4 id="三角函数的正交性"><a href="#三角函数的正交性" class="headerlink" title="三角函数的正交性"></a>三角函数的正交性</h4><p>这是为下一步傅里叶级数展开时所用积分的准备知识。一个三角函数系：$\left\{\sin nw_0t,\cos nw_0t \right\} (n=0,1,2,…;w_0=\frac{2\pi}{T})$ 如果这一堆函数（包括常数1）中任何两个不同函数的乘积在区间$[t_0, t_0+T]$上的积分等于零，就说三角函数系在区间$[t_0, t_0+T]$上正交，即有如下式子：</p><script type="math/tex;mode=display">
\begin{eqnarray}\int_{t_0}^{t_0+T}cos\ nw_0tdt=0\quad\quad\quad(n=1,2,3,...)\\ \int_{t_0}^{t_0+T}sin\ nw_0tdt=0\quad\quad\quad(n=1,2,3,...)\\ \int_{t_0}^{t_0+T}sin\ kw_0t\cdot cos\ nw_0tdt=0\quad\quad\quad(k,n=1,2,3,...)\\ \int_{t_0}^{t_0+T}cos\ kw_0t\cdot cos\ nw_0tdt=0\quad\quad\quad(k,n=1,2,3,...;k\ne n)\\ \int_{t_0}^{t_0+T}sin\ kw_0t\cdot sin\ nw_0tdt=0\quad\quad\quad(k,n=1,2,3,...;k\ne n)\end{eqnarray}</script><p>以上各式在区间$[t_0, t_0+T]$的定积分均为0，前两个式子可视为三角函数$cos$和$sin$与１相乘的积分；后面几个式子则为$sin$和$cos$的不同组合相乘的积分式。除了这5个式子外，不可能再有其他的组合了。注意，后两个式子中，$k$不能等于$n$，否则就不属于“三角函数系中任意两个不同函数”的定义了，变成同一函数的平方了。但第3个式中，$k$与$n$可以相等，相等时也是二个不同函数。下面通过计算第4式的定积分来验证其正确性，第4式中二函数相乘可以写成：</p><script type="math/tex;mode=display">
\begin{eqnarray}cos\ kw_0t\cdot cos\ nw_0t=\frac{1}{2}[cos(k+n)w_0t+cos(k-n)w_0t]\\ sin\ kw_0t\cdot sin\ nw_0t=-\frac{1}{2}[cos(k+n)w_0t-cos(k-n)w_0t]\\ sin\ kw_0t\cdot cos\ nw_0t=\frac{1}{2}[sin(k+n)w_0t+sin(k-n)w_0t]\end{eqnarray}</script><p>当 $k\ne n$时，有：</p><script type="math/tex;mode=display">
\begin{eqnarray}\begin{aligned}\int_{t_0}^{t_0+T}cos\ kw_0t \cdot cos\ nw_0tdt=&\frac{1}{2}\int_{t_0}^{t_0+T}[cos(k+n)w_0t + cos(k-n)w_0t]dt\\ =&\frac{1}{2}[\frac{sin(k+n)w_0t}{k+n}\ +\ \frac{sin(k-n)w_0t}{k-n}]|_{t_0}^{t_0+T}\\ \overset{\text{sin函数周期内积分为0}}{=}&\frac{1}{2}[0+0]=0\end{aligned}\end{eqnarray}</script><p>可见在指定$[t_0, t_0+T]$的区间里，该式的定积分为0。其他式也可逐一验证。</p><h4 id="三角型傅里叶级数"><a href="#三角型傅里叶级数" class="headerlink" title="三角型傅里叶级数"></a>三角型傅里叶级数</h4><p>以$T$为周期的连续周期信号$f(t)$，如果满足<strong>狄利克雷条件</strong>：</p><ul><li>在一个周期内只有有限个间断点；</li><li>在一个周期内有有限个极值点；</li><li>在一个周期内函数绝对可积，即</li></ul><script type="math/tex;mode=display">
\begin{eqnarray}\int_{t_0}^{t_0+T} |f(t)| dt < \infty\end{eqnarray}</script><p>则可展开为<strong>完备的正交三角函数基</strong>$\left\{\sin nw_0t,\cos nw_0t \right\} (n=0,1,2,…)$的线性组合</p><script type="math/tex;mode=display">
\begin{eqnarray}f(t)=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left(a_n\cos nw_0t + b_n \sin nw_0t\right), n=0,1,2,...\end{eqnarray}</script><p><strong>该式子称为正弦余弦形式的三角型傅里叶级数</strong>。其中，$w_0=\frac{2\pi}{T}$（推导过程中可以当做为常数），$a_0、a_n、b_n$为傅里叶系数。该式子是关于自变量$t$的函数，$nw_0$可以看做为常数。根据上面提到的矢量表示为正交矢量集方法，我们需要首先定义内积，利用内积的性质求傅里叶系数。</p><p>我们回忆一下刚才讲的正交的概念：两个向量的内积为0。如果一个空间有一组基两两正交，那么它就叫做一组<strong>正交基</strong>。</p><p>平方可积空间$L^2[a,b]$定义：</p><script type="math/tex;mode=display">
\begin{eqnarray}L^2[a,b] = \left\{ x(t) \mid \int_a^b \mid x(t)^2 \mid dt < \infty \right\}\end{eqnarray}</script><p>其中，$x(t)$称为平方可积函数。</p><p>我们可以证明，定义在$[t_0,t_0+T]$上的所有平方可积的函数构成线性空间（内积是在线性空间上定义的），规定其内积（正交是由内积引出的）为</p><script type="math/tex;mode=display">
\begin{eqnarray}\left\langle f,g \right\rangle=\int_{t_0}^{t_0+T}f(t)\overline{g(t)}dt\end{eqnarray}</script><blockquote><p><strong>注意：</strong>对于三角型傅里叶级数，计算内积的时候均是在时域计算的，所以不需要考虑共轭。但是对于指数型傅里叶级数，需要在复频域计算内积，所以需要考虑共轭。另外，对于傅里叶变换，因为积分区间为$[-\infty,\infty]$，所以不能保证函数平方可积，也就不能使用该定义。</p></blockquote><p>现在我们想计算每一个<strong>基的系数</strong>。按照<code>矢量表示为正交矢量集</code>小节总结的计算系数的方法，试着计算一下它和其中一项$\cos nw_0t$的<strong>内积</strong>，比如</p><script type="math/tex;mode=display">
\begin{eqnarray} \begin{split} \int_{t_0}^{t_0+T}f(t)\cos nw_0t\mathrm{d}t&=& \frac{a_0}{2}\int_{t_0}^{t_0+T}\cos nw_0t\mathrm{d}t\\&+&\sum_{m=1}^{\infty}a_m\int_{t_0}^{t_0+T}\cos mw_0t \cos nw_0t\mathrm{d}t\\&+&\sum_{m=1}^{\infty}b_m\int_{t_0}^{t_0+T}\sin mw_0t \cos nw_0t\mathrm{d}t \end{split}\end{eqnarray}</script><p>不过注意到不同的基向量都是正交的（这就是正交基向量的优点），所以非零项只有右边第二项$m=n$的情况，所以</p><script type="math/tex;mode=display">
\begin{eqnarray}a_n\int_{t_0}^{t_0+T}\cos ^2 nw_0t\mathrm{d}t=\frac{T}{2}a_n\end{eqnarray}</script><p><strong>证明：</strong></p><script type="math/tex;mode=display">
\begin{eqnarray}\int_{t_0}^{t_0+T}\cos ^2 nw_0t\mathrm{d}t = \int_{t_0}^{t_0+T}\frac{cos2nw_0t+1}{2}dt \\=
\frac{1}{2} \int_{t_0}^{t_0+T} \cos2nw_0t dt + \frac{1}{2} \int_{t_0}^{t_0+T} dt \\=
\frac{1}{2\times 2nw_0} sin 2nw_0t \mid _{t_0}^{t_0+T} + \frac{T}{2} \\ = \frac{1}{4nw_0}\left(sin2nw_0(t_0+T) - sin2nw_0t_0\right)+ \frac{T}{2}\end{eqnarray}</script><p>因为$w_0=\frac{2\pi}{T}$，所以上式第一项括号内可以继续化简如下：</p><script type="math/tex;mode=display">
\begin{eqnarray}sin2nw_0(t_0+T) - sin2nw_0t_0 \\= sin2nw_0t_0cos2nw_0T+cos2nw_0t_0sin2nw_0T - sin2nw_0t_0 \\=
sin2nw_0t- sin2nw_0t = 0\end{eqnarray}</script><p>所以$a_n\int_{t_0}^{t_0+T}\cos ^2 nw_0t\mathrm{d}t$中积分项为$\frac{T}{2}$，所以得证。</p><p>好简单啊！也就是说，</p><script type="math/tex;mode=display">
\begin{eqnarray}a_n=\frac{2}{T}\int_{t_0}^{t_0+T}f(t)\cos(nw_0t)\mathrm{d}t, n=0,1,2,..,\end{eqnarray}</script><p>当$n=0$的时候，带入$({31})$积分式，可以得到</p><script type="math/tex;mode=display">
\begin{eqnarray}a_0T=\int_{t_0}^{t_0+T}f(t)\mathrm{d}t\end{eqnarray}</script><p>所以$a_0$的系数为$\frac{1}{T}\int_{t_0}^{t_0+T}f(t)\mathrm{d}t$。所以式子$({30})$中$a_0$的系数乘上$\frac{1}{2}$，$a_0$的系数变成了$\frac{2}{T}\int_{t_0}^{t_0+T}f(t)\mathrm{d}t$，与$n&gt;0$的时候，$a_n$的表达式统一。</p><p>同理，当我们使用$sinnw_0t$内积的时候，可以得到</p><script type="math/tex;mode=display">
\begin{eqnarray}b_n=\frac{2}{T}\int_{t_0}^{t_0+T}f(t)\sin(nw_0t)\mathrm{d}t, n=1,2,...\end{eqnarray}</script><p>显然，傅里叶系数$a_n$和$b_n$都是$nw_0$的函数，<strong>$a_n$是$n$或者$nw_0$的偶函数，$b_n$是$n$或者$nw_0$的偶函数。</strong></p><p><strong>式子$({27})$、$({39})$和$({41})$组成了我们的傅里叶级数。</strong></p><p>我将$a_n$的表达式重新写成下面的形式：</p><script type="math/tex;mode=display">
\begin{eqnarray}a_n=\frac{\left \langle f(t),\cos nw_0t\right \rangle}{\left< \cos nw_0t,\cos nw_0t\right>}\end{eqnarray}</script><p>类比一下上一小节在欧几里得空间，将矢量表示为正交矢量集的方法，完全一样。</p><p>结合广义内积可以看成一种广义意义上的投影，从式子$({27})$可以看出，<strong>$a_n$和$b_n$可以理解为为$f(t)$映射到$\left\{\sin nw_0t,\cos nw_0t \right\} (n=0,1,2,…)$完备正交基时的坐标，这也可以解释为何说$a_n$和$b_n$为时域到频域的转换（转换也就是说将一个坐标系转到另外一个坐标系）。在式子$({27})$中$\sin nw_0t,\cos nw_0t$均是关于$nw_0$的函数，正好对应三角函数的频率，所以称$\left\{\sin nw_0t,\cos nw_0t \right\} (n=0,1,2,…)$完备正交基组成的空间为频域。</strong></p><p>那么现在我们已经理解了，<strong>傅里叶基其实就是这个函数构成的空间中的一组正交基，而傅里叶级数就是把空间里的元素写成基的线性组合(分解)。</strong></p><h4 id="指数型傅里叶级数"><a href="#指数型傅里叶级数" class="headerlink" title="指数型傅里叶级数"></a>指数型傅里叶级数</h4><p>三角型傅里叶级数，物理含义明确，但运算不便，因而常用指数型的傅里叶级数。</p><p>以$T$为周期的周期信号$f(t)$的指数型傅里叶级数的定义为：</p><script type="math/tex;mode=display">
\begin{eqnarray}f(t)=\sum_{n=-\infty}^\infty F_n e^{jnw_0t}, w_0 = \frac{2\pi}{T}\end{eqnarray}</script><p>该函数是<strong>关于自变量$t$的函数，</strong>$n、w_0、T$为已知量。其中，</p><script type="math/tex;mode=display">
\begin{eqnarray}F_n = \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-jnw_0t}dt, n=0,\pm1,\pm2,...\end{eqnarray}</script><p>称为指数型傅里叶级数的系数，又称为傅里叶复系数。</p><p><strong>同时称上面两个式子构成一对周期信号傅里叶级数变换</strong>，记作$f(t)\leftrightarrow F_n$</p><p><strong>证明：</strong></p><script type="math/tex;mode=display">
\begin{eqnarray}e^{j\theta}=cos(\theta)+jsin(\theta)\end{eqnarray}</script><p>欧拉公式，可以变形为：</p><script type="math/tex;mode=display">
\begin{eqnarray}cos(\theta)=\frac{e^{j\theta}+e^{-j\theta}}{2}\end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}sin(\theta)=\frac{e^{j\theta}-e^{-j\theta}}{2j}=-j\cdot \frac{e^{j\theta}-e^{-j\theta}}{2}\end{eqnarray}</script><p>将 $sin(\theta) 、 cos(\theta)$ 代入傅里叶级数$({27})$求得：</p><script type="math/tex;mode=display">
\begin{eqnarray}f(t)=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}{[a_{n}\frac{e^{jn\omega t}+e^{-jn\omega t}}{2}-jb_{n}\frac{e^{jn\omega t}-e^{-jn\omega t}}{2}]} \\ =\frac{a_{0}}{2}+\sum_{n=1}^{\infty}{[\frac{a_{n}-jb_{n}}{2}e^{jn\omega t}+\frac{a_{n}+jb_{n}}{2}e^{-jn\omega t}]} \\\overset{\text{系数$a_n$和$b_n$奇偶性}}{=} \frac{a_{0}}{2}+ \sum_{n=1}^{\infty} \frac{a_{n}-jb_{n}}{2}e^{jn\omega t} + \sum_{n=-\infty}^{-1}\frac{a_{n}-jb_{n}}{2}e^{jn\omega t} \\= F_0 +\sum_{n=-\infty}^{-1} F_n e^{jnw_0t} + \sum_{n=1}^{\infty} F_n e^{jnw_0t} \\= \sum_{n=-\infty}^{\infty} F_n e^{jnw_0t}\end{eqnarray}</script><p>其中</p><script type="math/tex;mode=display">
\begin{eqnarray}F_0 = \frac{a_0}{2}=\frac{1}{T}\int_{t_0}^{t_0+T} f(t)dt \overset{\text{f(t)周期函数}}{=} \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-j0w_0t}dt \end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}F_n=\frac{a_{n}-jb_{n}}{2}=\frac{1}{T} \int_{t_0}^{t_0+T}f(t)(cosnw_0t-jsinnw_0t)dt \\ \overset{\text{周期性}}{=} \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-jnw_0t}dt \end{eqnarray}</script><p>综上，可得到</p><script type="math/tex;mode=display">
\begin{eqnarray}F_n = \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-jnw_0t}dt, n=0,\pm1,\pm2,...\end{eqnarray}</script><p>同样的，我也可以把这个式子写成内积的形式，如下：</p><script type="math/tex;mode=display">
\begin{eqnarray}F_n = \frac{\left \langle f(t),e^{jnw_0t} \right>}{\left< e^{jnw_0t},e^{jnw_0t}\right>} \end{eqnarray}</script><p><strong>证明：</strong><br>分子显而易见等于$\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)e^{-jnw_0t}dt$。</p><p>对于分母，</p><script type="math/tex;mode=display">
\begin{eqnarray}\left< e^{jnw_0t},e^{jnw_0t}\right> \\= \int_{-\frac{T}{2}}^{\frac{T}{2}}e^{jnw_0t}e^{-jnw_0t}dt \\= \int_{-\frac{T}{2}}^{\frac{T}{2}}(cos^2nw_0t + sin^2nw_0t)dt= \int_{-\frac{T}{2}}^{\frac{T}{2}} dt = T\end{eqnarray}</script><p>综合分子和分母，可得证。</p><p>结合广义内积可以看成一种广义意义上的投影，<strong>可以理解为$F_n$为$f(t)$映射到$e^{jnw_0t}$完备正交基时的坐标。</strong>但我们注意到这里的函数是$[t_0,t_0+T]$的，如果换一个区间，结果会如何呢？</p><h3 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h3><p><strong>傅里叶级数是针对周期信号而言的，但是对于非周期信号，可以看做是周期无限长的周期信号，从而可以使用极限求得傅里叶变换。</strong></p><p>在介绍傅里叶变换之前，我们首先引入<strong>频谱密度函数</strong>的概念。</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/2019-05-17-22-30-181558108099937.png" alt></p><p>从频谱分析的观点来看，当$T$增加时，基波频率$w_0$变小，离散谱线变密，频谱幅度变小，但频谱包络线的形状保持不变。在极限的情况下，当周期$T$趋于无穷大时，其谱线间隔与幅度将会趋于无穷小，原来有很多谱线组成的周期信号的离散频谱就会连成一片，称为面频谱，，并且从$\mid F_n \mid- nw_0$平面消失，如上图所示。这时，非周期信号的$\mid F_n \mid$成为无限小量而却再也看不见，傅里叶级数在此已经没有意义。为了描述非周期信号的频谱特性，引入频谱密度函数。</p><p>分析上图可知，当周期$T$趋近于无限大时，虽然相邻谱线的间隔$w_0$趋于无穷小而成为为变量$\Delta w$，但各频率分量的幅度$\mid F_n \mid$也都趋于无穷小，这些无穷小之间仍保持一定的比例关系。这给人以启示，可否利用相对频谱的概念来描述非周期信号的频谱特性呢？这就是频谱密度的概念。</p><script type="math/tex;mode=display">
\begin{eqnarray}F ( \mathrm { j } \omega ) \stackrel { \mathrm { def } } { = } \lim _ { \Delta \rightarrow 0 } \frac { F _ { n } } { \Delta f } = \lim _ { T \rightarrow \infty } T F _ { n }\end{eqnarray}</script><p>上式为频谱密度函数，从式中可以看出，$F(jw)$表示了单位频带$\Delta f$上的复振幅$F_n$分布情况，类似于物理学中质量线密度$\rho _ { l } = \lim _ { \Delta \rightarrow 0 } \frac { \Delta m } { \Delta l }$，$F(jw)$可理解为一种密度频谱，它表示了<strong>非周期信号在任意频率$w$处的频谱密度函数</strong>。</p><p>针对傅里叶级数的定义式，有</p><script type="math/tex;mode=display">
\begin{eqnarray}T\times F_n=\int_{-\frac{T}{2}}^{\frac{T}{2}}f_T(t)e^{-jnw_0t}dt\end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}f_T(t)=\sum_{n=-\infty}^{\infty}F_ne^{jnw_0t}\end{eqnarray}</script><p>当$T$去趋近于无限大时，上式中的各变量将作以下变化：$w_0=\frac{2\pi}{T}$趋于无穷小，$w_0\rightarrow \Delta w \rightarrow dw$，取其为$dw$；离散变量$nw_0$则变为连续变量$w$；$F_n$变为$\frac{dw}{2\pi}F(jw)$；离散和变为连续和（积分）。于是</p><script type="math/tex;mode=display">
\begin{eqnarray}F ( j \omega ) = \lim _ { T \rightarrow \infty } T \times F _ { n } = \lim _ { T \rightarrow \infty } \frac { T } { T } \int _ { - \frac { T } { 2 } } ^ { \frac { T } { 2 } } f _ { T } ( t ) \mathrm { e } ^ { - \mathrm jn \omega _ { 0 } t } \mathrm { d } t = \int _ { - \infty } ^ { \infty } f ( t ) \mathrm { e } ^ { - \mathrm { j } \omega t } \mathrm { d } t\end{eqnarray}</script><p>同理，</p><script type="math/tex;mode=display">
\begin{eqnarray}\begin{aligned} f ( t ) & = \lim _ { T \rightarrow \infty } f _ { T } ( t ) = \lim _ { T \rightarrow \infty } \sum _ { n = - \infty } ^ { \infty } F _ { n } e ^ { j m _ { 0 } t } \\  & = \lim _ { T \rightarrow \infty } \sum _ { n = - \infty } ^ { \infty } \frac { 1 } { 2 \pi } F ( j \omega ) \mathrm { e } ^ { i n \omega _ { 0 } t } \Delta \omega = \frac { 1 } { 2 \pi } \int _ { - \infty } ^ { \infty } F ( j \omega ) \mathrm { e } ^ { j \omega t } \mathrm { d } \omega \end{aligned}\end{eqnarray}</script><p>所以可得到傅里叶变换</p><script type="math/tex;mode=display">
\begin{eqnarray}F ( j \omega ) = \int _ { - \infty } ^ { \infty } f ( t ) \mathrm { e } ^ { - j \omega t } d t\end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}f ( t ) = \frac { 1 } { 2 \pi } \int _ { - \infty } ^ { \infty } F ( j \omega ) \mathrm { e } ^ { \mathrm { j } \omega t } \mathrm { d } \omega\end{eqnarray}</script><p><strong>上面两个式子称为傅里叶变换对。式$({66})$称为傅里叶正变换，式$({67})$称为傅里叶逆变换。</strong></p><p>我们将$F(jw)$的表达式带入到$f(t)$中，可以得到</p><script type="math/tex;mode=display">
\begin{eqnarray}f ( t ) = \frac { 1 } { 2 \pi } \int _ { - \infty } ^ { \infty } \left(\int _ { - \infty } ^ { \infty } f ( t ) \mathrm { e } ^ { - j \omega t } d t \right) \mathrm { e } ^ { \mathrm { j } \omega t } \mathrm { d } \omega\end{eqnarray}</script><p>可以看到，这里把$f(t)$表示成另外一个复平面内一组基$e^{jwt}$的线性组合形式(积分是一种广义上的求和)，而线性组合的系数为$F(jw)$。</p><p><strong>傅里叶正变换是关于$w$的函数。从式子$({67})$可以看出，傅里叶正变换得到的在$w$处的函数值$F(jw)$表示函数$f(t)$在$w$对应的基$e^{jwt}$上的系数，这也解释了为何说式子$({66})$为从时域到频域的转换（转换也就是说将一个坐标系转到另外一个坐标系）。在式子$({67})$中固定$t$，基$e^{jwt}$是关于$w$的表达式，$w$可以取遍整个频率轴，所以称该组基组成的空间为频域。</strong></p><p>同时，我们也可以将系数写成内积的形式：</p><script type="math/tex;mode=display">
\begin{eqnarray}F(jw) = \frac{\left\langle f(t),e^{jwt} \right>}{\left< e^{jwt},e^{jwt}\right>}\end{eqnarray}</script><blockquote><p>这里不给出证明，因为暂时我还不清楚积分区间$[-\infty,\infty]$的函数内积的定义。</p></blockquote><p><strong>结合广义内积可以看成一种广义意义上的投影，从内积的角度也可以理解$F(jw)$为$f(t)$映射到$e^{jwt}$完备正交基时的坐标。</strong></p><p>至此我们就完成了傅里叶变换从基变换（分解）角度的介绍。</p><h2 id="傅里叶分析的通俗理解"><a href="#傅里叶分析的通俗理解" class="headerlink" title="傅里叶分析的通俗理解"></a>傅里叶分析的通俗理解</h2><p>下面正式开始原作者的表演，之所以抄下面部分的内容，是因为原作者可以让打击通俗易懂的理解傅里叶分析的本质。</p><hr><p>傅里叶分析不仅仅是一个数学工具，更是一种可以彻底颠覆一个人以前世界观的思维模式。但不幸的是，傅里叶分析的公式看起来太复杂了，所以很多大一新生上来就懵圈并从此对它深恶痛绝。老实说，这么有意思的东西居然成了大学里的杀手课程，不得不归咎于编教材的人实在是太严肃了。（您把教材写得好玩一点会死吗？会死吗？）所以我一直想写一个有意思的文章来解释傅里叶分析，有可能的话高中生都能看懂的那种。所以，不管读到这里的您从事何种工作，我保证您都能看懂，并且一定将体会到通过傅里叶分析看到世界另一个样子时的快感。至于对于已经有一定基础的朋友，也希望不要看到会的地方就急忙往后翻，仔细读一定会有新的发现。</p><p>p.s.本文无论是cos还是sin，都统一用“正弦波”（Sine Wave）一词来代表简谐波。</p><h3 id="什么是频域"><a href="#什么是频域" class="headerlink" title="什么是频域"></a>什么是频域</h3><p>从我们出生，我们看到的世界都以时间贯穿，股票的走势、人的身高、汽车的轨迹都会随着时间发生改变。这种以时间作为参照来观察动态世界的方法我们称其为时域分析。而我们也想当然的认为，世间万物都在随着时间不停的改变，并且永远不会静止下来。但如果我告诉你，用另一种方法来观察世界的话，你会发现世界是永恒不变的，你会不会觉得我疯了？我没有疯，这个静止的世界就叫做频域。</p><p>先举一个公式上并非很恰当，但意义上再贴切不过的例子：</p><p>在你的理解中，一段音乐是什么呢？</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/2ca39677363c65a305207a5491b75825_hd.jpg" alt></p><p>这是我们对音乐最普遍的理解，一个随着时间变化的震动。但我相信对于乐器小能手们来说，音乐更直观的理解是这样的：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/8e1fce9d7607d97cebf73e1f36f03f06_hd.jpg" alt></p><p>好的！下课，同学们再见。<br>是的，其实这一段写到这里已经可以结束了。上图是音乐在时域的样子，而下图则是音乐在频域的样子。所以频域这一概念对大家都从不陌生，只是从来没意识到而已。</p><p>现在我们可以回过头来重新看看一开始那句痴人说梦般的话：世界是永恒的。</p><p>将以上两图简化：</p><p>时域：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/d4fa1de0327eb491a6941ac84a56e432_hd.jpg" alt></p><p>频域：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/1ca366b593d877a16c8a49773774b5b9_hd.jpg" alt></p><p>在时域，我们观察到钢琴的琴弦一会上一会下的摆动，就如同一支股票的走势；而在频域，只有那一个永恒的音符。</p><p>所以</p><p>你眼中看似落叶纷飞变化无常的世界，实际只是躺在上帝怀中一份早已谱好的乐章。<br>抱歉，这不是一句鸡汤文，而是黑板上确凿的公式：<strong>傅里叶同学告诉我们，任何周期函数，都可以看作是不同振幅，不同相位正弦波的叠加</strong>。在第一个例子里我们可以理解为，利用对不同琴键不同力度，不同时间点的敲击，可以组合出任何一首乐曲。</p><p>而贯穿时域与频域的方法之一，就是传中说的傅里叶分析。<strong>傅里叶分析可分为傅里叶级数（Fourier Serie）和傅里叶变换(Fourier Transformation)</strong> ，我们从简单的开始谈起。</p><h3 id="傅里叶级数-Fourier-Series-的频谱"><a href="#傅里叶级数-Fourier-Series-的频谱" class="headerlink" title="傅里叶级数(Fourier Series)的频谱"></a>傅里叶级数(Fourier Series)的频谱</h3><p>还是举个栗子并且有图有真相才好理解。</p><p>如果我说我能用前面说的正弦曲线波叠加出一个带90度角的矩形波来，你会相信吗？你不会，就像当年的我一样。但是看看下图：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/055bf33bb84555a952804c5dbeb75dd9_hd.jpg" alt></p><p>第一幅图是一个郁闷的正弦波$cos（x）$</p><p>第二幅图是2个卖萌的正弦波的叠加$cos(x)+a.cos(3x)$</p><p>第三幅图是4个发春的正弦波的叠加</p><p>第四幅图是10个便秘的正弦波的叠加</p><p>随着正弦波数量逐渐的增长，他们最终会叠加成一个标准的矩形，大家从中体会到了什么道理？</p><p>（只要努力，弯的都能掰直！）</p><p>随着叠加的递增，<strong>所有正弦波中上升的部分逐渐让原本缓慢增加的曲线不断变陡，而所有正弦波中下降的部分又抵消了上升到最高处时继续上升的部分使其变为水平线</strong>。一个矩形就这么叠加而成了。但是要多少个正弦波叠加起来才能形成一个标准90度角的矩形波呢？不幸的告诉大家，答案是无穷多个。（上帝：我能让你们猜着我？）</p><p>不仅仅是矩形，你能想到的<strong>任何波形都是可以如此方法用正弦波叠加起来的</strong>。这是没<br>有接触过傅里叶分析的人在直觉上的第一个难点，但是一旦接受了这样的设定，游戏就开始有意思起来了。</p><p>还是上图的正弦波累加成矩形波，我们换一个角度来看看：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/563deb4a6599d052b3ba108661872c57_hd.jpg" alt></p><p>在这几幅图中，<strong>最前面黑色的线就是所有正弦波叠加而成的总和</strong>，也就是越来越接近矩形波的那个图形。而后面依不同颜色排列而成的正弦波就是组合为矩形波的各个分量。这些正弦波按照频率从低到高从前向后排列开来，而每一个波的振幅都是不同的。一定有细心的读者发现了，每两个正弦波之间都还有一条直线，那并不是分割线，而是<strong>振幅为0的正弦波！也就是说，为了组成特殊的曲线，有些正弦波成分是不需要的。</strong></p><p>这里，不同频率的正弦波我们成为频率分量。</p><p>好了，关键的地方来了！！</p><p>如果我们把第一个频率最低的频率分量看作“1”，我们就有了构建频域的最基本单元。</p><p>对于我们最常见的有理数轴，数字“1”就是有理数轴的基本单元。</p><p>时域的基本单元就是“1秒”，如果我们将一个角频率为$\omega_{0}$ 的正弦波$cos（\omega_{0} t）$看作基础，<strong>那么频域的基本单元就是$\omega_{0} $</strong>。</p><p>有了“1”，还要有“0”才能构成世界，那么频域的“0”是什么呢？<strong>cos（0t）就是一个周期无限长的正弦波，也就是一条直线！所以在频域，0频率也被称为直流分量，在傅里叶级数的叠加中，它仅仅影响全部波形相对于数轴整体向上或是向下而不改变波的形状。</strong></p><p>接下来，让我们回到初中，回忆一下已经死去的八戒，啊不，已经死去的老师是怎么定义正弦波的吧。</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/81ca9447d6c45c162c2d76df75a6690a_hd.jpg" alt></p><p>正弦波就是一个圆周运动在一条直线上（$y$轴）的投影（正弦波的$x$轴为旋转的角度）。<strong>所以频域的基本单元也可以理解为一个始终在旋转的圆。</strong></p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/Fourier_series_square_wave_circles_animation.gif" alt></p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/Fourier_series_sawtooth_wave_circles_animation.gif" alt></p><p>介绍完了频域的基本组成单元，我们就可以看一看一个矩形波，在频域里的另一个模样了：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/e2e3c0af3bdbcba721c5415a4c65da9e_hd.jpg" alt></p><p>这是什么奇怪的东西？<br>这就是矩形波在频域的样子，是不是完全认不出来了？教科书一般就给到这里然后留给了读者无穷的遐想，以及无穷的吐槽，其实教科书只要补一张图就足够了：频域图像，也就是俗称的频谱，就是——</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/19679c871bd33d94e2fc8b174f0d14ab_hd.jpg" alt></p><p>再清楚一点：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/40cf849e55ed95732a60b52d4019d609_hd.jpg" alt></p><p>可以发现，在频谱中，偶数项的振幅都是0，也就对应了图中的彩色直线。振幅为0的正弦波。</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/Fourier_series_and_transform.gif" alt></p><p>老实说，在我学傅里叶变换时，维基的这个图还没有出现，那时我就想到了这种表达方法，而且，后面还会加入维基没有表示出来的另一个谱——相位谱。</p><p>但是在讲相位谱之前，我们先回顾一下刚刚的这个例子究竟意味着什么。记得前面说过的那句“世界是静止的”吗？估计好多人对这句话都已经吐槽半天了。想象一下，世界上每一个看似混乱的表象，实际都是一条时间轴上不规则的曲线，但实际这些曲线都是由这些无穷无尽的正弦波组成。我们看似不规律的事情反而是规律的正弦波在时域上的投影，而正弦波又是一个旋转的圆在直线上的投影。那么你的脑海中会产生一个什么画面呢？</p><p>我们眼中的世界就像皮影戏的大幕布，幕布的后面有无数的齿轮，大齿轮带动小齿轮，小齿轮再带动更小的。在最外面的小齿轮上有一个小人——那就是我们自己。我们只看到这个小人毫无规律的在幕布前表演，却无法预测他下一步会去哪。而幕布后面的齿轮却永远一直那样不停的旋转，永不停歇。这样说来有些宿命论的感觉。说实话，这种对人生的描绘是我一个朋友在我们都是高中生的时候感叹的，当时想想似懂非懂，直到有一天我学到了傅里叶级数……</p><h3 id="傅里叶级数（Fourier-Series）的相位谱"><a href="#傅里叶级数（Fourier-Series）的相位谱" class="headerlink" title="傅里叶级数（Fourier Series）的相位谱"></a>傅里叶级数（Fourier Series）的相位谱</h3><p>上一章的关键词是：从侧面看。这一章的关键词是：从下面看。</p><p>在这一章最开始，我想先回答很多人的一个问题：傅里叶分析究竟是干什么用的？这段相对比较枯燥，已经知道了的同学可以直接跳到下一个分割线。</p><p>先说一个最直接的用途。无论听广播还是看电视，我们一定对一个词不陌生——频道。<strong>频道频道，就是频率的通道，不同的频道就是将不同的频率作为一个通道来进行信息传输</strong>。下面大家尝试一件事：</p><p>先在纸上画一个$sin（x）$，不一定标准，意思差不多就行。不是很难吧。</p><p>好，接下去画一个$sin（3x）+sin（5x）$的图形。</p><p>别说标准不标准了，曲线什么时候上升什么时候下降你都不一定画的对吧？</p><p>好，画不出来不要紧，我把$sin（3x）+sin（5x）$的曲线给你，但是前提是你不知道这个曲线的方程式，现在需要你把$sin（5x）$给我从图里拿出去，看看剩下的是什么。这基本是不可能做到的。</p><p>但是在频域呢？则简单的很，无非就是几条竖线而已。</p><p>所以很多在<strong>时域看似不可能做到的数学操作，在频域相反很容易。这就是需要傅里叶变换的地方</strong>。尤其是<strong>从某条曲线中去除一些特定的频率成分，这在工程上称为滤波，是信号处理最重要的概念之一，只有在频域才能轻松的做到</strong>。</p><p>再说一个更重要，但是稍微复杂一点的用途——求解微分方程。（这段有点难度，看不懂的可以直接跳过这段）微分方程的重要性不用我过多介绍了。各行各业都用的到。但是求解微分方程却是一件相当麻烦的事情。因为除了要计算加减乘除，还要计算微分积分。而傅里叶变换则可以<strong>让时域微分和积分在频域中变为除法和乘法</strong>，大学数学瞬间变小学算术有没有。</p><p>傅里叶分析当然还有其他更重要的用途，我们随着讲随着提。</p><p>————————————————————————————————————</p><p>下面我们继续说相位谱：</p><p>通过时域到频域的变换，我们得到了一个从侧面看的频谱，但是这个频谱并没有包含时域中全部的信息。因为频谱只代表每一个对应的正弦波的振幅是多少，而没有提到相位。基础的正弦波$A.sin(wt+θ)$中，振幅，频率，相位缺一不可，不同相位决定了波的位置，所以对于频域分析，仅仅有频谱（振幅谱）是不够的，我们还需要一个相位谱。那么这个相位谱在哪呢？我们看下图，这次为了避免图片太混论，我们用7个波叠加的图。</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/07199fc0250791d768771b50c098e26a_hd.jpg" alt></p><p>鉴于正弦波是周期的，我们需要设定一个用来标记正弦波位置的东西。在图中就是那些小红点。<strong>小红点是距离频率轴最近的波峰</strong>，而这个波峰所处的位置离频率轴有多远呢？为了看的更清楚，我们将红色的点投影到下平面，投影点我们用粉色点来表示。当然，<strong>这些粉色的点只标注了波峰距离频率轴的距离(可理解为时域中的时间差)，并不是相位。</strong></p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/e1985fe86283a7b14d1fc7e11d322fcb_hd.jpg" alt></p><p>这里需要纠正一个概念：<strong>时间差并不是相位差</strong>。如果将全部周期看作$2\pi$或者360度的话，相位差则是时间差在一个周期中所占的比例。我们<strong>将时间差除周期再乘$2\pi$，就得到了相位差。</strong></p><p>在完整的立体图中，我们将投影得到的时间差依次除以所在频率的周期，就得到了最下面的相位谱。所以，频谱是从侧面看，相位谱是从下面看。下次偷看女生裙底被发现的话，可以告诉她：“对不起，我只是想看看你的相位谱。”</p><p>注意到，本例的相位谱中的相位除了0，就是$\pi$。因为$cos（t+\pi）=-cos（t）$，所以实际上相位为$\pi$的波只是上下翻转了而已。<strong>对于周期方波的傅里叶级数</strong>，这样的相位谱已经是很简单的了。另外值得注意的是，由于$cos（t+2\pi）=cos（t）$，所以相位差是周期的，$\pi$和$3\pi$，$5\pi$，$7\pi$都是相同的相位。人为定义相位谱的值域为$(-\pi，\pi]$，所以图中的相位差均为$\pi$。</p><p>最后来一张大集合：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/4695ce06197677bab880cd55b6846f12_hd.jpg" alt></p><h3 id="傅里叶变换（Fourier-Transformation）"><a href="#傅里叶变换（Fourier-Transformation）" class="headerlink" title="傅里叶变换（Fourier Transformation）"></a>傅里叶变换（Fourier Transformation）</h3><p>相信通过前面三章，大家对频域以及傅里叶级数都有了一个全新的认识。但是文章在一开始关于钢琴琴谱的例子我曾说过，这个栗子是一个公式错误，但是概念典型的例子。所谓的公式错误在哪里呢？</p><p><strong>傅里叶级数的本质是将一个周期的信号分解成无限多分开的（离散的）正弦波</strong>，但是宇宙似乎并不是周期的。曾经在学数字信号处理的时候写过一首打油诗：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">往昔连续非周期，</span><br><span class="line">回忆周期不连续，</span><br><span class="line">任你ZT、DFT，</span><br><span class="line">还原不回去。</span><br></pre></td></tr></table></figure><p></p><p>在这个世界上，有的事情一期一会，永不再来，并且时间始终不曾停息地将那些刻骨铭心的往昔连续的标记在时间点上。但是这些事情往往又成为了我们格外宝贵的回忆，在我们大脑里隔一段时间就会周期性的蹦出来一下，可惜这些回忆都是零散的片段，往往只有最幸福的回忆，而平淡的回忆则逐渐被我们忘却。因为，往昔是一个连续的非周期信号，而回忆是一个周期离散信号。</p><p>是否有一种数学工具将连续非周期信号变换为周期离散信号呢？抱歉，真没有。</p><p>比如<strong>傅里叶级数，在时域是一个周期且连续的函数，而在频域是一个非周期离散的函数。</strong>这句话比较绕嘴，实在看着费事可以干脆回忆第一章的图片。</p><p>而在我们接下去要讲的<strong>傅里叶变换，则是将一个时域非周期的连续信号，转换为一个在频域非周期的连续信号。</strong></p><p>算了，还是上一张图方便大家理解吧：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/419cd0b2e965aca25d5f8a5a6362d728_hd.jpg" alt></p><p>或者我们也可以换一个角度理解：<strong>傅里叶变换实际上是对一个周期无限大的函数进行傅里叶级数。</strong><br>所以说，钢琴谱其实并非一个连续的频谱，而是很多在时间上离散的频率，但是这样的一个贴切的比喻真的是很难找出第二个来了。</p><p>因此在傅里叶变换在频域上就从离散谱变成了连续谱。那么连续谱是什么样子呢？</p><p><strong>你见过大海么？</strong></p><p>为了方便大家对比，我们这次从另一个角度来看频谱，还是傅里叶级数中用到最多的那幅图，我们从频率较高的方向看。</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/a185be412974fd73a7925cf1f1cc5372_hd.jpg" alt></p><p>以上是离散谱，那么连续谱是什么样子呢？<br>尽情的发挥你的想象，想象这些离散的正弦波离得越来越近，逐渐变得连续……</p><p>直到变得像波涛起伏的大海：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/ece53f825c6de629befba3de12f929a7_hd.jpg" alt></p><p>很抱歉，为了能让这些波浪更清晰的看到，我没有选用正确的计算参数，而是选择了一些让图片更美观的参数，不然这图看起来就像屎一样了。</p><p>不过通过这样两幅图去比较，大家应该可以理解如何从离散谱变成了连续谱的了吧？<strong>原来离散谱的叠加，变成了连续谱的累积。所以在计算上也从求和符号变成了积分符号。</strong></p><p>不过，这个故事还没有讲完，接下去，我保证让你看到一幅比上图更美丽壮观的图片，但是这里需要介绍到一个数学工具才能然故事继续，这个工具就是——</p><h3 id="宇宙耍帅第一公式：欧拉公式"><a href="#宇宙耍帅第一公式：欧拉公式" class="headerlink" title="宇宙耍帅第一公式：欧拉公式"></a>宇宙耍帅第一公式：欧拉公式</h3><p>虚数$i$这个概念大家在高中就接触过，但那时我们只知道它是-1的平方根，可是它真正的意义是什么呢?</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/42e1f6dc43e8868b4962f5ba389a5df4_hd.jpg" alt></p><p>这里有一条数轴，在数轴上有一个红色的线段，它的长度是1。当它乘以3的时候，它的长度发生了变化，变成了蓝色的线段，而当它乘以-1的时候，就变成了绿色的线段，或者说线段在数轴上围绕原点旋转了180度。</p><p>我们知道乘-1其实就是乘了两次$i$使线段旋转了180度，那么乘一次$i$呢——答案很简单——旋转了90度。</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/3e88e9463e4667e50ebdda51dee88358_hd.jpg" alt></p><p>同时，我们获得了一个垂直的虚数轴。实数轴与虚数轴共同构成了一个复数的平面，也称复平面。这样我们就了解到，<strong>乘虚数$i$的一个功能——旋转。</strong></p><p>现在，就有请宇宙第一耍帅公式欧拉公式隆重登场——</p><script type="math/tex;mode=display">
\begin{eqnarray}e^{i t}=\cos t+i \sin t\end{eqnarray}</script><p>这个公式在数学领域的意义要远大于傅里叶分析，但是乘它为宇宙第一耍帅公式是因为它的特殊形式——当$x$等于$\pi$的时候。</p><script type="math/tex;mode=display">
\begin{eqnarray}e^{i \pi}+1=0\end{eqnarray}</script><p>经常有理工科的学生为了跟妹子表现自己的学术功底，用这个公式来给妹子解释数学之美：”石榴姐你看，这个公式里既有自然底数e，自然数1和0，虚数$i$还有圆周率$\pi$，它是这么简洁，这么美丽啊！“但是姑娘们心里往往只有一句话：”臭屌丝……“</p><p>这个公式关键的作用，是将正弦波统一成了简单的指数形式。我们来看看图像上的涵义：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/974efc6a99e06dcd623193e960ccbe93_hd.jpg" alt></p><p>欧拉公式所描绘的，是一个随着时间变化，在复平面上做圆周运动的点，随着时间$t$的改变，在时间轴上就成了一条螺旋线。如果只看它的实数部分，也就是螺旋线在左侧的投影，就是一个最基础的余弦函数。而右侧的投影则是一个正弦函数。<br>关于复数更深的理解，大家可以参考：</p><p><a href="http://www.zhihu.com/question/23234701/answer/26017000" target="_blank" rel="noopener">复数的物理意义是什么？</a></p><p>这里不需要讲的太复杂，足够让大家理解后面的内容就可以了。</p><h3 id="指数形式的傅里叶变换"><a href="#指数形式的傅里叶变换" class="headerlink" title="指数形式的傅里叶变换"></a>指数形式的傅里叶变换</h3><p>有了欧拉公式的帮助，我们便知道：<strong>正弦波的叠加，也可以理解为螺旋线的叠加在实数空间的投影。</strong>而螺旋线的叠加如果用一个形象的栗子来理解是什么呢？</p><p><strong>光波</strong></p><p>高中时我们就学过，自然光是由不同颜色的光叠加而成的，而最著名的实验就是牛顿师傅的三棱镜实验：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/c2d7bfc819ebcbea8d6f2c8271d4791d_hd.jpg" alt></p><p>所以其实我们在很早就接触到了<strong>光的频谱</strong>，只是并没有了解频谱更重要的意义。</p><p>但不同的是，<strong>傅里叶变换出来的频谱不仅仅是可见光这样频率范围有限的叠加，而是频率从0到无穷所有频率的组合。</strong></p><p>这里，我们可以用两种方法来理解正弦波：</p><p>第一种前面已经讲过了，就是螺旋线在实轴的投影。</p><p>另一种需要借助欧拉公式的另一种形式去理解：</p><script type="math/tex;mode=display">
\begin{eqnarray}\begin{array}{l}{e^{i t}=\cos (t)+i . \sin (t)} \\ {e^{-i t}=\cos (t)-i . \sin (t)}\end{array}\end{eqnarray}</script><p>将以上两式相加再除2，得到：</p><script type="math/tex;mode=display">
\begin{eqnarray}\cos (t)=\frac{e^{i t}+e^{-i t}}{2}\end{eqnarray}</script><p>这个式子可以怎么理解呢？</p><p>我们刚才讲过，$e^{it}$可以理解为一条逆时针旋转的螺旋线，那么$e^{-it}$则可以理解为一条顺时针旋转的螺旋线。而$cos(t)$则是这两条旋转方向不同的螺旋线叠加的一半，因为这两条螺旋线的虚数部分相互抵消掉了！</p><p>举个例子的话，就是极化方向不同的两束光波，磁场抵消，电场加倍。</p><p>这里，逆时针旋转的我们称为正频率，而顺时针旋转的我们称为负频率（注意不是复频率）。</p><p>好了，刚才我们已经看到了大海——连续的傅里叶变换频谱，现在想一想，连续的螺旋线会是什么样子：</p><p>想象一下再往下翻：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/f116ae26859bdc80b28ea0f8f894ccc0_hd.jpg" alt></p><p>是不是很漂亮？</p><p>你猜猜，这个图形在时域是什么样子？</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/0fdfa0a9b6eea036703ab2499381080c_hd.jpg" alt></p><p>哈哈，是不是觉得被狠狠扇了一个耳光。数学就是这么一个把简单的问题搞得很复杂的东西。<br>顺便说一句，那个像大海螺一样的图，为了方便观看，我仅仅展示了其中正频率的部分，负频率的部分没有显示出来。</p><p>如果你认真去看，海螺图上的每一条螺旋线都是可以清楚的看到的，<strong>每一条螺旋线都有着不同的振幅（旋转半径），频率（旋转周期）以及相位。而将所有螺旋线连成平面，就是这幅海螺图了。</strong></p><p>好了，讲到这里，相信大家对傅里叶变换以及傅里叶级数都有了一个形象的理解了，我们最后用一张图来总结一下：</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/097c9051af221c171730d4bc8f436a72_hd.jpg" alt></p><h2 id="卷积和傅里叶分析的关系"><a href="#卷积和傅里叶分析的关系" class="headerlink" title="卷积和傅里叶分析的关系"></a>卷积和傅里叶分析的关系</h2><p>我们在上面分别介绍了时域中的卷积操作和如何通过傅里叶级数或者傅里叶变换将时域中的信号转换到频域。但是时域中的卷积和傅里叶分析有什么关系呢，这就用到了我们下面所说的卷积定理。</p><h3 id="卷积定理"><a href="#卷积定理" class="headerlink" title="卷积定理"></a>卷积定理</h3><p><strong>卷积定理指出，函数卷积的傅里叶变换是函数傅里叶变换的乘积。即一个域中的卷积对应于另一个域中的乘积，例如时域中的卷积对应于频域中的乘积。</strong></p><blockquote><p> 注意下面$←→$负号并不是相等的意思。</p></blockquote><h4 id="时域卷积定理"><a href="#时域卷积定理" class="headerlink" title="时域卷积定理"></a>时域卷积定理</h4><p>若${f_1}(t) \leftarrow \to {F_1}(j\omega ),{f_2}(t) \leftarrow \to {F_2}(j\omega )$，则${f_1}(t)*{f_2}(t) \leftarrow \to {F_1}(j\omega ){F_2}(j\omega )$</p><p><strong>证明：</strong></p><script type="math/tex;mode=display">
\begin{eqnarray}f_1\left( t \right) * f_2\left( t \right) = \int_{ - \infty }^\infty f_1 \left( \tau \right)f_2\left( t - \tau  \right)d\tau \end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}F[f_1(t)*f_2(t)] = \int_{ - \infty }^\infty \left[ {\int_{ - \infty }^\infty {f_1} \left( \tau \right)f_2\left( t - \tau \right)d\tau } \right] \rm{e}^{ - j\omega t}dt \\ = \int_{ - \infty }^\infty f_1 \left( \tau \right)\left[ \int_{ - \infty }^\infty f_2 \left( t - \tau \right)\rm{e}^{ - j\omega t}dt \right]d\tau \end{eqnarray}</script><p>令$x=t-\tau$，有$dt=dx$，则上式可以写成</p><script type="math/tex;mode=display">
\begin{eqnarray}\begin{aligned} \int_{ - \infty }^\infty f_1(\tau) \left[ \int_{ - \infty }^\infty f_2(x)e^{-jw(x+\tau)}dx \right] d\tau  &  \\= \int_{ - \infty }^\infty f_1(\tau) e^{-jwt}d\tau \int_{ - \infty }^\infty f_2(x)e^{-jwx}dx & = F_1(jw)F_2(jw) \end{aligned}\end{eqnarray}</script><p>也就是说</p><script type="math/tex;mode=display">
\begin{eqnarray} F[f_1(t)*f_2(t)] = F_1(jw)F_2(jw) \\ f_1(t)*f_2(t)= \mathscr { F } ^ { - 1 }[ F_1(jw)F_2(jw)] \end{eqnarray}</script><h4 id="频域卷积定理"><a href="#频域卷积定理" class="headerlink" title="频域卷积定理"></a>频域卷积定理</h4><p>若$f_1(t) ←→F_1(jω)，f_2(t) ←→F_2(jω)$，则$f_1(t) f_2(t) ←→ \frac{1}{2\pi}F_1(jω)*F_2(jω)$。</p><p><strong>证明：</strong>，下面的$\mathscr{F}^ { - 1 }$为傅里叶逆变换。</p><script type="math/tex;mode=display">
\begin{eqnarray}\mathscr { F } ^ { - 1 } \left[ \frac { 1 } { 2 \pi } F _ { 1 } ( j \omega ) * F _ { 2 } ( j \omega ) \right] \end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}={ \left( \frac { 1 } { 2 \pi } \right) ^ { 2 } \int _ { - \infty } ^ { \infty } \left\{ \int _ { - \infty } ^ { \infty } F _ { 1 } ( j u ) F _ { 2 } [ j ( \omega - u ) ] \mathrm { d } u \right\} \mathrm { e } ^ { j \omega t } \mathrm { d } \omega } \end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray} = { \left( \frac { 1 } { 2 \pi } \right) ^ { 2 } \int _ { - \infty } ^ { \infty } F _ { 1 } ( j u ) \left\{ \int _ { - \infty } ^ { \infty } F _ { 2 } [ j ( \omega - u ) ] \mathrm { e } ^ { j \omega t } \mathrm { d } \omega \right\} \mathrm { d } u }\end{eqnarray}</script><p>令$v=w-u$，有$dw=dv$，则上式可以写成</p><script type="math/tex;mode=display">
\begin{eqnarray}\left( \frac { 1 } { 2 \pi } \right) ^ { 2 } \int _ { - \infty } ^ { \infty } F _ { 1 } ( j u ) \left[ \int _ { - \infty } ^ { \infty } F _ { 2 } ( j v ) e ^ { \mathrm { j } ( v + u) t } \mathrm { d } v \right] d u\end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}= \frac { 1 } { 2 \pi } \int _ { - \infty } ^ { \infty } F _ { 1 } ( j u ) \mathrm { e } ^ { \mathrm { ju } t } \mathrm { d } u \frac { 1 } { 2 \pi } \int _ { - \infty } ^ { \infty } F _ { 2 } ( j v ) \mathrm { e } ^ { \mathrm { i } v t } \mathrm { d } v\end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}=f_1(t)f_2(t)\end{eqnarray}</script><p>也就是说</p><script type="math/tex;mode=display">
\begin{eqnarray} f_1(t) f_2(t) = \mathscr { F } ^ { - 1 } \left[ \frac { 1 } { 2 \pi } F _ { 1 } ( j \omega ) * F _ { 2 } ( j \omega ) \right]  \\  F[f_1(t)f_2(t)] =\frac { 1 } { 2 \pi } F _ { 1 } ( j \omega ) * F _ { 2 } ( j \omega )\end{eqnarray}</script><h3 id="图像处理中的卷积"><a href="#图像处理中的卷积" class="headerlink" title="图像处理中的卷积"></a>图像处理中的卷积</h3><p>用一个模板和一幅图像进行卷积，对于图像上的每一个点，让模板的原点和该点重合，然后模板上的点和图像上对应的点相乘，然后各点的积相加，就得到了该点的卷积值。对图像上的每个点都这样处理。<strong>由于大多数模板都是对称的，所以模板不旋转。卷积是一种积分运算，用来求两个曲线重叠区域面积。可以看作加权求和，可以用来消除噪声、特征增强、图像滤波。</strong></p><p>卷积是一种线性运算,图像处理中常见的mask运算都是卷积，广泛应用于图像滤波。</p><p>卷积在数据处理中用来平滑，卷积有平滑效应和展宽效应.</p><p>对比时域中的二维离散卷积运算：</p><ul><li>在二维离散卷积运算中，需要进行旋转，而图像处理中的卷积核通常是对称的，所以不需要进行翻转，而深度学习中的卷积核每一个值均是反向传播得到的，并不是固定值，因此也就不需要翻转。</li><li>在二维离散卷积运算中，需要进行平移，而图像处理中的卷积和深度学习中的卷积均需要划窗操作，也就是平移操作。</li></ul><h4 id="卷积的本质"><a href="#卷积的本质" class="headerlink" title="卷积的本质"></a>卷积的本质</h4><p>卷积定理中告诉我们，<strong>时域卷积=频域相乘</strong>，这个定理对于下面的理解特别重要。</p><p>卷积核又被称为滤波器（英文名字为filter）。滤波器这个名词想必大家也不陌生，比如带有『降噪』功能的麦克风，说白了就是把高频的噪音信号给过滤掉。更专业一点，滤波器是能过滤某些特定频段，留下需要信号的部件，比如低通滤波器(只留下低频分量)、高通滤波器(只留下高频分量)、带通滤波器(只留下特定范围内的分量)。</p><p>那滤波跟卷积的有什么关系么？别忘了我们刚刚特别强调的概念</p><blockquote><p>时域卷积=频域相乘</p></blockquote><p>假设时域信号f1和f2做卷积，从f1的角度看，它的频谱函数要跟f2对应的频谱函数相乘，而如果f1的某些频率分量，在f2上是没有的，那么相乘之后的结果是0，所以得到的f3信号，在这些频率上值为0，于是对f1而言，f2把它的某些分量『过滤』掉了，所以f2是『滤波器』，f1是原始信号，f3是过滤之后的信号。</p><p>一个理想的低通滤波器如下所示</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-c76d2c7ce11d38d76addf847face35d2_hd.png" alt></p><p>它的横坐标是频率，幅值为1，这是典型的『低通滤波器』，如果有信号跟它做卷积，那这个信号只会留下在0附近的低频信号，其他高频分量都被过滤掉了。</p><p>更形象一点，我们再回顾下刚刚的方波信号(注意，这里的方波信号横坐标是时间t，不是上面的低通滤波器)</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-4729710146abe4cbeeba1b315bbf620d_hd.png" alt></p><p>它是由不同频率、相位、幅度的正弦信号组成。如果我们现在通过某种『低通滤波器』，过滤掉一些高频的正弦信号，则这个方波信号将变成下面的形状</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-3fcaa905013995eb3c4f23b6538840fe_hd.png" alt></p><p>我们发现，这个方形信号没那么『方』了，两边尖锐的『棱角』变缓和了，也没有棱角附近小幅度高频振动的波形了。我们继续加大滤波力度，将得到下面结果</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-387fe282ad5d70113e69679b81499d62_hd.png" alt></p><p>我们可以看到，方形波的『棱角』基本被磨平了。所以</p><blockquote><p>波形里的『棱角』其实是一种突变信号，它里面包含了很多高频分量.</p></blockquote><p>上面这个发现非常重要，将有助于我们后面理解图像的『卷积』和『滤波』。</p><p>上面的讨论均是在一维空间中的，图像卷积是在二维空间中进行的，所以下面我们讨论<strong>在二维空间的情况</strong>。</p><p>虽然我们一直在强调『时间信号』，但不知大家注意到没有，其实这里的时间t，完全可以换成其他符号比如x，从而所谓的时间信号f(t)，可以写成f(x)，进而，<strong>图像可以看成一个离散的二维信号f(x,y)</strong>，x 和 y 决定了图像的像素点，f是像素点在该处的取值。更形象地理解，图像就仿佛是一个『水池』，像素点就是『水分子』，像素点的取值大小，从视觉上看代表图像亮度的强弱，而类比到水池里，就是不同位置水分子的运动幅度，在水池里泛起涟漪。</p><p>进而，我们很自然地想到，一维函数的『傅里叶变换』，能否扩展到二维呢？</p><p>答案是肯定的。不过二维空间的傅里叶变换公式我们就不贴出来了，大家有兴趣可以详细阅读<a href="http://www.robots.ox.ac.uk/~az/lectures/ia/lect2.pdf" target="_blank" rel="noopener">参考资料</a>。一维函数f(x)的频谱函数F(w)，是一维信号的不同频率分量，而二维函数f(x,y)的频谱函数，是一个二维函数F(w,v)，也反应了二维函数的频率特性(不过理解起来不那么直观，这里略过，有兴趣的同学请阅读参考资料——&lt;数字信号处理&gt; 奥本海默)。这里我们直接结合前面说的『滤波器』来理解卷积过程</p><p>卷积核本质上是一个二维函数，有对应的频谱函数，因而可以看成某种『滤波器』</p><p>下面是几种常见卷积核的频谱图像</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-a30d021f90d53c4089bf472a6ed517e6_hd.jpg" alt></p><p>这是一个低通滤波器，频率接近原点附近的幅值很大(频率低的通过)，越往两边越小(频率高的过滤)。下面这个高通滤波器恰恰相反</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-b00f391d3ba69f1c8bd6807e28ca79a4_hd.jpg" alt></p><p>滤波器的概念有了，那么问题来了，我们该如何理解『图像卷积』和『滤波』的关系呢？</p><blockquote><p>当我们将图像跟『高通滤波器』做卷积时，明暗变化会被保留，而缓和的变化会被过滤</p></blockquote><p>回顾下我们上节的发现——『波形里的「棱角」其实是一种突变信号，它里面包含了很多高频分量』。我们沿用上面『水池』的类比，图像像素值变化陡峭的地方（如边界、噪声等），反映在图像上，就是那块区域明暗变化明显，即为高频分量，而类比到『水池』里，就是水波在该区域快速振动，『棱角』分明。所以反映到图像上，就是『锐化』效果，即图像的边缘被加强，大色块的背景被过滤。同理，跟低通滤波器做卷积，效果相反。我们看看直观的效果</p><p><img src="/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/v2-1cc14436b43f795bb5bcb91ca8ac52ec_hd.png" alt></p><p>当我们把图像跟多种卷积核作用时，就能得到不同频段的信号，这也就是卷积神经网络中，『卷积层』的本质作用。</p><p>至此，我们完美阐释了图像处理中卷积的本质。</p><p><strong>图像卷积的本质，是提取图像不同『频段』的特征。</strong></p><p>上面卷积核都是在频域进行解释的，这是为了探讨卷积的本质，这里补充一点在时域的解释：对于低通滤波器，就是将中心像素值与周围临近的像素进行平均，自然就能“削峰填谷”，实现平滑处理；对于高通滤波器，就是将中心像素值复制n份，然后减去周围的n个临近像素值。所以如果是在区域位置（四周像素值都很相近），像素值肯定就“减”为0；只有在边缘位置（与周围的像素值差别比较大），相减后才能留下。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>傅里叶级数和傅里叶变换的实质为基变换或者说是分解。</p><p>本篇中的核心思想为：时域中的卷积相当于频域中的相乘，由时域变换到频域需要使用到傅里叶变换或者傅里叶级数。而频域理所当然的可以想到频率，<strong>在图像中变换较大的如边界、噪声等，均是高频信息；而其余的为低频信息</strong>。在图像处理中，经常使用滤波器，在时域看来相当于二维离散卷积（划窗操作可以对应为二维离散卷积运算中的平移操作，而大多数均是对称的，所以不需要二维离散卷积中的翻转操作），而若使用傅里叶变换将其变换到频域范围内，就可以理解为滤波器的实质——是提取图像不同『频段』的特征。而卷积神经网络中的卷积其实就是图像处理中的卷积（划窗操作可以对应为二维离散卷积运算中的平移操作，因为卷积核参数是反向传播优化出来的并不是固定的，所以也不需要二维离散卷积中的翻转操作，实际上从运算来看这里的卷积是信号与系统中的相关操作），因此卷积神经网络之所以效果这么好，可以理解为使用了多个卷积核，提取了图像的不同频段信息，然后综合了起来。</p><p>另外，从二维离散卷积的运算来看，也就可以理解卷积操作中的参数共享、局部感知、多核等性质是如何提出来的了。</p><p>小小地拓展一下，与本文无关 这种频域相乘的特性可以用于快速求一些特定函数的积分，因为<strong>『卷积』的运算本质是积分（离散时为加权求和）</strong>，而很多特定函数存在傅里叶变换和反变换，所以与其直接求解积分函数，不如把他们变换到频域，直接进行频谱函数『相乘』，然后再反变换回来，就得到积分结果了。</p><h2 id="问题与思考"><a href="#问题与思考" class="headerlink" title="问题与思考"></a>问题与思考</h2><ul><li>傅里叶变换为何选择$e^{jwt}$作为底</li><li>既然深度学习中的卷积来着图像处理中的卷积，那么图像处理中有哪些操作可以代替卷积操作进而可以创造出一个新的网络呢？</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/tiandijun/article/details/40080823" target="_blank" rel="noopener">卷积的本质及物理意义（全面理解卷积）</a><br><a href="https://blog.csdn.net/qq_26093511/article/details/55670390" target="_blank" rel="noopener">卷积的本质及物理意义（全面理解卷积）</a><br><a href="https://zhuanlan.zhihu.com/p/19763358" target="_blank" rel="noopener">傅里叶分析之掐死教程（完整版）更新于2014.06.06</a><br><a href="https://www.zhihu.com/question/20099543" target="_blank" rel="noopener">图像上的频率指的是什么？</a><br><a href="https://stonema.github.io/2018/04/24/%E9%AB%98%E9%A2%91%E4%BF%A1%E5%8F%B7%E4%B8%8E%E4%BD%8E%E9%A2%91%E4%BF%A1%E5%8F%B7-%E5%9B%BE%E5%83%8F%E9%94%90%E5%8C%96%E4%B8%8E%E6%A8%A1%E7%B3%8A/" target="_blank" rel="noopener">高频信号与低频信号,图像锐化与模糊</a><br><a href="https://blog.csdn.net/discoverer100/article/details/56676479" target="_blank" rel="noopener">图像卷积与信号卷积对照理解</a><br><a href="https://zhuanlan.zhihu.com/p/28478034" target="_blank" rel="noopener">[CV] 通俗理解『卷积』——从傅里叶变换到滤波器</a><br><a href="https://www.jianshu.com/p/e1b7caf3d736" target="_blank" rel="noopener">傅里叶变换和卷积与图像滤波的关系</a><br><a href="https://zhuanlan.zhihu.com/p/41455378" target="_blank" rel="noopener">傅里叶系列（一）傅里叶级数的推导</a><br><a href="https://zhuanlan.zhihu.com/p/41875010" target="_blank" rel="noopener">傅里叶系列（二）傅里叶变换的推导</a><br><a href="https://blog.csdn.net/zhu_hongji/article/details/81562746" target="_blank" rel="noopener">【OpenCV学习笔记】之卷积及卷积算子（convolution）</a><br><a href="https://blog.csdn.net/iracer/article/details/49330533" target="_blank" rel="noopener">如何构造频域滤波器——图像频域滤波的信号与系统基本理论</a><br><a href="https://www.zhihu.com/question/52237725/answer/545340892" target="_blank" rel="noopener">哪位高手能解释一下卷积神经网络的卷积核？</a><br><a href="https://zhuanlan.zhihu.com/p/23739221" target="_blank" rel="noopener">我理解的傅里叶变换</a>.<br><a href="https://www.zhihu.com/question/30242595" target="_blank" rel="noopener">傅里叶变换的意义是什么？</a><br><a href="https://zhuanlan.zhihu.com/p/32574762" target="_blank" rel="noopener">信号处理中的“基”</a><br><a href="https://baike.baidu.com/item/%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0%E5%85%AC%E5%BC%8F/4374733?fr=aladdin" target="_blank" rel="noopener">三角函数公式</a><br><a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E5%AE%9A%E7%90%86" target="_blank" rel="noopener">卷积定理</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/" title="卷积的本质及物理意义（全面理解卷积）">https://www.zdaiot.com/MachineLearning/卷积神经网络/卷积的本质及物理意义（全面理解卷积）/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/CNN/" rel="tag"><i class="fa fa-tag"></i> CNN</a><a href="/tags/傅里叶变换/" rel="tag"><i class="fa fa-tag"></i> 傅里叶变换</a><a href="/tags/卷积/" rel="tag"><i class="fa fa-tag"></i> 卷积</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MachineLearning/机器学习/谱聚类（spectral clustering）原理总结/" rel="prev" title="谱聚类（spectral clustering）原理总结"><i class="fa fa-chevron-left"></i> 谱聚类（spectral clustering）原理总结</a></div><div class="post-nav-item"> <a href="/Math/如何理解特征值和特征向量/" rel="next" title="如何理解特征值和特征向量">如何理解特征值和特征向量<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#信号的定义"><span class="nav-number">1.</span> <span class="nav-text">信号的定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积"><span class="nav-number">2.</span> <span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积来源"><span class="nav-number">2.1.</span> <span class="nav-text">卷积来源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积意义"><span class="nav-number">2.2.</span> <span class="nav-text">卷积意义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#信号处理角度"><span class="nav-number">2.2.1.</span> <span class="nav-text">信号处理角度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#幽默笑话——谈卷积的物理意义"><span class="nav-number">2.2.2.</span> <span class="nav-text">幽默笑话——谈卷积的物理意义</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积计算"><span class="nav-number">2.3.</span> <span class="nav-text">卷积计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#连续"><span class="nav-number">2.3.1.</span> <span class="nav-text">连续</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#一维卷积"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">一维卷积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#二维卷积"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">二维卷积</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#离散"><span class="nav-number">2.3.2.</span> <span class="nav-text">离散</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#一维卷积-1"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">一维卷积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#二维卷积-1"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">二维卷积</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积在具体学科中的应用"><span class="nav-number">2.4.</span> <span class="nav-text">卷积在具体学科中的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图像处理"><span class="nav-number">2.4.1.</span> <span class="nav-text">图像处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#电磁学"><span class="nav-number">2.4.2.</span> <span class="nav-text">电磁学</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信号处理"><span class="nav-number">2.4.3.</span> <span class="nav-text">信号处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多项式"><span class="nav-number">2.4.4.</span> <span class="nav-text">多项式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从基变换角度理解傅里叶变换"><span class="nav-number">3.</span> <span class="nav-text">从基变换角度理解傅里叶变换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基础理论"><span class="nav-number">3.1.</span> <span class="nav-text">基础理论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#域"><span class="nav-number">3.1.1.</span> <span class="nav-text">域</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性空间"><span class="nav-number">3.1.2.</span> <span class="nav-text">线性空间</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基与维数"><span class="nav-number">3.1.3.</span> <span class="nav-text">基与维数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内积"><span class="nav-number">3.1.4.</span> <span class="nav-text">内积</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#傅里叶基和傅里叶级数"><span class="nav-number">3.2.</span> <span class="nav-text">傅里叶基和傅里叶级数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#矢量表示为正交矢量集"><span class="nav-number">3.2.1.</span> <span class="nav-text">矢量表示为正交矢量集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三角函数的正交性"><span class="nav-number">3.2.2.</span> <span class="nav-text">三角函数的正交性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三角型傅里叶级数"><span class="nav-number">3.2.3.</span> <span class="nav-text">三角型傅里叶级数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#指数型傅里叶级数"><span class="nav-number">3.2.4.</span> <span class="nav-text">指数型傅里叶级数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#傅里叶变换"><span class="nav-number">3.3.</span> <span class="nav-text">傅里叶变换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#傅里叶分析的通俗理解"><span class="nav-number">4.</span> <span class="nav-text">傅里叶分析的通俗理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是频域"><span class="nav-number">4.1.</span> <span class="nav-text">什么是频域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#傅里叶级数-Fourier-Series-的频谱"><span class="nav-number">4.2.</span> <span class="nav-text">傅里叶级数(Fourier Series)的频谱</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#傅里叶级数（Fourier-Series）的相位谱"><span class="nav-number">4.3.</span> <span class="nav-text">傅里叶级数（Fourier Series）的相位谱</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#傅里叶变换（Fourier-Transformation）"><span class="nav-number">4.4.</span> <span class="nav-text">傅里叶变换（Fourier Transformation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#宇宙耍帅第一公式：欧拉公式"><span class="nav-number">4.5.</span> <span class="nav-text">宇宙耍帅第一公式：欧拉公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数形式的傅里叶变换"><span class="nav-number">4.6.</span> <span class="nav-text">指数形式的傅里叶变换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积和傅里叶分析的关系"><span class="nav-number">5.</span> <span class="nav-text">卷积和傅里叶分析的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积定理"><span class="nav-number">5.1.</span> <span class="nav-text">卷积定理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#时域卷积定理"><span class="nav-number">5.1.1.</span> <span class="nav-text">时域卷积定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#频域卷积定理"><span class="nav-number">5.1.2.</span> <span class="nav-text">频域卷积定理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图像处理中的卷积"><span class="nav-number">5.2.</span> <span class="nav-text">图像处理中的卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积的本质"><span class="nav-number">5.2.1.</span> <span class="nav-text">卷积的本质</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#问题与思考"><span class="nav-number">7.</span> <span class="nav-text">问题与思考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">8.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">320</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">54</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">386</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.5m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">37:48</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '8a2090d6b9fbb97731d550ba83b11897',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>