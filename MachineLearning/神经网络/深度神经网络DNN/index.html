<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="深度神经网络（Deep Neural Networks，以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型、前向传播算法与反向传播算法做一个总结。 从感知机到神经网络在感知机原理小结中，作者介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:  输出和输入之间学习到一个线性关系，得到中间输出结果："><meta name="keywords" content="反向传播,DNN"><meta property="og:type" content="article"><meta property="og:title" content="深度神经网络DNN"><meta property="og:url" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="深度神经网络（Deep Neural Networks，以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型、前向传播算法与反向传播算法做一个总结。 从感知机到神经网络在感知机原理小结中，作者介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:  输出和输入之间学习到一个线性关系，得到中间输出结果："><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936532.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936533.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936535.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936539.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936539_1.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936539_2.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/15430659365401543657888425.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/tikz18.png"><meta property="og:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/tikz19.png"><meta property="og:updated_time" content="2019-09-15T03:30:18.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="深度神经网络DNN"><meta name="twitter:description" content="深度神经网络（Deep Neural Networks，以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型、前向传播算法与反向传播算法做一个总结。 从感知机到神经网络在感知机原理小结中，作者介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:  输出和输入之间学习到一个线性关系，得到中间输出结果："><meta name="twitter:image" content="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/1543065936532.png"><link rel="canonical" href="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>深度神经网络DNN | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 深度神经网络DNN<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/MachineLearning/神经网络/深度神经网络DNN.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-11-24 21:23:06" itemprop="dateCreated datePublished" datetime="2018-11-24T21:23:06+08:00">2018-11-24</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-09-15 11:30:18" itemprop="dateModified" datetime="2019-09-15T11:30:18+08:00">2019-09-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/神经网络/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>250</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>深度神经网络（Deep Neural Networks，以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型、前向传播算法与反向传播算法做一个总结。</p><h2 id="从感知机到神经网络"><a href="#从感知机到神经网络" class="headerlink" title="从感知机到神经网络"></a>从感知机到神经网络</h2><p>在<a href="http://www.cnblogs.com/pinard/p/6042320.html" target="_blank" rel="noopener">感知机原理小结</a>中，作者介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/1543065936532.png" alt></p><p>输出和输入之间学习到一个线性关系，得到中间输出结果：</p><script type="math/tex;mode=display">z=\sum\limits_{i=1}^mw_ix_i + b</script><p>接着是一个神经元激活函数:</p><script type="math/tex;mode=display">sign(z)= \begin{cases} -1& {z<0}\\\\\ 1& {z\geq 0} \end{cases}</script><p>从而得到我们想要的输出结果1或者-1。 这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。</p><p>而神经网络则在感知机的模型上做了扩展，总结下主要有三点：</p><p>1) <strong>加入了隐藏层</strong>，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/1543065936533.png" alt></p><p>2) <strong>输出层的神经元也可以不止一个输出，可以有多个输出</strong>，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/1543065936535.png" alt></p><blockquote><p>由该图可以看出，该输出层输出的维度为4，由$z=\sum\limits_{i=1}^mw_ix_i + b$可得输出层由4个偏置，即等于输出层神经元的数目。同理，对于图中的hidden layer层，一个神经元对应一个偏置。</p></blockquote><p>3）<strong>对激活函数做扩展</strong>，感知机的激活函数是$sign(z)$,虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：</p><script type="math/tex;mode=display">f(z)=\frac{1}{1+e^{-z}}</script><p>还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。</p><h2 id="DNN的基本结构"><a href="#DNN的基本结构" class="headerlink" title="DNN的基本结构"></a>DNN的基本结构</h2><p>上一节我们了解了神经网络基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准,多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，<strong>DNN有时也叫做多层感知机</strong>（Multi-Layer perceptron,MLP）,名字实在是多。后面我们讲到的神经网络都默认为DNN。</p><p>从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/1543065936539.png" alt></p><p><strong>层与层之间是全连接的</strong>，也就是说，第$i$层的任意一个神经元一定与第$i+1$层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系$z=\sum\limits w_ix_i + b$加上一个激活函数$\sigma(z)$。</p><p>由于DNN层数多，则我们的线性关系系数$w$和偏置$b$的数量也就是很多了。具体的参数在DNN是如何定义的呢？</p><p>首先我们来看看线性关系系数$w$的定义。以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w_{24}^3$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是$w_{42}^3$,而是$w_{24}^3$呢？这主要是为了便于模型用于矩阵表示运算，如果是$w_{42}^3$而每次进行矩阵运算是$w^Tx+b$，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置，即直接为$wx+b$。证明将在下面给出。总结下，第$l-1$层的第k个神经元到第$l$层的第j个神经元的线性系数定义为$w_{jk}^l$。注意，<strong>输入层是没有$w$参数的</strong>。</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/1543065936539_1.png" alt></p><p>再来看看偏置$b$的定义。我们使用$b^l_j$代表第$l$层中第$j$个元素的偏置，而$a^l_j$第$l$层中第$j$个元素的激活值。还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b_3^{2}$。其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三个的第一个神经元的偏倚应该表示为$b_1^{3}$。同样的，<strong>输入层是没有偏置参数$b$的</strong>。</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/1543065936539_2.png" alt></p><h2 id="DNN前向传播算法数学原理"><a href="#DNN前向传播算法数学原理" class="headerlink" title="DNN前向传播算法数学原理"></a>DNN前向传播算法数学原理</h2><p>在上一节，我们已经介绍了DNN各层线性关系系数$w$,偏置$b$的定义。假设我们选择的激活函数是$\sigma(z)$，隐藏层和输出层的输出值为$a$，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/15430659365401543657888425.png" alt></p><p>对于第二层的的输出$a_1^2,a_2^2,a_3^2$，我们有：</p><script type="math/tex;mode=display">a_1^2=\sigma(z_1^2) = \sigma(w_{11}^2x_1 + w_{12}^2x_2 + w_{13}^2x_3 + b_1^{2})</script><script type="math/tex;mode=display">a_2^2=\sigma(z_2^2) = \sigma(w_{21}^2x_1 + w_{22}^2x_2 + w_{23}^2x_3 + b_2^{2})</script><script type="math/tex;mode=display">a_3^2=\sigma(z_3^2) = \sigma(w_{31}^2x_1 + w_{32}^2x_2 + w_{33}^2x_3 + b_3^{2})</script><p>对于第三层的的输出$a_1^3$，我们有：</p><script type="math/tex;mode=display">a_1^3=\sigma(z_1^3) = \sigma(w_{11}^3a_1^2 + w_{12}^3a_2^2 + w_{13}^3a_3^2 + b_1^{3})</script><p>将上面的例子一般化，假设第$l-1$层共有m个神经元，则对于第$l$层的第j个神经元的输出$a_j^l$，我们有：</p><script type="math/tex;mode=display">\begin{eqnarray}
a_j^l = \sigma(z_j^l) = \sigma(\sum\limits_{k=1}^mw_{jk}^la_k^{l-1} + b_j^l)
\end{eqnarray}</script><p>其中，如果$l=2$，则对于的$a_k^1$即为输入层的$x_k$。</p><blockquote><p>从表达式中可以看出来，在DNN中<strong>每一个输出都对应一个偏置</strong>，偏置的数量与该层输出的神经元数目有关。而在CNN中，输入特征经过一个卷积核输出一个特征图，<strong>每一个特征图对应一个偏置</strong>，而<strong>一个batch之内的相同位置特征图的偏置是同一个偏置</strong>。之所以这样，跟偏置的作用是增加表达式的非线性有关。思考，对于DNN或者CNN而言，每一个输出都是线性组合，为了增加非线性，直接在输出增加非线性即可。若对于每一个权重都增加非线性，会浪费计算量，并且偏置都是常数，一个常数可以看成是由多个常数之和。</p></blockquote><p>从上面可以看出，使用代数法一个个的表示输出比较复杂，而如果使用矩阵法则比较的简洁。假设第$l-1$层共有$m$个神经元，而第$l$层共有$n$个神经元，则第$l$层的线性系数$w$<strong>组成了一个$n\times m$的矩阵$W^l$,</strong> 第$l$层的偏置$b$组成了一个$n \times 1$的向量$b^l$ ,第$l-1$层的的输出$a$组成了一个$m \times 1$的向量$a^{l-1}$，第$l$层的的未激活前线性输出$z$组成了一个$n \times 1$的向量$z^{l}$, 第$l$层的的输出$a$组成了一个$n \times 1$的向量$a^{l}$。最后我们需要对$\sigma$完成向量化，记做$\sigma(v)$，其中$\sigma(v)_j = \sigma(v_j)$。例如我们的函数为$f(x) = x^2$，向量化$f$有如下表达式：</p><script type="math/tex;mode=display">f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)
  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]
  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right]</script><p>将式子$({1})$用矩阵的形式表示出来。</p><script type="math/tex;mode=display">\begin{eqnarray}
a^l = \sigma(z^l) = \sigma(W^la^{l-1} + b^l)
\end{eqnarray}</script><p>这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。另外，这个表达式为我们提供了一种更全面的思考一层中的激活值与上一层中的激活值之间的关系的方法：我们只需将权重矩阵与上一层激活值相乘，然后添加偏置向量，最后应用$σ$函数。这样就可以使用矩阵形式进行运算，编程中可以使用大量的库。</p><p>关于上面所说的为什么不使用$w^l_{kj}$。如果我们使用的是$w^l_{kj}$，假设第$l-1$层的维度为$m$，而第$l$层的维度为$n$,则$W$的维度为$m \times n$，第$a^{l-1}$层的维度为$m \times 1$，我们写成矩阵形式的化就需要写成$a^{l} = \sigma((W^l)^T a^{l-1}+b^l)$，即对$W$加上转置。</p><p>当计算$a^l$的时候，我们需要计算中间变量$z^l \equiv W^l a^{l-1}+b^l$，我们称$z^l$为层$l$中神经元的加权输入。$z^l$也可以拆开为$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$。$z^l_j$含义为第$l$层第$j$个加权输入。</p><h2 id="DNN前向传播算法"><a href="#DNN前向传播算法" class="headerlink" title="DNN前向传播算法"></a>DNN前向传播算法</h2><p>有了上一节的数学推导，DNN的前向传播算法也就不难了。所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵$W$,偏置向量$b$来和输入值向量$x$进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。</p><p>输入: 总层数L，所有隐藏层和输出层对应的矩阵$W$,偏置向量$b$，输入值向量$x$</p><p>输出：输出层的输出$a^L$</p><p>1) 初始化$a^1 = x $</p><p>2) for $l = 2$ to $L$, 计算：<script type="math/tex">a^l = \sigma(z^l) = \sigma(W^la^{l-1} + b^l)</script><br>最后的结果即为输出$a^L$。</p><h2 id="DNN反向传播算法要解决的问题"><a href="#DNN反向传播算法要解决的问题" class="headerlink" title="DNN反向传播算法要解决的问题"></a>DNN反向传播算法要解决的问题</h2><p>在上一节中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。</p><p>在了解DNN的反向传播算法前，我们先要知道DNN反向传播算法要解决的问题，也就是说，什么时候我们需要这个反向传播算法？</p><p>回到我们监督学习的一般问题，假设我们有$m$个训练样本：$(x_1,y_1), (x_2,y_2), …,(x_m,y_m)$,其中$x$为输入向量，特征维度为$n_{in}$,而$y$为输出向量，特征维度为${n_{out}}$。我们需要利用这$m$个样本训练出一个模型，当有一个新的测试样本$(x_{test},?)$来到时,我们可以预测$y_{test}$向量的输出。</p><p>如果我们采用DNN的模型，即我们使输入层有$n_{in}$个神经元，而输出层有$n_{out}$个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵$W$,偏置向量$b$,让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。怎么找到合适的参数呢？</p><p>如果大家对传统的机器学习的算法优化过程熟悉的话，这里就很容易联想到我们可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵$W$，偏置向量$b$即为我们的最终结果。对于DNN而言，我们无法找到精确地数值解，所以在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。如果大家对梯度下降法不熟悉，建议先阅读一个大神写的<a href="http://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="noopener">梯度下降（Gradient Descent）小结</a>。</p><p>对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。</p><p>可以理解为反向传播算法将损失函数值反向传播到每一个权重和偏置上，即<strong>计算当我们改变权重和偏差时，损失函数的变化有多快</strong>，对应的数学表达式为$\partial J / \partial W$与$\partial J / \partial b$。其中$J$是损失函数，而$W$是权重，$b$为偏置。而梯度下降法、SGD等是如何将计算出的$\partial J / \partial W$与$\partial J / \partial b$的值进行权重和偏置更新的。</p><h2 id="损失函数的两个假设"><a href="#损失函数的两个假设" class="headerlink" title="损失函数的两个假设"></a>损失函数的两个假设</h2><p>在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。你也许会问：训练样本计算出的输出是怎么得来的？这个输出是随机选择一系列$W,b$,用我们上一节的前向传播算法计算出来的。即通过一系列的计算：$a^l = \sigma(z^l) = \sigma(W^la^{l-1} + b^l)$。计算到输出层第$L$层对应的$a^L$即为前向传播算法计算出来的输出。</p><p>这里我们定义激活函数：</p><script type="math/tex;mode=display">\begin{eqnarray}
  J(W,b,x,y) = \frac{1}{2m} \sum_x \|y(x)-a^L(x)\|_2^2,
\end{eqnarray}</script><p>其中，$m$是训练样本的总个数。我们关于损失函数的<strong>第一个假设</strong>是$J$可以写成$J = \frac{1}{n} \sum_x J_x$，$J_x$为每个独立训练样本$x$的损失值，具体表达式为$J_x = \frac{1}{2} |y-a^L |_2^2$。之所以做这样的假设，是因为反向传播中使用梯度下降法进行优化，经常让求每个训练样本的偏导数$\partial J_x / \partial W$和$\partial J_x / \partial b$。通过对$\partial J_x / \partial W$和$\partial J_x / \partial b$求平均，我们可以得到$\partial J / \partial W$和$\partial J / \partial b$。为了表达的简便，我们使用$J$代替$J_x$。</p><p>我们关于损失函数的<strong>第二个假设</strong>是，它可以写成神经网络输出的函数：</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/tikz18.png" alt></p><blockquote><p>上图中使用了$C$表示误差，在我们文中使用$J$表示误差。</p></blockquote><p>例如，我们这里的对于<strong>一个样本的平方误差函数</strong>就可以满足条件：</p><script type="math/tex;mode=display">\begin{eqnarray}
J(W,b,x,y) = \frac{1}{2} \|y-a^L\|_2^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,
\end{eqnarray}</script><blockquote><p>我们使用$y(x)$表示样本$x$的输出，而使用$y_j$表示某一个特定样本的输出中的第$j$个维度。</p></blockquote><p>这里不把损失函数看成是$y$的函数，是因为$x$固定的那么$y$也是固定的，也就是说对于神经网络这不是一个可以学习的参数。因此，把损失函数看做是$a^L$的函数更有意义。而$y$仅仅是确定损失函数值的一个参数。</p><h2 id="DNN反向传播算法的基本思路"><a href="#DNN反向传播算法的基本思路" class="headerlink" title="DNN反向传播算法的基本思路"></a>DNN反向传播算法的基本思路</h2><p>DNN可选择的损失函数有不少，通过上节关于损失函数两个假设的讨论，这里我们使用最常见的均方差来度量损失。即<strong>对于每个样本</strong>，我们期望最小化下式：</p><script type="math/tex;mode=display">\begin{eqnarray} J(W,b,x,y) = \frac{1}{2}||a^L-y||_2^2 \end{eqnarray}</script><p>其中，$a^L$和$y$为特征维度为$n_{out}$的向量，而$||a^L-y||_2$为$a^L-y$的L2范数。</p><p>反向传播实际上计算的是$\partial J / \partial W^l_{jk}$与$\partial J / \partial b^l_j$。在这之前，我们需要计算中间量$\delta^l_j$，它是第$l$层第$j$个神经元的损失值。</p><h3 id="中间量-delta-l-j"><a href="#中间量-delta-l-j" class="headerlink" title="中间量$\delta^l_j$"></a>中间量$\delta^l_j$</h3><p>为了理解中间量$\delta^l_j$是如何定义的，想象一下我们的神经网络中有一个恶魔：</p><p><img src="/MachineLearning/神经网络/深度神经网络DNN/tikz19.png" alt></p><p>恶魔所在的位置是第$l$层的第$j$个神经元位置，当神经元的输入进来时，恶魔就会扰乱神经元的运作,对于神经元的加权输入增加变化$\Delta z^l_j$，则我们需要计算的是$\sigma(z^l_j+\Delta z^l_j)$而不是$\sigma(z^l_j)$。这种变化会在网络的后续层中传播，最终导致总损失函数值的变化$\frac{\partial J}{\partial z^l_j} \Delta z^l_j$。</p><p>现在假设恶魔是好的，他试图帮助你降低损失函数值。假设$\frac{\partial J}{\partial z^l_j}$有一个比较大的值，无论是正还是负，只要$\Delta z^l_j$的符号与$\frac{\partial J}{\partial z^l_j}$相反，总损失函数值的变化$\frac{\partial J}{\partial z^l_j} \Delta z^l_j$就会小于零，神经网络就会得到优化。当$\frac{\partial J}{\partial z^l_j}$的值接近零的时候，通过对$z^l_j$干扰，不能使得损失函数值下降。这时神经网络已经取得最优解。受到这个启发，使用$\frac{\partial J}{\partial z^l_j}$度量神经元的损失值。</p><p>第$l$层第$j$个神经元的损失值$\delta^l_j$可以如下定义：</p><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^l_j \equiv \frac{\partial J}{\partial z^l_j}.
\end{eqnarray}</script><p>我们定义$\delta^l$为层$l$误差的向量形式。</p><p>之所以恶魔选择改变$z^l_j$和$\frac{\partial J}{\partial z^l_j}$，而不是$a^l_j$和$\frac{\partial J}{\partial a^l_j}$，是因为选择后者可以得到相同的结论，但是会使得反向传播公式比较复杂。</p><p>现在，损失函数$J$有了，中间量$\delta^l_j$来源介绍了。现在我们开始用梯度下降法迭代求解每一层的$W,b$。</p><h3 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h3><p>假设$s$和$t$是两个维度相同的向量，我们使用$s \odot t$表示两个向量之间的点乘，也就是说$(s \odot t)_j = s_j t_j$。例如：</p><script type="math/tex;mode=display">
\left[\begin{array}{c} 1 \\ 2 \end{array}\right] 
  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].</script><h3 id="输出层的-delta-L"><a href="#输出层的-delta-L" class="headerlink" title="输出层的$\delta^L$"></a>输出层的$\delta^L$</h3><p>首先写出$\delta^L$<strong>离散形式</strong></p><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^L_j = \frac{\partial J}{\partial a^L_j} \times \frac{\partial a^L_j}{\partial z^L_j} = \frac{\partial J}{\partial a^L_j} \sigma'(z^L_j).
\tag{BP1}\end{eqnarray}</script><p>$\partial J / \partial a^L_j$衡量损失函数随第$j$次输出的激活函数值而变化的速度有多快。第二个$\sigma’(z^L_j)$衡量激活函数$σ$在$z^L_j$处变化的速度。</p><p>这个式子中$z^L_j$在前向传播过程中已经被计算，而之前假设了损失函数可以写成输出的形式，$\partial J / \partial a^L_j$也就很好计算。例如损失函数为$J = \frac{1}{2} \sum_j (y_j-a^L_j)^2$，则$\partial J / \partial a^L_j = (a_j^L-y_j)$。这个也很好计算。</p><p>因此，第一个方程可以进一步写作：</p><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^L_j = \frac{\partial J}{\partial a^L_j} \times \frac{\partial a^L_j}{\partial z^L_j} = \frac{\partial J}{\partial a^L_j} \sigma'(z^L_j)= (a_j^L-y_j)\times \sigma^{'}(z_j^L)
\tag{BP11}\end{eqnarray}</script><p>为了方便计算，我们将公式$({BP1})$写成<strong>矩阵形式</strong></p><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^L= \frac{\partial J}{\partial a^L}\odot \sigma'(z^L) = \nabla_a J \odot \sigma'(z^L).
\tag{BP1a}\end{eqnarray}</script><p>其中，$\nabla_a J$是$\partial J / \partial a^L_j$向量形式。又因为对于均方差误差函数</p><script type="math/tex;mode=display">\begin{eqnarray} \nabla_a J = \frac{\partial J}{\partial a^L}= (a^L-y) \end{eqnarray}</script><p>因此对于均方误差函数$({BP1})$的<strong>完整矩阵形式</strong></p><script type="math/tex;mode=display">\begin{eqnarray} \delta^L = \frac{\partial J(W,b,x,y)}{\partial z^L} = (a^L-y)\odot \sigma^{'}(z^L) \tag{BP1aa} \end{eqnarray}</script><h4 id="式子-BP1-证明"><a href="#式子-BP1-证明" class="headerlink" title="式子$({BP1})$证明"></a>式子$({BP1})$证明</h4><p>这里我们使用下标$j$代表第$L$层中间输出$z$的第$j$元素；下标$k$代表第$L$层输出$a$的第$k$元素。</p><script type="math/tex;mode=display">\begin{eqnarray}
\delta^L_j = \frac{\partial C}{\partial z^L_j} = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j}
\end{eqnarray}</script><p>因为$k \neq j$的时候，$\partial a^L_k / \partial z^L_j$消失。所以上式可以写为：</p><script type="math/tex;mode=display">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.
\end{eqnarray}</script><p>因为$a^L_j = \sigma(z^L_j)$，所以右边第二项可以写为$\sigma’(z^L_j)$，可得下式得证。</p><script type="math/tex;mode=display">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j),
\end{eqnarray}</script><h4 id="式子-BP11-证明"><a href="#式子-BP11-证明" class="headerlink" title="式子$({BP11})$证明"></a>式子$({BP11})$证明</h4><p>关于式子$({BP11})$的推导，可以将分子写成$\frac{1}{2} \left\{(y_1-a^L_1)^2+(y_2-a^L_2)^2+\dots \right\}$，这是一个标量，接着将式子按照<strong>分母布局</strong>运算格式展开。对于第$j$项，</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial J}{\partial a^L_j} = \frac{\partial( \frac{1}{2} \left\{(y_1-a^L_1)^2+(y_2-a^L_2)^2+\dots \right\})} {\partial a^L_j }\end{eqnarray}</script><p>可以看出来，分子还是求和形式，但是只有$\frac{1}{2}(y_j-a^L_j)^2$与$a^L_j$有关，因此求导可得 $(a_j^L-y_j)$。带入式子$({BP11})$中第三项即可得证。</p><h4 id="式子-BP1aa-证明"><a href="#式子-BP1aa-证明" class="headerlink" title="式子$({BP1aa})$证明"></a>式子$({BP1aa})$证明</h4><p>注意到式${(5)}$中$\frac{1}{2}|| \sigma(z^L)-y ||_2^2=\frac{1}{2} \sum_{i=1}^{n_{out}}(\sigma_i(z^L)-y_i)^2$，那么可得：</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial J(W,b,x,y)}{\partial z^L}=\frac{\partial \frac{1}{2} \sum_{i=1}^{n_{out}}(\sigma_i(z^L)-y_i)^2}{\partial z^L} \end{eqnarray}</script><p>而在上式中，$\frac{1}{2} \sum_{i=1}^{n_{out}}(\sigma(z_i^L)-y_i)^2$为标量，$\partial z^L$为向量，按照分母布局，根据标量对向量的求导公式，可以得到下式：</p><script type="math/tex;mode=display">\begin{eqnarray}  \frac{\partial J(W,b,x,y)}{\partial z^L}=\frac{\partial \frac{1}{2} \sum_{i=1}^{n_{out}}(\sigma(z_i^L)-y_i)^2}{\partial z^L} = \frac{1}{2} \frac{\partial \left \{(\sigma(z_1^L)-y_1)^2 + (\sigma(z_2^L)-y_2)^2 + \dots + (\sigma(z_{n_{out}}^L)-y_{n_{out}}) ^2 \right \} }{\partial z^L} \\=\frac{1}{2} \begin{aligned}& \begin{bmatrix}
\frac{\partial  \left \{(\sigma(z_1^L)-y_1)^2 + (\sigma(z_2^L)-y_2)^2 + \dots + (\sigma(z_{n_{out}}^L)-y_{n_{out}}) ^2 \right \}}{\partial z_1^L}\\
\frac{\partial  \left \{(\sigma(z_1^L)-y_1)^2 + (\sigma(z_2^L)-y_2)^2 + \dots + (\sigma(z_{n_{out}}^L)-y_{n_{out}}) ^2 \right \}}{\partial z_2^L}\\
\vdots\\
\frac{\partial  \left \{(\sigma(z_1^L)-y_1)^2 + (\sigma(z_2^L)-y_2)^2 + \dots + (\sigma(z_{n_{out}}^L)-y_{n_{out}}) ^2 \right \}}{\partial z_{n_{out}}^L}\\
\end{bmatrix}
\end{aligned}  \begin{aligned} &=\begin{bmatrix}
(\sigma(z_1^L)-y_1) \times \sigma' (z_1^L)\\
(\sigma(z_2^L)-y_2) \times \sigma' (z_2^L) \\
\vdots\\
(\sigma(z_{n_{out}}^L)-y_{n_{out}}) \times \sigma' (z_{n_{out}}^L) \\
\end{bmatrix}
\end{aligned} = (a^L-y)\odot \sigma^{'}(z^L) \end{eqnarray}</script><blockquote><p>其中，$\sigma^{‘}(z^L)$也是一个向量，可以这么计算。例如这里假设$\sigma=ln(x)$，将$z^L$向量拆成$z_i^L$的标量，带入到$\sigma^{‘}=\frac{1}{x}$即可。</p></blockquote><h3 id="已知-delta-l-1-推导-delta-l"><a href="#已知-delta-l-1-推导-delta-l" class="headerlink" title="已知$\delta^{l+1}$推导$\delta^l$"></a>已知$\delta^{l+1}$推导$\delta^l$</h3><p>这里我们用数学归纳法，第$L$层的$\delta^{L}$上面我们已经求出，假设第$l+1$层的$\delta^{l+1}$已经求出来了，那么我们如何求出第$l$层的$\delta^{l}$呢？我们先给出结论。</p><p>首先写出$\delta^l$的离散形式</p><script type="math/tex;mode=display">\begin{eqnarray}
  \delta^l_j = \sum_k W^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{BP2}\end{eqnarray}</script><p>为了计算方便，我们把上式表示成矩阵的形式</p><script type="math/tex;mode=display">\begin{eqnarray} \delta^{l} = (W^{l+1})^T\delta^{l+1}\odot \sigma^{'}(z^l)  \tag{BP2a} \end{eqnarray}</script><h4 id="式子-BP2-证明"><a href="#式子-BP2-证明" class="headerlink" title="式子$({BP2})$证明"></a>式子$({BP2})$证明</h4><p>这里我们使用下标$j$代表第$l$层第$j$元素；下标$k$代表第$l+1$层第$k$元素。我们假设第$l$层的维度为$n \times 1$，而第$l+1$层的维度为$m \times 1$</p><script type="math/tex;mode=display">\begin{eqnarray}
  \delta^l_j & = & \frac{\partial J}{\partial z^l_j} \\
  & = & \sum_k\frac{\partial J}{\partial z^{l+1}_k}  \frac{\partial z^{l+1}_k}{\partial z^l_j} \\ 
  & = &\sum_k  \delta^{l+1}_k \frac{\partial z^{l+1}_k}{\partial z^l_j} ,
\end{eqnarray}</script><p>在这里，因为对于特定的$j,k$，$\delta^{l+1}_k$与$\frac{\partial z^{l+1}_k}{\partial z^l_j}$均是标量，因此前后顺序无所谓。</p><p>又因为</p><script type="math/tex;mode=display">\begin{eqnarray}
z^{l+1}_k = \sum_j W^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j W^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k 
= W^{l+1}_{k1} \sigma(z^l_1) + W^{l+1}_{k2} \sigma(z^l_2) + \dots + W^{l+1}_{kn} \sigma(z^l_n) +b^{l+1}_k 
\end{eqnarray}</script><p>所以</p><script type="math/tex;mode=display">\begin{eqnarray}
\frac{\partial z^{l+1}_k}{\partial z^l_j}
= \frac{\partial(W^{l+1}_{k1} \sigma(z^l_1) + W^{l+1}_{k2} \sigma(z^l_2) + \dots + W^{l+1}_{kn} \sigma(z^l_n) +b^{l+1}_k) }{\partial z^l_j}
\end{eqnarray}</script><p>按照分母布局，最终$\frac{\partial z^{l+1}_k}{\partial z^l_j}$的维度为$m \times n$，我们按照分母布局对上式进行分析，分析过程如下：</p><p>对于结果中第一行，即当$j=1$的时候，取$k=1$，求导结果为$w_{11}^{l+1}\sigma’(z^l_1)$；取$k=2$，求导结果为$w_{21}^{l+1}\sigma’(z^l_1)$；取$k=n$，求导结果为$w_{n1}^{l+1}\sigma’(z^l_1)$。</p><p>对于结果中第二行，即当$j=2$的时候，取$k=1$，求导结果为$w_{12}^{l+1}\sigma’(z^l_2)$；取$k=2$，求导结果为$w_{22}^{l+1}\sigma’(z^l_2)$；取$k=n$，求导结果为$w_{n2}^{l+1}\sigma’(z^l_2)$。</p><p>对于结果中第m行，即当$j=m$的时候，取$k=1$，求导结果为$w_{1m}^{l+1}\sigma’(z^l_m)$；取$k=2$，求导结果为$w_{2m}^{l+1}\sigma’(z^l_m)$；取$k=n$，求导结果为$w_{nm}^{l+1}\sigma’(z^l_m)$。</p><p>综合上述分析，可以得到最终的结果</p><script type="math/tex;mode=display">\begin{eqnarray}
\frac{\partial z^{l+1}_k}{\partial z^l_j}
= \frac{\partial(W^{l+1}_{k1} \sigma(z^l_1) + W^{l+1}_{k2} \sigma(z^l_2) + \dots + W^{l+1}_{kn} \sigma(z^l_n) +b^{l+1}_k) }{\partial z^l_j}
= W^{l+1}_{kj} \sigma'(z^l_j)
\end{eqnarray}</script><p>对于特定的$j,k$，$W^{l+1}_{kj}$与$ \sigma’(z^l_j)$均是标量，而$\delta^{l+1}_k$也是一个标量，因此可以互换顺序，代回方程$({17})$，即可得证。</p><h4 id="式子-BP2a-证明"><a href="#式子-BP2a-证明" class="headerlink" title="式子$({BP2a})$证明"></a>式子$({BP2a})$证明</h4><p>这里我们假设所有的向量均为<strong>列向量</strong>，而所有的标量对向量求导均采取<strong>分母布局</strong>的形式，假设$z^{l+1}$的维度为$n \times 1$，$z^l$的维度为$m \times 1$。根据分母布局，$\frac{\partial z^{l+1}}{\partial z^l}$的维度为$m \times n$，而$\delta^{l+1}$的维度为$n \times 1$。为了保证矩阵可以正常相乘，这里需要将$\frac{\partial z^{l+1}}{\partial z^{l}} $放到前面，将$\delta^{l+1}$放到后面。</p><script type="math/tex;mode=display">\begin{eqnarray} \delta^{l} = \frac{\partial J(W,b,x,y)}{\partial z^l} = \frac{\partial z^{l+1}}{\partial z^{l}} \frac{\partial J(W,b,x,y)}{\partial z^{l+1}}= \frac{\partial z^{l+1}}{\partial z^{l}}
\delta^{l+1}\end{eqnarray}</script><p>可见，用归纳法递推$\delta^{l+1}$和$\delta^{l}$的关键在于求解$\frac{\partial z^{l+1}}{\partial<br>z^{l}}$。 而$z^{l+1}$和$z^{l}$的关系其实很容易找出：</p><script type="math/tex;mode=display">\begin{eqnarray} z^{l+1}= W^{l+1}a^{l} + b^{l+1} = W^{l+1}\sigma(z^l) + b^{l+1} \end{eqnarray}</script><p>注意到这里的$z^{l+1}$是$n \times 1$的向量，而$z^l$是$m \times 1$的向量，$w^{l+1}$是$n \times m$的向量，$n^{l+1}$是$n \times 1$的向量。则$z^{l+1}$可以表示如下形式：</p><script type="math/tex;mode=display">\begin{eqnarray} \begin{aligned}
z^{l+1}&=\begin{bmatrix}
W_{11}^{l+1} & W_{12}^{l+1} & \dots & W_{1m}^{l+1}\\
W_{21}^{l+1} & W_{22}^{l+1} & \dots & W_{2m}^{l+1}\\
\vdots & \vdots & \dots & \vdots\\
W_{n1}^{l+1} & W_{n2}^{l+1} & \dots & W_{nm}^{l+1}\\
\end{bmatrix} \times \begin{bmatrix}
\sigma'(z_1^l)\\
\sigma'(z_2^l)\\
\vdots\\
\sigma'(z_m^l)\\
\end{bmatrix}
\end{aligned} + \begin{bmatrix}
b_1^{l+1}\\
b_2^{l+1}\\
\vdots\\
b_m^{l+1}\\
\end{bmatrix}\\= \begin{bmatrix}
W_{11}^{l+1}\times \sigma'(z_1^l) + W_{12}^{l+1}\times \sigma'(z_2^l) + \dots + W_{1m}^{l+1} \times \sigma'(z_m^l) + b_1^{l+1}\\
W_{21}^{l+1}\times \sigma'(z_1^l) + W_{22}^{l+1}\times \sigma'(z_2^l) + \dots + W_{2m}^{l+1} \times \sigma'(z_m^l) + b_2^{l+1}\\
\vdots \\
W_{n1}^{l+1}\times \sigma'(z_1^l) + W_{n2}^{l+1}\times \sigma'(z_2^l) + \dots + W_{nm}^{l+1} \times \sigma'(z_m^l) + b_m^{l+1}\\
\end{bmatrix} \end{eqnarray}</script><p>在上式中，这是一个$n \times 1$的列向量，而$z^l$是$m \times 1$的列向量，这里用到了列向量对列向量求导，我们采用<strong>分母布局</strong>计算此式，即标量对列向量求导为列向量，列向量对列向量求导为行向量。所以可得下式：</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial z^{l+1}}{\partial z^{l}}=\begin{aligned}\begin{bmatrix}
\frac{\partial z^{l+1}}{\partial z_1^{l}}\\
\frac{\partial z^{l+1}}{\partial z_2^{l}}\\
\vdots\\
\frac{\partial z^{l+1}}{\partial z_m^{l}}\\
\end{bmatrix} \end{aligned}=\begin{bmatrix}
\frac{\partial z_1^{l+1}}{\partial z_1^{l}} & \frac{\partial z_2^{l+1}}{\partial z_1^{l}} & \dots & \frac{\partial z_n^{l+1}}{\partial z_1^{l}}\\
\frac{\partial z_1^{l+1}}{\partial z_2^{l}} & \frac{\partial z_2^{l+1}}{\partial z_2^{l}} & \dots & \frac{\partial z_n^{l+1}}{\partial z_2^{l}}\\
\vdots  & \vdots & \dots & \vdots\\
\frac{\partial z_1^{l+1}}{\partial z_m^{l}} & \frac{\partial z_2^{l+1}}{\partial z_m^{l}} & \dots & \frac{\partial z_n^{l+1}}{\partial z_m^{l}}\\
\end{bmatrix} \\
=\begin{bmatrix}
W_{11}^{l+1}\times  \sigma'(z_1^l) & W_{21}^{l+1}\times \sigma'(z_1^l) & \dots & W_{n1}^{l+1}\times \sigma'(z_1^l) \\
W_{12}^{l+1}\times \sigma'(z_2^l) & W_{22}^{l+1}\times \sigma'(z_2^l) & \dots & W_{n2}^{l+1}\times \sigma'(z_2^l)\\
\vdots  & \vdots & \dots & \vdots\\
W_{1m}^{l+1} \times \sigma'(z_m^l) & W_{2m}^{l+1} \times \sigma'(z_m^l) & \dots & W_{nm}^{l+1} \times \sigma'(z_m^l)\\
\end{bmatrix}\end{eqnarray}</script><p>这样也就很容易写成下式：</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial z^{l+1}}{\partial z^{l}} = (W^{l+1})^T\odot \underbrace{(\sigma^{'}(z^l),..,\sigma^{'}(z^l))}_{n_{l+1}}\end{eqnarray}</script><p>将上式带入上面$\delta^{l+1}$和$\delta^{l}$关系式我们可以得到下式：</p><script type="math/tex;mode=display">\begin{eqnarray} \delta^{l} = \frac{\partial z^{l+1}}{\partial z^{l}}\delta^{l+1} = 
\begin{bmatrix}
W_{11}^{l+1}\times  \sigma'(z_1^l) \times \delta_1^{l+1} + W_{21}^{l+1}\times \sigma'(z_1^l) \times \delta_2^{l+1} + \dots + W_{n1}^{l+1}\times \sigma'(z_1^l)\times \delta_n^{l+1} \\
W_{12}^{l+1}\times  \sigma'(z_2^l) \times \delta_1^{l+1} + W_{22}^{l+1}\times \sigma'(z_2^l) \times \delta_2^{l+1} + \dots + W_{n2}^{l+1}\times \sigma'(z_2^l)\times \delta_n^{l+1}\\
\vdots\\
W_{1m}^{l+1} \times \sigma'(z_m^l) \times \delta_1^{l+1} + W_{2m}^{l+1} \times \sigma'(z_m^l)\times \delta_2^{l+1} + \dots + W_{nm}^{l+1} \times \sigma'(z_m^l)\times \delta_n^{l+1}\\
\end{bmatrix} = \\
\begin{bmatrix}
W_{11}^{l+1}\times  \sigma'(z_1^l) + W_{21}^{l+1}\times \sigma'(z_1^l) + \dots + W_{n1}^{l+1}\times \sigma'(z_1^l) \\
W_{12}^{l+1}\times \sigma'(z_2^l) + W_{22}^{l+1}\times \sigma'(z_2^l) + \dots + W_{n2}^{l+1}\times \sigma'(z_2^l)\\
\vdots\\
W_{1m}^{l+1} \times \sigma'(z_m^l) + W_{2m}^{l+1} \times \sigma'(z_m^l) + \dots + W_{nm}^{l+1} \times \sigma'(z_m^l)\\
\end{bmatrix}
\odot
\begin{bmatrix}
\sigma'(z_1^l)\\
\sigma'(z_2^l)\\
\vdots\\
\sigma'(z_m^l)\\
\end{bmatrix}= (W^{l+1})^T\delta^{l+1}\odot \sigma^{'}(z^l)\end{eqnarray}</script><p>所以得证。</p><p>现在我们得到了$\delta^{l}$的递推关系式，只要求出了某一层的$\delta^{l}$，求解$W^l,b^l$的对应梯度就很简单的。</p><h3 id="损失函数对-b-的偏导"><a href="#损失函数对-b-的偏导" class="headerlink" title="损失函数对$b$的偏导"></a>损失函数对$b$的偏导</h3><p>首先写出离散形式：</p><script type="math/tex;mode=display">\begin{eqnarray}  \frac{\partial J}{\partial b^l_j} =
  \delta^l_j.
\tag{BP3}\end{eqnarray}</script><p>为了计算方便，我们把上式表示成矩阵的形式：</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial J(W,b,x,y)}{\partial b^l} = \frac{\partial z^l}{\partial b^l} \frac{\partial J(W,b,x,y)}{\partial z^l}= \delta^{l} \tag{BP3a}\end{eqnarray}</script><h4 id="式子-BP3-证明"><a href="#式子-BP3-证明" class="headerlink" title="式子$({BP3})$证明"></a>式子$({BP3})$证明</h4><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial J}{\partial b^l_j} = \frac{\partial J}{\partial z^l_j} \times \frac{\partial z^l_j}{\partial b^l_j}\end{eqnarray}</script><p>因为存在</p><script type="math/tex;mode=display">\begin{eqnarray} z^l_j= \sum_k w_{jk}a_k^{l-1}+b_j= w_{j1}a_1^{l-1} + w_{j2}a_2^{l-1} + \dots +b_j \end{eqnarray}</script><p>所以</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial z^l_j}{\partial b^l_j} = 1 \end{eqnarray}</script><p>带入到式子$({25})$</p><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial J}{\partial b^l_j} = \frac{\partial J}{\partial z^l_j} \times \frac{\partial z^l_j}{\partial b^l_j} = \frac{\partial J}{\partial z^l_j}  =   \delta^l_j \end{eqnarray}</script><p>所以得证</p><h4 id="式子-BP3a-证明"><a href="#式子-BP3a-证明" class="headerlink" title="式子$({BP3a})$证明"></a>式子$({BP3a})$证明</h4><p>首先将$z^l= W^la^{l-1} + b^l$式子进行展开，这里表示方便，假设$W$的维度为$n \times m$，$a^{l-1}$的维度为$m \times 1$，$b^l$和$z^l$的维度均为$n \times 1$。所以：</p><script type="math/tex;mode=display">\begin{aligned}
z^{l}&=\begin{bmatrix}
W_{11}^{l} & W_{12}^{l} & \dots & W_{1m}^{l}\\
W_{21}^{l} & W_{22}^{l} & \dots & W_{2m}^{l}\\
\vdots & \vdots & \dots & \vdots\\
W_{n1}^{l} & W_{n2}^{l} & \dots & W_{nm}^{l}\\
\end{bmatrix} \times \begin{bmatrix}
a_1^{l-1}\\
a_2^{l-1}\\
\vdots\\
a_m^{l-1}\\
\end{bmatrix}
\end{aligned} + \begin{bmatrix}
b_1^{l}\\
b_2^{l}\\
\vdots\\
b_m^{l}\\
\end{bmatrix}= \begin{bmatrix}
W_{11}^{l}\times a_1^{l-1} + W_{12}^{l}\times a_2^{l-1} + \dots + W_{1m}^{l} \times a_m^{l-1} + b_1^{l} \\
W_{21}^{l}\times a_1^{l-1} + W_{22}^{l}\times a_2^{l-1} + \dots + W_{2m}^{l} \times a_m^{l-1} + b_2^{l}\\
\vdots \\
W_{n1}^{l}\times a_1^{l-1} + W_{n2}^{l}\times a_2^{l-1} + \dots + W_{nm}^{l} \times a_m^{l-1} + b_m^{l}\\
\end{bmatrix}</script><p>接着按照分母布局，得到列向量对列向量求导，如下：</p><script type="math/tex;mode=display">\frac{\partial z^l}{\partial b^l} =  \begin{bmatrix}
\frac{\partial z_1^l}{\partial b_1^l} & \frac{\partial z_2^l}{\partial b_1^l} & \cdots & \frac{\partial z_n^l}{\partial b_1^l}\\
\frac{\partial z_1^l}{\partial b_2^l} & \frac{\partial z_2^l}{\partial b_2^l} & \cdots & \frac{\partial z_n^l}{\partial b_2^l}\\
\vdots & \vdots & \cdots & \vdots \\
\frac{\partial z_1^l}{\partial b_n^l} & \frac{\partial z_2^l}{\partial b_n^l} & \cdots & \frac{\partial z_n^l}{\partial b_n^l}\\
\end{bmatrix} = \begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \cdots & \vdots \\
0 & 0 & \cdots & 1\\
\end{bmatrix}</script><p>得到的是$n \times n$的单位矩阵。带入到式子$({BP3a})$中得到：</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial J(W,b,x,y)}{\partial b^l} = \frac{\partial z^l}{\partial b^l} \frac{\partial J(W,b,x,y)}{\partial z^l}=  \frac{\partial J(W,b,x,y)}{\partial z^l} = \delta^{l} \end{eqnarray}</script><h3 id="损失函数对-W-的偏导"><a href="#损失函数对-W-的偏导" class="headerlink" title="损失函数对$W$的偏导"></a>损失函数对$W$的偏导</h3><p>首先写出离散形式：</p><script type="math/tex;mode=display">\begin{eqnarray}
  \frac{\partial J}{\partial W^l_{jk}} = a^{l-1}_k \delta^l_j.
\tag{BP4}\end{eqnarray}</script><p>为了计算方便，我们把上式表示成矩阵的形式</p><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial J(W,b,x,y)}{\partial W^l} = \frac{\partial J(W,b,x,y)}{\partial z^l}
\frac{\partial z^l}{\partial W^l} = \delta^{l}(a^{l-1})^T\tag{BP4a}\end{eqnarray}</script><h4 id="式子-BP4-证明"><a href="#式子-BP4-证明" class="headerlink" title="式子$({BP4})$证明"></a>式子$({BP4})$证明</h4><p>这里我们使用下标$j$代表第$l$层$z$的第$j$元素；下标$k$代表第$l-1$层$a$第$k$元素。</p><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial J}{\partial W^l_{jk}} = \frac{\partial J}{\partial z^l_j} \times \frac{\partial z^l_j}{\partial W^l_{jk}}\end{eqnarray}</script><p>因为存在</p><script type="math/tex;mode=display">\begin{eqnarray} z^l_j= \sum_k w_{jk}a_k^{l-1}+b_j= w_{j1}a_1^{l-1} + w_{j2}a_2^{l-1} + \dots +b_j \end{eqnarray}</script><p>所以有如下式子，其中分子始终是一个求和的形式，只是求和的项与$j$取值、$k$的个数有关。</p><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial z^l_j}{\partial W^l_{jk}} = \frac{\partial (w_{j1}a_1^{l-1} + w_{j2}a_2^{l-1} + \dots +b_j)}{\partial W^l_{jk}}\end{eqnarray}</script><p>当$j=1$，$k$依次从1取到第$l-1$层总个数，上式的结果依次为$a_1^{l-1},a_2^{l-1},\dots$</p><p>当$j=2$，$k$依次从1取到第$l-1$层总个数，上式的结果依次为$a_1^{l-1},a_2^{l-1},\dots$</p><p>可以看出来，结果只和$k$的取值有关，而和$j$的取值无关。所以可得</p><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial J}{\partial W^l_{jk}} = \frac{\partial J}{\partial z^l_j} \times \frac{\partial z^l_j}{\partial W^l_{jk}} = \delta^l_j  a^{l-1}_k=  a^{l-1}_k \delta^l_j\end{eqnarray}</script><h4 id="式子-BP4a-证明"><a href="#式子-BP4a-证明" class="headerlink" title="式子$({BP4a})$证明"></a>式子$({BP4a})$证明</h4><p>式子中第一项是显而易见的，这里对主要是对第二项$\frac{\partial z^l}{\partial W^l}= (a^{l-1})^T$进行证明，不论是依据行向量对矩阵求导还是列向量对矩阵求导，其结果矩阵不可能是$a^{l-1}$的维度大小，肯定大很多。这里正确的做法是将矩阵形式拆开成每个分量进行计算，推导如下：</p><p>首先将$z^l= W^la^{l-1} + b^l$式子进行展开，这里表示方便，假设$W$的维度为$n \times m$，$a^{l-1}$的维度为$m \times 1$，$b^l$和$z^l$的维度均为$n \times 1$。所以：</p><script type="math/tex;mode=display">\begin{eqnarray} \begin{aligned}
z^{l}&=\begin{bmatrix}
W_{11}^{l} & W_{12}^{l} & \dots & W_{1m}^{l}\\
W_{21}^{l} & W_{22}^{l} & \dots & W_{2m}^{l}\\
\vdots & \vdots & \dots & \vdots\\
W_{n1}^{l} & W_{n2}^{l} & \dots & W_{nm}^{l}\\
\end{bmatrix} \times \begin{bmatrix}
a_1^{l-1}\\
a_2^{l-1}\\
\vdots\\
a_m^{l-1}\\
\end{bmatrix}
\end{aligned} + \begin{bmatrix}
b_1^{l}\\
b_2^{l}\\
\vdots\\
b_m^{l}\\
\end{bmatrix}= \begin{bmatrix}
W_{11}^{l}\times a_1^{l-1} + W_{12}^{l}\times a_2^{l-1} + \dots + W_{1m}^{l} \times a_m^{l-1} + b_1^{l} \\
W_{21}^{l}\times a_1^{l-1} + W_{22}^{l}\times a_2^{l-1} + \dots + W_{2m}^{l} \times a_m^{l-1} + b_2^{l}\\
\vdots \\
W_{n1}^{l}\times a_1^{l-1} + W_{n2}^{l}\times a_2^{l-1} + \dots + W_{nm}^{l} \times a_m^{l-1} + b_m^{l}\\
\end{bmatrix}\end{eqnarray}</script><p>将$\frac{\partial z^l}{\partial W^l}$写成离散的形式，第$l$层使用$j$下标进行索引，而$l-1$层使用$k$下标进行索引。可以得到下式</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial z^l}{\partial W^l} = \frac{\partial z_j^l}{\partial W_{jk}^l}\end{eqnarray}</script><p>上式中最终分子和分母均为标量，分子始终是一个求和的形式，只是求和的项与$j$取值、$k$的个数有关。$W$的维度为$n \times m$，因此可以依次固定$j$，再依次取$k$。具体分析如下：</p><p>当$j=1$，即分母取$z^l$的第1行，$k$依次从1取到第$l-1$层总个数，上式的结果依次为$a_1^{l-1},a_2^{l-1},\dots,a_m^{l-1}$</p><p>当$j=2$，即分母取$z^l$的第2行，$k$依次从1取到第$l-1$层总个数，上式的结果依次为$a_1^{l-1},a_2^{l-1},\dots,a_m^{l-1}$</p><p>当$j=n$，即分母取$z^l$的第n行，$k$依次从1取到第$l-1$层总个数，上式的结果依次为$a_1^{l-1},a_2^{l-1},\dots,a_m^{l-1}$</p><p>可以看出来，结果只和$k$的取值有关，而和$j$的取值无关。</p><p>所以可得，下式中$a^{l-1}$取转置，是因为它是一个列向量，而由上面的分析，当$j$固定的时候，结果是一个行向量。</p><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial z^l}{\partial W^l} = \frac{\partial z_j^l}{\partial W_{jk}^l} = a_k^{l-1}=(a^{l-1})^T\end{eqnarray}</script><p>通过以上分析，可以发现，之所以使用矩阵求导法则会出现漏洞，并不是因为矩阵求导法则出错了，而是$\frac{\partial z_j^l}{\partial W_{jk}^l}$中分子和分母具有绑定关系，即当分子中$j=1$的时候，在运算中只存在对于$W_{1k}^l$的导数；而且，最终得到的结论是求导的是结果和$j$的取值无关，可以进一步简化结果，所以最终得到了上式结果。</p><blockquote><p>另外，<a href="https://blog.csdn.net/zb1165048017/article/details/80105310#comments" target="_blank" rel="noopener">矩阵求导与BP的证明的建议</a>中说到，不论是依据行向量对矩阵求导还是列向量对矩阵求导，其结果矩阵不可能是$a^{l-1}$的维度大小，肯定大很多。但是由$\frac{\partial z^l}{\partial W}$可以推出来等于${\sigma}’\cdot (a^{l-1})^T$,其实这一部分推导时候不能用矩阵求导方法，而是直接拆分。</p><script type="math/tex;mode=display">a_j^l=\sigma(\sum_i W_{ji}^l\cdot a_i^{l-1}+b_j^l)</script><script type="math/tex;mode=display">\frac{\partial a_j^l}{\partial W_{ji}^l}= {\sigma}'\left(\sum_i W_{ji}\cdot a_i^{l-1}+b_j^l\right)\cdot a_i^{l-1}= a_i^{l-1}</script><p>所以</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial a^l}{\partial W}&=\begin{bmatrix}
\frac{\partial a_1^l}{\partial W_{11}^l} & \cdots & \frac{\partial a_n^l}{\partial W_{1n}^l}\\
\vdots & \ddots & \vdots\\
\frac{\partial a_1^l}{\partial w_{m1}^l} & \cdots & \frac{\partial a_n^l}{\partial w_{mn}^l}\\
\end{bmatrix}\\
&=\begin{bmatrix}
{\sigma}'_{1}a_1^{l-1} &\cdots & {\sigma}'_{m}a_n^{l-1}\\
\vdots & \ddots & \vdots\\
{\sigma}'_{m}\cdot a_1^{l-1} &\cdots & {\sigma}'_{m}\cdot a_n^{l-1}\\
\end{bmatrix}\\
&=\begin{bmatrix}
{\sigma}'_{1}\\
\vdots\\
{\sigma}'_{m} 
\end{bmatrix}
\cdot
\begin{bmatrix}
a_1^{l-1}&\cdots&a_n^{l-1}
\end{bmatrix}
\\&=
{\sigma}'\cdot (a^{l-1})^T
\end{aligned}</script></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="离散形式的四大方程"><a href="#离散形式的四大方程" class="headerlink" title="离散形式的四大方程"></a>离散形式的四大方程</h4><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^L_j = \frac{\partial J}{\partial a^L_j} \times \frac{\partial a^L_j}{\partial z^L_j} = \frac{\partial J}{\partial a^L_j} \sigma'(z^L_j).
\tag{BP1}\end{eqnarray}</script><script type="math/tex;mode=display">\begin{eqnarray}
  \delta^l_j = \sum_k W^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{BP2}\end{eqnarray}</script><script type="math/tex;mode=display">\begin{eqnarray}  \frac{\partial J}{\partial b^l_j} =
  \delta^l_j.
\tag{BP3}\end{eqnarray}</script><script type="math/tex;mode=display">\begin{eqnarray}
  \frac{\partial J}{\partial W^l_{jk}} = a^{l-1}_k \delta^l_j.
\tag{BP4}\end{eqnarray}</script><p>若我们选择均方误差函数，则第一个方程可以进一步写作：</p><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^L_j = \frac{\partial J}{\partial a^L_j} \times \frac{\partial a^L_j}{\partial z^L_j} = \frac{\partial J}{\partial a^L_j} \sigma'(z^L_j)= (a_j^L-y)\times \sigma^{'}(z_j^L)
\tag{BP11}\end{eqnarray}</script><p>从这4个式子中，我们考虑$({BP1})$中的$\sigma’(z^L_j)$，当$\sigma$为sigmoid函数的时候，$\sigma(z^L_j)$的值接近与0或1，这时偏导数$\sigma’(z^L_j) \approx 0$。在这种情况下，通常会说输出神经元已经饱和，这时，权重停止了学习(或学习缓慢)。类似的观点也适用于输出神经元的偏差。</p><p>同样的情况也适用于$({BP2})$中的$\sigma’(z^l)$，这个时候，$\delta^l_j$会变得很小，这又意味着，<strong>输入到饱和神经元的任何权值都会慢慢地学习</strong>。</p><blockquote><p>当然，上述讨论不适用于当${w^{l+1}}^T \delta^{l+1}$特别大可以弥补$\sigma’(z^l_j)$的小的情形。上述讨论只是一种总体趋势。</p></blockquote><p>综上所述，我们了解到，如果输入神经元处于低激活状态，或者输出神经元处于饱和状态，则权值的学习速度较慢。这上面的观察结果可以帮助我们设计较好的激活函数。</p><h4 id="矩阵形式的四大方程"><a href="#矩阵形式的四大方程" class="headerlink" title="矩阵形式的四大方程"></a>矩阵形式的四大方程</h4><p>因为在计算的时候，矩阵形式可以利用线性代数的一些库，使得运算更加方便，因此，在这里我们把矩阵形式的四大方程列出来：</p><script type="math/tex;mode=display">\begin{eqnarray} 
  \delta^L= \frac{\partial J}{\partial a^L}\odot \sigma'(z^L) = \nabla_a J \odot \sigma'(z^L).
\tag{BP1a}\end{eqnarray}</script><script type="math/tex;mode=display">\begin{eqnarray} \delta^{l} = (W^{l+1})^T\delta^{l+1}\odot \sigma^{'}(z^l)  \tag{BP2a} \end{eqnarray}</script><script type="math/tex;mode=display">\begin{eqnarray} \frac{\partial J(W,b,x,y)}{\partial b^l} =\delta^{l} \tag{BP3a}\end{eqnarray}</script><script type="math/tex;mode=display">\begin{eqnarray}\frac{\partial J(W,b,x,y)}{\partial W^l} = \delta^{l}(a^{l-1})^T\tag{BP4a}\end{eqnarray}</script><p>若我们选择均方误差函数，则第一个方程可以进一步写作：</p><script type="math/tex;mode=display">\begin{eqnarray} \delta^L = (a^L-y)\odot \sigma^{'}(z^L) \tag{BP1aa} \end{eqnarray}</script><h4 id="反向传播方程的另外一种形式"><a href="#反向传播方程的另外一种形式" class="headerlink" title="反向传播方程的另外一种形式"></a>反向传播方程的另外一种形式</h4><p>$({BP1a})$和$({BP2a})$含有Hadamard积，然而当对Hadamard积不是很了解的时候，可以写成矩阵的形式方便大家理解。</p><p>$({BP1a})$可以写成下面的形式：</p><script type="math/tex;mode=display">\begin{eqnarray}
 \delta^L = \Sigma'(z^L) \nabla_a J,
\tag{33}\end{eqnarray}</script><p>其中，$\Sigma’(z^L)$方阵，对角元素是$\sigma’(z^L_j)$，非对角元素值为零。</p><p>$({BP2a})$可以写成下面的形式：</p><script type="math/tex;mode=display">\begin{eqnarray}
    \delta^l = \Sigma'(z^l) (w^{l+1})^T \delta^{l+1}.
  \tag{34}\end{eqnarray}</script><p>通过以上两个式子，可以得到</p><script type="math/tex;mode=display">\begin{eqnarray}
    \delta^l = \Sigma'(z^l) (w^{l+1})^T \ldots \Sigma'(z^{L-1}) (w^L)^T 
    \Sigma'(z^L) \nabla_a J
  \tag{35}\end{eqnarray}</script><p>上述这种矩阵的表示方式可能更好理解，并且数值计算的时候，方便。</p><h2 id="DNN反向传播算法过程"><a href="#DNN反向传播算法过程" class="headerlink" title="DNN反向传播算法过程"></a>DNN反向传播算法过程</h2><p>现在我们总结下DNN反向传播算法的过程。由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的随机梯度下降法和批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。不过区别仅仅在于迭代时训练样本的选择而已。</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>1) <strong>输入$x$：</strong>即$a^1$</p><p>2) <strong>前向计算：</strong>对于每一个$l = 2, 3, \ldots, L$，计算$z^{l} = W^l a^{l-1}+b^l$，并且$a^{l} = \sigma(z^{l})$。</p><p>3) <strong>计算$\delta^L$：</strong>$\delta^{L} = \nabla_a J \odot \sigma’(z^L)$</p><p>4) <strong>损失反向传播：</strong>对于每一个$l = L-1, L-2, \ldots, 2$，计算$\delta^{l} = ((W^{l+1})^T \delta^{l+1}) \odot<br> \sigma’(z^{l})$</p><p>5) <strong>输出：</strong>计算梯度，$\frac{\partial J}{\partial W^l_{jk}} = a^{l-1}_k \delta^l_j$和$\frac{\partial J}{\partial b^l_j} = \delta^l_j$。</p><h3 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h3><p>输入:总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长$\alpha$,最大迭代次数MAX与停止迭代阈值$\epsilon$，输入的m个训练样本$(x_1,y_1),(x_2,y_2), …, (x_m,y_m)$</p><p>输出：各隐藏层与输出层的线性关系系数矩阵$W$和偏倚向量$b$</p><p>1)初始化各隐藏层与输出层的线性关系系数矩阵$W$和偏倚向量$b$的值为一个随机值。</p><p>2）for iter to 1 to MAX：</p><p>　　2-1) for i =1 to m：</p><p>　　　　a) 将DNN输入$a^1$设置为$x_i$</p><p>　　　　b) for $l$=2 to L，进行前向传播算法计算$a^{i,l} = \sigma(z^{i,l}) = \sigma(W^la^{i,l-1} + b^l)$</p><p>　　　　c) 通过损失函数计算输出层的$\delta^{i,L}= \frac{\partial J}{\partial a^{i,L}}\odot \sigma’(z^{i,L}) $</p><p>　　　　d) for $l$= L-1 to 2, 进行反向传播算法计算$\delta^{i,l} = (W^{l+1})^T\delta^{i,l+1}\odot \sigma^{‘}(z^{i,l})$</p><p>　　2-2) for $l$ = 2 to L，更新第$l$层的$W^l,b^l$:</p><script type="math/tex;mode=display">W^l = W^l- \frac{\alpha}{m} \sum\limits_{i=1}^m \delta^{i,l}(a^{i, l-1})^T</script><script type="math/tex;mode=display">b^l = b^l -\frac{\alpha}{m} \sum\limits_{i=1}^m \delta^{i,l}</script><p>　　2-3)如果所有$W，b$的变化值都小于停止迭代阈值$\epsilon$，则跳出迭代循环到步骤3。</p><p>3）输出各隐藏层与输出层的线性关系系数矩阵$W$和偏倚向量$b$。</p><h2 id="DNN代码实现"><a href="#DNN代码实现" class="headerlink" title="DNN代码实现"></a>DNN代码实现</h2><p>Python2:<a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener">github链接</a></p><p>Python3:<a href="https://github.com/MichalDanielDobrzanski/DeepLearningPython35" target="_blank" rel="noopener">github链接</a></p><p>这里为了理解，贴出Python3的实现过程，注意下面使用的损失函数为平方误差损失函数。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"># %load network.py</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">network.py</span><br><span class="line">~~~~~~~~~~</span><br><span class="line">IT WORKS</span><br><span class="line"></span><br><span class="line">A module to implement the stochastic gradient descent learning</span><br><span class="line">algorithm for a feedforward neural network.  Gradients are calculated</span><br><span class="line">using backpropagation.  Note that I have focused on making the code</span><br><span class="line">simple, easily readable, and easily modifiable.  It is not optimized,</span><br><span class="line">and omits many desirable features.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">#### Libraries</span><br><span class="line"># Standard library</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"># Third-party libraries</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">class Network(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, sizes):</span><br><span class="line">        &quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the</span><br><span class="line">        respective layers of the network.  For example, if the list</span><br><span class="line">        was [2, 3, 1] then it would be a three-layer network, with the</span><br><span class="line">        first layer containing 2 neurons, the second layer 3 neurons,</span><br><span class="line">        and the third layer 1 neuron.  The biases and weights for the</span><br><span class="line">        network are initialized randomly, using a Gaussian</span><br><span class="line">        distribution with mean 0, and variance 1.  Note that the first</span><br><span class="line">        layer is assumed to be an input layer, and by convention we</span><br><span class="line">        won&apos;t set any biases for those neurons, since biases are only</span><br><span class="line">        ever used in computing the outputs from later layers.&quot;&quot;&quot;</span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        for x, y in zip(sizes[:-1], sizes[1:])]</span><br><span class="line"></span><br><span class="line">    def feedforward(self, a):</span><br><span class="line">        &quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span><br><span class="line">        for b, w in zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        return a</span><br><span class="line"></span><br><span class="line">    def SGD(self, training_data, epochs, mini_batch_size, eta,</span><br><span class="line">            test_data=None):</span><br><span class="line">        &quot;&quot;&quot;Train the neural network using mini-batch stochastic</span><br><span class="line">        gradient descent.  The ``training_data`` is a list of tuples</span><br><span class="line">        ``(x, y)`` representing the training inputs and the desired</span><br><span class="line">        outputs.  The other non-optional parameters are</span><br><span class="line">        self-explanatory.  If ``test_data`` is provided then the</span><br><span class="line">        network will be evaluated against the test data after each</span><br><span class="line">        epoch, and partial progress printed out.  This is useful for</span><br><span class="line">        tracking progress, but slows things down substantially.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        training_data = list(training_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line"></span><br><span class="line">        if test_data:</span><br><span class="line">            test_data = list(test_data)</span><br><span class="line">            n_test = len(test_data)</span><br><span class="line"></span><br><span class="line">        for j in range(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                for k in range(0, n, mini_batch_size)]</span><br><span class="line">            for mini_batch in mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            if test_data:</span><br><span class="line">                print(&quot;Epoch &#123;&#125; : &#123;&#125; / &#123;&#125;&quot;.format(j,self.evaluate(test_data),n_test));</span><br><span class="line">            else:</span><br><span class="line">                print(&quot;Epoch &#123;&#125; complete&quot;.format(j))</span><br><span class="line"></span><br><span class="line">    def update_mini_batch(self, mini_batch, eta):</span><br><span class="line">        &quot;&quot;&quot;Update the network&apos;s weights and biases by applying</span><br><span class="line">        gradient descent using backpropagation to a single mini batch.</span><br><span class="line">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span><br><span class="line">        is the learning rate.&quot;&quot;&quot;</span><br><span class="line">        nabla_b = [np.zeros(b.shape) for b in self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) for w in self.weights]</span><br><span class="line">        for x, y in mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        for w, nw in zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       for b, nb in zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    def backprop(self, x, y):</span><br><span class="line">        &quot;&quot;&quot;Return a tuple ``(nabla_b, nabla_w)`` representing the</span><br><span class="line">        gradient for the cost function C_x.  ``nabla_b`` and</span><br><span class="line">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span><br><span class="line">        to ``self.biases`` and ``self.weights``.&quot;&quot;&quot;</span><br><span class="line">        nabla_b = [np.zeros(b.shape) for b in self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) for w in self.weights]</span><br><span class="line">        # feedforward</span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] # list to store all the activations, layer by layer</span><br><span class="line">        zs = [] # list to store all the z vectors, layer by layer</span><br><span class="line">        for b, w in zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        # backward pass</span><br><span class="line">        delta = self.cost_derivative(activations[-1], y) * \</span><br><span class="line">            sigmoid_prime(zs[-1])</span><br><span class="line">        nabla_b[-1] = delta</span><br><span class="line">        nabla_w[-1] = np.dot(delta, activations[-2].transpose())</span><br><span class="line">        # Note that the variable l in the loop below is used a little</span><br><span class="line">        # differently to the notation in Chapter 2 of the book.  Here,</span><br><span class="line">        # l = 1 means the last layer of neurons, l = 2 is the</span><br><span class="line">        # second-last layer, and so on.  It&apos;s a renumbering of the</span><br><span class="line">        # scheme in the book, used here to take advantage of the fact</span><br><span class="line">        # that Python can use negative indices in lists.</span><br><span class="line">        for l in range(2, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())</span><br><span class="line">        return (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    def evaluate(self, test_data):</span><br><span class="line">        &quot;&quot;&quot;Return the number of test inputs for which the neural</span><br><span class="line">        network outputs the correct result. Note that the neural</span><br><span class="line">        network&apos;s output is assumed to be the index of whichever</span><br><span class="line">        neuron in the final layer has the highest activation.&quot;&quot;&quot;</span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        for (x, y) in test_data]</span><br><span class="line">        return sum(int(x == y) for (x, y) in test_results)</span><br><span class="line"></span><br><span class="line">    def cost_derivative(self, output_activations, y):</span><br><span class="line">        &quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span><br><span class="line">        \partial a for the output activations.&quot;&quot;&quot;</span><br><span class="line">        return (output_activations-y)</span><br><span class="line"></span><br><span class="line">#### Miscellaneous functions</span><br><span class="line">def sigmoid(z):</span><br><span class="line">    &quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span><br><span class="line">    return 1.0/(1.0+np.exp(-z))</span><br><span class="line"></span><br><span class="line">def sigmoid_prime(z):</span><br><span class="line">    &quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span><br><span class="line">    return sigmoid(z)*(1-sigmoid(z))</span><br></pre></td></tr></table></figure><p></p><p>主函数：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># </span><br><span class="line"># - network.py example:</span><br><span class="line">import network</span><br><span class="line"></span><br><span class="line">net = network.Network([784, 30, 10])</span><br><span class="line">net.SGD(training_data, 30, 10, 3.0, test_data=test_data)</span><br></pre></td></tr></table></figure><p></p><h2 id="DNN反向传播算法小结"><a href="#DNN反向传播算法小结" class="headerlink" title="DNN反向传播算法小结"></a>DNN反向传播算法小结</h2><p>有了DNN反向传播算法，我们就可以很方便的用DNN的模型去解决第一节里面提到了各种监督学习的分类回归问题。当然DNN的参数众多，矩阵运算量也很大，直接使用会有各种各样的问题。有哪些问题以及如何尝试解决这些问题并优化DNN模型与算法，我们在下一篇讲。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/pinard/p/6418668.html" target="_blank" rel="noopener">深度神经网络（DNN）模型与前向传播算法</a><br><a href="https://www.cnblogs.com/pinard/p/6422831.html" target="_blank" rel="noopener">深度神经网络（DNN）反向传播算法(BP)</a><br><a href="https://blog.csdn.net/zb1165048017/article/details/80117574" target="_blank" rel="noopener">BP推导——续</a><br><a href="https://blog.csdn.net/zb1165048017/article/details/80105310#comments" target="_blank" rel="noopener">矩阵求导与BP的证明的建议</a><br><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">How the backpropagation algorithm works</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/" title="深度神经网络DNN">https://www.zdaiot.com/MachineLearning/神经网络/深度神经网络DNN/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/反向传播/" rel="tag"><i class="fa fa-tag"></i> 反向传播</a><a href="/tags/DNN/" rel="tag"><i class="fa fa-tag"></i> DNN</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MachineLearning/神经网络/感知器/" rel="prev" title="感知器"><i class="fa fa-chevron-left"></i> 感知器</a></div><div class="post-nav-item"> <a href="/MachineLearning/卷积神经网络/卷积神经网络前向与反向传播/" rel="next" title="卷积神经网络前向与反向传播">卷积神经网络前向与反向传播<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#从感知机到神经网络"><span class="nav-number">1.</span> <span class="nav-text">从感知机到神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN的基本结构"><span class="nav-number">2.</span> <span class="nav-text">DNN的基本结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN前向传播算法数学原理"><span class="nav-number">3.</span> <span class="nav-text">DNN前向传播算法数学原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN前向传播算法"><span class="nav-number">4.</span> <span class="nav-text">DNN前向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN反向传播算法要解决的问题"><span class="nav-number">5.</span> <span class="nav-text">DNN反向传播算法要解决的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数的两个假设"><span class="nav-number">6.</span> <span class="nav-text">损失函数的两个假设</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN反向传播算法的基本思路"><span class="nav-number">7.</span> <span class="nav-text">DNN反向传播算法的基本思路</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#中间量-delta-l-j"><span class="nav-number">7.1.</span> <span class="nav-text">中间量$\delta^l_j$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hadamard积"><span class="nav-number">7.2.</span> <span class="nav-text">Hadamard积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#输出层的-delta-L"><span class="nav-number">7.3.</span> <span class="nav-text">输出层的$\delta^L$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP1-证明"><span class="nav-number">7.3.1.</span> <span class="nav-text">式子$({BP1})$证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP11-证明"><span class="nav-number">7.3.2.</span> <span class="nav-text">式子$({BP11})$证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP1aa-证明"><span class="nav-number">7.3.3.</span> <span class="nav-text">式子$({BP1aa})$证明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#已知-delta-l-1-推导-delta-l"><span class="nav-number">7.4.</span> <span class="nav-text">已知$\delta^{l+1}$推导$\delta^l$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP2-证明"><span class="nav-number">7.4.1.</span> <span class="nav-text">式子$({BP2})$证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP2a-证明"><span class="nav-number">7.4.2.</span> <span class="nav-text">式子$({BP2a})$证明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数对-b-的偏导"><span class="nav-number">7.5.</span> <span class="nav-text">损失函数对$b$的偏导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP3-证明"><span class="nav-number">7.5.1.</span> <span class="nav-text">式子$({BP3})$证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP3a-证明"><span class="nav-number">7.5.2.</span> <span class="nav-text">式子$({BP3a})$证明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数对-W-的偏导"><span class="nav-number">7.6.</span> <span class="nav-text">损失函数对$W$的偏导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP4-证明"><span class="nav-number">7.6.1.</span> <span class="nav-text">式子$({BP4})$证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#式子-BP4a-证明"><span class="nav-number">7.6.2.</span> <span class="nav-text">式子$({BP4a})$证明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">7.7.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#离散形式的四大方程"><span class="nav-number">7.7.1.</span> <span class="nav-text">离散形式的四大方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵形式的四大方程"><span class="nav-number">7.7.2.</span> <span class="nav-text">矩阵形式的四大方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播方程的另外一种形式"><span class="nav-number">7.7.3.</span> <span class="nav-text">反向传播方程的另外一种形式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN反向传播算法过程"><span class="nav-number">8.</span> <span class="nav-text">DNN反向传播算法过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#随机梯度下降法"><span class="nav-number">8.1.</span> <span class="nav-text">随机梯度下降法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#批量梯度下降法"><span class="nav-number">8.2.</span> <span class="nav-text">批量梯度下降法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN代码实现"><span class="nav-number">9.</span> <span class="nav-text">DNN代码实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN反向传播算法小结"><span class="nav-number">10.</span> <span class="nav-text">DNN反向传播算法小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">11.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">317</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">54</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">372</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">35:08</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '41893c23700e55dc578546c17d2375b0',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>