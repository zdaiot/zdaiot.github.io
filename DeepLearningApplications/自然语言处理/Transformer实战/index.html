<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前面我们详细的介绍了Transformer的原理，但是有的细节还是一头雾水，所以我们接下来介绍一下Transformer的实现，主要参考了文章The Annotated Transformer，github地址。  本文的代码部分来自于github，而图来源于The Annotated Transformer。  Prelims12345678910111213141516171819202122"><meta name="keywords" content="Transformer,NLP"><meta property="og:type" content="article"><meta property="og:title" content="Transformer"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="前面我们详细的介绍了Transformer的原理，但是有的细节还是一头雾水，所以我们接下来介绍一下Transformer的实现，主要参考了文章The Annotated Transformer，github地址。  本文的代码部分来自于github，而图来源于The Annotated Transformer。  Prelims12345678910111213141516171819202122"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_14_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_31_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_33_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_38_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_49_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/v2-680089c6c10bec7cb81b99640536de16_b.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_69_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_74_0.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_76_0.png"><meta property="og:updated_time" content="2022-04-28T12:18:03.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Transformer"><meta name="twitter:description" content="前面我们详细的介绍了Transformer的原理，但是有的细节还是一头雾水，所以我们接下来介绍一下Transformer的实现，主要参考了文章The Annotated Transformer，github地址。  本文的代码部分来自于github，而图来源于The Annotated Transformer。  Prelims12345678910111213141516171819202122"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_14_0.png"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Transformer | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> Transformer<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/自然语言处理/Transformer实战.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-04-28 20:18:03" itemprop="dateCreated datePublished" datetime="2022-04-28T20:18:03+08:00">2022-04-28</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/自然语言处理/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>51k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>46 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>前面我们详细的介绍了Transformer的原理，但是有的细节还是一头雾水，所以我们接下来介绍一下Transformer的实现，主要参考了文章<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a>，<a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">github地址</a>。</p><blockquote><p>本文的代码部分来自于github，而图来源于<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a>。</p></blockquote><h2 id="Prelims"><a href="#Prelims" class="headerlink" title="Prelims"></a>Prelims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> exists</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> log_softmax, pad</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt</span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">import</span> torchtext.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> GPUtil</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set to False to skip notebook execution (e.g. for debugging)</span></span><br><span class="line">RUN_EXAMPLES = <span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some convenience helper functions used throughout the notebook</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_interactive_notebook</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> __name__ == <span class="string">"__main__"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_example</span><span class="params">(fn, args=[])</span>:</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span> <span class="keyword">and</span> RUN_EXAMPLES:</span><br><span class="line">        <span class="keyword">return</span> fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute_example</span><span class="params">(fn, args=[])</span>:</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span> <span class="keyword">and</span> RUN_EXAMPLES:</span><br><span class="line">        fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DummyOptimizer</span><span class="params">(torch.optim.Optimizer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.param_groups = [&#123;<span class="string">"lr"</span>: <span class="number">0</span>&#125;]</span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span><span class="params">(self, set_to_none=False)</span>:</span></span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DummyScheduler</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="literal">None</span></span><br></pre></td></tr></table></figure><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>大多数的neural sequence transduction模型都使用了encoder-decoder结构，encoder结构将一个用符号（symbols）表示的输入系列$(x_1, …, x_n)$，表示成为连续表征$\mathbf{z} = (z_1, …, z_n)$。给出$\mathbf{z}$，decoder生成输出序列$(y_1,…,y_m)$，并且一次生成一个元素。在每一步，模型都是自回归的（auto-regressive），在生成下一步时，使用先前生成的符号作为附加输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many</span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>Transformer总体结构如下，encoder和decoder结构都是堆叠self-attention and point-wise, fully connected layers。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_14_0.png" alt="png"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder由$N=6$个一模一样的层（EncoderLayer）组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>我们在each of the two sub-layers使用残差连接，并且后接layer normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><p>因此，每一个sub-layer的输出是$\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$。我们还添加了Dropout层。为了facilitate这些残差连接，模型中所有sub-layer和embedding layers的输出维度均是$d_{\text{model}}=512$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>每一个layer含有两个sub-layers，第一个是multi-head self-attention mechanism，第二个是simple, position-wise fully connected feed-forward network。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder同样由$N=6$个一模一样的层（encoder layer）组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>在每个encoder layer除了两个 sub-layers 外，还插入了第三个sub-layer，它在encoder stack的输出上执行multi-head attention。与encoder相同，我们在each of the two sub-layers使用残差连接，并且后接layer normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>我们还修改了decoder中的self-attention sub-layer，以防止它利用到后续位置的信息。This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$ .</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=<span class="number">1</span>).type(</span><br><span class="line">        torch.uint8</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_mask</span><span class="params">()</span>:</span></span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"Subsequent Mask"</span>: subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>][x, y].flatten(),</span><br><span class="line">                    <span class="string">"Window"</span>: y,</span><br><span class="line">                    <span class="string">"Masking"</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">20</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">20</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .properties(height=<span class="number">250</span>, width=<span class="number">250</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">"Window:O"</span>),</span><br><span class="line">            alt.Y(<span class="string">"Masking:O"</span>),</span><br><span class="line">            alt.Color(<span class="string">"Subsequent Mask:Q"</span>, scale=alt.Scale(scheme=<span class="string">"viridis"</span>)),</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_mask)</span><br></pre></td></tr></table></figure><p>下图展示了each tgt word（row），被允许看到的信息（column）。单词在训练过程中被遮挡，使模型关注预测下一个words。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_31_0.png" alt="png"></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>attention函数可以被描述为 mapping a query and a set of key-value pairs to an output，其中query, keys, values, and output都是向量。output是values的加权求和，其中每个value的权重是通过query with the corresponding key的compatibility function计算得到。</p><p>我们将这种特别的attention称为“Scaled Dot-Product Attention”。它的输入由$d_k$维度的queries、keys，$d_v$维度的values组成。 We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_33_0.png" alt="png"></p><p>实际上我们会同时在一系列的的queries上计算attention 函数，对应的会有一系列的keys $K$、values $V$。attention函数的输出为：</p><script type="math/tex;mode=display">
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = scores.softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>最常用的两个attention函数是additive attention和dot-product (multiplicative) attention。后者除了没有缩放$\frac{1}{\sqrt{d_k}}$，其余与我们的相同。而Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. 虽然两者在理论复杂性上相似，但dot-product attention在实践中要快得多，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>虽然对于较小的$d_k$值，这两种机制的性能相似，但对于较大的$d_k$值，additive attention优于dot product attention。我们怀疑较大的$d_k$值，dot product的幅度会增大，从而将Softmax函数推入其梯度极小的区域。(To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$. Then their dot product, $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance$d_k$.). 为了抵消这种影响，我们使用$\frac{1}{\sqrt{d_k}}$对dot products进行缩放。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_38_0.png" alt="png"></p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 当仅有一个 attention head，平均化抑制了这一点。</p><script type="math/tex;mode=display">
\mathrm{MultiHead}(Q, K, V) =
    \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)</script><p>Where the projections are parameter matrices $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$。在本工作中，我们使用了$h=8$个平行attention layers, or heads。对于其中每一个，我们使用了$d_k=d_v=d_{\text{model}}/h=64$。由于each head的维度降低了，所以总的计算量与full dimensionality的single-head attention相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            lin(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> lin, x <span class="keyword">in</span> zip(self.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, self.attn = attention(</span><br><span class="line">            query, key, value, mask=mask, dropout=self.dropout</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        x = (</span><br><span class="line">            x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">del</span> query</span><br><span class="line">        <span class="keyword">del</span> key</span><br><span class="line">        <span class="keyword">del</span> value</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><p>Transformer以三种不同的方式使用了multi-head attention。</p><ol><li>在“encoder-decoder attention” layers中，queries来自于之前的decoder layer， memory keys and values 来自encoder的输出。This allows every position in the decoder to attend over all positions in the input sequence. 这模仿了sequence-to-sequence模型中典型的encoder-decoder attention机制。</li><li>encoder中的self-attention layers. 这里的self-attention layers中，所有的 keys, values and queries均来自于上一层的输出。Each position in the encoder can attend to all positions in the previous layer of the encoder.</li><li>decoder中的self-attention layers. self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position（up to and including：直到并包括）。我们需要防止信息在decoder中向左流动，以保持自回归（auto-regressive）特性。我们在scaled dot-product attention中实现了这一点，通过屏蔽Softmax输入中对应于非法连接的所有值（设置为$-\infty$）。</li></ol><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>除了attention sub-layers，encoder and decoder中的每一层都包含一个fully connected feed-forward network（完全连接的前馈网络），该网络分别且相同地应用于每个位置。它由两个线性变换组成，中间有一个ReLU激活。</p><script type="math/tex;mode=display">
\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2</script><p>虽然在不同位置上都是线性变换，但它们在不同的层之间使用不同的参数。另一种描述方式是将其描述为核大小为1的两个卷积。input和output的维度为$d_{\text{model}}=512$，内层的维度为$d_{ff}=2048$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(self.w_1(x).relu()))</span><br></pre></td></tr></table></figure><h3 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h3><p>与其它的sequence transduction models相似，we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\text{model}}$。我们还使用常用的 linear transformation and softmax function 将 decoder output转换为 predicted next-token probabilities. 在我们的模型中，我们在two embedding layers 和pre-softmax linear transformation共享相同的权重矩阵。在embedding layers，我们multiply those weights by $\sqrt{d_{\text{model}}}$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>由于我们的模型不包含recurrence和卷积，为了使模型利用序列的顺序，我们必须注入一些关于tokens in the sequence的相对或绝对位置的信息。为此，我们将“positional encodings”添加到 encoder 和 decoder 堆栈底部的input embeddings中。 positional encodings具有与embeddings相同的维度$d_{\text{model}}$，因此这两个模型可以求和。positional encodings有许多选择，学习的和固定的。</p><p>在这项工作中，我们使用不同频率的正弦和余弦函数。其中$pos$表示单词在句子中的位置，$2i$ 表示偶数的维度，$2i+1$ 表示奇数维度。也就是说，位置编码的每个维度对应于一个正弦。波长形成从2π到10000⋅2π的几何级数。我们选择这个函数是因为我们假设它将允许模型更容易学习相对位置，因为对于任何固定的偏移量$k$，$PE_{pos+k}$可以表示为$PE_{pos}$的线性函数。</p><script type="math/tex;mode=display">
PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}}) \\
PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})</script><p>初次之外，我们还将dropout应用于the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. 这里dropout的比例为$P_{drop}=0.1$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)</span><br><span class="line">        )</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">"pe"</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:, : x.size(<span class="number">1</span>)].requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p>Below the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_positional</span><span class="params">()</span>:</span></span><br><span class="line">    pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">    y = pe.forward(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">    data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"embedding"</span>: y[<span class="number">0</span>, :, dim],</span><br><span class="line">                    <span class="string">"dimension"</span>: dim,</span><br><span class="line">                    <span class="string">"position"</span>: list(range(<span class="number">100</span>)),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> dim <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">800</span>)</span><br><span class="line">        .encode(x=<span class="string">"position"</span>, y=<span class="string">"embedding"</span>, color=<span class="string">"dimension:N"</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_positional)</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_49_0.png" alt="png"></p><p>我们还试验了使用学习的positional embeddings，发现两个版本产生的结果几乎相同。我们选择正弦版本是因为它可能允许模型推广到比训练期间遇到的序列长度更长的序列长度。</p><h3 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was important from their code.</span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/v2-680089c6c10bec7cb81b99640536de16_b.jpg" alt="img" style="zoom:80%"></p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在这里，我们执行一个forward，以生成模型的预测。我们尝试使用我们的transformer来记忆输入。正如您将看到的，由于模型尚未经过训练，因此输出是随机生成的。在下一个教程中，我们将构建训练函数，并尝试训练我们的模型记住从1到10的数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference_test</span><span class="params">()</span>:</span></span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.eval()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Example Untrained Model Prediction:"</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_tests</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(run_tests)</span><br></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>接下来我们来介绍训练流程，在此之前我们先介绍train a standard encoder decoder model所需的工具。首先我们定义一个batch object保存用于训练的 src and target sentences、masks。</p><h3 id="Batches-and-Masking"><a href="#Batches-and-Masking" class="headerlink" title="Batches and Masking"></a>Batches and Masking</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"""Object for holding a batch of data with mask during training."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, tgt=None, pad=<span class="number">2</span>)</span>:</span>  <span class="comment"># 2 = &lt;blank&gt; for IWST</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        <span class="keyword">if</span> tgt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.tgt = tgt[:, :<span class="number">-1</span>]</span><br><span class="line">            self.tgt_y = tgt[:, <span class="number">1</span>:]</span><br><span class="line">            self.tgt_mask = self.make_std_mask(self.tgt, pad)</span><br><span class="line">            self.ntokens = (self.tgt_y != pad).data.sum()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(</span><br><span class="line">            tgt_mask.data</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure><h3 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h3><p>接下来我们创建一个通用的训练和打分函数，来跟踪损失。我们传入一个损失函数，它还会执行参数更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainState</span>:</span></span><br><span class="line">    <span class="string">"""Track number of steps, examples, and tokens processed"""</span></span><br><span class="line"></span><br><span class="line">    step: int = <span class="number">0</span>  <span class="comment"># Steps in the current epoch</span></span><br><span class="line">    accum_step: int = <span class="number">0</span>  <span class="comment"># Number of gradient accumulation steps</span></span><br><span class="line">    samples: int = <span class="number">0</span>  <span class="comment"># total # of examples used</span></span><br><span class="line">    tokens: int = <span class="number">0</span>  <span class="comment"># total # of tokens processed</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    data_iter,</span></span></span><br><span class="line"><span class="function"><span class="params">    model,</span></span></span><br><span class="line"><span class="function"><span class="params">    loss_compute,</span></span></span><br><span class="line"><span class="function"><span class="params">    optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">    scheduler,</span></span></span><br><span class="line"><span class="function"><span class="params">    mode=<span class="string">"train"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    accum_iter=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    train_state=TrainState<span class="params">()</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">"""Train a single epoch"""</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    n_accum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(</span><br><span class="line">            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span><br><span class="line">        )</span><br><span class="line">        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        <span class="comment"># loss_node = loss_node / accum_iter</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"train"</span> <span class="keyword">or</span> mode == <span class="string">"train+log"</span>:</span><br><span class="line">            loss_node.backward()</span><br><span class="line">            train_state.step += <span class="number">1</span></span><br><span class="line">            train_state.samples += batch.src.shape[<span class="number">0</span>]</span><br><span class="line">            train_state.tokens += batch.ntokens</span><br><span class="line">            <span class="keyword">if</span> i % accum_iter == <span class="number">0</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">                n_accum += <span class="number">1</span></span><br><span class="line">                train_state.accum_step += <span class="number">1</span></span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">40</span> == <span class="number">1</span> <span class="keyword">and</span> (mode == <span class="string">"train"</span> <span class="keyword">or</span> mode == <span class="string">"train+log"</span>):</span><br><span class="line">            lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">"lr"</span>]</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            print(</span><br><span class="line">                (</span><br><span class="line">                    <span class="string">"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f "</span></span><br><span class="line">                    + <span class="string">"| Tokens / Sec: %7.1f | Learning Rate: %6.1e"</span></span><br><span class="line">                )</span><br><span class="line">                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)</span><br><span class="line">            )</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">        <span class="keyword">del</span> loss</span><br><span class="line">        <span class="keyword">del</span> loss_node</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens, train_state</span><br></pre></td></tr></table></figure><h3 id="Training-Data-and-Batching"><a href="#Training-Data-and-Batching" class="headerlink" title="Training Data and Batching"></a>Training Data and Batching</h3><p>我们在标准的WMT 2014英语-德语数据集上进行了训练，该数据集由大约450万个句子对组成。Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. 对于英语-法语，我们使用了更大的2014年WMT英语-法语数据集，包括3600万个句子和split tokens into a 32000 word-piece vocabulary.</p><p>Sentence pairs were batched together by approximate sequence length. 每个训练批次包含一组句子对，其中包含大约25000个source tokens和25000个target tokens。</p><h3 id="Hardware-and-Schedule"><a href="#Hardware-and-Schedule" class="headerlink" title="Hardware and Schedule"></a>Hardware and Schedule</h3><p>我们在一台配备8个NVIDIA P100图形处理器的机器上训练了我们的模型。对于使用本文中描述的超参数的基本模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总共100,000步或12小时的培训。对于我们的大型模型，step time是1.0秒。这些大模型接受了300,000步(3.5天)的训练。</p><h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><p>我们使用adam作为优化器，$\beta_1=0.9$, $\beta_2=0.98$ and $\epsilon=10^{-9}$，在训练中学习率也是变化的，变化方式是：</p><script type="math/tex;mode=display">
lrate = d_{\text{model}}^{-0.5} \cdot

 \min({step\_num}^{-0.5},

  {step\_num} \cdot {warmup\_steps}^{-1.5})</script><p>这对应于在前面$warmup_steps$线性增加学习率，此后按与步数的平方根倒数成比例递减。这里$warmup_steps=4000$。</p><blockquote><p>注意：这部分非常重要。 需要使用这种模型设置进行训练。</p><p>该模型曲线的示例，用于不同的模型大小和优化超参数。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def rate(step, model_size, factor, warmup):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    we have to default the step to 1 for LambdaLR function</span><br><span class="line">    to avoid zero raising to negative power.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if step == 0:</span><br><span class="line">        step = 1</span><br><span class="line">    return factor * (</span><br><span class="line">        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_learning_schedule</span><span class="params">()</span>:</span></span><br><span class="line">    opts = [</span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 1</span></span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>],  <span class="comment"># example 2</span></span><br><span class="line">        [<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 3</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    dummy_model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    learning_rates = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we have 3 examples in opts list.</span></span><br><span class="line">    <span class="keyword">for</span> idx, example <span class="keyword">in</span> enumerate(opts):</span><br><span class="line">        <span class="comment"># run 20000 epoch for each example</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            dummy_model.parameters(), lr=<span class="number">1</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">        )</span><br><span class="line">        lr_scheduler = LambdaLR(</span><br><span class="line">            optimizer=optimizer, lr_lambda=<span class="keyword">lambda</span> step: rate(step, *example)</span><br><span class="line">        )</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="comment"># take 20K dummy training steps, save the learning rate at each step</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">            tmp.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">"lr"</span>])</span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        learning_rates.append(tmp)</span><br><span class="line"></span><br><span class="line">    learning_rates = torch.tensor(learning_rates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Enable altair to handle more than 5000 rows</span></span><br><span class="line">    alt.data_transformers.disable_max_rows()</span><br><span class="line"></span><br><span class="line">    opts_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"Learning Rate"</span>: learning_rates[warmup_idx, :],</span><br><span class="line">                    <span class="string">"model_size:warmup"</span>: [<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>][</span><br><span class="line">                        warmup_idx</span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">"step"</span>: range(<span class="number">20000</span>),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> warmup_idx <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(opts_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">600</span>)</span><br><span class="line">        .encode(x=<span class="string">"step"</span>, y=<span class="string">"Learning Rate"</span>, color=<span class="string">"model_size:warmup:N"</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">example_learning_schedule()</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_69_0.png" alt="png"></p><h3 id="Regularization（Label-Smoothing）"><a href="#Regularization（Label-Smoothing）" class="headerlink" title="Regularization（Label Smoothing）"></a>Regularization（Label Smoothing）</h3><p>在训练过程中我们使用了label smoothing，$\epsilon_{ls}=0.1$。虽然模型学会了更多的不确定，但提高了准确性和BLEU的分数。</p><p>我们使用KL div loss实现label smoothing. Instead of using a one-hot target distribution, we create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">"sum"</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure><p>Here we can see an example of how the mass is distributed to the words based on confidence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_label_smoothing</span><span class="params">()</span>:</span></span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">    predict = torch.FloatTensor(</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    crit(x=predict.log(), target=torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>]))</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"target distribution"</span>: crit.true_dist[x, y].flatten(),</span><br><span class="line">                    <span class="string">"columns"</span>: y,</span><br><span class="line">                    <span class="string">"rows"</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">5</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">5</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect(color=<span class="string">"Blue"</span>, opacity=<span class="number">1</span>)</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">"columns:O"</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Y(<span class="string">"rows:O"</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Color(</span><br><span class="line">                <span class="string">"target distribution:Q"</span>, scale=alt.Scale(scheme=<span class="string">"viridis"</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_label_smoothing)</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_74_0.png" alt="png"></p><p>Label smoothing actually starts to penalize the model if it gets very confident about a given choice.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x, crit)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d]])</span><br><span class="line">    <span class="keyword">return</span> crit(predict.log(), torch.LongTensor([<span class="number">1</span>])).data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">penalization_visualization</span><span class="params">()</span>:</span></span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">    loss_data = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"Loss"</span>: [loss(x, crit) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)],</span><br><span class="line">            <span class="string">"Steps"</span>: list(range(<span class="number">99</span>)),</span><br><span class="line">        &#125;</span><br><span class="line">    ).astype(<span class="string">"float"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(loss_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">350</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            x=<span class="string">"Steps"</span>,</span><br><span class="line">            y=<span class="string">"Loss"</span>,</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(penalization_visualization)</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_76_0.png" alt="png"></p><h2 id="A-First-Example"><a href="#A-First-Example" class="headerlink" title="A First Example"></a>A First Example</h2><p>我们可以从尝试一项简单的抄写任务开始。给定一组来自较小词汇表的随机输入symbols，目标是生成相同的symbols。</p><h3 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch_size, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="Loss-Computation"><a href="#Loss-Computation" class="headerlink" title="Loss Computation"></a>Loss Computation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        sloss = (</span><br><span class="line">            self.criterion(</span><br><span class="line">                x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), y.contiguous().view(<span class="number">-1</span>)</span><br><span class="line">            )</span><br><span class="line">            / norm</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br></pre></td></tr></table></figure><h3 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h3><p>为简单起见，此代码使用Greedy Decoding来预测翻译。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_simple_model</span><span class="params">()</span>:</span></span><br><span class="line">    V = <span class="number">11</span></span><br><span class="line">    criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">    model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, model_size=model.src_embed[<span class="number">0</span>].d_model, factor=<span class="number">1.0</span>, warmup=<span class="number">400</span></span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">80</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">20</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">"train"</span>,</span><br><span class="line">        )</span><br><span class="line">        model.eval()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">5</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">"eval"</span>,</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">    max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">execute_example(example_simple_model)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">3.023465</span> Tokens per Sec: <span class="number">403.074173</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.920030</span> Tokens per Sec: <span class="number">641.689380</span></span><br><span class="line"><span class="number">1.9274832487106324</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.940011</span> Tokens per Sec: <span class="number">432.003378</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.699767</span> Tokens per Sec: <span class="number">641.979665</span></span><br><span class="line"><span class="number">1.657595729827881</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.860276</span> Tokens per Sec: <span class="number">433.320240</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.546011</span> Tokens per Sec: <span class="number">640.537198</span></span><br><span class="line"><span class="number">1.4888023376464843</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.682198</span> Tokens per Sec: <span class="number">432.092305</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.313169</span> Tokens per Sec: <span class="number">639.441857</span></span><br><span class="line"><span class="number">1.3485562801361084</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.278768</span> Tokens per Sec: <span class="number">433.568756</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.062384</span> Tokens per Sec: <span class="number">642.542067</span></span><br><span class="line"><span class="number">0.9853351473808288</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.269471</span> Tokens per Sec: <span class="number">433.388727</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.590709</span> Tokens per Sec: <span class="number">642.862135</span></span><br><span class="line"><span class="number">0.5686767101287842</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.997076</span> Tokens per Sec: <span class="number">433.009746</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.343118</span> Tokens per Sec: <span class="number">642.288427</span></span><br><span class="line"><span class="number">0.34273059368133546</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.459483</span> Tokens per Sec: <span class="number">434.594030</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.290385</span> Tokens per Sec: <span class="number">642.519464</span></span><br><span class="line"><span class="number">0.2612409472465515</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.031042</span> Tokens per Sec: <span class="number">434.557008</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.437069</span> Tokens per Sec: <span class="number">643.630322</span></span><br><span class="line"><span class="number">0.4323212027549744</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.617165</span> Tokens per Sec: <span class="number">436.652626</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.258793</span> Tokens per Sec: <span class="number">644.372296</span></span><br><span class="line"><span class="number">0.27331129014492034</span></span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure><h2 id="A-Real-World-Example"><a href="#A-Real-World-Example" class="headerlink" title="A Real World Example"></a>A Real World Example</h2><p>现在，我们考虑一个使用IWSLT德语-英语翻译任务的真实世界示例。这项任务比本文考虑的WMT任务小得多，但它能说明整个流程。我们还展示了如何使用多GPU处理来实现真正的速度。</p><h3 id="Data-Loading"><a href="#Data-Loading" class="headerlink" title="Data Loading"></a>Data Loading</h3><p>We will load the dataset using torchtext and spacy for tokenization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load spacy tokenizer models, download them if they haven't been</span></span><br><span class="line"><span class="comment"># downloaded already</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_tokenizers</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_de = spacy.load(<span class="string">"de_core_news_sm"</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">"python -m spacy download de_core_news_sm"</span>)</span><br><span class="line">        spacy_de = spacy.load(<span class="string">"de_core_news_sm"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_en = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">"python -m spacy download en_core_web_sm"</span>)</span><br><span class="line">        spacy_en = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> spacy_de, spacy_en</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(text, tokenizer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> tokenizer.tokenizer(text)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yield_tokens</span><span class="params">(data_iter, tokenizer, index)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> from_to_tuple <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(from_to_tuple[index])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocabulary</span><span class="params">(spacy_de, spacy_en)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Building German Vocabulary ..."</span>)</span><br><span class="line">    train, val, test = datasets.IWSLT2016(language_pair=(<span class="string">"de"</span>, <span class="string">"en"</span>))</span><br><span class="line">    vocab_src = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_de, index=<span class="number">0</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">"&lt;s&gt;"</span>, <span class="string">"&lt;/s&gt;"</span>, <span class="string">"&lt;blank&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Building English Vocabulary ..."</span>)</span><br><span class="line">    train, val, test = datasets.IWSLT2016(language_pair=(<span class="string">"de"</span>, <span class="string">"en"</span>))</span><br><span class="line">    vocab_tgt = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_en, index=<span class="number">1</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">"&lt;s&gt;"</span>, <span class="string">"&lt;/s&gt;"</span>, <span class="string">"&lt;blank&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    vocab_src.set_default_index(vocab_src[<span class="string">"&lt;unk&gt;"</span>])</span><br><span class="line">    vocab_tgt.set_default_index(vocab_tgt[<span class="string">"&lt;unk&gt;"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_vocab</span><span class="params">(spacy_de, spacy_en)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(<span class="string">"vocab.pt"</span>):</span><br><span class="line">        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)</span><br><span class="line">        torch.save((vocab_src, vocab_tgt), <span class="string">"vocab.pt"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab_src, vocab_tgt = torch.load(<span class="string">"vocab.pt"</span>)</span><br><span class="line">    print(<span class="string">"Finished.\nVocabulary sizes:"</span>)</span><br><span class="line">    print(len(vocab_src))</span><br><span class="line">    print(len(vocab_tgt))</span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    <span class="comment"># global variables used later in the script</span></span><br><span class="line">    spacy_de, spacy_en = show_example(load_tokenizers)</span><br><span class="line">    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])</span><br></pre></td></tr></table></figure><h3 id="Iterators"><a href="#Iterators" class="headerlink" title="Iterators"></a>Iterators</h3><p>Batching对训练速度很重要。我们希望非常均匀的划分批次（with absolutely minimal padding）。要做到这一点，我们必须修改一下默认的torchtext batching。这段代码修改了它们的默认批处理，以确保我们搜索足够多的句子来找到紧凑的批处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_batch</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    batch,</span></span></span><br><span class="line"><span class="function"><span class="params">    src_pipeline,</span></span></span><br><span class="line"><span class="function"><span class="params">    tgt_pipeline,</span></span></span><br><span class="line"><span class="function"><span class="params">    src_vocab,</span></span></span><br><span class="line"><span class="function"><span class="params">    tgt_vocab,</span></span></span><br><span class="line"><span class="function"><span class="params">    device,</span></span></span><br><span class="line"><span class="function"><span class="params">    max_padding=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_id=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    bs_id = torch.tensor([<span class="number">0</span>], device=device)  <span class="comment"># &lt;s&gt; token id</span></span><br><span class="line">    eos_id = torch.tensor([<span class="number">1</span>], device=device)  <span class="comment"># &lt;/s&gt; token id</span></span><br><span class="line">    src_list, tgt_list = [], []</span><br><span class="line">    <span class="keyword">for</span> (_src, _tgt) <span class="keyword">in</span> batch:</span><br><span class="line">        processed_src = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    src_vocab(src_pipeline(_src)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        processed_tgt = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    tgt_vocab(tgt_pipeline(_tgt)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        src_list.append(</span><br><span class="line">            <span class="comment"># warning - overwrites values for negative values of padding - len</span></span><br><span class="line">            pad(</span><br><span class="line">                processed_src,</span><br><span class="line">                (</span><br><span class="line">                    <span class="number">0</span>,</span><br><span class="line">                    max_padding - len(processed_src),</span><br><span class="line">                ),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        tgt_list.append(</span><br><span class="line">            pad(</span><br><span class="line">                processed_tgt,</span><br><span class="line">                (<span class="number">0</span>, max_padding - len(processed_tgt)),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    src = torch.stack(src_list)</span><br><span class="line">    tgt = torch.stack(tgt_list)</span><br><span class="line">    <span class="keyword">return</span> (src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataloaders</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    device,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_src,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_tgt,</span></span></span><br><span class="line"><span class="function"><span class="params">    spacy_de,</span></span></span><br><span class="line"><span class="function"><span class="params">    spacy_en,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size=<span class="number">12000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    max_padding=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    is_distributed=True,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="comment"># def create_dataloaders(batch_size=12000):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> collate_batch(</span><br><span class="line">            batch,</span><br><span class="line">            tokenize_de,</span><br><span class="line">            tokenize_en,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            device,</span><br><span class="line">            max_padding=max_padding,</span><br><span class="line">            pad_id=vocab_src.get_stoi()[<span class="string">"&lt;blank&gt;"</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = datasets.IWSLT2016(</span><br><span class="line">        language_pair=(<span class="string">"de"</span>, <span class="string">"en"</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_iter_map = to_map_style_dataset(</span><br><span class="line">        train_iter</span><br><span class="line">    )  <span class="comment"># DistributedSampler needs a dataset len()</span></span><br><span class="line">    train_sampler = (</span><br><span class="line">        DistributedSampler(train_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line">    valid_iter_map = to_map_style_dataset(valid_iter)</span><br><span class="line">    valid_sampler = (</span><br><span class="line">        DistributedSampler(valid_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(</span><br><span class="line">        train_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    valid_dataloader = DataLoader(</span><br><span class="line">        valid_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(valid_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=valid_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br></pre></td></tr></table></figure><h2 id="Training-the-System"><a href="#Training-the-System" class="headerlink" title="Training the System"></a>Training the System</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_worker</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    gpu, ngpus_per_node, vocab_src, vocab_tgt, spacy_de, spacy_en, config</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    print(<span class="string">f"Train worker process using GPU: <span class="subst">&#123;gpu&#125;</span> for training"</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line">    is_main_process = gpu == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    pad_idx = vocab_tgt[<span class="string">"&lt;blank&gt;"</span>]</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    model = make_model(len(vocab_src), len(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda(gpu)</span><br><span class="line">    module = model</span><br><span class="line"></span><br><span class="line">    dist.init_process_group(</span><br><span class="line">        <span class="string">"nccl"</span>, init_method=<span class="string">"env://"</span>, rank=gpu, world_size=ngpus_per_node</span><br><span class="line">    )</span><br><span class="line">    model = DDP(model, device_ids=[gpu])</span><br><span class="line">    module = model.module</span><br><span class="line"></span><br><span class="line">    criterion = LabelSmoothing(</span><br><span class="line">        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=<span class="number">0.1</span></span><br><span class="line">    )</span><br><span class="line">    criterion.cuda(gpu)</span><br><span class="line"></span><br><span class="line">    train_dataloader, valid_dataloader = create_dataloaders(</span><br><span class="line">        gpu,</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=config[<span class="string">"batch_size"</span>] // ngpus_per_node,</span><br><span class="line">        max_padding=config[<span class="string">"max_padding"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">"base_lr"</span>], betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, d_model, factor=<span class="number">1</span>, warmup=config[<span class="string">"warmup"</span>]</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line">    train_state = TrainState()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(config[<span class="string">"num_epochs"</span>]):</span><br><span class="line"></span><br><span class="line">        train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">        valid_dataloader.sampler.set_epoch(epoch)</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        print(<span class="string">f"[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Training ===="</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        _, train_state = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> train_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">"train+log"</span>,</span><br><span class="line">            accum_iter=config[<span class="string">"accum_iter"</span>],</span><br><span class="line">            train_state=train_state,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        GPUtil.showUtilization()</span><br><span class="line">        <span class="keyword">if</span> is_main_process:</span><br><span class="line">            file_path = <span class="string">"%s%.2d.pt"</span> % (config[<span class="string">"file_prefix"</span>], epoch)</span><br><span class="line">            torch.save(module.state_dict(), file_path)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">f"[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Validation ===="</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        model.eval()</span><br><span class="line">        sloss = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> valid_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">"eval"</span>,</span><br><span class="line">        )</span><br><span class="line">        print(sloss)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_main_process:</span><br><span class="line">        file_path = <span class="string">"%sfinal.pt"</span> % config[<span class="string">"file_prefix"</span>]</span><br><span class="line">        torch.save(module.state_dict(), file_path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> the_annotated_transformer <span class="keyword">import</span> train_worker</span><br><span class="line"></span><br><span class="line">    ngpus = torch.cuda.device_count()</span><br><span class="line">    os.environ[<span class="string">"MASTER_ADDR"</span>] = <span class="string">"localhost"</span></span><br><span class="line">    os.environ[<span class="string">"MASTER_PORT"</span>] = <span class="string">"12356"</span></span><br><span class="line">    print(<span class="string">f"Number of GPUs detected: <span class="subst">&#123;ngpus&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">"Spawning training processes ..."</span>)</span><br><span class="line">    mp.spawn(</span><br><span class="line">        train_worker,</span><br><span class="line">        nprocs=ngpus,</span><br><span class="line">        args=(</span><br><span class="line">            ngpus,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            spacy_de,</span><br><span class="line">            spacy_en,</span><br><span class="line">            config,</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_trained_model</span><span class="params">(create_model)</span>:</span></span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">"batch_size"</span>: <span class="number">150</span>,</span><br><span class="line">        <span class="string">"num_epochs"</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">"accum_iter"</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">"base_lr"</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">"max_padding"</span>: <span class="number">72</span>,</span><br><span class="line">        <span class="string">"warmup"</span>: <span class="number">3000</span>,</span><br><span class="line">        <span class="string">"file_prefix"</span>: <span class="string">"iwslt_model_"</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> create_model:</span><br><span class="line">        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span><br><span class="line"></span><br><span class="line">    model = make_model(len(vocab_src), len(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">"iwslt_model_final.pt"</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    model = load_trained_model(create_model=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>一旦经过训练，我们就可以对模型进行decode，以产生一组翻译。在这里，我们只需翻译验证集中的第一句话。这个数据集非常小，因此使用greedy search的翻译相当准确。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Translation:	&lt;unk&gt; &lt;unk&gt; . In my language , that means , thank you very much . </span><br><span class="line">Gold:	&lt;unk&gt; &lt;unk&gt; . It means in my language , thank you very much .</span><br></pre></td></tr></table></figure><h2 id="Additional-Components-BPE-Search-Averaging"><a href="#Additional-Components-BPE-Search-Averaging" class="headerlink" title="Additional Components: BPE, Search, Averaging"></a>Additional Components: BPE, Search, Averaging</h2><p>上述代码主要介绍了Transformer自身的实现，还有四个函数我们没有实现。</p><h3 id="BPE-Word-piece"><a href="#BPE-Word-piece" class="headerlink" title="BPE/ Word-piece"></a>BPE/ Word-piece</h3><p>我们使用了subword units库对数据进行预处理。它将把训练数据转换成如下所示的形式：▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</p><h3 id="Shared-Embeddings"><a href="#Shared-Embeddings" class="headerlink" title="Shared Embeddings"></a>Shared Embeddings</h3><p>当使用共享vocabulary的BPE时，我们可以在source / target / generator之间共享权重向量。详细信息可以阅读<a href="https://arxiv.org/abs/1608.05859" target="_blank" rel="noopener">cite</a>。要将此想法实现到模型中，只需执行以下操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure><h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>详情可以看<a href="https://github.com/OpenNMT/OpenNMT-py/blob/onmt/translate/Beam.py" target="_blank" rel="noopener">OpenNMT-py</a>。</p><h3 id="Model-Averaging"><a href="#Model-Averaging" class="headerlink" title="Model Averaging"></a>Model Averaging</h3><p>文章中平均了最后k个checkpoints来达到集成效果。如果我们有一堆checkpoint，我们可以在事后做这件事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">(model, models)</span>:</span></span><br><span class="line">    <span class="string">"Average models into model"</span></span><br><span class="line">    <span class="keyword">for</span> ps <span class="keyword">in</span> zip(*[m.params() <span class="keyword">for</span> m <span class="keyword">in</span> [model] + models]):</span><br><span class="line">        ps[<span class="number">0</span>].copy_(torch.sum(*ps[<span class="number">1</span>:]) / len(ps[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>在WMT 2014英德翻译任务中，big transformer模型(表2中的Transformer (big) )的表现超过了已有的最好的模型(包括集成)，BLEU的表现高上了2.0%，创造了最好的BLEU 28.4分的新纪录。表3的底部列出了该模型参数配置。在8个P100 GPU上进行了3.5天的训练。甚至我们的base model模型也超过了已有的所有模型和集成模型，而训练成本只是任何已有模型的一小部分。</p><p>在2014年WMT英法翻译任务中，我们的大模型达到了BLEU的41.0分，超过了之前已有的所有单一模型，培训成本不到以前最好模型的四分之一。用于英法翻译的Transformer (big) 的dropout 系数等于0.1，而不是0.3。</p><blockquote><p>我们在这里编写的代码是base model的一个版本。完整版本可以看 <a href="http://opennmt.net/Models-py/" target="_blank" rel="noopener">(Example Models)</a>。</p><p>使用上以小节的附加扩展，OpenNMT-py 在EN-DE WMT数据集上达到了26.9的BLEU。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load data and model for output checks</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_outputs</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    valid_dataloader,</span></span></span><br><span class="line"><span class="function"><span class="params">    model,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_src,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_tgt,</span></span></span><br><span class="line"><span class="function"><span class="params">    n_examples=<span class="number">15</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_idx=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    eos_string=<span class="string">"&lt;/s&gt;"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    results = [()] * n_examples</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(n_examples):</span><br><span class="line">        print(<span class="string">"\nExample %d ========\n"</span> % idx)</span><br><span class="line">        b = next(iter(valid_dataloader))</span><br><span class="line">        rb = Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx)</span><br><span class="line">        greedy_decode(model, rb.src, rb.src_mask, <span class="number">64</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        src_tokens = [</span><br><span class="line">            vocab_src.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.src[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line">        tgt_tokens = [</span><br><span class="line">            vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.tgt[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        print(</span><br><span class="line">            <span class="string">"Source Text (Input)        : "</span></span><br><span class="line">            + <span class="string">" "</span>.join(src_tokens).replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        )</span><br><span class="line">        print(</span><br><span class="line">            <span class="string">"Target Text (Ground Truth) : "</span></span><br><span class="line">            + <span class="string">" "</span>.join(tgt_tokens).replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        )</span><br><span class="line">        model_out = greedy_decode(model, rb.src, rb.src_mask, <span class="number">72</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        model_txt = (</span><br><span class="line">            <span class="string">" "</span>.join(</span><br><span class="line">                [vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> model_out <span class="keyword">if</span> x != pad_idx]</span><br><span class="line">            ).split(eos_string, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            + eos_string</span><br><span class="line">        )</span><br><span class="line">        print(<span class="string">"Model Output               : "</span> + model_txt.replace(<span class="string">"\n"</span>, <span class="string">""</span>))</span><br><span class="line">        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_model_example</span><span class="params">(n_examples=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> vocab_src, vocab_tgt, spacy_de, spacy_en</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Preparing Data ..."</span>)</span><br><span class="line">    _, valid_dataloader = create_dataloaders(</span><br><span class="line">        torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        is_distributed=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Loading Trained Model ..."</span>)</span><br><span class="line"></span><br><span class="line">    model = make_model(len(vocab_src), len(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(</span><br><span class="line">        torch.load(<span class="string">"iwslt_model_final.pt"</span>, map_location=torch.device(<span class="string">"cpu"</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Checking Model Outputs:"</span>)</span><br><span class="line">    example_data = check_outputs(</span><br><span class="line">        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model, example_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">execute_example(run_model_example)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Translation:	&lt;s&gt; ▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</span><br></pre></td></tr></table></figure><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>即使使用greedy decoder，翻译看起来也很好。我们可以进一步把它形象化，看看注意力的每一层都在发生什么。</p><h3 id="Encoder-Self-Attention"><a href="#Encoder-Self-Attention" class="headerlink" title="Encoder Self Attention"></a>Encoder Self Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mtx2df</span><span class="params">(m, max_row, max_col, row_tokens, col_tokens)</span>:</span></span><br><span class="line">    <span class="string">"convert a dense matrix to a data frame with row and column indices"</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                float(m[r, c]),</span><br><span class="line">                <span class="string">"%.3d %s"</span></span><br><span class="line">                % (r, row_tokens[r] <span class="keyword">if</span> len(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">                <span class="string">"%.3d %s"</span></span><br><span class="line">                % (c, col_tokens[c] <span class="keyword">if</span> len(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> range(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        <span class="comment"># if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],</span></span><br><span class="line">        columns=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attn_map</span><span class="params">(attn, layer, head, row_tokens, col_tokens, max_dim=<span class="number">30</span>)</span>:</span></span><br><span class="line">    df = mtx2df(</span><br><span class="line">        attn[<span class="number">0</span>, head].data,</span><br><span class="line">        max_dim,</span><br><span class="line">        max_dim,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">"col_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">"row_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            color=<span class="string">"value"</span>,</span><br><span class="line">            tooltip=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">        )</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encoder</span><span class="params">(model, layer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model.encoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_decoder_self</span><span class="params">(model, layer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_decoder_src</span><span class="params">(model, layer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].src_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_layer</span><span class="params">(model, layer, getter_fn, ntokens, row_tokens, col_tokens)</span>:</span></span><br><span class="line">    <span class="comment"># ntokens = last_example[0].ntokens</span></span><br><span class="line">    attn = getter_fn(model, layer)</span><br><span class="line">    n_heads = attn.shape[<span class="number">1</span>]</span><br><span class="line">    charts = [</span><br><span class="line">        attn_map(</span><br><span class="line">            attn,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            h,</span><br><span class="line">            row_tokens=row_tokens,</span><br><span class="line">            col_tokens=col_tokens,</span><br><span class="line">            max_dim=ntokens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_heads)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">assert</span> n_heads == <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(</span><br><span class="line">        charts[<span class="number">0</span>]</span><br><span class="line">        | charts[<span class="number">1</span>]</span><br><span class="line">        | charts[<span class="number">2</span>]</span><br><span class="line">        | charts[<span class="number">3</span>]</span><br><span class="line">        | charts[<span class="number">4</span>]</span><br><span class="line">        | charts[<span class="number">5</span>]</span><br><span class="line">        | charts[<span class="number">6</span>]</span><br><span class="line">        | charts[<span class="number">7</span>]</span><br><span class="line">        <span class="comment"># layer + 1 due to 0-indexing</span></span><br><span class="line">    ).properties(title=<span class="string">"Layer %d"</span> % (layer + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="Encoder-Self-Attention-1"><a href="#Encoder-Self-Attention-1" class="headerlink" title="Encoder Self Attention"></a>Encoder Self Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_encoder_self</span><span class="params">()</span>:</span></span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[</span><br><span class="line">        len(example_data) - <span class="number">1</span></span><br><span class="line">    ]  <span class="comment"># batch object for the final example</span></span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model, layer, get_encoder, len(example[<span class="number">1</span>]), example[<span class="number">1</span>], example[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_encoder_self)</span><br></pre></td></tr></table></figure><h3 id="Decoder-Self-Attention"><a href="#Decoder-Self-Attention" class="headerlink" title="Decoder Self Attention"></a>Decoder Self Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_decoder_self</span><span class="params">()</span>:</span></span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[len(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_self,</span><br><span class="line">            len(example[<span class="number">1</span>]),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_self)</span><br></pre></td></tr></table></figure><h3 id="Decoder-Src-Attention"><a href="#Decoder-Src-Attention" class="headerlink" title="Decoder Src Attention"></a>Decoder Src Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_decoder_src</span><span class="params">()</span>:</span></span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[len(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_src,</span><br><span class="line">            max(len(example[<span class="number">1</span>]), len(example[<span class="number">2</span>])),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">2</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_src)</span><br></pre></td></tr></table></figure><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><h3 id="Transformer-mask"><a href="#Transformer-mask" class="headerlink" title="Transformer mask"></a>Transformer mask</h3><p>在《<a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">the annotated transformer</a>》中有多个mask，这里总结一下。</p><p>整个模型中使用到的mask主要就是source mask和target mask，其各自的作用如下所示：</p><ol><li>source mask：</li></ol><ul><li><p>source长短不一而无法形成batch，因此引入了pad。将source mask传入到encoder中，让attention在计算$\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})$时，pad位置的值不起作用。</p></li><li><p>同时这个mask还需要传入每个decoderLayer第二个multi-head attention模块中，就是防止来自encoder的key和来自decoder的query在计算多头注意力的时候算了target中的词和source中pad的权重</p></li></ul><ol><li>target mask：需要分training和testing进行讨论</li></ol><ul><li><strong>训练</strong>时，用于防止target的ground truth长短不一引入pad造成的误差，以及<strong>避免在自回归时看到正在预测的字和以后字的ground truth</strong></li><li><strong>测试</strong>时，逻辑上decoder不需要target mask，但出于编程方便的考虑引入mask，假装用于防止看到后面的ground truth，target mask的最后两维的shape和目前生成出来的序列长度相同，但实际上每次都会有一些重复运算在里面，比如目前在预测第10个词时，第1-9个词还需要重新算一遍。核心原因是：模型在写的时候主要考虑的是训练，执行一次attention函数翻译完一个batch的所有句子，而测试时必须是单个或多个句子word by word进行计算</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">annotated-transformer</a><br><a href="https://zhuanlan.zhihu.com/p/504408727" target="_blank" rel="noopener">the annotated transformer中的关于mask的问题 - lumino的文章 - 知乎</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/" title="Transformer">https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a><a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a></div><div class="post-nav"><div class="post-nav-item"><a href="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/" rel="prev" title="fairseq翻译任务解读"><i class="fa fa-chevron-left"></i> fairseq翻译任务解读</a></div><div class="post-nav-item"> <a href="/DeepLearningApplications/自然语言处理/bert模型详解/" rel="next" title="bert模型详解">bert模型详解<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Prelims"><span class="nav-number">1.</span> <span class="nav-text">Prelims</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Architecture"><span class="nav-number">2.</span> <span class="nav-text">Model Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">2.1.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">2.2.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-number">2.3.</span> <span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Applications-of-Attention-in-our-Model"><span class="nav-number">2.4.</span> <span class="nav-text">Applications of Attention in our Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Position-wise-Feed-Forward-Networks"><span class="nav-number">2.5.</span> <span class="nav-text">Position-wise Feed-Forward Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embeddings-and-Softmax"><span class="nav-number">2.6.</span> <span class="nav-text">Embeddings and Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-number">2.7.</span> <span class="nav-text">Positional Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Full-Model"><span class="nav-number">2.8.</span> <span class="nav-text">Full Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference"><span class="nav-number">2.9.</span> <span class="nav-text">Inference</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training"><span class="nav-number">3.</span> <span class="nav-text">Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Batches-and-Masking"><span class="nav-number">3.1.</span> <span class="nav-text">Batches and Masking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Loop"><span class="nav-number">3.2.</span> <span class="nav-text">Training Loop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Data-and-Batching"><span class="nav-number">3.3.</span> <span class="nav-text">Training Data and Batching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hardware-and-Schedule"><span class="nav-number">3.4.</span> <span class="nav-text">Hardware and Schedule</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimizer"><span class="nav-number">3.5.</span> <span class="nav-text">Optimizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization（Label-Smoothing）"><span class="nav-number">3.6.</span> <span class="nav-text">Regularization（Label Smoothing）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-First-Example"><span class="nav-number">4.</span> <span class="nav-text">A First Example</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Synthetic-Data"><span class="nav-number">4.1.</span> <span class="nav-text">Synthetic Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-Computation"><span class="nav-number">4.2.</span> <span class="nav-text">Loss Computation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Decoding"><span class="nav-number">4.3.</span> <span class="nav-text">Greedy Decoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Real-World-Example"><span class="nav-number">5.</span> <span class="nav-text">A Real World Example</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Loading"><span class="nav-number">5.1.</span> <span class="nav-text">Data Loading</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Iterators"><span class="nav-number">5.2.</span> <span class="nav-text">Iterators</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-the-System"><span class="nav-number">6.</span> <span class="nav-text">Training the System</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Additional-Components-BPE-Search-Averaging"><span class="nav-number">7.</span> <span class="nav-text">Additional Components: BPE, Search, Averaging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BPE-Word-piece"><span class="nav-number">7.1.</span> <span class="nav-text">BPE/ Word-piece</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shared-Embeddings"><span class="nav-number">7.2.</span> <span class="nav-text">Shared Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beam-Search"><span class="nav-number">7.3.</span> <span class="nav-text">Beam Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Averaging"><span class="nav-number">7.4.</span> <span class="nav-text">Model Averaging</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Results"><span class="nav-number">8.</span> <span class="nav-text">Results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-Visualization"><span class="nav-number">8.1.</span> <span class="nav-text">Attention Visualization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Self-Attention"><span class="nav-number">8.2.</span> <span class="nav-text">Encoder Self Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Self-Attention-1"><span class="nav-number">8.3.</span> <span class="nav-text">Encoder Self Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder-Self-Attention"><span class="nav-number">8.4.</span> <span class="nav-text">Decoder Self Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder-Src-Attention"><span class="nav-number">8.5.</span> <span class="nav-text">Decoder Src Attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Question"><span class="nav-number">9.</span> <span class="nav-text">Question</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer-mask"><span class="nav-number">9.1.</span> <span class="nav-text">Transformer mask</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">10.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">327</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">55</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">383</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">35:21</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '60af59f2efdd474df82e5424774a0468',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>