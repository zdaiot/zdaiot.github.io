<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="最近需要用到fairseq框架中的翻译任务，这里记录一下。 从实战开始首先下载翻译模型： 123456mkdir -p modelcd modelwget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2bunzip2 wmt16.en-de.joined-dict.t"><meta name="keywords" content="NLP,fairseq,translation"><meta property="og:type" content="article"><meta property="og:title" content="fairseq翻译任务解读"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="最近需要用到fairseq框架中的翻译任务，这里记录一下。 从实战开始首先下载翻译模型： 123456mkdir -p modelcd modelwget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2bunzip2 wmt16.en-de.joined-dict.t"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/2019051614192910.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427221019923.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427223531357.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427233419637.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427105444788.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427110136773.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220428003141180.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220428003252483.png"><meta property="og:updated_time" content="2022-04-27T02:42:03.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="fairseq翻译任务解读"><meta name="twitter:description" content="最近需要用到fairseq框架中的翻译任务，这里记录一下。 从实战开始首先下载翻译模型： 123456mkdir -p modelcd modelwget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2bunzip2 wmt16.en-de.joined-dict.t"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/2019051614192910.jpg"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>fairseq翻译任务解读 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> fairseq翻译任务解读<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/自然语言处理/fairseq翻译任务解读.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-04-27 10:42:03" itemprop="dateCreated datePublished" datetime="2022-04-27T10:42:03+08:00">2022-04-27</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/自然语言处理/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>36k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>33 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>最近需要用到<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">fairseq</a>框架中的翻译任务，这里记录一下。</p><h2 id="从实战开始"><a href="#从实战开始" class="headerlink" title="从实战开始"></a>从实战开始</h2><p>首先下载翻译模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p model</span><br><span class="line"><span class="built_in">cd</span> model</span><br><span class="line">wget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2</span><br><span class="line"></span><br><span class="line">bunzip2 wmt16.en-de.joined-dict.transformer.tar.bz2</span><br><span class="line">tar -xvf wmt16.en-de.joined-dict.transformer.tar</span><br></pre></td></tr></table></figure><p>解压后的文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./model</span><br><span class="line">├── wmt16.en-de.joined-dict.transformer</span><br><span class="line">│   ├── bpecodes</span><br><span class="line">│   ├── dict.de.txt</span><br><span class="line">│   ├── dict.en.txt</span><br><span class="line">│   └── model.pt</span><br></pre></td></tr></table></figure><p>然后调用翻译模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fairseq.models.transformer <span class="keyword">import</span> TransformerModel</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_fairseq_tm</span><span class="params">(path, device)</span>:</span></span><br><span class="line">    <span class="comment"># data_name_or_path和bpe_codes可以省略</span></span><br><span class="line">    model = TransformerModel.from_pretrained(</span><br><span class="line">        path,</span><br><span class="line">        checkpoint_file=<span class="string">'model.pt'</span>,</span><br><span class="line">        data_name_or_path=<span class="string">'.'</span>,</span><br><span class="line">        bpe=<span class="string">'subword_nmt'</span>,</span><br><span class="line">        bpe_codes=path+<span class="string">"/bpecode"</span></span><br><span class="line">    )</span><br><span class="line">    model.cuda(device=device)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">bt_model = load_fairseq_tm(<span class="string">'./model/wmt16.en-de.joined-dict.transformer'</span>, <span class="number">0</span>)</span><br><span class="line">output = bt_model.translate(<span class="string">'Hello world!'</span>)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><p>得到的结果是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hallo Welt !</span><br></pre></td></tr></table></figure><p>那么，这个过程都干了哪些事呢？我们对此进行了详细的分析。</p><h2 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h2><p>在分析之前，我们先介绍一下BPE算法。以下内容来源于<a href="http://txshi-mt.com/2019/02/28/NMT-Tutorial-3e2-subword/" target="_blank" rel="noopener">NMT Tutorial 3扩展e第2部分. Subword</a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>按照布隆菲尔德的理论，词被认为是人类语言中能自行独立存在的最小单位，是“最小自由形式”。因此，对西方语言做NLP时，以词为基石是一个很自然的想法。</p><p>但是将某个语言的词穷举出来是不太现实的。首先，名词、动词、形容词、副词这四种属于开放词类，总会有新的词加入进来。其次，网络用语会创造出更多新词，或者为某个词给出不规则的变形。最后，以德语为代表的语言通常会将几个基本词组合起来，形成一个复合词，例如Abwasserbehandlungsanlage “污水处理厂”可以被细分为Abwasser、behandlungs和Anlage三个部分。</p><p>即便是存在某个语言能获得其完整词表，词表的数量也会非常庞大，使得模型复杂度很高，训练起来很难。对于以德语、西班牙语、俄语为代表的<strong>屈折语</strong>，也会存在类似的问题（例如西班牙语动词可能有80种变化）。</p><p>因此，在机器翻译等任务中，从训练语料构造词表时，通常会过滤掉出现频率很低的单词，并将这些单词统一标记为<strong>UNK（Unknown）</strong>。根据Zipf定律，这种做法能筛掉很多不常见词，简化模型结构，而且可以起到部分防止过拟合的作用。此外，模型上线做推断时，也有很大概率会遇到在训练语料里没见过的词，这些词也会被标为UNK。所有不在词表里被标记为UNK的词，通常被称作<strong>集外词</strong>（Out Of Vocabulary，OOV）或者<strong>未登录词</strong>。</p><p>对未登录词的处理是机器翻译领域里一个十分重要的问题。<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">sennrich2016</a>认为，对于某些未登录词的翻译可能是”透明“的，包括</p><ul><li>命名实体，例如人名、地名等。对于这些词，如果目标语言和源语言的字母体系相同，可能可以直接抄写；如果不同，需要做些转写。例如将英语的Barack Obama转写成俄语的Барак Обама</li><li>借词，可以通过字母级别的翻译做到，例如将claustrophobia翻译成德语的Klaustrophobie和俄语的Клаустрофобия</li><li>词素复杂的词，例如通过组合或者屈折变化得到的词，可以将其进一步拆分为词素，通过分别翻译各个词素的得到结果。例如将英语的solar system翻译成德语的Sonnensystem或者匈牙利语的Naprendszer</li></ul><p>因此，将词拆分为更细粒度的subword，可以有助于处理OOV问题。另外传统tokenization方法不利于模型学习词缀之间的关系。E.g. 模型学到的“old”, “older”, and “oldest”之间的关系无法泛化到“smart”, “smarter”, and “smartest”。</p><p>由此，<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">sennrich2016</a>文章还同时指出使用一种称为<strong>“比特对编码”（Byte Pair Encoding——BPE）</strong>的算法可以将词拆分为更细粒度的subword。但是BPE对单词的划分是纯基于统计的，得到的subword所蕴含的词素，或者说形态学信息，并不明显。除此BPE之外，Morfessor是一种基于形态学的分词器，它使用的是无监督学习的方法，能达到不错的准确率。最后，2016年FAIR提出的一种基于subword的词嵌入表示方法fastText。但是本文只关注BPE算法，其余可以参考文章<a href="http://txshi-mt.com/2019/02/28/NMT-Tutorial-3e2-subword/" target="_blank" rel="noopener">NMT Tutorial 3扩展e第2部分. Subword</a>。</p><p>除去subword方法以外，还可以将词拆成字符，为每个字符训练一个字符向量。这种方法很直观，也很有效，不过无需太费笔墨来描述。关于字符向量的优秀工作，可以参考<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00051/43387/Enriching-Word-Vectors-with-Subword-Information" target="_blank" rel="noopener">Bojanowski2017</a>的“相关工作”部分。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>BPE算法[gage1994]的本质实际上是一种数据压缩算法。<strong>数据压缩的一般做法都是将常见比特串替换为更短的表示方法</strong>，而BPE也不例外。更具体地说，BPE是找出最常出现的相邻字节对，将其替换成一个在原始数据里没有出现的字节，一直循环下去，直到找不到最常出现的字节对或者所有字节都用光了为止。后期使用时需要一个替换表来重建原始数据。例如，对”lwlwlwlwrr”使用BPE算法，会先把lw替换为a，得到”aaaarr”，然后把”aa”替换为”b”，得到”bbrr”。此时所有相邻字节对”bb”、”br”、”rr”的出现次数相等，迭代结束，输出替换表{“b” -&gt; “aa”, “a” -&gt; “lw”}。</p><ul><li>优点：可以有效地平衡词汇表大小和步数(编码句子所需的token数量)。</li><li>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li></ul><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ol><li>准备足够大的训练语料</li><li>确定期望的subword词表大小</li><li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li><li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li><li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li></ol><p>停止符”&lt;/w&gt;”的意义在于表示subword是词后缀。举例来说：”st”字词不加”&lt;/w&gt;”可以出现在词首如”st ar”，加了”&lt;/w&gt;”表明改字词位于词尾，如”wide st&lt;/w&gt;”，二者意义截然不同。</p><p>每次合并后词表可能出现3种变化：</p><ul><li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li><li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li><li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li></ul><p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p><p><strong>例子</strong></p><p>输入：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”。输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w es t &lt;/w&gt;': 6, 'w i d es t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”。输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est &lt;/w&gt;': 6, 'w i d est &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 3, 以此类推，最高频连续字节对为”est”和”&lt;/w&gt;” 输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>……</p><p>Iter n, 继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。</p><p>BPE算法的核心学习过程可以写做如下Python代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab)</span>:</span></span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols)<span class="number">-1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair, v_in)</span>:</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line">vocab = &#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w e s t &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d e s t &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print output</span></span><br><span class="line"><span class="comment"># ('e', 's')</span></span><br><span class="line"><span class="comment"># ('es', 't')</span></span><br><span class="line"><span class="comment"># ('est', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('l', 'o')</span></span><br><span class="line"><span class="comment"># ('lo', 'w')</span></span><br><span class="line"><span class="comment"># ('n', 'e')</span></span><br><span class="line"><span class="comment"># ('ne', 'w')</span></span><br><span class="line"><span class="comment"># ('new', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('w', 'i')</span></span><br><span class="line"><span class="comment"># ('wi', 'd')</span></span><br><span class="line"><span class="comment"># ('wid', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', 'e')</span></span><br><span class="line"><span class="comment"># ('lowe', 'r')</span></span><br><span class="line"><span class="comment"># ('lower', '&lt;/w&gt;')</span></span><br></pre></td></tr></table></figure><h3 id="编码和解码"><a href="#编码和解码" class="headerlink" title="编码和解码"></a>编码和解码</h3><h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4><p>在之前的算法中，我们已经得到了subword的词表（即常说的<code>code</code>文件），且该词表已经按照频率从高到低进行排序了。那么我们就可以对单词进行编码（下文的<code>subword-nmt</code>小节中，利用得到的<code>code.file</code>对<code>./en.txt</code>进行编码得到<code>result1.txt</code>就利用了当前要介绍的编码过程）。</p><p>以单词“where”为例，首先按照字符拆分开，然后查找<code>code</code>文件，逐对合并，优先合并频率靠前的字符对。<code>85 319 9 15</code> 表示在该字符对在<code>code</code>文件中的频率排名。</p><blockquote><p>根据我自己的实验，<code>e&lt;/w&gt;</code>可以直接合并，所以这里的频率排名直接是1，即使<code>code</code>文件中无<code>e &lt;/w&gt;</code>。</p></blockquote><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/2019051614192910.jpg" alt="img" style="zoom:67%"></p><p> 如果仍然有子字符串没被替换但所有token都已迭代完毕，则有两种做法，一种是将剩余的子词替换为特殊token，如<unk>。另外一种比较常用，由于未登录词通常会被这种方法拆成若干个subword，因此通常会向不在原来词表的subword后面写明一个分隔符，通常是@@。例如，假如要编码的词是said</unk></p><ol><li>若这个词的子词<code>s a</code>在词表中，但是<code>sa i</code>和<code>i d&lt;/w&gt;</code>不在词表里，<code>encode</code>只能得到<code>(&#39;sa&#39;, &#39;i&#39;, &#39;d&#39;)</code>，那么输出会是<code>sa@@ i@@ d</code>。</li><li>若子词<code>s a</code>和<code>i d</code>在词表中，但是<code>sa i</code>和<code>i d&lt;/w&gt;</code>不在词表里，那么输出仍然是<code>sa@@ i@@ d</code>。</li><li>若子词<code>s a</code>和<code>i d&lt;/w&gt;</code>在词表中，但是<code>sa id&lt;/w&gt;</code>不在词表里，那么输出是<code>sa@@ id</code>。</li><li>若子词<code>s a</code>，<code>i d&lt;/w&gt;</code>和<code>sa id&lt;/w&gt;</code>在词表中，那么输出是<code>said</code>。</li><li>若仅有<code>sa id&lt;/w&gt;</code>在词表中，那么输出是<code>s@@ a@@ i@@ d</code>。</li><li>若仅有<code>i d&lt;/w&gt;</code>和<code>sa id&lt;/w&gt;</code>在词表中，那么输出是<code>s@@ a@@ id</code>。</li></ol><p>编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。</p><h4 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h4><p>将所有的tokens拼在一起，如果有<code>@@</code>符号则去除（下文的<code>后处理</code>小节中<code>self.remove_bpe</code>函数，就利用了当前小节要介绍的解码过程）。</p><p>例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 编码序列</span><br><span class="line">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 解码序列</span><br><span class="line">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure><h3 id="subword-nmt"><a href="#subword-nmt" class="headerlink" title="subword-nmt"></a>subword-nmt</h3><p>安装subword-nmt</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install subword-nmt</span><br></pre></td></tr></table></figure><h4 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h4><p>先准备一个语料库。例如：链接：<a href="https://pan.baidu.com/s/1BAWDeAw5QYXS7xCrLIBIAw，提取码：kfy9" target="_blank" rel="noopener">https://pan.baidu.com/s/1BAWDeAw5QYXS7xCrLIBIAw，提取码：kfy9</a></p><p><strong>生成codevocabulary和：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt learn-joint-bpe-and-vocab -i ./en.txt -o ./code.file --write-vocabulary voc.txt</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li>-i后面的参数是输入文件名</li><li>-o后面是输出的code文件文件名</li><li>—write-vocabulary后面是输出字典的文件名</li></ul><p>其他参数说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">usage: subword-nmt learn-joint-bpe-<span class="keyword">and</span>-vocab [-h] --input PATH [PATH ...]</span><br><span class="line">                                             --output PATH [--symbols SYMBOLS]</span><br><span class="line">                                             [--separator STR]</span><br><span class="line">                                             --write-vocabulary PATH</span><br><span class="line">                                             [PATH ...] [--min-frequency FREQ]</span><br><span class="line">                                             [--total-symbols] [--verbose]</span><br><span class="line"></span><br><span class="line">learn BPE-based word segmentation</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message <span class="keyword">and</span> exit</span><br><span class="line">  --input PATH [PATH ...], -i PATH [PATH ...]</span><br><span class="line">                        Input texts (multiple allowed).</span><br><span class="line">  --output PATH, -o PATH</span><br><span class="line">                        Output file <span class="keyword">for</span> BPE codes.</span><br><span class="line">  --symbols SYMBOLS, -s SYMBOLS</span><br><span class="line">                        Create this many new symbols (each representing a</span><br><span class="line">                        character n-gram) (default: <span class="number">10000</span>))</span><br><span class="line">  --separator STR       Separator between non-final subword units (default:</span><br><span class="line">                        <span class="string">'@@'</span>))</span><br><span class="line">  --write-vocabulary PATH [PATH ...]</span><br><span class="line">                        Write to these vocabulary files after applying BPE.</span><br><span class="line">                        One per input text. Used <span class="keyword">for</span> filtering <span class="keyword">in</span> apply_bpe.py</span><br><span class="line">  --min-frequency FREQ  Stop <span class="keyword">if</span> no symbol pair has frequency &gt;= FREQ (default:</span><br><span class="line">                        <span class="number">2</span>))</span><br><span class="line">  --total-symbols, -t   subtract number of characters <span class="keyword">from</span> the symbols to be</span><br><span class="line">                        generated (so that <span class="string">'--symbols'</span> becomes an estimate <span class="keyword">for</span></span><br><span class="line">                        the total number of symbols needed to encode text).</span><br><span class="line">  --verbose, -v         verbose mode.</span><br></pre></td></tr></table></figure><p>我们可以看一下生成的code.file和voc.txt。</p><p>code.file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#version: 0.2</span><br><span class="line">t h</span><br><span class="line">i n</span><br><span class="line">th e&lt;/w&gt;</span><br><span class="line">a n</span><br><span class="line">r e</span><br><span class="line">t i</span><br><span class="line">e n</span><br><span class="line">o n</span><br><span class="line">an d&lt;/w&gt;</span><br><span class="line">e r</span><br><span class="line">···</span><br></pre></td></tr></table></figure><p>voc.txt部分内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">···</span><br><span class="line">ary 14</span><br><span class="line">apart 14</span><br><span class="line">conscientiously 14</span><br><span class="line">flight 14</span><br><span class="line">association 14</span><br><span class="line">represent 14</span><br><span class="line">th 14</span><br><span class="line">activity 14</span><br><span class="line">standard 14</span><br><span class="line">call 14</span><br><span class="line">jia 14</span><br><span class="line">solid 14</span><br><span class="line">seven 14</span><br><span class="line">···</span><br></pre></td></tr></table></figure><p>这里需要注意的是，<code>code.file</code>文件一共有10001行，而<code>voc.txt</code>文件一共有8760行，并且<code>voc.txt</code>含有一部分带有<code>@@</code>的行。</p><p>那么这里的<code>code.file</code>和<code>voc.txt</code>有什么关系呢？我们继续进行探索。</p><p>安装完subword-nmt之后，我们可以在终端输入<code>subword-nmt -h</code>，得到内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">(base) PS E:\Working\learn_bpe&gt; subword-nmt -h</span><br><span class="line">usage: subword-nmt [-h] &#123;learn-bpe,apply-bpe,get-vocab,learn-joint-bpe-and-vocab&#125; ...</span><br><span class="line"></span><br><span class="line">subword-nmt: unsupervised word segmentation <span class="keyword">for</span> neural machine translation and text generation</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  &#123;learn-bpe,apply-bpe,get-vocab,learn-joint-bpe-and-vocab&#125;</span><br><span class="line">                        <span class="built_in">command</span> to run. Run one of the commands with <span class="string">'-h'</span> <span class="keyword">for</span> more info.</span><br><span class="line"></span><br><span class="line">                        apply-bpe: apply given BPE operations to input text.</span><br><span class="line">                        learn-joint-bpe-and-vocab: executes recommended workflow <span class="keyword">for</span> joint BPE.</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br></pre></td></tr></table></figure><p>也就是说，<code>subword-nmt</code>有<code>learn-bpe,apply-bpe,get-vocab,learn-joint-bpe-and-vocab</code>方法，继续输入<code>subword-nmt learn-bpe -h</code>可以查看子函数的用法。</p><p>详细的探索这几个函数的用法之后，可以发现如下结论。</p><ol><li><code>learn-joint-bpe-and-vocab</code>其实是三条指令的合体。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt learn-joint-bpe-and-vocab -i ./en.txt -o ./code.file --write-vocabulary voc.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述指令等价于</span></span><br><span class="line">subword-nmt learn-bpe -i ./en.txt  -o ./code.file</span><br><span class="line">subword-nmt apply-bpe -i ./en.txt -c ./code.file -o result1.txt</span><br><span class="line">subword-nmt get-vocab -i ./result1.txt -o voc.txt</span><br></pre></td></tr></table></figure><ol><li><p><code>get-vocab</code>函数会对文件中出现的单词以及对应的频率进行统计，得到<code>voc.txt</code>文件，该过程不需要<code>code.file</code>文件。</p></li><li><p><code>code.file</code>和<code>voc.txt</code>关系是：首先利用<code>learn-bpe</code>从<code>en.txt</code>文件中学习bpe分词规则，然后利用该规则对<code>en.txt</code>编码，统计编码之后文件的词语和词频得到<code>voc.txt</code>文件。所以两者并不是意义对应的关系，而且哪个文件行数更多也不一定。</p></li></ol><p><strong>使用bpe编码</strong></p><p>在使用learn-bpe功能得到code后，可以使用apply-bpe来对语料进行编码。值得注意的是，这里解码时并不需要用到<code>voc.txt</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt apply-bpe -i ./en.test.txt -c ./code.file -o result.txt</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>-i</code> 后面是输入的待解码文件名</li><li><code>-c</code> 后面跟着learn-bpe步骤得到的code文件</li><li><code>-o</code> 结果输出文件</li></ul><p>我们可以查看结果，就会自动根据bpe生成的code文件对语料进行分割。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">beijing , 1 mar ( xinhua ) -- tian feng@@ shan , former heilongjiang governor who is 5@@ 9 years old , was appointed minister of land and resources today .</span><br><span class="line">tian feng@@ shan , who was born in zhao@@ yuan county , heilongjiang province , took part in work since july 196@@ 1 and joined the cpc in march 1970 .</span><br><span class="line">this should be a natural process set off by economic development ; the &quot; third tier construction &quot; of the 1960s involving fac@@ tory relocation was something entirely different .</span><br><span class="line">we must also realize however that from the angle of changing the pattern of resource allocation , we have not yet made the big breakthrough in reform .</span><br><span class="line">with regard to joining the world trade organization , one recent reaction has been blind optim@@ ism and the belief that china will profit whatever it does .</span><br><span class="line">since these areas where objective conditions are not particularly good can achieve this , other areas where conditions are better can naturally do the same .</span><br><span class="line">the objective trend of globalization is calling for international cooperation on a global scale , and a global cooperation has far exceeded the scope of the economy .</span><br></pre></td></tr></table></figure><p><strong>解码</strong></p><p>那么我们的文件怎么恢复到bpe编码之前的结果呢？</p><p>只需要执行下面指令即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sed -r <span class="string">'s/(@@ )|(@@ ?$)//g'</span> result.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将解码结果保存到文件中</span></span><br><span class="line">sed -r <span class="string">'s/(@@ )|(@@ ?$)//g'</span> result.txt &gt; restore.txt</span><br></pre></td></tr></table></figure><p>我们恢复之后的结果是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">beijing , 1 mar ( xinhua ) -- tian fengshan , former heilongjiang governor who is 59 years old , was appointed minister of land and resources today .</span><br><span class="line">tian fengshan , who was born in zhaoyuan county , heilongjiang province , took part in work since july 1961 and joined the cpc in march 1970 .</span><br><span class="line">this should be a natural process set off by economic development ; the &quot; third tier construction &quot; of the 1960s involving factory relocation was something entirely different .</span><br><span class="line">we must also realize however that from the angle of changing the pattern of resource allocation , we have not yet made the big breakthrough in reform .</span><br><span class="line">with regard to joining the world trade organization , one recent reaction has been blind optimism and the belief that china will profit whatever it does .</span><br><span class="line">since these areas where objective conditions are not particularly good can achieve this , other areas where conditions are better can naturally do the same .</span><br><span class="line">the objective trend of globalization is calling for international cooperation on a global scale , and a global cooperation has far exceeded the scope of the economy .</span><br></pre></td></tr></table></figure><h4 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h4><p>可以用命令<code>pip install subword-nmt</code>安装包<code>subword-nmt</code>以后，可以使用如下代码得到BPE的分词结果，以及将BPE的分词方法用到测试语料上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> subword_nmt <span class="keyword">import</span> apply_bpe, learn_bpe</span><br><span class="line"><span class="comment"># 得到分词结果，写到../data/toy_bpe.txt这个文件中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../data/toy_vocab.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> in_file, \</span><br><span class="line">        open(<span class="string">'../data/toy_bpe.txt'</span>, <span class="string">'w+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> out_file:</span><br><span class="line">    <span class="comment"># 得到分词结果，写到../data/toy_bpe.txt这个文件中</span></span><br><span class="line">    <span class="comment"># 1500是最后BPE词表大小，is_dict说明输入文件是个词表文件，格式为"&lt;单词&gt; &lt;次数&gt;"</span></span><br><span class="line">    learn_bpe.learn_bpe(in_file, out_file, <span class="number">1500</span>, verbose=<span class="literal">True</span>, is_dict=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取../data/toy_bpe.txt分词结果，并作用于../data/bpe_test_raw.txt中的文本，最后写到../data/bpe_test_processed.txt文件中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../data/bpe_test_raw.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> in_file, \</span><br><span class="line">        open(<span class="string">'../data/bpe_test_processed.txt'</span>, <span class="string">'w+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> out_file, \</span><br><span class="line">        open(<span class="string">'../data/toy_bpe.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> code_file:</span><br><span class="line">    <span class="comment"># 构造BPE词表</span></span><br><span class="line">    bpe = apply_bpe.BPE(code_file)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> in_file:</span><br><span class="line">        <span class="comment"># 使用BPE分词</span></span><br><span class="line">        out_file.write(bpe.process_line(line))</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>subword可以平衡词汇量和对未知词的覆盖。 极端的情况下，我们只能使用26个token（即字符）来表示所有英语单词。一般情况，建议使用16k或32k子词足以取得良好的效果，Facebook <a href="https://github.com/pytorch/fairseq/tree/main/examples/roberta" target="_blank" rel="noopener">RoBERTa</a>甚至建立的多达50k的词表。</p><h2 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h2><p>补充完毕BPE算法的原理之后，我们开始对该源码进行分析。首先来看模型加载部分。</p><p>模型加载的核心函数为<code>fairseq/hub_utils.py: from_pretrained</code>函数。在该函数的会调用<code>checkpoint_utils.load_model_ensemble_and_task</code>函数。该函数不仅加载了模型权重，而且会初始化task。我们重点关注这个初始化过程。初始化该task时，默认会初始化为<code>translation</code>任务。</p><p>然后跳入函数<code>fairseq/tasks/translation.py</code>中，可以看到在<code>setup_task</code>函数中，会读取<code>model/wmt16.en-de.joined-dict.transformer/dict.en.txt</code>和<code>model/wmt16.en-de.joined-dict.transformer/dict.de.txt</code>文件，然后放到<code>fairseq.tasks.translation.TranslationTask</code>的<code>src_dict</code>和<code>tgt_dict</code>中。另外值得注意的是<code>fairseq.data.dictionary.Dictionary</code>的实例，在实例化该类的时候，会在最前面加上<code>bos=&quot;&lt;s&gt;&quot;, pad=&quot;&lt;pad&gt;&quot;, eos=&quot;&lt;/s&gt;&quot;, unk=&quot;&lt;unk&gt;&quot;</code>，因此虽然这两个txt文件中都有32764行（两个文件内容一模一样），最终都会有32768行，与翻译模型的输出维度一致。</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427221019923.png" alt="image-20220427221019923" style="zoom:50%"></p><p>加载模型并初始化task之后，<code>from_pretrained</code>函数接着实例化了<code>hub_utils.GeneratorHubInterface</code>。我们接着看该类在实例化的时候会做些什么。</p><p>从下图可以看到该初始化函数依次做了：从task中创建<code>src_dict</code>和<code>tgt_dict</code>属性（与<code>fairseq.tasks.translation.TranslationTask</code>的<code>src_dict</code>和<code>tgt_dict</code>一致），然后加载了<code>align_dict、tokenizer、bpe</code>。</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427223531357.png" alt="image-20220427223531357" style="zoom:50%"></p><p>我们这里重点关注一下<code>bpe</code>的初始化过程，单步调试进入到<code>fairseq/registry.py</code>文件后，可以发现fairseq支持的所有bpe有：<code>dict_keys([&#39;bytes&#39;, &#39;gpt2&#39;, &#39;hf_byte_bpe&#39;, &#39;bert&#39;, &#39;characters&#39;, &#39;fastbpe&#39;, &#39;byte_bpe&#39;, &#39;sentencepiece&#39;, &#39;subword_nmt&#39;])</code>。我们这里在初始化模型时传入了<code>bpe=&#39;subword_nmt</code>参数，所以我们重点关注一下<code>subword_nmt</code>的初始化方式。</p><p>该初始化过程的详细过程在<code>fairseq/data/encoders/subword_nmt_bpe.py</code>文件的<code>SubwordNMTBPE</code>类的<code>__init__</code>函数中。从下图中可以看出，该函数会读取<code>args.bpe_codes</code>对应的文件，也就是<code>&#39;./model/wmt16.en-de.joined-dict.transformer/bpecodes&#39;</code>文件，用于实例化<code>subword_nmt.apply_bpe.BPE</code>得到对应<code>self.bpe</code>，同时<code>SubwordNMTBPE</code>类还有对应的<code>encode</code>和<code>decode</code>函数。</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427233419637.png" alt="image-20220427233419637" style="zoom:50%"></p><p>介绍完BPE的初始化，我们接着回到<code>hub_utils.GeneratorHubInterface</code>类中，此时的<code>self.bpe</code>的类型为<code>fairseq.data.encoders.subword_nmt_bpe.SubwordNMTBPE</code>。</p><p>由此，翻译模型的模型加载部分已经介绍完了。</p><h2 id="前向推理"><a href="#前向推理" class="headerlink" title="前向推理"></a>前向推理</h2><p>从上面的调用关系来看，翻译模型进行推理的函数是<code>translate</code>。我们调试进入该函数，发现该函数位于<code>/root/anaconda3/lib/python3.8/site-packages/fairseq/hub_utils.py</code>文件中。核心代码如下：</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427105444788.png" alt="image-20220427105444788" style="zoom:50%"></p><p>可以看到，翻译时需要经过三个关键步骤：<code>encode</code>、<code>generate</code>和<code>decode</code>。这里我们先关注预处理和后处理步骤。关键代码如下：</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427110136773.png" alt="image-20220427110136773" style="zoom:50%"></p><p>可以看出来预处理主要流程为<code>分词-&gt;BPE-&gt;binarize</code>，后处理的主要步骤是<code>string-&gt;去除bpe-&gt;去分词</code>。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>我们接着来看预处理过程。在使用该模型的时候，并没有用到分词，而是直接使用了BPE的方式，所以我们跳过<code>self.tokenize</code>函数，首先来看<code>self.apply_bpe</code>函数。该函数会调用<code>SubwordNMTBPE.encode of &lt;fairseq.data.encoders.subword_nmt_bpe.SubwordNMTBPE&gt;</code>，我们这里先不管这个函数干了啥，先介绍它的输入输出。其输入为：<code>&#39;Hello world!&#39;</code>，输出为<code>&#39;H@@ ello world@@ !&#39;</code>。</p><p>接着我们来看<code>self.binarize</code>，它的输入是<code>&#39;H@@ ello world@@ !&#39;</code>，输出是<code>tensor([ 190, 7016, 29382, 88, 2])</code>。该函数会调用<code>Dictionary.encode_line of &lt;fairseq.data.dictionary.Dictionary&gt;</code>。查询前面的<code>src_dict</code>，将字符串映射到唯一ID上（ID简单理解为<code>model/wmt16.en-de.joined-dict.transformer/dict.en.txt</code>中的行数+4）。</p><h2 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h2><p>介绍完预处理流程，我们来看下网络结构，网络结构如下（因为该网络结构很长，所以只摘出来关键部分）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">GeneratorHubInterface(</span><br><span class="line">  (models): ModuleList(</span><br><span class="line">    (<span class="number">0</span>): TransformerModel(</span><br><span class="line">      (encoder): TransformerEncoder(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (embed_tokens): Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">        (embed_positions): SinusoidalPositionalEmbedding()</span><br><span class="line">        (layers): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): TransformerEncoderLayer(</span><br><span class="line">            (self_attn): MultiheadAttention(</span><br><span class="line">              (dropout_module): FairseqDropout()</span><br><span class="line">              (k_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (v_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (q_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (out_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">            (self_attn_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (dropout_module): FairseqDropout()</span><br><span class="line">            (activation_dropout_module): FairseqDropout()</span><br><span class="line">            (fc1): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (fc2): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (final_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          <span class="comment"># 省略(1~5)TransformerEncoderLayer</span></span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (decoder): TransformerDecoder(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (embed_tokens): Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">        (embed_positions): SinusoidalPositionalEmbedding()</span><br><span class="line">        (layers): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): TransformerDecoderLayer(</span><br><span class="line">            (dropout_module): FairseqDropout()</span><br><span class="line">            (self_attn): MultiheadAttention(</span><br><span class="line">              (dropout_module): FairseqDropout()</span><br><span class="line">              (k_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (v_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (q_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (out_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">            (activation_dropout_module): FairseqDropout()</span><br><span class="line">            (self_attn_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (encoder_attn): MultiheadAttention(</span><br><span class="line">              (dropout_module): FairseqDropout()</span><br><span class="line">              (k_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (v_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (q_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (out_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">            (encoder_attn_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (fc1): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (fc2): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (final_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          <span class="comment"># 省略(1~5)TransformerDecoderLayer</span></span><br><span class="line">        )</span><br><span class="line">        (output_projection): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">32768</span>, bias=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>总结概括一下该结构，如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TransformerEncoder(</span><br><span class="line">    FairseqDropout(), </span><br><span class="line">    Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>), </span><br><span class="line">    SinusoidalPositionalEmbedding(), </span><br><span class="line">    <span class="number">6</span>个TransformerEncoderLayer</span><br><span class="line">)</span><br><span class="line">TransformerDecoder(</span><br><span class="line">    FairseqDropout(), </span><br><span class="line">    Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>), </span><br><span class="line">    SinusoidalPositionalEmbedding(), </span><br><span class="line">    <span class="number">6</span>个TransformerDecoderLayer, </span><br><span class="line">    Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">32768</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>也就是说，在该模型中，使用了<a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener">torch.nn.Embedding</a>层对输入进行了Embedding并学习。</p><p>接着我们看下模型推理部分——<code>generate</code>函数。</p><p>该函数首先会调用<code>FairseqTask.build_generator of &lt;fairseq.tasks.translation.TranslationTask&gt;</code>函数，并传入<code>gen_args</code>参数（该参数中包含了<code>beam</code>）。在该函数会执行<code>search_strategy = search.BeamSearch(self.target_dictionary)</code>函数实例化BeamSearch（使用到了<code>model/wmt16.en-de.joined-dict.transformer/dict.de.txt</code>），并与模型一块放到<code>SequenceGenerator</code>中进行实例化，而实际进行推理时也是调用的<code>SequenceGenerator.generate of SequenceGenerator</code>，同时进行模型推理+BeamSearch过程。</p><p>具体细节我们先不关注，先说下输入输出。其输入为</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220428003141180.png" alt="image-20220428003141180" style="zoom:50%"></p><p>经过推理之后，输出结果为（下面5个结果的tokens是不一样的，这里显示不出来）：</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220428003252483.png" alt="image-20220428003252483" style="zoom:50%"></p><h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>最后，我们来看下后处理流程。后处理的对应的代码是<code>[self.decode(hypos[0][&quot;tokens&quot;]) for hypos in batched_hypos]</code>。也就是将<code>tensor([12006, 165, 488, 88, 2], device=&#39;cuda:0&#39;)</code>输入到<code>self.decode</code>函数中。该函数的主要流程是<code>string-&gt;去除bpe-&gt;去分词</code>。</p><p>我们先来看<code>self.string</code>函数，该函数与<code>self.binarize</code>函数相反，它会调用<code>Dictionary.string of &lt;fairseq.data.dictionary.Dictionary&gt;</code>，查询前面的<code>tgt_dict</code>，将ID映射回字符串（ID简单理解为<code>model/wmt16.en-de.joined-dict.transformer/dict.de.txt</code>中的行数+4）。它的输入为<code>tensor([12006, 165, 488, 88, 2], device=&#39;cuda:0&#39;)</code>，输出为<code>&#39;Hall@@ o Welt !&#39;</code>。</p><p>接着来看<code>self.remove_bpe</code>函数，它与<code>self.apply_bpe</code>函数作用相反，该函数会调用<code>SubwordNMTBPE.decode of &lt;fairseq.data.encoders.subword_nmt_bpe.SubwordNMTBPE&gt;</code>，我们这里先不管这个函数干了啥，先介绍它的输入输出。其输入为：<code>&#39;Hall@@ o Welt !&#39;</code>，输出为<code>&#39;Hallo Welt !&#39;</code>。</p><p>同样的，最后，该过程并没有调用<code>self.detokenize</code>，这里先不管。</p><h2 id="训练数据准备"><a href="#训练数据准备" class="headerlink" title="训练数据准备"></a>训练数据准备</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>下文主要来源于<a href="https://www.cnblogs.com/mmxy/p/14076930.html" target="_blank" rel="noopener">WMT14 en-de翻译数据集预处理步骤</a></p><p>fairseq提供了一份wmt14英德数翻译据集的<a href="https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-wmt14en2de.sh" target="_blank" rel="noopener">预处理脚本</a>，简单结合其代码分析一下其处理步骤：</p><p>1、下载mosesdecoder。mosesdecoder的使用文档在<a href="http://www.statmt.org/moses/?n=Moses.Baseline" target="_blank" rel="noopener">这里</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Moses github repository (for tokenization scripts)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/moses-smt/mosesdecoder.git</span><br></pre></td></tr></table></figure><p>2、下载subword nmt。这个开源库是用于构造bpecodes及其字典的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Subword NMT repository (for BPE pre-processing)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/rsennrich/subword-nmt.git</span><br></pre></td></tr></table></figure><p>3、</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">SCRIPTS</span>=mosesdecoder/scripts      <span class="comment"># 定义SCRIPTS变量，指向mosesdecoder的脚本文件夹</span></span><br><span class="line"><span class="attr">TOKENIZER</span>=<span class="variable">$SCRIPTS</span>/tokenizer/tokenizer.perl      <span class="comment"># 定义TOKENIZER变量，指向mosesdecoder的tokenizer.perl, 用来分词</span></span><br><span class="line"><span class="attr">CLEAN</span>=<span class="variable">$SCRIPTS</span>/training/clean-corpus-n.perl      <span class="comment"># 定义CLEAN变量，指向mosesdecoder的clean-corpus-n.perl，clean的主要作用是保留指定长度的数据</span></span><br><span class="line"><span class="attr">NORM_PUNC</span>=<span class="variable">$SCRIPTS</span>/tokenizer/normalize-punctuation.perl      <span class="comment"># 定义NORM_PUNC变量，指向normalize-punctuation.perl,用来将标点符号规范化</span></span><br><span class="line"><span class="attr">REM_NON_PRINT_CHAR</span>=<span class="variable">$SCRIPTS</span>/tokenizer/remove-non-printing-char.perl      <span class="comment"># 定义REM_NON_PRINT_CHAR变量，指向remove-non-printing-char.perl,去除语料中的非打印字符 </span></span><br><span class="line"><span class="attr">BPEROOT</span>=subword-nmt/subword_nmt      <span class="comment"># 定义BPEROOT变量，指向subword_nmt根目录。</span></span><br><span class="line"><span class="attr">BPE_TOKENS</span>=<span class="number">40000</span>      <span class="comment"># 指定BPE TOKENS的数量为40000</span></span><br></pre></td></tr></table></figure><p>4、</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定语料来源，其中包括了训练、验证、测试语料</span></span><br><span class="line">URLS=(</span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/dev.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt14/test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 指定文件名，和上面URLS对应</span></span><br><span class="line">FILES=(</span><br><span class="line">    <span class="string">"training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"dev.tgz"</span></span><br><span class="line">    <span class="string">"test-full.tgz"</span>      <span class="comment"># 只要test-full是测试集，上面四个都是训练+验证集。</span></span><br><span class="line">)</span><br><span class="line">CORPORA=(</span><br><span class="line">    <span class="string">"training/europarl-v7.de-en"</span></span><br><span class="line">    <span class="string">"commoncrawl.de-en"</span></span><br><span class="line">    <span class="string">"training/news-commentary-v12.de-en"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>5、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1705.03122</span></span><br><span class="line"><span class="comment"># 如果指定参数--icml17，就将语料2替换成wmt14的语料，而不是使用wmt17的语料，这是为了和ConvS2S论文保持一致</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> == <span class="string">"--icml17"</span> ]; <span class="keyword">then</span></span><br><span class="line">    URLS[2]=<span class="string">"http://statmt.org/wmt14/training-parallel-nc-v9.tgz"</span></span><br><span class="line">    FILES[2]=<span class="string">"training-parallel-nc-v9.tgz"</span></span><br><span class="line">    CORPORA[2]=<span class="string">"training/news-commentary-v9.de-en"</span></span><br><span class="line">    OUTDIR=wmt14_en_de      <span class="comment"># 指定输出文件夹名</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    OUTDIR=wmt17_en_de</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>6、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">src=en      <span class="comment"># 源语言为英文</span></span><br><span class="line">tgt=de      <span class="comment"># 目标语言是德语</span></span><br><span class="line">lang=en-de      <span class="comment"># 语言对为英德</span></span><br><span class="line">prep=<span class="variable">$OUTDIR</span>      <span class="comment"># 文件夹前缀为$OUTDIR</span></span><br><span class="line">tmp=<span class="variable">$prep</span>/tmp      <span class="comment"># 文件夹$OUTDIR内有一个tmp文件夹</span></span><br><span class="line">orig=orig      <span class="comment"># orig=orig</span></span><br><span class="line">dev=dev/newstest2013      <span class="comment"># 开发集使用newstest2013</span></span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$orig</span> <span class="variable">$tmp</span> <span class="variable">$prep</span>      <span class="comment"># 递归创建上面定义的文件夹，包括orig文件夹，$OUTDIR/tmp文件夹，$OUTDIR文件夹</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$orig</span>      <span class="comment"># 切换到orig文件夹中</span></span><br></pre></td></tr></table></figure><p>7、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$&#123;#URLS[@]&#125;</span>;++i)); <span class="keyword">do</span>      <span class="comment"># 迭代每一个URLS</span></span><br><span class="line">    file=<span class="variable">$&#123;FILES[i]&#125;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span> already exists, skipping download"</span>      <span class="comment"># 如果文件之前已经下载下来了，就跳过</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        url=<span class="variable">$&#123;URLS[i]&#125;</span>      </span><br><span class="line">        wget <span class="string">"<span class="variable">$url</span>"</span>      <span class="comment"># 否则下载</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span>      </span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> successfully  downloaded."</span>       <span class="comment"># 下载完文件存在表示下载成功</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> not successfully downloaded."</span>  <span class="comment"># 查无此人，下载失败</span></span><br><span class="line">            <span class="built_in">exit</span> -1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tgz"</span> ]; <span class="keyword">then</span>      <span class="comment"># 对于.tgz格式的文件，用zxvf命令解压</span></span><br><span class="line">            tar zxvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">elif</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tar"</span> ]; <span class="keyword">then</span>      <span class="comment"># 对于.tar格式的文件，用xvf命令解压</span></span><br><span class="line">            tar xvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure><p>执行完毕之后，<code>$OUTDIR</code>文件夹存放的内容有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br></pre></td></tr></table></figure><p>8、<strong>重点来了</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing train data..."</span>      <span class="comment"># 预处理训练语料</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    rm <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span>      <span class="comment"># 如果存在，先移除</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;CORPORA[@]&#125;</span>"</span>; <span class="keyword">do</span>      </span><br><span class="line">        cat <span class="variable">$orig</span>/<span class="variable">$f</span>.<span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$NORM_PUNC</span> <span class="variable">$l</span> | \      <span class="comment"># 先标准化符号</span></span><br><span class="line">            perl <span class="variable">$REM_NON_PRINT_CHAR</span> | \      <span class="comment"># 移除非打印字符</span></span><br><span class="line">            perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt;&gt; <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span>  <span class="comment"># 分词</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing test data..."</span>      <span class="comment"># 预处理测试语料</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$l</span>"</span> == <span class="string">"<span class="variable">$src</span>"</span> ]; <span class="keyword">then</span>      </span><br><span class="line">        t=<span class="string">"src"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        t=<span class="string">"ref"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    grep <span class="string">'&lt;seg id'</span> <span class="variable">$orig</span>/<span class="built_in">test</span>-full/newstest2014-deen-<span class="variable">$t</span>.<span class="variable">$l</span>.sgm | \      <span class="comment">#这一块操作没看懂</span></span><br><span class="line">        sed -e <span class="string">'s/&lt;seg id="[0-9]*"&gt;\s*//g'</span> | \      </span><br><span class="line">        sed -e <span class="string">'s/\s*&lt;\/seg&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">"s/\’/\'/g"</span> | \</span><br><span class="line">    perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/<span class="built_in">test</span>.<span class="variable">$l</span>      <span class="comment"># 分词</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── orig  <span class="comment"># 原始数据集的tgz文件+解压之后的结果</span></span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── <span class="built_in">test</span>-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br><span class="line">    ├── train.tags.en-de.tok.de</span><br><span class="line">    └── train.tags.en-de.tok.en</span><br></pre></td></tr></table></figure><p>预处理完毕之后，<code>test.en</code>的其中一条语句为<code>They are not even 100 metres apart : On Tuesday , the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights .</code>。可以看出来，标点符号已经和字母分开了。</p><p>9、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"splitting train and valid..."</span>      <span class="comment"># 划分训练集和验证集</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 == 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/valid.<span class="variable">$l</span>      <span class="comment"># 从训练集中，每100个句子抽1个句子作为验证集</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 != 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/train.<span class="variable">$l</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── orig  <span class="comment"># 原始数据集的tgz文件+解压之后的结果</span></span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── <span class="built_in">test</span>-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br><span class="line">    ├── train.de</span><br><span class="line">    ├── train.en</span><br><span class="line">    ├── train.tags.en-de.tok.de</span><br><span class="line">    ├── train.tags.en-de.tok.en</span><br><span class="line">    ├── valid.de</span><br><span class="line">    └── valid.en</span><br></pre></td></tr></table></figure><p>10、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">TRAIN=<span class="variable">$tmp</span>/train.de-en      <span class="comment"># 训练语料（包含src和tgt)</span></span><br><span class="line">BPE_CODE=<span class="variable">$prep</span>/code      <span class="comment"># BPECODE文件</span></span><br><span class="line">rm -f <span class="variable">$TRAIN</span>      <span class="comment"># train.de-en如果存在就删掉</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span>      </span><br><span class="line">    cat <span class="variable">$tmp</span>/train.<span class="variable">$l</span> &gt;&gt; <span class="variable">$TRAIN</span>  <span class="comment"># 其实就是简单地将src语料和tgt语料按顺序放到一个文件中，方便后面联合学习bpe</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"learn_bpe.py on <span class="variable">$&#123;TRAIN&#125;</span>..."</span>      <span class="comment"># 学习BPE</span></span><br><span class="line">python <span class="variable">$BPEROOT</span>/learn_bpe.py -s <span class="variable">$BPE_TOKENS</span> &lt; <span class="variable">$TRAIN</span> &gt; <span class="variable">$BPE_CODE</span>       <span class="comment"># 这里是将源语言和目标语言的语料联合起来学BPE的，因为我们用的是train.de-en</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> train.<span class="variable">$L</span> valid.<span class="variable">$L</span> <span class="built_in">test</span>.<span class="variable">$L</span>; <span class="keyword">do</span>      <span class="comment"># 用学到的bpecode应用到三份语料中（训练语料，验证语料，测试语料）</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"apply_bpe.py to <span class="variable">$&#123;f&#125;</span>..."</span></span><br><span class="line">        python <span class="variable">$BPEROOT</span>/apply_bpe.py -c <span class="variable">$BPE_CODE</span> &lt; <span class="variable">$tmp</span>/<span class="variable">$f</span> &gt; <span class="variable">$tmp</span>/bpe.<span class="variable">$f</span>      <span class="comment"># 输出到tmp中对应的文件，以bpe.作为前缀</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>在执行<code>learn_bpe.py</code>的时候，刚开始的速度特别慢，但是速度会越来越快，最终得到<code>code</code>文件。</p><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── code</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">    ├── bpe.test.de</span><br><span class="line">    ├── bpe.test.en</span><br><span class="line">    ├── bpe.train.de</span><br><span class="line">    ├── bpe.train.en</span><br><span class="line">    ├── bpe.valid.de</span><br><span class="line">    ├── bpe.valid.en</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br><span class="line">    ├── train.de</span><br><span class="line">    ├── train.de-en</span><br><span class="line">    ├── train.en</span><br><span class="line">    ├── train.tags.en-de.tok.de</span><br><span class="line">    ├── train.tags.en-de.tok.en</span><br><span class="line">    ├── valid.de</span><br><span class="line">    └── valid.en</span><br></pre></td></tr></table></figure><p>11、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250      <span class="comment"># 按照长度对训练语料和验证语料进行clean，只保留前250个token（cutoff 1-250），并将结果输出到output文件夹中</span></span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br></pre></td></tr></table></figure><p>中间输出结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhaodali@ubuntua:wmt14$ perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250</span><br><span class="line">clean-corpus.perl: processing /mnt/private_zhaodali_cq/datasets/security/wmt14/tmp/bpe.train.en &amp; .de to /mnt/private_zhaodali_cq/datasets/security/wmt14/train, cutoff 1-250, ratio 1.5</span><br><span class="line">..........(100000)..........(200000)..........(300000)..........(400000)..........(500000)..........(600000)..........(700000)..........(800000)..........(900000)..........(1000000)..........(1100000)..........(1200000)..........(1300000)..........(1400000)..........(1500000)..........(1600000)..........(1700000)..........(1800000)..........(1900000)..........(2000000)..........(2100000)..........(2200000)..........(2300000)..........(2400000)..........(2500000)..........(2600000)..........(2700000)..........(2800000)..........(2900000)..........(3000000)..........(3100000)..........(3200000)..........(3300000)..........(3400000)..........(3500000)..........(3600000)..........(3700000)..........(3800000)..........(3900000)..........(4000000)..........(4100000)..........(4200000)..........(4300000)..........(4400000)..........(4500000)....</span><br><span class="line">Input sentences: 4544200  Output sentences:  3961179</span><br><span class="line">zhaodali@ubuntua:wmt14$ perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br><span class="line">clean-corpus.perl: processing /mnt/private_zhaodali_cq/datasets/security/wmt14/tmp/bpe.valid.en &amp; .de to /mnt/private_zhaodali_cq/datasets/security/wmt14/valid, cutoff 1-250, ratio 1.5</span><br><span class="line">....</span><br><span class="line">Input sentences: 45901  Output sentences:  40058</span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── code</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">│   ├── bpe.test.de</span><br><span class="line">│   ├── bpe.test.en</span><br><span class="line">│   ├── bpe.train.de</span><br><span class="line">│   ├── bpe.train.en</span><br><span class="line">│   ├── bpe.valid.de</span><br><span class="line">│   ├── bpe.valid.en</span><br><span class="line">│   ├── test.de</span><br><span class="line">│   ├── test.en</span><br><span class="line">│   ├── train.de</span><br><span class="line">│   ├── train.de-en</span><br><span class="line">│   ├── train.en</span><br><span class="line">│   ├── train.tags.en-de.tok.de</span><br><span class="line">│   ├── train.tags.en-de.tok.en</span><br><span class="line">│   ├── valid.de</span><br><span class="line">│   └── valid.en</span><br><span class="line">├── train.de</span><br><span class="line">├── train.en</span><br><span class="line">├── valid.de</span><br><span class="line">└── valid.en</span><br></pre></td></tr></table></figure><p>12、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cp <span class="variable">$tmp</span>/bpe.test.<span class="variable">$L</span> <span class="variable">$prep</span>/<span class="built_in">test</span>.<span class="variable">$L</span>      <span class="comment"># 对于test语料，不进行clean，直接放到output文件夹。</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── code</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">│   ├── bpe.test.de  # bpe之后的结果</span><br><span class="line">│   ├── bpe.test.en</span><br><span class="line">│   ├── bpe.train.de</span><br><span class="line">│   ├── bpe.train.en</span><br><span class="line">│   ├── bpe.valid.de</span><br><span class="line">│   ├── bpe.valid.en</span><br><span class="line">│   ├── test.de</span><br><span class="line">│   ├── test.en</span><br><span class="line">│   ├── train.de  # 训练集与验证划分之后的结果</span><br><span class="line">│   ├── train.de-en</span><br><span class="line">│   ├── train.en</span><br><span class="line">│   ├── train.tags.en-de.tok.de  # 训练集与验证集</span><br><span class="line">│   ├── train.tags.en-de.tok.en</span><br><span class="line">│   ├── valid.de</span><br><span class="line">│   └── valid.en</span><br><span class="line">├── train.de  # clean之后的结果</span><br><span class="line">├── train.en</span><br><span class="line">├── valid.de</span><br><span class="line">└── valid.en</span><br><span class="line">├── test.de</span><br><span class="line">├── test.en</span><br></pre></td></tr></table></figure><h3 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h3><p>执行完上述指令之后，我们需要继续将数据Binarize。并且统计词频，得到vocabulary文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../</span><br><span class="line">TEXT=./wmt17_en_de  <span class="comment"># 放置前面小节文件的根目录</span></span><br><span class="line">fairseq-preprocess \</span><br><span class="line">    --<span class="built_in">source</span>-lang en --target-lang de \</span><br><span class="line">    --trainpref <span class="variable">$TEXT</span>/train --validpref <span class="variable">$TEXT</span>/valid --testpref <span class="variable">$TEXT</span>/<span class="built_in">test</span> \</span><br><span class="line">    --destdir data-bin/wmt17_en_de --thresholdtgt 0 --thresholdsrc 0 \</span><br><span class="line">    --workers 20</span><br></pre></td></tr></table></figure><p>中间输出为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2022-04-29 19:38:39 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion=<span class="string">'cross_entropy'</span>, dataset_impl=<span class="string">'mmap'</span>, destdir=<span class="string">'data-bin/wmt17_en_de'</span>, dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler=<span class="string">'fixed'</span>, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path=<span class="string">'/tmp/plasma'</span>, profile=False, quantization_config_path=None, reset_logging=False, scoring=<span class="string">'bleu'</span>, seed=1, simul_type=None, source_lang=<span class="string">'en'</span>, srcdict=None, suppress_crashes=False, target_lang=<span class="string">'de'</span>, task=<span class="string">'translation'</span>, tensorboard_logdir=None, testpref=<span class="string">'wmt14//test'</span>, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=<span class="string">'wmt14//train'</span>, use_plasma_view=False, user_dir=None, validpref=<span class="string">'wmt14//valid'</span>, wandb_project=None, workers=20)</span><br><span class="line">2022-04-29 19:38:57 | INFO | fairseq_cli.preprocess | [en] Dictionary: 40360 types</span><br><span class="line">2022-04-29 19:39:39 | INFO | fairseq_cli.preprocess | [en] wmt14//train.en: 3961179 sents, 116600288 tokens, 0.0% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:39:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 40360 types</span><br><span class="line">2022-04-29 19:39:42 | INFO | fairseq_cli.preprocess | [en] wmt14//valid.en: 40058 sents, 1180285 tokens, 0.00322% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:39:42 | INFO | fairseq_cli.preprocess | [en] Dictionary: 40360 types</span><br><span class="line">2022-04-29 19:39:44 | INFO | fairseq_cli.preprocess | [en] wmt14//test.en: 3003 sents, 81185 tokens, 0.00246% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:39:44 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42720 types</span><br><span class="line">2022-04-29 19:40:26 | INFO | fairseq_cli.preprocess | [de] wmt14//train.de: 3961179 sents, 119369232 tokens, 0.0% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:40:26 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42720 types</span><br><span class="line">2022-04-29 19:40:31 | INFO | fairseq_cli.preprocess | [de] wmt14//valid.de: 40058 sents, 1209744 tokens, 0.00116% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:40:31 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42720 types</span><br><span class="line">2022-04-29 19:40:32 | INFO | fairseq_cli.preprocess | [de] wmt14//test.de: 3003 sents, 84629 tokens, 0.907% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:40:32 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt17_en_de</span><br></pre></td></tr></table></figure><p>执行完毕之后，可以得到的文件结构如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data-bin</span><br><span class="line">│   ├── preprocess.log</span><br><span class="line">│   └── wmt17_en_de</span><br><span class="line">│       ├── dict.de.txt  <span class="comment"># vocabulary文件</span></span><br><span class="line">│       ├── dict.en.txt</span><br><span class="line">│       ├── preprocess.log</span><br><span class="line">│       ├── test.en-de.de.bin</span><br><span class="line">│       ├── test.en-de.de.idx</span><br><span class="line">│       ├── test.en-de.en.bin</span><br><span class="line">│       ├── test.en-de.en.idx</span><br><span class="line">│       ├── train.en-de.de.bin</span><br><span class="line">│       ├── train.en-de.de.idx</span><br><span class="line">│       ├── train.en-de.en.bin</span><br><span class="line">│       ├── train.en-de.en.idx</span><br><span class="line">│       ├── valid.en-de.de.bin</span><br><span class="line">│       ├── valid.en-de.de.idx</span><br><span class="line">│       ├── valid.en-de.en.bin</span><br><span class="line">│       └── valid.en-de.en.idx</span><br><span class="line">└── wmt17_en_de</span><br><span class="line">    ├── code</span><br><span class="line">    ├── orig  <span class="comment"># 原始数据集的tgz文件+解压之后的结果</span></span><br><span class="line">    │   ├── dev</span><br><span class="line">    │   ├── <span class="built_in">test</span>-full</span><br><span class="line">    │   ├── training</span><br><span class="line">    ├── tmp</span><br><span class="line">    │   ├── bpe.test.de</span><br><span class="line">    │   ├── bpe.test.en</span><br><span class="line">    │   ├── bpe.train.de</span><br><span class="line">    │   ├── bpe.train.en</span><br><span class="line">    │   ├── bpe.valid.de</span><br><span class="line">    │   ├── bpe.valid.en</span><br><span class="line">    │   ├── test.de</span><br><span class="line">    │   ├── test.en</span><br><span class="line">    │   ├── train.de</span><br><span class="line">    │   ├── train.de-en</span><br><span class="line">    │   ├── train.en</span><br><span class="line">    │   ├── train.tags.en-de.tok.de</span><br><span class="line">    │   ├── train.tags.en-de.tok.en</span><br><span class="line">    │   ├── valid.de</span><br><span class="line">    │   └── valid.en</span><br><span class="line">    ├── train.de</span><br><span class="line">    ├── train.en</span><br><span class="line">    ├── valid.de</span><br><span class="line">    └── valid.en</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br></pre></td></tr></table></figure><p>得到的<code>data-bin</code>文件夹就是我们处理完之后的结果，可以直接用来训练和测试。因为它其中的文件已经使用bpe编码了，所以不需要<code>code</code>文件，但是仍然需要<code>dict.de.txt</code>和<code>dict.en.txt</code>用于字符与ID之间的转换。</p><p>若直接测试句子的话，仍然需要<code>code</code>文件对该句子进行编码，然后需要<code>dict.de.txt</code>和<code>dict.en.txt</code>用于字符与ID之间的转换。</p><p>另外需要注意的是，因为<code>joined_dictionary=False</code>，所以<code>dict.de.txt</code>与<code>dict.en.txt</code>文件内容是不一样的。</p><blockquote><p>joined_dictionary：源端和目标端使用同一个词表，对于相似语言（如英语和西班牙语）来说，有很多的单词是相同的，使用同一个词表可以降低词表和参数的总规模。</p></blockquote><p>所以<a href="https://github.com/pytorch/fairseq/tree/main/examples/translation#iwslt14-german-to-english-transformer" target="_blank" rel="noopener">官方教程</a>在训练时用的<code>--share-decoder-input-output-embed</code>参数。而我看另外一个<code>dict.de.txt</code>与<code>dict.en.txt</code>文件内容一致的，训练时用了<code>--share-all-embeddings</code>参数。</p><blockquote><p>可以看<a href="https://github.com/pytorch/fairseq/issues/2537#issuecomment-683989567" target="_blank" rel="noopener">这里</a>： when you specify —share-all-embeddings then the embedding matrices for encoder input, decoder input and decoder output are all shared. when you specify —share-decoder-input-output-embed, then the matrices for decoder input and output are shared, but encoder has its own embeddings.</p></blockquote><p>补充一下，当<code>--share-decoder-input-output-embed</code>时，实际对应的代码如下（<code>fairseq/models/transformer/transformer_decoder.py</code>文件中的<code>build_output_projection</code>函数）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> self.share_input_output_embed:</span><br><span class="line">    self.output_projection = nn.Linear(</span><br><span class="line">        self.embed_tokens.weight.shape[<span class="number">1</span>],</span><br><span class="line">        self.embed_tokens.weight.shape[<span class="number">0</span>],</span><br><span class="line">        bias=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    self.output_projection.weight = self.embed_tokens.weight  <span class="comment"># torch.Size([37056, 512])</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://txshi-mt.com/2019/02/28/NMT-Tutorial-3e2-subword/" target="_blank" rel="noopener">NMT Tutorial 3扩展e第2部分. Subword</a><br><a href="https://zhuanlan.zhihu.com/p/86965595" target="_blank" rel="noopener">深入理解NLP Subword算法：BPE、WordPiece、ULM</a><br><a href="https://blog.csdn.net/weixin_38937984/article/details/103995209" target="_blank" rel="noopener">moses(mosesdecoder)数据预处理&amp;BPE分词&amp;moses用法总结</a><br><a href="https://blog.csdn.net/jmh1996/article/details/89286898" target="_blank" rel="noopener">机器翻译 bpe——bytes-pair-encoding以及开源项目subword-nmt快速入门</a><br><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" target="_blank" rel="noopener">Byte Pair Encoding</a><br><a href="https://www.dengbocong.cn/Deep-Learning/38fa61dd1a7b/" target="_blank" rel="noopener">有必要了解的Subword算法模型</a><br><a href="http://www.noobyard.com/article/p-kcottpoh-uv.html" target="_blank" rel="noopener">bpe分词算法的原理</a><br><a href="https://juejin.cn/post/7088322473640329230" target="_blank" rel="noopener">BPE 算法原理及使用指南【深入浅出】</a><br><a href="https://wmathor.com/index.php/archives/1517/" target="_blank" rel="noopener">BPE 算法详解</a><br><a href="https://www.cnblogs.com/mmxy/p/14076930.html" target="_blank" rel="noopener">WMT14 en-de翻译数据集预处理步骤</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/" title="fairseq翻译任务解读">https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a><a href="/tags/fairseq/" rel="tag"><i class="fa fa-tag"></i> fairseq</a><a href="/tags/translation/" rel="tag"><i class="fa fa-tag"></i> translation</a></div><div class="post-nav"><div class="post-nav-item"><a href="/DeepLearningApplications/自然语言处理/NLP基本概念/" rel="prev" title="NLP基本概念"><i class="fa fa-chevron-left"></i> NLP基本概念</a></div><div class="post-nav-item"> <a href="/DeepLearningApplications/自然语言处理/Transformer实战/" rel="next" title="Transformer">Transformer<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#从实战开始"><span class="nav-number">1.</span> <span class="nav-text">从实战开始</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BPE"><span class="nav-number">2.</span> <span class="nav-text">BPE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前言"><span class="nav-number">2.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原理"><span class="nav-number">2.2.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法"><span class="nav-number">2.3.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编码和解码"><span class="nav-number">2.4.</span> <span class="nav-text">编码和解码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#编码"><span class="nav-number">2.4.1.</span> <span class="nav-text">编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解码"><span class="nav-number">2.4.2.</span> <span class="nav-text">解码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#subword-nmt"><span class="nav-number">2.5.</span> <span class="nav-text">subword-nmt</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#命令行接口"><span class="nav-number">2.5.1.</span> <span class="nav-text">命令行接口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Python接口"><span class="nav-number">2.5.2.</span> <span class="nav-text">Python接口</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">2.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型加载"><span class="nav-number">3.</span> <span class="nav-text">模型加载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向推理"><span class="nav-number">4.</span> <span class="nav-text">前向推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预处理"><span class="nav-number">5.</span> <span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型推理"><span class="nav-number">6.</span> <span class="nav-text">模型推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后处理"><span class="nav-number">7.</span> <span class="nav-text">后处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练数据准备"><span class="nav-number">8.</span> <span class="nav-text">训练数据准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据预处理"><span class="nav-number">8.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二值化"><span class="nav-number">8.2.</span> <span class="nav-text">二值化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">9.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">324</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">56</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">381</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">34:13</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '5f6afdc5f29334cba088c1e981229612',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>