<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="这篇论文，讲道理我个人不是很喜欢，因为实验感觉也不是很充分，理论上感觉创新性也没那么高，只是提供了一个强有力的baseline，连作者本人也说大家对这篇文章的关注度过高了。不过，既然能引起大家广泛关注，还是有吸引人的地方的，那么对这篇论文进行介绍。 创新点 提出了一个feature map groups之间的attention机制，即Split-Attention block。通过像ResNet那"><meta name="keywords" content="图像分类,ResNeSt"><meta property="og:type" content="article"><meta property="og:title" content="ResNeSt：Split-Attention Networks"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="这篇论文，讲道理我个人不是很喜欢，因为实验感觉也不是很充分，理论上感觉创新性也没那么高，只是提供了一个强有力的baseline，连作者本人也说大家对这篇文章的关注度过高了。不过，既然能引起大家广泛关注，还是有吸引人的地方的，那么对这篇论文进行介绍。 创新点 提出了一个feature map groups之间的attention机制，即Split-Attention block。通过像ResNet那"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511193928118.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511201930903.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511202102954.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511202345248.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511210935468.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511221128425.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511220906788.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511234812531.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511235853118.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200512000458103.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200512000427899.png"><meta property="og:updated_time" content="2020-05-11T09:53:11.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="ResNeSt：Split-Attention Networks"><meta name="twitter:description" content="这篇论文，讲道理我个人不是很喜欢，因为实验感觉也不是很充分，理论上感觉创新性也没那么高，只是提供了一个强有力的baseline，连作者本人也说大家对这篇文章的关注度过高了。不过，既然能引起大家广泛关注，还是有吸引人的地方的，那么对这篇论文进行介绍。 创新点 提出了一个feature map groups之间的attention机制，即Split-Attention block。通过像ResNet那"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention%20Networks/image-20200511193928118.png"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>ResNeSt：Split-Attention Networks | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> ResNeSt：Split-Attention Networks<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-05-11 17:53:11" itemprop="dateCreated datePublished" datetime="2020-05-11T17:53:11+08:00">2020-05-11</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/图像分类/" itemprop="url" rel="index"><span itemprop="name">图像分类</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>6.7k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>6 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>这篇论文，讲道理我个人不是很喜欢，因为实验感觉也不是很充分，理论上感觉创新性也没那么高，只是提供了一个强有力的baseline，连作者本人也说大家对这篇文章的关注度过高了。不过，既然能引起大家广泛关注，还是有吸引人的地方的，那么对这篇论文进行介绍。</p><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ol><li>提出了一个feature map groups之间的attention机制，即Split-Attention block。通过像ResNet那样堆叠这些Split-Attention blocks，可以得到一个新的ResNet变体——ResNeSt。该模型保留了ResNet的整体结构，能够被应用到下游任务，且没有引入额外的计算量。</li><li>ResNeSt模型比其他ResNet变体表现更好。如ResNeSt-50在图像分类任务上比最好的ResNet变体性能提高了1%。这种分类性能上的提升也带动了下游任务精度的提升。</li></ol><h2 id="创新点来源"><a href="#创新点来源" class="headerlink" title="创新点来源"></a>创新点来源</h2><p>虽然图像分类技术方面已经取得了巨大的进步，但是延伸出来的其他应用如目标检测、图像分割仍然使用ResNet及其变体作为backbone，因为这些结构简单且模块化。</p><ul><li>使用NAS搜索出的模型，虽然分类性能更强，但是并没有对训练性能、推理性能以及内存占用进行优化，限制了将这些模型应用到下游任务上。</li><li>ResNet模型只是为图像分类任务设计的，可能并不适合不同的下游任务。因为其有限的感受野并且缺少通道间的交互。因此在实际应用时，需要修改ResNet以适配特定任务。</li></ul><p>然而这些更改通常只适用于特定的任务，那么就有一个问题：能够创建一个具有通用改进的backbone，从而同时提高各种下游任务的性能。Cross channel information被证明在下游任务很重要，那么创建一个有cross-channel information的backbone是值得期待的。</p><h2 id="Split-Attention-Networks"><a href="#Split-Attention-Networks" class="headerlink" title="Split-Attention Networks"></a>Split-Attention Networks</h2><p>Split-Attention block是一个计算单元，主要包含feature-map group和split attention操作。图1(右)给出了split-attention block的总体框图。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511193928118.png" alt="image-20200511193928118" style="zoom:80%"></p><p><strong>feature-map group.</strong> 正如ResNeXt blocks那样，feature能够被划分为几组，feature-map groups的数量由一个cardinality hyperparameter $K$给出，将产生的feature-map groups称为<strong>cardinal groups</strong>。然后作者引入了一个新的radix hyperparameter $R$将每个cardinal group进一步划分为$R$个splits，所以得到的feature groups总数为$G=KR$。对每一个独立group都进行一个变换$\cal F_i$，每组的中间量就可以用$U_i = {\cal F}_i (X),i \in {1,2,\cdots G}$表示。</p><p><strong>Split Attention in Cardinal Groups.</strong> 首先，每一个cardinal group的<strong>融合特征</strong>可以通过将不同splits之间对应元素相加得到，如第$k$个cardinal group的融合特征用公式表示为$\hat U^k = \sum_{j=R(k-1)+1}^{RK} U_j$（对应于图2中第二行的加号符号），其中$U_j \in {\mathbb R}^{H \times W \times C/K}$（见图1右Split Attention模块上面，每一个Split的输出维度）；$\hat U^k \in {\mathbb R}^{H \times W \times C/K},k\in 1,2,\cdots K$；$H,W,C$是block的输出特征图。接着，通过<strong>全局平均池化</strong>操作获得每个通道的统计信息，作为全局上下文信息$s^k \in {\mathbb R}^{C/K}$（对应于图2中第3行的Global pooling），其第$c$个元素（标量）通过下面式子计算得到：</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511201930903.png" alt="image-20200511201930903" style="zoom:80%"></p><p>这些全局上下文信息用于融合同一cardinal group中的所有splits（乘法操作对应于图2中第7行的乘号操作和加法操作对应于第8行的加号操作）。对于第$k$个cardinal group的最终特征图，其第$c$个通道就是将不同split的第$c$个通道按照特定权重加权求和，具体计算方式如下：</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511202102954.png" alt="image-20200511202102954" style="zoom:80%"></p><p>其中，$k$表示第$k$个cardinal group，$i$表示第$i$个split，$c$表示第$c$个通道。$U_{R(k-1)+i}$表示第$k$个cardinal group的第$i$个split，维度为${\mathbb R}^{H \times W \times C/K}$。$a_i^k(c)$表示第$k$个cardinal group，第$i$个划分中第$c$个channel的权重，为一个标量，可以由下面式子计算得到（对应于图2第6行中的$r-$Softmax操作）：</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511202345248.png" alt="image-20200511202345248" style="zoom:80%"></p><p>基于全局上下文信息$s^k$，映射函数$\cal G_i^c$决定了每个split中的第$c$个通道对最终第$k$个cardinal group产生的特征图的第$c$个通道特征的贡献值。如果splits数目$R=1$，则使用sigmoid函数完成归一化；若$R&gt;1$，则使用softmax函数完成归一化。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511210935468.png" alt="image-20200511210935468" style="zoom:80%"></p><p><strong>ResNeSt Block.</strong> 沿着channel维度将$K$个cardinal group的特征进行拼接：$V=Concat\{V^1,V^2, \cdots V^K\}$（对应于图1右最下面的Concatenate操作）。正如在标准残差blocks那样，若输入和输出特征图大小相同，Split-Attention block在产生输出$Y$时也采用了一个跳转连接：$Y=V+X$；若blocks有步长，则在产生$Y$时先对$X$采用一个合适的变换$\cal T$：$Y=V+{\cal T}(X)$，例如$\cal T$可以是带有步长的卷积或者带有池化的卷积。</p><p><strong>Instantiation, Acceleration, and Computational Costs.</strong> 图1右为Split-Attention block的一个实例，其中上面介绍的${\cal F}_i$由一个$1 \times 1$和$3 \times 3$卷积组成。$\cal G$由两层全连接加ReLU激活函数组成。Split-Attention block的参数总数和FLOPs和有相同cardinal和channels数的标准残差模块粗略相等。</p><p>感觉上面这个操作还是挺复杂的，公式符号很多，这里总结一下：</p><ol><li>首先，将输入特征图划分为$K$个cardinal group，然后将每一个cardinal group划分为$R$个split，这样就将输入特征图实际划分为了$KR$组。每一个split的输入维度为${\mathbb R}^{H \times W \times C/K/R}$，然后经过一个$1\times1$和$3 \times 3$卷积，每个split的输出维度为${\mathbb R}^{H \times W \times C/K}$。</li><li>将第$k$个cardinal group内所有split的输出按元素相加起来，得到融合后的特征$\hat U^k \in {\mathbb R}^{H \times W \times C/K},k\in 1,2,\cdots K$。</li><li>将融合后的特征$\hat U^k$经过一个全局平均池化层得到全局上下文信息$s^k \in {\mathbb R}^{C/K}$。</li><li>将全局上下文信息$s^k$经过映射函数${\cal G}_i^c$得到第$k$个cardinal group内第$i$ split第$c$个通道的权重。</li><li>将第$k$个cardinal group内所有split的权重<strong>按通道进行归一化</strong>。然后使用归一化的权值将所有split<strong>按照通道进行加权求和</strong>，即可得到最终第$k$个cardinal group经过Split Attention后的特征图。</li><li>将$k$个cardinal group经过Split Attention后的特征图进行拼接，并加上残差结构就完成了Split-Attention block。</li></ol><p><strong>Relation to Existing Attention Methods.</strong> 在SE-Net中，使用了一个全局上下文信息预测通道之间的权重，但是这是建立在整个block上面的；在SK-Net中，使用了。TODO</p><h2 id="Network-and-Training"><a href="#Network-and-Training" class="headerlink" title="Network and Training"></a>Network and Training</h2><p>对于网络部分，主要有以下改进：</p><ol><li>对于dense prediction tasks如目标检测或分割，保留空间信息特别重要，因此通常使用带步长的$3 \times 3$去卷积去代替$1 \times 1$的卷积。卷积层需要使用补零操作来处理特征图边界，这在转移到其他dense prediction tasks时通常是次优的。我们不使用带步长的卷积进行下采样，而是使用3×3的平均池化层。</li><li>将第一个$7 \times 7$的卷积替换为三个$3 \times 3$的卷积层，和原始的设计有相同的计算量和相同的感受野。</li><li>A 2 × 2 average pooling layer is added to the shortcut connection prior to the 1 × 1 convolutional layer for the transitioning blocks with stride of two.</li></ol><p>对于训练部分，主要用各种trick的叠加，比如说标签平滑损失、数据自动增强技术、Mixup增强技术、输入图像Crop时更大、各种正则化手段等。这里不再赘述。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>ResNeSt模型主要是基于ResNet-D模型的。Mixup训练策略将ResNet-D模型的准确率从78.31%提升到了79.15%，数据自动增强技术进一步提高0.26%。使用Split-Attention block组成一个ResNeSt-50-fast模型，精度进一步的提升到80.64%。对于ResNeSt-fast设置中，在$3 \times 3$卷积前使用一个有效的平均下采样方式避免在模型中引入额外的计算量。将该下采样操作移动到$3 \times 3$卷积层后面，ResNeSt-50取得了81.13%的精度。</p><p><strong>Radix vs. Cardinality.</strong> 采用不同的Radix和Cardinality，就可以得到不同的ResNeSt变体。在每一个变体中，对网络的宽度进行合适的调整，保证其总体计算量和ResNet变体相似。结果在表2中给出，其中$\rm s$表示radix，$\rm x$表示cardinality，$\rm d$表示网络的宽度（0$\rm s$ 为ResNet-D中标准残差结构）。可以发现，将radix从0增加到4会不断提高top-1的准确性，同时也会增加延迟和内存使用量。虽然我们希望在radix/Cardinality更大的情况下进一步提高精度，但我们在后续实验中使用了2s1x64d设置的Split-Attention，以确保这些块可以扩展到更深层次的网络，并在速度、精度和内存使用之间进行良好的权衡。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511221128425.png" alt="image-20200511221128425" style="zoom:80%"></p><h3 id="Comparing-against-the-State-of-the-Art"><a href="#Comparing-against-the-State-of-the-Art" class="headerlink" title="Comparing against the State-of-the-Art"></a>Comparing against the State-of-the-Art</h3><p><strong>ResNet Variants.</strong> 将本文所提方法和ResNet变体进行比较，如表3所示，在相近参数量和FLOPs下，本文模型效果更好。ResNeSt-50模型在ImageNet上的top-1取得了80.64%的精度，这是第一个使用50层网络性能超过80%的ResNet变体。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511220906788.png" alt="image-20200511220906788" style="zoom:80%"></p><p><strong>Other CNN Models.</strong> 对于更深的模型，作者加大了crop size。具体而言，ResNeSt-200使用大小为$256 \times 256$的crop size，ResNeSt-269使用$320 \times 320$的crop size。结果在表4中给出，除了参数量之外，我们还对比了推理时间。我们发现，尽管广泛使用的深度可分离卷积在参数量上具有优势，但它并没有优化推理速度。与通过神经结构搜索找到的模型相比，所提出的ResNeSt具有更好的准确性和推理时间。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511234812531.png" alt="image-20200511234812531"></p><blockquote><p>上面EfficientNet准确率应该是来自于原论文。</p></blockquote><h2 id="Transfer-Learning-Results"><a href="#Transfer-Learning-Results" class="headerlink" title="Transfer Learning Results"></a>Transfer Learning Results</h2><h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h3><p>为了进行比较，我们只将普通ResNet backbone替换为ResNeST，使用同时的超参数和detection heads。详细结果见表5，与使用标准ResNet的baseline相比，我们的backbone能够将FAST-RCNNs和CascadeRCNNs的平均精度提高约3%。结果表明，我们的backbone具有很好的泛化能力，可以很容易地移植到下游任务中去。值得注意的是，我们的ResNeSt-50在FAST-RCNN和CascadeRCNN检测模型上都优于ResNet101，使用的参数要少得多。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200511235853118.png" alt="image-20200511235853118" style="zoom:80%"></p><h3 id="Instance-Segmentation"><a href="#Instance-Segmentation" class="headerlink" title="Instance Segmentation"></a>Instance Segmentation</h3><p>我们以ResNeST-50和ResNeST-101为backbone，对Mask-RCNN和Cascade-Mask-RCNN模型进行了评估。<br>所有模型都使用了FPN和synchronized batch normalization。对于数据增强，输入图像的短边被随机缩放到(640,672,704,736,768,800)中的一个。为了与其他方法进行公平的比较，保持训练超参数不变。使用标准ResNet对作为backbone，将这些baseline按照前面说的设置进行重新训练。</p><p>如表6所示，本文所提的backbone实现了更好的性能。对于Mask-RCNN，ResNeSt-50在box/mask的精度方面比baseline提高了2.85%/2.09%，ResNeSt101表现得更好，分别提高了4.03%/3.14%。对于Cascade-Mask-RCNN，更换为ResNeSt50或ResNeSt101产生的增益分别为3.13%/2.36%或3.51%/3.04%。这表明，如果一个模型包含更多的Split-Attention modules，那么它会更好。在检测结果中观察到，ResNeSt50的MAP超过了标准的ResNet101主干的结果，这表明使用我们提出的模块的小模型的容量更高。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200512000458103.png" alt="image-20200512000458103"></p><h3 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h3><p>对于Cityscapes数据集，结果在表7右给出，本文的ResNeST backbone将DeepLabV3模型实现的mIoU提高了约1%，同时整体模型复杂性相似。值得注意的是，用ResNeST-50作为backbone的DeepLabV3模型的性能比使用更大的ResNet-101作为backbone的DeepLabV3的性能更好。</p><p>对于ADE20K数据集，表7左展示了pixAcc和mIoU，通过采用我们的ResNeST backbone，DeepLabV3的性能大大提高。与之前的结果类似，使用我们的ResNeST-50作为backbone的DeepLabv3模型的性能已经超过了使用更深的ResNet-101作为backbone的DeepLabv3。采用ResNeST-101作为backbone的DeepLabV3实现了82.07%的PixAcc和46.91%的mIoU，据我们所知，这是目前为止，在ADE20K上最好的单一模型。</p><p><img src="/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/image-20200512000427899.png" alt="image-20200512000427899" style="zoom:80%"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的目的很明确，就是提供一个特别适用于下游任务的强有力的backbone，作者从通道间的信息融合入手，将一个cardinal group进一步细分为多组splits。然后在一个cardinal group内将多组splits<strong>按照通道进行加权求和</strong>，也就是本文所说的Split-Attention机制。最后验证了在图像分类任务和各种迁移学习任务上的性能，证明了这个backbone的强大。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_24548569/article/details/105751377" target="_blank" rel="noopener">《ResNeSt: Split-Attention Networks》笔记</a><br><a href="https://github.com/zhanghang1989/ResNeSt/issues/4" target="_blank" rel="noopener">Similar to the attention of SKNet.</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/" title="ResNeSt：Split-Attention Networks">https://www.zdaiot.com/DeepLearningApplications/图像分类/ResNeSt：Split-Attention Networks/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/图像分类/" rel="tag"><i class="fa fa-tag"></i> 图像分类</a><a href="/tags/ResNeSt/" rel="tag"><i class="fa fa-tag"></i> ResNeSt</a></div><div class="post-nav"><div class="post-nav-item"><a href="/DeepLearningApplications/图像分类/Designing Network Design Spaces/" rel="prev" title="Designing Network Design Spaces"><i class="fa fa-chevron-left"></i> Designing Network Design Spaces</a></div><div class="post-nav-item"> <a href="/DeepLearningApplications/极化SAR/PolSAR Image Classification Based on Dilated Convolution and Pixel-Refining Parallel Mapping network in the Complex Domain/" rel="next" title="PolSAR Image Classification Based on Dilated Convolution and Pixel-Refining Parallel Mapping network in the Complex Domain">PolSAR Image Classification Based on Dilated Convolution and Pixel-Refining Parallel Mapping network in the Complex Domain<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#创新点"><span class="nav-number">1.</span> <span class="nav-text">创新点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创新点来源"><span class="nav-number">2.</span> <span class="nav-text">创新点来源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Split-Attention-Networks"><span class="nav-number">3.</span> <span class="nav-text">Split-Attention Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Network-and-Training"><span class="nav-number">4.</span> <span class="nav-text">Network and Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验结果"><span class="nav-number">5.</span> <span class="nav-text">实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ablation-Study"><span class="nav-number">5.1.</span> <span class="nav-text">Ablation Study</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Comparing-against-the-State-of-the-Art"><span class="nav-number">5.2.</span> <span class="nav-text">Comparing against the State-of-the-Art</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transfer-Learning-Results"><span class="nav-number">6.</span> <span class="nav-text">Transfer Learning Results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#目标检测"><span class="nav-number">6.1.</span> <span class="nav-text">目标检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Instance-Segmentation"><span class="nav-number">6.2.</span> <span class="nav-text">Instance Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-Segmentation"><span class="nav-number">6.3.</span> <span class="nav-text">Semantic Segmentation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">8.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">324</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">56</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">381</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">34:13</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '5ec8e52359dc24eed9d58d3f3dfbe237',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>