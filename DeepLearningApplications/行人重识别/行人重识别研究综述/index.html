<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="行人重识别定义行人重识别 (Person re-identiflcation) 也称行人再识别, 被广泛认为是一个图像检索的子问题, 是利用计算机视觉技术判断图像或者视频中是否存在特定行人的技术, 即给定一个监控行人图像检索跨设备下的该行人图像。 深度学习之前，该领域主要聚焦于如何手工设计更好的视觉特征和如何学习更好的相似性度量，深度学习出现后，它可以自动提取较好的行人图像特征，同时学习得到较好的"><meta name="keywords" content="综述,行人重识别"><meta property="og:type" content="article"><meta property="og:title" content="行人重识别研究综述"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="行人重识别定义行人重识别 (Person re-identiflcation) 也称行人再识别, 被广泛认为是一个图像检索的子问题, 是利用计算机视觉技术判断图像或者视频中是否存在特定行人的技术, 即给定一个监控行人图像检索跨设备下的该行人图像。 深度学习之前，该领域主要聚焦于如何手工设计更好的视觉特征和如何学习更好的相似性度量，深度学习出现后，它可以自动提取较好的行人图像特征，同时学习得到较好的"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573006406243.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573007754134.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573007916679.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/58098bc0eea6a.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/58098bc22b75b.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/58098be3ea50a.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-4eb252ed2bbcb4d78a51fc8d68cfdd10_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-0e942e3f97dc992104684dc1b29c64e9_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-cc7f31e9f2681f50f8fa4f106a60ccd2_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-470540752c1d8f4fdccfd00fd9557d4e_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573026705709.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-21f0d2845c3ed72d52f44a10e9943e64_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-d37e5ce5794bd23f36146f0ed7552cad_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-4367e3b00cd2120e914a4254fc5d8b88_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-a7c4e13ec4ac9354758d5f89a72339d8_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-d01de4281c4079b3fd25e58a0351c27f_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-9ed4efb7e67f29891e9ca0bda0729a6a_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-7152ff6c6def49a9326dedfa9c74a61b_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-011cdc19979938d5ffd6c5172d40ce33_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-c2643bfcaefb7d3ff101bcc432ed7315_hd.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573039423366.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573040795238.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041570104.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041462259.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041781960.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041947661.png"><meta property="og:updated_time" content="2019-11-06T01:34:48.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="行人重识别研究综述"><meta name="twitter:description" content="行人重识别定义行人重识别 (Person re-identiflcation) 也称行人再识别, 被广泛认为是一个图像检索的子问题, 是利用计算机视觉技术判断图像或者视频中是否存在特定行人的技术, 即给定一个监控行人图像检索跨设备下的该行人图像。 深度学习之前，该领域主要聚焦于如何手工设计更好的视觉特征和如何学习更好的相似性度量，深度学习出现后，它可以自动提取较好的行人图像特征，同时学习得到较好的"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/1573006406243.png"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>行人重识别研究综述 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 行人重识别研究综述<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/行人重识别/行人重识别研究综述.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-11-06 09:34:48" itemprop="dateCreated datePublished" datetime="2019-11-06T09:34:48+08:00">2019-11-06</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/行人重识别/" itemprop="url" rel="index"><span itemprop="name">行人重识别</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>20k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>19 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="行人重识别定义"><a href="#行人重识别定义" class="headerlink" title="行人重识别定义"></a>行人重识别定义</h2><p>行人重识别 (Person re-identiflcation) 也称行人再识别, 被广泛认为是一个图像检索的子问题, 是利用计算机视觉技术判断图像或者视频中是否存在特定行人的技术, 即给定一个监控行人图像检索跨设备下的该行人图像。</p><p>深度学习之前，该领域主要聚焦于如何手工设计更好的视觉特征和如何学习更好的相似性度量，深度学习出现后，它可以自动提取较好的行人图像特征，同时学习得到较好的相似性度量。</p><p>行人重识别领域知名学者郑良博士在论文 [3] 中将<strong>行人重识别系统总结为行人检测加上行人重识别</strong>。如下图所示：</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573006406243.png" alt="1573006406243"></p><p>随着深度学习的发展, 行人检测技术已逐渐成熟, 本文不再做具体阐述. <strong>目前大部分数据集直接将检测出来的行人图片作为训练集和测试集</strong>, 并且剔除了一些遮挡较严重的低质量图片. <strong>行人重识别技术将行人检测结果作为先验知识, 直接对行人图片进行跨摄像头检索.</strong></p><p>行人重识别任务主要包含特征提取和相似度度量两个步骤。 在行人重识别问题上, 基于深度学习的方法可以自动学习出复杂的特征描述, 并且用简单的欧式距离进行相似度度量便可以取得很好的性能.换句话说, 深度学习可以端对端地实现行人重识别任务, 这使得任务变得更加简单.</p><h2 id="相关数据集介绍"><a href="#相关数据集介绍" class="headerlink" title="相关数据集介绍"></a>相关数据集介绍</h2><p>VIPeR数据集是早期的一个小型行人重识别数据集, 图像来自 <strong>2 个摄像头</strong>. 该数据集总共包含 632 个行人的 1264 张图片, <strong>每个行人有两张不同摄像头拍摄的图片</strong>. 数据集随机分为相等的两部分, 一部分作为训练集, 一部分作为测试集. 由于采集时间较早, 该数据集的图像分辨率非常低, 所以识别难度较大。</p><p>PRID2011是 2011 年提出的一个数据集,图像来自于 <strong>2 个不同的摄像头</strong>. 该数据集总共包含934 个行人的 24541 张行人图片, <strong>所有的检测框都是人工手动提取</strong>. 图像大小的分辨率统一为 128x64的分辨率。</p><p>CUHK03在香港中文大学采集, 图像来自 <strong>2 个不同的摄像头</strong>. 该数据集提供机器<strong>自动检测和手动检测</strong>两个数据集. 其中检测数据集包含一些检测误差, 更接近实际情况. 数据集总共包括 1467个行人的 14097 张图片, 平均每个人有 9.6 张训练数据.</p><p>Market1501是在清华大学校园中采集,图像来自 <strong>6 个不同的摄像头</strong>, 其中有一个摄像头为低分辨率. 同时该数据集提供训练集和测试集. 训练集包含 12936 张图像, 测试集包含 19732 张图像。<strong>图像由检测器自动检测并切割</strong>， 所以包含一些检测误差 (接近实际使用情况). 训练数据中一共有 751人, 测试集中有 750 人. 所以在训练集中, 平均每类(每个人) 有 17.2 张训练数据.</p><p> DukeMTMC-reID[16] 在杜克大学内采集,<strong>图像来自 8 个不同摄像头</strong>, 行人图像的边框由人工标注完成. 该数据集提供训练集和测试集. 训练集包含 16522 张图像, 测试集包含 17661 张图像. 训练数据中一共有 702 人, 平均每个人有 23.5 张训练数据. 该数据集是 ICCV2017 会议之前最大的行人重识别数据集, 并且提供了行人属性 (性别/长短袖/是否背包等) 的标注。</p><p>除此之外，还有别的经典数据集，如下表所示，大 部 分 数 据 集 使 用 Deformable Part-basedModel(DPM) 或者手动标注的方法检测行人, 两个还未开放下载的同源数据集 MSMT17 和 LVreID使用了最新的 Faster RCNN 检测器, MARS 在提取序列的时候还辅助了 Generalized MaximumMulti Clique problem (GMMCP) 跟踪器。几乎目前主流的数据集都使用<strong>累计匹配 (CumulativeMatch Characteristics, CMC) 曲线和平均准确度(Mean Average Precision, mAP) 准确度评估</strong>.</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573007754134.png" alt="1573007754134"></p><p>下图展示了一些行人重识别数据集的图片, 从图中可以看出, 行人重识别是一个非常有挑战性的问题. 其中最主要的难点主要有: <strong>不同行人之间的外观可能高度相似, 而相同的行人在不同的时空下姿态也可能不同, 行人主体遭遇遮挡以及不同相机拍摄的光线条件差异</strong>等. 这些难点也使得行人重识别和一般的图像检索问题有所不同, 目前深度学习的方法除了扩大训练数据和改善网络结构以外, 也会针对于这些难点设计专用于 ReID 任务的算法.</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573007916679.png" alt="1573007916679"></p><h2 id="行人重识别深度学习方法"><a href="#行人重识别深度学习方法" class="headerlink" title="行人重识别深度学习方法"></a>行人重识别深度学习方法</h2><h3 id="基于表征学习的方法"><a href="#基于表征学习的方法" class="headerlink" title="基于表征学习的方法"></a>基于表征学习的方法</h3><p>首先，需要了解一下什么叫做表征学习，在周志华CNCC2016的PPT上有这样的几张图很好的解释了这个概念：</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/58098bc0eea6a.jpg" alt="CNCC 2016 | 周志华 57 张 PPT 揭开机器学习本质"></p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/58098bc22b75b.jpg" alt="CNCC 2016 | 周志华 57 张 PPT 揭开机器学习本质"></p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/58098be3ea50a.jpg" alt="CNCC 2016 | 周志华 57 张 PPT 揭开机器学习本质"></p><p>维基百科上给出的概念如下：在机器学习中，特征学习或表征学习是学习一个特征的技术的集合：<strong>将原始数据转换成为能够被机器学习来有效开发的一种形式</strong>。它<strong>避免了手动提取特征的麻烦</strong>，允许计算机学习使用特征的同时，也学习如何提取特征：学习如何学习。机器学习任务，例如分类问题，通常都要求输入在数学上或者在计算上都非常便于处理，在这样的前提下，特征学习就应运而生了。然而，在我们现实世界中的数据例如图片，视频，以及传感器的测量值都非常的复杂，冗余并且多变。那么，如何有效的提取出特征并且将其表达出来就显得非常重要。传统的手动提取特征需要大量的人力并且依赖于非常专业的知识。同时，还不便于推广。这就要求特征学习技术的整体设计非常有效，自动化，并且易于推广。大概了解了表征学习的概念后，我们回到这个问题上。</p><p>虽然行人重识别的最终目标是为了学习出两张图片之间的相似度, 但是表征学习的方法并没有直接在训练网络的时候考虑图片间的相似度, 而<strong>把行人重识别任务当做分类 (Classiflcation) 问题或者验证(Veriflcation) 问题来看待</strong>. 这类方法的特点就是网络的最后一层全连接 (Fully connected, FC) 层输出的并不是最终使用的图像特征向量, 而是经过一个Softmax 激活函数来计算表征学习损失, <strong>前一层 (倒数第二层)FC 层通常为特征向量层</strong>. 具体言之, 分类问题是指利用行人的 ID 或者属性等作为训练标签来训练模型, 每次只需要输入一张图片; 验证问题是指输入一对 (两张) 行人图片, 让网络来学习这两张图片是否属于同一个行人.</p><h4 id="分类网络"><a href="#分类网络" class="headerlink" title="分类网络"></a>分类网络</h4><p>分类网络常用的两种损失分别是行人 ID 损失(Identiflcation loss) 和属性损失 (Attribute loss)。</p><p>文献[3, 29] 将每一个行人当做分类问题的一个类别, 用行人的 ID 作为训练数据的标签来训练 CNN网络, 这个网络损失被称为 ID 损失, 而这种网络被称为 IDE(ID embedding) 网络. IDE 网络是行人重识别领域非常重要的<strong>baseline 基准。</strong>假设有$K$个行人的$n$张图片，将图片$x$输入IDE网络$f$，网络最后一层输出该图片的ID预测向量$z=[z_1,z_2,\cdots,z_k] \in R^K$。因此图片$x$属于第$k$（$k \in 1,2,\cdots,K$）个行人的概率为$p(k|x)=\frac{exp(z_k)}{\sum_{i=1}^K exp(z_i)}$。于是，IDE网络该图片的ID损失为：</p><script type="math/tex;mode=display">
L_{ID}(f,x)=-\sum_{k=1}^{K}q(k)log p(k|x)</script><p>其中$q(k)$为指示变量，若图片$x$的ID标签为$y$，则当$k=y$时，$q(k)=1$，当$k \not= y$时，$q(k)=0$。</p><p>后来部分研究者认为, 光靠行人的 ID 信息不足以学习出一个泛化能力足够强的模型. 因此, 他们利用了额外标注的行人图片的属性信息, 例如性别、头发、 衣着等属性, 通过<strong>引入行人属性标签计算属性损失</strong>. 训练好的网络不但要准确地预测出行人 ID,还要预测出各项行人属性, 这大大增加了网络的泛化能力, 多数论文也显示这种方法是有效的[3,26-27]。下图是一个例子，从图中可以看出，从图中可以看出, 网络输出的特征后面引出两个分支. 一个分支用于计算 ID 损失$L_{ID}$, 此分支和上文一致; 另一个分支用于计算属性损失$L_{Att}$。 假设图片$x$有$M$个属性标注，我们针对其中<strong>每一个属性计算一个损失</strong>。若<strong>某个属性共有$m$种类型</strong>，类似地我们计算图片$x$属于第$j,j=1,2,3,\cdots,m$的概率$p(j|x)=\frac{exp(z_k)}{\sum_{j=1}^m exp(z_j)}$。因此<strong>该图片该属性</strong>的属性损失为：</p><script type="math/tex;mode=display">
L_{Att}(f,x)=-\sum_{j=1}^m q(j)log p(j|x)</script><p>其中$q(j)$为指示变量，若图片$x$的属性标注为为$y_m$，则当$j=y_m$时，$q(j)=1$，当$j \not= y_m$时，$q(j)=0$。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-4eb252ed2bbcb4d78a51fc8d68cfdd10_r.jpg" alt="preview"></p><p>最终网络的总损失由 ID 损失和 M 个属性损失组成, 即</p><script type="math/tex;mode=display">
L = \lambda L_{ID} + \frac{1}{M} \sum_{i=1}^{M}L_{Att}^{i}</script><p>其中$\lambda$是平衡两个损失的权重因子，$L_{Att}^i$是第$i$个属性的损失值。该网络提取的图像特征个不仅用于预测行人的 ID 信息, 还用于预测各项行人属性. 通过结合 ID 损失和属性损失能够提高网络的泛化能力。</p><h4 id="验证网络"><a href="#验证网络" class="headerlink" title="验证网络"></a>验证网络</h4><p>验证网络是另外一种常用于行人重识别任务的表征学习方法[25,31]. 和分类网络不同之处在于, 验证网络每次需要输入两张图片, 这两张图片经过一个<strong>共享</strong>的 CNN 网络, 将网络输出的<strong>两个特征向量融合起来</strong>输入到<strong>一个只有两个神经元的 FC 层</strong>, 来预测这两幅图片是否属于同一个行人. 因此, <strong>验证网络本质上是一个多输入单输出的二分类网络</strong>.</p><p>通常, 仅仅使用验证损失训练网络是非常低效的, 所以验证损失会与 ID 损失一起使用来训练网络. 下图是一个使用融合验证损失和 ID 损失的行人重识别网络. 网络输入为若干对行人图片, 包括<strong>分类子网络(Classiflcation Subnet) 和验证子网络 (Veriflcation Subnet).</strong> 分类子网络对图片进行 ID 预测, 根据预测的 ID 来计算 ID 损失 $L_{ID}$, 这部分和前文一致. 验证子网络融合两张图片的特征, 判断这两张图片是否属于同一个行人, <strong>该子网络实质上等于一个二分类网络</strong>. 假设网络输入一对图像对$X=\{x_a, x_b\}$，它们的ID标签分别为$y_a$和$y_b$。网络输出一个2维的向量，其验证损失为：</p><script type="math/tex;mode=display">
L_V = -\sum_{i=1}^2 y(i)log v(i)</script><p>若$y_a=y_b$，则$y=\{1,0\}$，反之若$y_a \not=y_b$，则$y=\{0,1\}$。这样解释理解比较费劲，个人感觉可以这样解释：将$L_V$看成是一个二分类交叉熵，$y(i)$为指示标量，当$y_a=y_b$时，$y_1=1,y_2=0$；当$y_a \not=y_b$时，$y_1=0,y_2=1$。最终网络的总损失为$L=L_{ID}+L_{V}$。经过足够数据的训练，在推理阶段再次输入一张测试图片, 网络将自动提取出一个特征, 这个特征用于行人重识别任务。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-0e942e3f97dc992104684dc1b29c64e9_r.jpg" alt="preview"></p><p>如今依然有大量工作是基于表征学习，<strong>表征学习也成为了ReID领域的一个非常重要的baseline，并且表征学习的方法比较鲁棒，训练比较稳定，结果也比较容易复现</strong>。但是个人的实际经验感觉<strong>表征学习容易在数据集的domain上过拟合</strong>，并且<strong>当训练ID增加到一定程度的时候会显得比较乏力</strong>。</p><h3 id="基于度量学习的方法"><a href="#基于度量学习的方法" class="headerlink" title="基于度量学习的方法"></a>基于度量学习的方法</h3><p>度量学习 (Metric learning) 是广泛用于图像检索领域的一种方法. <strong>不同于表征学习, 度量学习旨在通过网络学习出两张图片的相似度</strong>. 在行人重识别问题上, 表现为同一行人的不同图片间的相似度大于不同行人的不同图片. 具体为，定义一个映射$f(x)：R^F \rightarrow R^D$，将图片从<strong>原始域映射到特征域</strong>, 之后再定义一个<strong>距离度量函数</strong>$D(x,y):R^D \times R^D \rightarrow R$，来计算两个特征向量之间的距离。<strong>最后通过最小化网络的度量损失, 来寻找一个最优的映射$f(x)$，使得使得相同行人两张图片 (正样本对) 的距离尽可能小, 不同行人两张图片 (负样本对) 的距离尽可能大</strong>. 而这个映射$f(x)$, 就是我们训练得到的深度卷积网络。</p><p>常 用 的 度 量 学 习 损 失 方 法 包 括 对 比 损 失(Contrastive loss)、 三 元 组 损 失 (Triplet loss)、 四元组损失(Quadruplet loss)。首先, 假如有两张输入图片$I_1$ 和 $I_2$, 通过网络的前向传播我们可以得到它们 (归一化后) 的特征向量 $f_{I_1}$和 $f_{I_2}$. 之后我们需要定义一个距离度量函数, 这个函数并不唯一, <strong>只要能够在特征空间描述特征向量的相似度/差异度的函数均可以作为距离度量函数</strong>. 然而, <strong>为了实现端对端 (End-to-end) 训练的网络, 度量函数尽可能连续可导</strong>, 通常我们<strong>使用归一化特征的欧式距离或者特征的余弦距离作为度量函数</strong>，即两张图片在特征空间的距离定义为</p><script type="math/tex;mode=display">
d_{I_1,I_2}=\mid \mid f_{I_1} - f_{I_2} \mid \mid_2 \\
d_{I_1,I_2}=1-\frac{f_{I_1} \cdot f_{I_2}}{\mid \mid f_{I_1} \mid \mid_2 \mid \mid f_{I_2} \mid \mid_2}</script><p>当然曼哈顿距离、 汉明距离、 马氏距离等距离也可以作为度量学习的距离度量函数, 本文对此不做过多讨论.</p><h4 id="对比损失函数"><a href="#对比损失函数" class="headerlink" title="对比损失函数"></a>对比损失函数</h4><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-cc7f31e9f2681f50f8fa4f106a60ccd2_r.jpg" alt="preview"></p><p>对比损失用于训练孪生网络 (Siamese network). 孪生网络的输入为一对 (两张) 图片 $I_a$ 和$I_b$, 这两张图片可以为同一行人, 也可以为不同行人.每一对训练图片都有一个标签 $y$, 其中 $y = 1$ 表示两张图片属于同一个行人 (正样本对), 反之 $y = 0$ 表示它们属于不同行人 (负样本对). 之后, 对比损失函数写作:</p><script type="math/tex;mode=display">
L_C = yd^2_{I_a, I_b}+(1-y)(\alpha-d_{I_a, I_b})_+^2</script><p>其中，$(z)_+=max(z,0)$，$\alpha$是根据实际需求设置的训练阀值参数。为了最小化损失函数，当网络输入一对正样本对， $d_{I_a, I_b}$会逐渐变小，即相同ID的行人图片会逐渐在特征空间形成聚类。反之，当网络输入一对负样本对时，会最小化$(\alpha-d_{I_a, I_b})_+^2$，等价于最小化$\left( max(0, \alpha-d_{I_a, I_b}) \right)^2$，$d_{I_a, I_b}$ 会逐渐变大直到超过设定的 $\alpha$，此时$max(0, \alpha-d_{I_a, I_b}) $为零，所以整体损失为0。通过最小化 $L_C$，最后可以使得正样本对之间的距离逐渐变小，负样本对之间的距离逐渐变大，从而满足行人重识别任务的需要。</p><h4 id="三元组损失"><a href="#三元组损失" class="headerlink" title="三元组损失"></a>三元组损失</h4><p>三元组损失是一种被广泛应用的度量学习损失, 之后的大量度量学习方法也是基于三元组损失演变而来. 顾名思义, 三元组损失需要三张输入图片. <strong>和对比损失不同, 一个输入的三元组 (Triplet) 包括一对正样本对和一对负样本对</strong>. 三张图片分别命名为固定图片 (Anchor) a, 正样本图片 (Positive) p 和负样本图片 (Negative) n. 图片 a 和图片 p 为一对正样本对, 图片 a 和图片 n 为一对负样本对. 则三元组损失表示为:</p><script type="math/tex;mode=display">
L_t = \left( d_{a,p} -d_{a,n} + \alpha \right)_+</script><p>其中，$(z)_+=max(z,0)$，$\alpha$是根据实际需求设置的训练阀值参数。如下图所示，三元组可以拉近正样本对之间的距离，推开负样本对之间的距离，最后使得相同ID的行人图片<strong>在特征空间里形成聚类</strong>，达到行人重识别的目的。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-470540752c1d8f4fdccfd00fd9557d4e_r.jpg" alt="preview"></p><p>论文[8]认为原版的Triplet loss只考虑正负样本对之间的相对距离，而并没有考虑正样本对之间的绝对距离，为此提出<strong>改进三元组损失(Improved triplet loss)</strong>：</p><script type="math/tex;mode=display">
L_i^t = d_{a,p} + \left( d_{a,p} -d_{a,n} + \alpha \right)_+</script><p><strong>通过添加$d_{a,p}$ 项, 保证网络不仅能够在特征空间把正负样本推开, 也能保证正样本对之间的距离很近。</strong></p><h4 id="四元组损失"><a href="#四元组损失" class="headerlink" title="四元组损失"></a>四元组损失</h4><p>四元组损失是三元组损失的另一个改进版本. 顾名思义, 四元组 (Quadruplet) 需要四张输入图片, <strong>和三元组不同的是多了一张负样本图片</strong>. 即四张图片为固定图片$a$, 正样本图片$p$, 负样本图片1 $n1$ 和负样本图片2 $n2$. 其中 $n1$ 和 $n2$ 是<strong>两张不同行人ID 的图片</strong>. 则, 四元组损失表示为:</p><script type="math/tex;mode=display">
L_q = \left( d_{a,p} -d_{a,n1} + \alpha  \right)_+ + \left( d_{a,p} -d_{n1,n2} + \beta  \right)_+</script><p>其中，$\alpha$和$\beta$是手动设置的正常数，通常设置$\beta$小于$\alpha$，前一项称为强推动，后一项称为弱推动. 其中前一项和三元组损失一样, <strong>只考虑正负样本间的相对距离</strong>, 共享了固定图片 $a$。因此在推开负样本对 $a$ 和$n1$ 的同时, 也会直接影响 $a$ 的特征, <strong>造成正样本对$a$ 和 $p$ 的距离不好控制. 改进三元组损失通过直接约束 $a$ 和 $p$ 之间的距离来解决这个问题. 而四元组通过引入第二项弱推动实现</strong>, 添加的<strong>第二项中负样本对和正样本对不共享 ID，所以考虑的是正负样本间的绝对距离</strong>, 在推开负样本对的同时不会太过直接影响 $a$ 的特征. 因此, 四元组损失通常能让模型学习到更好的表征。</p><h4 id="难样本采样三元组损失"><a href="#难样本采样三元组损失" class="headerlink" title="难样本采样三元组损失"></a>难样本采样三元组损失</h4><p>以上度量学习方法样本示例如下图所示, 这些方法在计算度量损失时, 样本对都是从训练集中随机挑选. <strong>随机挑选样本对的方法可能经常挑选出一些容易识别的样本对组成训练批量 (Batch), 使得网络泛化能力受限</strong>. 为此, 部分学者提出了难样本采样 (Hard sample mining) 的方法, 来挑选出难样本本对训练网络[37,41]. 常用的思路是<strong>挑选出一个训练Batch 中特征向量距离比较大 (非常不像) 的正样本对和特征向量距离比较小 (非常像) 的负样本对来训练网络.</strong> 难样本采样技术可以明显改进度量学习方法的性能, 加快网络的收敛, 并且可以很方便地在原有度量学习方法上进行扩展, 是目前广泛采用的一种技术.</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573026705709.png" alt="1573026705709"></p><p>这种技术称为<strong>难样本采样三元组损失(Triplet loss with batch hard mining, TriHard loss)</strong>。下面对该损失进行详细介绍。</p><p>TriHard损失的核心思想是：对于每一个训练batch，随机挑选$P$个ID的行人，每个行人随机挑选$K$张不同的图片，即一个batch含有$P \times K$张图片。之后<strong>对于batch中的每一张图片$a$，我们可以挑选一个最难的正样本和一个最难的负样本和$a$组成一个三元组</strong>。</p><p>首先我们定义和$a$相同ID的图片集为$A$，剩下不同ID的图片集为$B$，则TriHard损失表示为：</p><script type="math/tex;mode=display">
L_{th}=\frac{1}{P \times K} \sum_{a\in batch}\left(max_{p \in A} d_{a,p}-min_{n \in B}d_{a,n}+\alpha \right)_+</script><p>其中，$\alpha$为人为设定的阈值参数。TriHard损失会计算$a$和batch中的每一张图片在特征空间的欧式距离，然后选出与 $a$ 距离最远（最不像）的正样本$p$和距离最近（最像）的负样本$n$ 来计算三元组损失。通常TriHard损失效果比传统的三元组损失要好。</p><blockquote><p>假设一个batch有$N$个样本，则对于难样本采样三元组损失只需要求$N$个损失之和，对于普通采样三元组损失需要求$N*N$的损失之和。</p></blockquote><h4 id="边界挖掘损失"><a href="#边界挖掘损失" class="headerlink" title="边界挖掘损失"></a>边界挖掘损失</h4><p>边界挖掘损失(Margin sample mining loss, MSML)是一种<strong>引入难样本采样思想的度量学习方法</strong>。三元组损失只考虑了正负样本对之间的相对距离。为了引入正负样本对之间的绝对距离，四元组损失加入一张负样本组成了四元组$\left\{ a,p,n_1,n_2 \right\}$。四元组损失也定义为：</p><script type="math/tex;mode=display">
L_q = \left( d_{a,p} -d_{a,n1} + \alpha  \right)_+ + \left( d_{a,p} -d_{n1,n2} + \beta  \right)_+</script><p>假如我们忽略$\alpha$和$\beta$的影响，我们可以用一种更加通用的形式表示四元组损失：</p><script type="math/tex;mode=display">
L_{q^\prime} = \left( d_{a,p} -d_{m,n} + \alpha  \right)_+</script><p>其中，$m$和$n$是一对负样本对，$m$和$a$既可以是一对正样本对也可以是一对负样本对。之后把TriHard loss的难样本挖掘思想引入进来，便可以得到：</p><script type="math/tex;mode=display">
L_{msml}=\left( max_{a,p} d_{a,p} - min_{m,n} d_{m,n} + \alpha \right)_+</script><p>其中$a,p,m,n$均为batch内的图片，$a,p$是batch中最不像的正样本对，$m,n$是batch中最像的负样本对，$a,m$可以是正样本对也可以是负样本对。概括而言<strong>TriHard损失是针对batch中的每一张图片都挑选了一个三元组，而MSML损失只挑选出最难的一个正样本对和最难的一个负样本对计算损失。</strong>所以MSML是比TriHard更难的一种难样本采样，此外$max_{a,p} d_{a,p}$可以看做是正样本对距离的上界，而$min_{m,n} d_{m,n}$可以看作是负样本对的下界。<strong>MSML是为了把正负样本对的边界给推开，因此命名为边界样本挖掘损失。总的概括，MSML是同时兼顾相对距离和绝对距离并引入了难样本采样思想的度量学习方法</strong>。其演变思想如下图：</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-21f0d2845c3ed72d52f44a10e9943e64_r.jpg" alt="preview"></p><h4 id="各种loss的性能对比"><a href="#各种loss的性能对比" class="headerlink" title="各种loss的性能对比"></a>各种loss的性能对比</h4><p>在论文[11]之中，对上面提到的主要损失函数在尽可能公平的实验的条件下进行性能对比，实验结果如下表所示。作为一个参考:</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-d37e5ce5794bd23f36146f0ed7552cad_r.jpg" alt="preview"></p><h3 id="基于局部特征的方法"><a href="#基于局部特征的方法" class="headerlink" title="基于局部特征的方法"></a>基于局部特征的方法</h3><p><strong>从网络的训练损失函数上进行分类可以分成表征学习和度量学习</strong>, 相关方法前文已经介绍. 另一个角度, <strong>从抽取图像特征进行分类, 行人重识别的方法可以分为基于全局特征 (Global feature) 和基于局部特征 (Local feature) 的方法</strong>. 全局特征比较简单,是指让网络对整幅图像提取一个特征, 这个特征不考虑一些局部信息. 正常的卷积网络提取的都是全局特征, 因此在此不做赘述. 然而, 随着行人数据集越来越复杂, 仅仅使用全局特征并不能达到性能要求, 因此提取更加复杂的局部特征成为一个研究热点. 局部特征是指让手动或者自动地让网络去关注关键的局部区域, 然后提取这些区域的局部特征. 常用的提取局部特征的思路主要有<strong>图像切块、 利用骨架关键点定位以及行人前景分割</strong>等等.</p><p>图片切块是一种很常见的提取局部特征方式。 因为人体结构的特殊性, 通常研究者会将图片从上到下均分为几等份 (头部、 上身、 腿部等). 下图是图片切块的一个典型示例, 网络采用的是<strong>经典的孪生网络</strong>, 损失函数为<strong>度量学习的对比损失</strong>, 输入的两幅图片均分为若干等分. 之后, 被分割好的若干块图像块<strong>按照顺序送到一个</strong>长短时记忆网络 (Long short term memory network, LSTM), 最后的<strong>特征融合了所有图像块的局部特征</strong>. 图片切换方法的<strong>缺点在于对图像对齐的要求比较高, 如果两幅图像没有上下对齐, 那么很可能出现头和上身对比的现象, 反而使得模型判断错误.</strong> 因此 Zhang 等人设计了一种动态对齐网络 AlignedReID, 可以在不需要额外信息的情况下实现图片块从上到下的自动对准.</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-4367e3b00cd2120e914a4254fc5d8b88_r.jpg" alt="preview"></p><p>为了解决图像不对齐情况下手动图像切片失效的问题，一些论文<strong>利用一些先验知识先将行人进行对齐，这些先验知识主要是预训练的人体姿态(Pose)和骨架关键点(Skeleton) 模型</strong>。论文[13]先用姿态估计的模型估计出行人的关键点，然后用仿射变换使得相同的关键点对齐。如下图所示，一个行人通常被分为14个关键点，这14个关键点把人体结果分为若干个区域。为了提取不同尺度上的局部特征，作者设定了三个不同的PoseBox组合。之后这三个PoseBox矫正后的图片和原始未矫正的图片一起送到网络里去提取特征，这个特征包含了全局信息和局部信息。特别提出，这个仿射变换可以在进入网络之前的预处理中进行，也可以在输入到网络后进行。如果是后者的话需要需要对仿射变换做一个改进，因为传统的仿射变化是不可导的。为了使得网络可以训练，需要引入可导的近似放射变化，在本文中不赘述相关知识。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-a7c4e13ec4ac9354758d5f89a72339d8_r.jpg" alt="preview"></p><p>行人的局部特征在最近逐渐被证明是一种有效的特征, 可以一定程度上解决行人姿态多样化的问题。因此, 融合全局和局部特征在行人重识别领域也渐渐变得流行, <strong>图片切块的方法简单但是需要图片比较规范化, 利用姿态点信息比较精确但是需要额外的姿态估计模型</strong>。高效且低耗的局部特征提取模型依然是该领域一个值得研究的切入点。</p><h3 id="基于视频序列的ReID方法"><a href="#基于视频序列的ReID方法" class="headerlink" title="基于视频序列的ReID方法"></a>基于视频序列的ReID方法</h3><p>目前主流的行人重识别方法大部分是基于单帧图像的, 然而单帧图像给予的信息终究是有限的.此外, <strong>单帧的方法要求图像质量很高</strong>, 这对于相机的布置和使用的场景是一个非常大的限制, 因此研究基于序列的方法便显得十分重要. <strong>基于单帧图像的 ReID 方法可以通过一个简单方法扩展到视频序列, 即用所有序列图像特征向量的平均池化或者最大池化作为该序列的最终特征</strong>. 但是仍然有很多工作在研究如何更好地利用视频序列来进行行人重识别. 这类方法除了考虑了图像的内容信息, 还会考虑: (1) 帧与帧之间的运动信息; (2) 更好的特征融合; (3) 对图像帧进行质量判断等. 总体来说, 基于序列的方法核心思想为通过融合更多的信息来<strong>解决图像噪声较大、 背景复杂</strong>等一系列质量不佳的问题. 本小节将会着重介绍几个典型方法, 以点带面的形式来总结该类方法。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-d01de4281c4079b3fd25e58a0351c27f_hd.jpg" alt="img"></p><p>基于单帧图像的方法主要思想是利用CNN来提取图像的空间特征，而基于视频序列的方法主要思想是<strong>利用CNN 来提取空间特征的同时利用递归循环网络(Recurrent neural networks, RNN)来提取时序特征</strong>。上图是非常典型的思路，网络输入为图像序列。每张图像都经过一个共享的CNN提取出图像空间内容特征，之后这些特征向量被输入到一个RNN网络去提取最终的特征。<strong>最终的特征融合了单帧图像的内容特征和帧与帧之间的运动特征</strong>。而<strong>这个特征用于代替前面单帧方法的图像特征来训练网络。</strong>最终的损失包含了ID分类损失和对比损失函数。</p><p>视频序列类的代表方法之一是累计运动背景网络(Accumulative motion context network, AMOC)。<strong>AMOC输入的包括原始的图像序列和提取的光流序列</strong>。通常提取光流信息需要用到传统的光流提取算法，但是这些算法计算耗时，并且无法与深度学习网络兼容。为了能够得到一个自动提取光流的网络，作者首先训练了一个运动信息网络(Motion network, Moti Nets)。这个运动网络输入为原始的图像序列，<strong>标签为传统方法提取的光流序列</strong>。如下图所示，原始的图像序列显示在第一排，提取的光流序列显示在第二排。网络有三个光流预测的输出，分别为Pred1，Pred2，Pred3，这三个输出能够预测<strong>三个不同尺度的光流图</strong>。最后网络<strong>融合了三个尺度上的光流预测输出来得到最终光流图</strong>，预测的光流序列在第三排显示。通过最小化预测光流图和提取光流图的误差，<strong>网络能够提取出较准确的运动特征</strong>。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-9ed4efb7e67f29891e9ca0bda0729a6a_r.jpg" alt="preview"></p><p>AMOC的核心思想在于网络<strong>除了要提取序列图像的特征，还要提取运动光流的运动特征</strong>，其网络结构图如下图所示。AMOC拥有<strong>空间信息网络</strong>(Spatial network, Spat Nets)和<strong>运动信息网络</strong>两个子网络。图像序列的<strong>每一帧图像都被输入到Spat Nets来提取图像的全局内容特征</strong>。而<strong>相邻的两帧将会送到Moti Nets来提取光流图特征</strong>。之后<strong>空间特征和光流特征融合</strong>后输入到一个RNN来提取时序特征。通过AMOC网络，<strong>每个图像序列都能被提取出一个融合了内容信息、运动信息的特征</strong>。网络采用了<strong>分类损失和对比损失</strong>来训练模型。融合了运动信息的序列图像特征能够提高行人重识别的准确度。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-7152ff6c6def49a9326dedfa9c74a61b_hd.jpg" alt="img"></p><p>论文[24]从另外一个角度展示了多帧序列弥补单帧信息不足的作用，目前大部分video based ReID方法还是不管三七二十一的把序列信息输给网络，让网络去自己学有用的信息，并没有直观的去解释为什么多帧信息有用。而论文[24]则很明确地指出<strong>当单帧图像遇到遮挡等情况的时候，可以用多帧的其他信息来弥补，直接诱导网络去对图片进行一个质量判断，降低质量差的帧的重要度</strong>。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-011cdc19979938d5ffd6c5172d40ce33_hd.jpg" alt="img"></p><p>如上图，文章认为在遮挡较严重的情况下，如果用一般的pooling会造成attention map变差，遮挡区域的特征会丢失很多。而利用论文的方法每帧进行一个质量判断，就可以着重考虑那些比较完整的几帧，使得attention map比较完整。而关键的实现就是利用一个pose estimation的网络，论文叫做landmark detector。<strong>当landmark不完整的时候就证明存在遮挡，则图片质量就会变差</strong>。之后pose feature map和global feature map都同时输入到网络，让网络对每帧进行一个权重判断，给高质量帧打上高权重，然后对feature map进行一个线性叠加。思路比较简单但是还是比较让人信服的。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/v2-c2643bfcaefb7d3ff101bcc432ed7315_hd.jpg" alt="img"></p><p>基于视频序列的行人重识别技术是该领域未来急需解决的一个问题. 总体而言, 和单帧方法相比,序列方法无论是从思路的多样性上, 还是从结果性能上, 都还存在一定的差距.</p><h3 id="基于GAN造图的ReID方法"><a href="#基于GAN造图的ReID方法" class="headerlink" title="基于GAN造图的ReID方法"></a>基于GAN造图的ReID方法</h3><p>GAN 在近几年得到了蓬勃的发展, 其中一个应用就是图片生成. 深度学习的方法需要依赖大量训练数据, 而目前行人重识别的数据集总体来说规模还是比较小. 因此, 利用 GAN 来做行人重识别任务逐渐开始变得流行.</p><p>因为我之前并没有对GAN进行过了解，所以这里不做更多的解释，具体内容可以参考论文以及知乎专栏。</p><h2 id="各类方法总结比较"><a href="#各类方法总结比较" class="headerlink" title="各类方法总结比较"></a>各类方法总结比较</h2><p>前文按照分类介绍一些基于深度学习的行人重识别方法, 本小节将对这些方法进行总结与比较. 基于 GAN 的方法更多是作为一种图像增广或者解决图像域偏差的技术而较为独立.从训练深度网络的角度, 我们将从三个角度来分析: <strong>表征学习与度量学习、 全局特征与局部特征、单帧图像与视频序列</strong>. 如表格 3 所示, 前文提到的代表算法所对应的类型都已标记出. 有的方法只使用了一种类型的损失函数或者特征类型, 而有的方法融合了多种损失函数或者特征类型来达到更高的性能水平。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573039423366.png" alt="1573039423366"></p><h3 id="基于表征学习与度量学习的方法"><a href="#基于表征学习与度量学习的方法" class="headerlink" title="基于表征学习与度量学习的方法"></a>基于表征学习与度量学习的方法</h3><p>按照<strong>网络训练损失分类</strong>, 行人重识别的方法可以分为表征学习和度量学习两类.</p><ul><li>表征学习的优点在于数据集量不大的时候<strong>收敛容易, 模型训练鲁棒性强, 训练时间短</strong>. 然而表征学习是将每一个 ID 的行人图片当做一个类别, 当 ID 数量增加到百万、 千万甚至更多的时候, <strong>网络最后一层是一个维度非常高的全连接层, 使得网络参数量巨大并且收敛困难</strong>.由于直接计算特征之间的距离,</li><li>度量学习的优点在于可以<strong>很方便的扩展到新的数据集, 不需要根据 ID数量来调整网络的结构</strong>, 此外也可以<strong>非常好的适应ID 数目巨大的训练数据</strong>. 然而, <strong>度量学习相对来说收敛困难</strong>, 需要比较丰富的训练经验来调整网络参数, 另外<strong>收敛训练时间也比表征学习要长</strong>.</li></ul><p><strong>度量学习可以近似看作为样本在特征空间进行聚类, 表征学习可以近似看作为学习样本在特征空间的分界面.</strong> 正样本距离拉近的过程使得类内距离缩小, 负样本距离推开的过程使得类间距离增大, <strong>最终收敛时样本在特征空间呈现聚类效应</strong>. 度量学习和表征学习相比, 优势在于网络末尾不需要接一个分类的全连接层, 因此<strong>对于训练集的行人 ID 数量并不敏感</strong>, 可以应用于训练超大规模数据集的网络. 总体而言, 度量学习比表征学习使用的更加广泛, 性能表现也略微优于表征学习. 但是<strong>目前行人重识别的数据集规模还依然有限, 表征学习的方法也依然得到使用</strong>, 而同时融合度量学习和表征学习训练网络的思路也在逐渐变得流行。联合的方式也比较直接, 在<strong>传统度量学习方法的基础上, 在特征层后面再添加一个全连接层进行 ID 分类学习</strong>. 网络同时优化表征学习损失和度量学习损失, 来共同优化特征层. 如AMOC 同时联合了 ID 损失和对比损失, 特征层之后分出了两个分支分别优化表征学习损失和度量学习损失.</p><blockquote><p>验证网络学习一个二分类，因为<strong>没有算distance</strong> ，而是直接根据feature进行分类，所以放到了表征学习里。</p></blockquote><h3 id="基于全局特征与局部特征的方法"><a href="#基于全局特征与局部特征的方法" class="headerlink" title="基于全局特征与局部特征的方法"></a>基于全局特征与局部特征的方法</h3><p>按照<strong>网络输出特征类型</strong>, 行人重识别方法可以分为基于全局特征与局部特征的方法.</p><p>全局特征一般是卷积网络的特征图直接通过一个全局池化层得到, <strong>推理阶段计算快速</strong>, 适合于需要帧率较高的实际应用. 然而由于<strong>全局池化层会使得图像的空间特征信息丢失</strong>, 因此在姿态不对齐、 行人图片不完整、只有局部细节不相似等情况下, 全局特征容易出现误识别.</p><p>而局部特征的优点在于可以一定程度上解决这些问题, 当然局部特征也有它自己的缺点. 对于分块的局部特征优点在于不需要引入额外的计算量, 但是通常并不能特别好的解决姿态不对齐的问题. 而利用姿态点估计模型估计出行人的姿态点, 然后再进行局部特征匹配可以较好的解决姿态不对齐的问题, 但是却需要一个额外的姿态点模型. 总体来说, 全局特征和局部特征是两个比较互补的特征类型, 通常不会单独使用局部特征. 广义上讲, 分块局部特征把所有的分块特征融合起来也包含了全局图像信息. 因此在不考虑推理阶段计算耗时的前提下,融合全局特征和局部特征是目前一种提高网络性能非常常用的手段.</p><p>目前融合全局特征和局部特征常用的思路是对于全局模块和局部模块分别提取特征, 之后再将全局特征和局部特征拼接在一起作为最终的特征. Spindle Net 就提取了全局特征和 7 个不同尺度的局部特征, 然后融合成最终的图像特征用于进行最后的相似度度量. AlignedReID 给出了另外一种融合方法, 即分别计算两幅图像全局特征距离和局部特征距离, 然后加权求和作为最终两幅图像在特征空间的距离. RQEN 则是利用一个姿态点模型来估计行人的可视性部分, 然后融合多帧信息得到一个较好的最终特征, 也可以看做是一个全局特征和局部特征融合的过程. 如何探究更好的全局特征和局部特征方法也是行人重识别未来一个重要的研究分支。</p><h3 id="基于单帧图像与视频序列的方法"><a href="#基于单帧图像与视频序列的方法" class="headerlink" title="基于单帧图像与视频序列的方法"></a>基于单帧图像与视频序列的方法</h3><p>按照<strong>网络输入数据</strong>, 行人重识别方法可以分为基于单帧图像与视频序列的方法. 这两类方法并没有太多重合的地方, 只是针对于不同的应用选择不同类型的网络输入.</p><p>基于单帧图像的方法训练简单,使用方便, 推理阶段耗时时间短. 然而它的缺点在于单帧图像信息有限, 对于图像质量要求较高, 一旦出现检测框错误或者行人遮挡等情况, 算法效果会大幅度下降.</p><p>基于视频序列的方法可以解决单帧图像信息不足的缺点, 并且可以融入运动信息加强鲁棒性, 然而由于每次要处理多张图像, 因此计算效率较低. 当然基于视频序列的方法大部分都是单帧图像方法的扩展延伸, 因此发展单帧图像的方法对于发展视频序列的方法也是有益的。</p><h2 id="典型算法比较"><a href="#典型算法比较" class="headerlink" title="典型算法比较"></a>典型算法比较</h2><p>行人重识别最主要的两个性能指标是一选准确率(rank-1) 和平均准确率 (mAP). 表格给出的结果均由论文中给出, 一些代表性的算法没有在 Market1501 上进行评测因此没有展示. <strong>除非算法本身是基于重排序的研究, 否则本文默认都是使用欧式距离的无重排序结果</strong>. 为了方便比较, 表格第一行给出了比较好的传统方法的结果, 作为传统方法的<br>基准. 第二部分是强监督的深度学习方法. 在第三部分的无监督学习方法方面, 大部分无监督学习方法都还是基于传统特征的研究. 而 CVPR2018 刚接收的SPGAN[62] 是比较具有代表性的基于深度学习的无监督行人重识别方法, 同时在 Market1501 数据集上也击败了目前已有的无监督学习方法.</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573040795238.png" alt="1573040795238"></p><h2 id="挑战和未来"><a href="#挑战和未来" class="headerlink" title="挑战和未来"></a>挑战和未来</h2><h3 id="目前的重要挑战"><a href="#目前的重要挑战" class="headerlink" title="目前的重要挑战"></a>目前的重要挑战</h3><p>目前学术界已存的数据集是清理之后的高质量图像, 然而在真实场景下行人重识别会遇到<strong>跨视角造成的姿态多变、 分辨率变化、 行人遮挡以及图像域变化</strong>等问题. 这些问题逐渐受到学者的重视, 本小节将会简单介绍一些克服<br>这些挑战的代表性。</p><p>1) 跨视角造成的姿态多变问题: 由于不同摄像头架设的角度、 位置不一, 拍摄图片中的行人姿态也十分多变. 目前已经有不少代表性的工作从不同角度上来解决这个问题, 而这些方法主要是依靠一个预训练的姿态模型来实现姿态的对齐. 除了 3.3 小节中介绍的 GLAD 和 SpindleNet 等工作以外, CVPR2018 提出的姿态敏感嵌入方PoseSensitive Embedding, PSE). 如图所示, PSE利用<strong>一个预训练的姿态模型估计行人的姿态点</strong>, 然后将姿态点信息输入到网络, <strong>网络的视角分支会估计行人的朝向及其概率</strong>. 另一方面, <strong>PSE 的特征分支分别得到前向、 背向和侧向三个视角的特征图</strong>, 之后<strong>与估计的视角概率加权得到最终的全局特征</strong>. 通过使用对齐后的全局特征, 可以更好地处理视角多变的行人图片.</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041570104.png" alt="1573041570104"></p><p>2) 行人图片分辨率变化: 由于摄像头中目标拍摄距离不一致, 拍摄的行人图片分辨率也不一样. 目前专门解决这个问题的方法较少, 论文提出了一个新的图像超分辨和行人身份识别联合学习 (Super-resolution and identity joint learning,SING) 的方法. 如下图所示, SING 通过联合学习图片的超分辨率和行人重识别的问题, 既能够提升低分辨率图片的分辨率, 又能提高低分辨率图片行人重识别任务的准确度. 为了得到低分辨率的图片,SING 先用高分辨率图片降采样得到一批低分辨率图片. 之后, 网络优化联合学习图像超分辨的重构损失和行人身份识别损失函数. 低分辨率图片经过网络高分辨率处理后再进行特征提取, 而正常分辨率图像则是直接进行特征提取. 由于不同分辨率的图片经过不同的方式提取特征, 因此 SING 网络能够较好的应对分辨率变化的问题。</p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041462259.png" alt="1573041462259"></p><p>3) 行人图片遮挡问题: 目前学术界的行人重识别数据集大多数清洗过的高质量图像. 然而在真实的使用场景, <strong>行人经常会被移动目标或者静态物体所遮挡, 造成行人图片的不完整</strong>. 由于失去了部分行人特征而引入了很多干扰特征, 使得很多基于全局特征的行人重识别算法效果大大下降. 为了解决这个问题, 一个思路是<strong>利用行人姿态模型来估计行人图像的可视部分, 然后对可视部分进行局部特征提取、 融合</strong>. 而 CVPR2018 的论文提出深度空间特征重建方法 (Deep Spatial feature Reconstruction, DSR) 来进行不完整图片和完整图片的匹配.如图所示, DSR 利用一个训练好的 ReID 网络对图片进行特征提取, 并且不对原图进行尺度变换的操作. 不同尺寸的图片经过网络后得到不同尺寸大小的特征图, 而两个不同尺寸的特征图并不能直接地进行相似度计算. 为了解决这个问题, DSR 利用空间特征重建 (Spatial Feature Reconstruction)的方法计算出两幅特征图之间的稀疏表达系数. <strong>完整图片的特征图经过乘以稀疏表达系数便可以与不完整图片的特征图进行欧氏距离的度量. 从而实现不同尺寸图片的特征图相似度的计算.</strong></p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041781960.png" alt="1573041781960"></p><p>4) 图像域变化的跨模态重识别. 图像域的变化是行人重识别应用上非常普遍的一个挑战. 图像域变化的类型也多种多样, 例如不同相机、 不同天气、不同时间、 不同城市拍摄的图像风格均可能不同. 此外, 夜晚 RGB 相机也会失效, 使用红外相机拍摄的图片没有颜色信息, 因此 RGB 图片与红外图片的行人重识别也是个典型的跨模态问题. 目前基于 GAN网络生成图像来解决图像域偏差是一个很流行的思路, 例如CamStyle 解决不同相机的图像域问题, PTGAN 解决不同城市的图像域问题. 而RGB 与红外图片域的跨模态重识别问题逐渐开始受到关注, ICCV17 接受的一篇论文提出了深度零填充模型 (Deep zero padding model) 首次利用深度网络来解决这一问题. 如图所示, 该方法的核心思想是<strong>在网络输入图片的时候, 对于不同域的图片在不同的通道上用零填充. 零填充通道记录了图像来源于哪个图像域的信息, 促使深度网络根据图像域来自适应提取不同的特征, 从而实现更好的跨模态行人重识别.</strong></p><p><img src="/DeepLearningApplications/行人重识别/行人重识别研究综述/1573041947661.png" alt="1573041947661"></p><h3 id="未来的研究方向"><a href="#未来的研究方向" class="headerlink" title="未来的研究方向"></a>未来的研究方向</h3><p>1) 构建更加适应真实环境的高质量标准数据库: 目前的数据集场景丰富度也不够高, 通常就是在一个环境下一个较短时间间隔采集的数据.目前行人重识别数据集之间的偏差依然很大, 而不同地域、 空间、 时间环境下的行人着装也各有不同,<strong>一个数据集训练的网络在另外一个数据集下性能会下降相当多</strong>.</p><p>2) 半监督、 无监督和迁移学习的方法: 采集的数据终究是有限的, 而标注数据的成本代价也很高.因此半监督和无监督学习的方法虽然在性能上可能比不上监督学习方法, 但是性价比很高. 迁移学习也是一个值得研究的方向. <strong>行人重识别技术的应用场景是无限的, 针对于每一个使用场景都训练一个专用模型是非常低效的</strong>. 如何通过迁移学习的方法将一个场景训练的模型适应新的场景是一个有价值的研究问题. 而半监督、无监督以及迁移学习的深度学习行人重识别技术已经有一些研究工作, 不过还有很大的提升空间。</p><p>3) 构造更加强大的特征: 提高行人重识别的性能主要是从特征提取和图像检索两个角度切入. <strong>一些重排序技术可以用消耗时间为代价提高检索准确度[75], 而一个好的特征可以更加经济提高性能</strong>. 具体而言, 行人重识别任务要想构造一个更好的特征,需要网络能够关注到更加关键的局部信息, 即更加合理的局部特征. 而利用更加丰富的序列特征也是构造特征的一个思路.</p><p>4) 丰富场景下的行人重识别: 目前行人重识别数据集以视野广阔的室外场景为主, 几个包含室内场景的数据集也能够保证行人是完整的全身. 但是在一些场景下, 例如无人超市、 商场、 地铁内等, 会存在非常多的半身图片. 而半身 - 半身、 全身 - 半身的“部分” 行人重识别技术便显得非常重要. 而夜间光照不佳情况下的行人重识别也是一个值得研究的问题. 目前的主流思路还是用红外相机在黑暗条件下采集图片, 随之引申出来的是红外行人重识别. 红外图片几乎只有轮廓, 失去了颜色信息给重识别任务带来了非常大的挑战. 除了以上举得例子, 其他场景的一些跨域行人重识别也值得关注</p><p>5) 行人重识别与行人检测、 行人跟踪的结合: 目前大部分的方法是在已经检测出行人的先验条件下进行的. 但是这需要一个非常鲁邦的行人检测模型, 如果行人重识别与行人检测结合起来, 则更加符合实际的应用需求.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[3] Zheng L, Yang Y, Hauptmann A G. Person re-identification:past, present and future. arXiv preprint arXiv:1610.02984,2016<br>[8] He K M, Zhang X Y, Ren S Q, Sun J. Delving deep into rectifiers: surpassing human-level performance on ImageNet classification. In: Proceedings of the 2015 IEEE International Conference on Computer Vision. Santiago, Chile: IEEE, 2015. 1026−1034<br>[11] irzer M, Beleznai C, Roth P M, Bischof H. Person reidentification by descriptive and discriminative classification. In: Proceedings of Scandinavian Conference on Image Analysis. Berlin, Heidelberg: Springer, 2011. 91−102<br>[13] Zheng L, Shen L Y, Tian L, Wang S J, Wang J D, Tian Q. Scalable person re-identification: a benchmark. In: Proceedings of the 2015 IEEE International Conference on Computer Vision. Santiago, Chile: IEEE, 2015. 1116−1124<br>[24] Karanam S, Gou M R, Wu Z Y, Rates-Borras A, Camps O,Radke R J. A Systematic evaluation and enchmark for person re-identification: features, metrics, and datasets. arXiv preprint arXiv:1605.09653, 2016.<br>[25] Geng M Y, Wang Y W, Xiang T, Tian Y H. Deep transfer learning for person re-identification. arXiv preprint arXiv:1611.05244, 2016.<br>[26] Lin Y T, Zheng L, Zheng Z D, Wu Y, Yang Y. Improving person re-identification by attribute and identity learning. arXiv preprint arXiv:1703.07220, 2017.<br>[27] Matsukawa T, Suzuki E. Person re-identification using CNN features learned from combination of attributes. In: Proceedings of the 23rd International Conference on Pattern Recognition. Cancun, Mexico: IEEE, 2016. 2428−2433<br>[29] Zheng L, Zhang H H, Sun S Y, Chandraker M, Yang Y,Tian Q. Person re-identification in the wild. In: Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, Hawaii, USA: IEEE, 2017.3346−3355<br>[31] Zheng Z D, Zheng L, Yang Y. A discriminatively learned CNN embedding for person reidentification. ACM Transactions on Multimedia Computing, Communications, and Applications, 2018, 14(1): Article No. 13<br>[37] Hermans A, Beyer L, Leibe B. In defense of the triplet loss for person re-identification. arXiv preprint<br>arXiv:1703.07737, 2017.<br>[41] Xiao Q Q, Luo H, Zhang C. Margin sample mining loss: a deep learning based method for person re-identification.arXiv preprint arXiv:1710.00478, 2017.<br>[75] Zhong Z, Zheng L, Cao D L, Li S Z. Re-ranking person re-identification with k-reciprocal encoding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, Hawaii, USA: IEEE, 2017.3652−3661<br><a href="https://www.leiphone.com/news/201610/rZ9EHIpeSwBv2Tvq.html" target="_blank" rel="noopener">CNCC 2016 | 周志华 57 张 PPT 揭开机器学习本质</a><br><a href="[https://zh.wikipedia.org/wiki/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0](https://zh.wikipedia.org/wiki/表征学习">表征学习</a>)</p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/" title="行人重识别研究综述">https://www.zdaiot.com/DeepLearningApplications/行人重识别/行人重识别研究综述/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/综述/" rel="tag"><i class="fa fa-tag"></i> 综述</a><a href="/tags/行人重识别/" rel="tag"><i class="fa fa-tag"></i> 行人重识别</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MLFrameworks/Pytorch/Pytorch tensor 索引/" rel="prev" title="Pytorch tensor 索引"><i class="fa fa-chevron-left"></i> Pytorch tensor 索引</a></div><div class="post-nav-item"> <a href="/DataStructureAlgorithm/03复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度/" rel="next" title="03复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度">03复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#行人重识别定义"><span class="nav-number">1.</span> <span class="nav-text">行人重识别定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#相关数据集介绍"><span class="nav-number">2.</span> <span class="nav-text">相关数据集介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#行人重识别深度学习方法"><span class="nav-number">3.</span> <span class="nav-text">行人重识别深度学习方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于表征学习的方法"><span class="nav-number">3.1.</span> <span class="nav-text">基于表征学习的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#分类网络"><span class="nav-number">3.1.1.</span> <span class="nav-text">分类网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#验证网络"><span class="nav-number">3.1.2.</span> <span class="nav-text">验证网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于度量学习的方法"><span class="nav-number">3.2.</span> <span class="nav-text">基于度量学习的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#对比损失函数"><span class="nav-number">3.2.1.</span> <span class="nav-text">对比损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三元组损失"><span class="nav-number">3.2.2.</span> <span class="nav-text">三元组损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#四元组损失"><span class="nav-number">3.2.3.</span> <span class="nav-text">四元组损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#难样本采样三元组损失"><span class="nav-number">3.2.4.</span> <span class="nav-text">难样本采样三元组损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#边界挖掘损失"><span class="nav-number">3.2.5.</span> <span class="nav-text">边界挖掘损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#各种loss的性能对比"><span class="nav-number">3.2.6.</span> <span class="nav-text">各种loss的性能对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于局部特征的方法"><span class="nav-number">3.3.</span> <span class="nav-text">基于局部特征的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于视频序列的ReID方法"><span class="nav-number">3.4.</span> <span class="nav-text">基于视频序列的ReID方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于GAN造图的ReID方法"><span class="nav-number">3.5.</span> <span class="nav-text">基于GAN造图的ReID方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#各类方法总结比较"><span class="nav-number">4.</span> <span class="nav-text">各类方法总结比较</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于表征学习与度量学习的方法"><span class="nav-number">4.1.</span> <span class="nav-text">基于表征学习与度量学习的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于全局特征与局部特征的方法"><span class="nav-number">4.2.</span> <span class="nav-text">基于全局特征与局部特征的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于单帧图像与视频序列的方法"><span class="nav-number">4.3.</span> <span class="nav-text">基于单帧图像与视频序列的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#典型算法比较"><span class="nav-number">5.</span> <span class="nav-text">典型算法比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#挑战和未来"><span class="nav-number">6.</span> <span class="nav-text">挑战和未来</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#目前的重要挑战"><span class="nav-number">6.1.</span> <span class="nav-text">目前的重要挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#未来的研究方向"><span class="nav-number">6.2.</span> <span class="nav-text">未来的研究方向</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">327</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">55</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">383</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">35:21</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : 'bdbf98c609961229816cd4112ae23288',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>