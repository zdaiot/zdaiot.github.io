<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本文主要参考了Faster R-CNN具体实现详解和Object Detection and Classification using R-CNNs。 在这篇文章中，将详细描述最近引入的基于深度学习的对象检测和分类方法，R-CNN（Regions with CNN features）是如何工作的。事实证明，R-CNN在检测和分类自然图像中的物体方面非常有效，其mAP远高于之前的方法。R-CNN方法"><meta name="keywords" content="目标检测,R-CNN"><meta property="og:type" content="article"><meta property="og:title" content="R-CNNs详解"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="本文主要参考了Faster R-CNN具体实现详解和Object Detection and Classification using R-CNNs。 在这篇文章中，将详细描述最近引入的基于深度学习的对象检测和分类方法，R-CNN（Regions with CNN features）是如何工作的。事实证明，R-CNN在检测和分类自然图像中的物体方面非常有效，其mAP远高于之前的方法。R-CNN方法"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa46e9e0bbd7.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5a9ffec911c19.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/person_06.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5bac3708478ce.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa6f44a0a9c7-1568628026330.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa6f476535f7.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/image-20200331201944487.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa05d3ecef3e-1568626559915.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/v2-c93db71cc8f4f4fd8cfb4ef2e2cef4f4_720w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa0695484e3e.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa5766d53b63.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa13d4d911d3.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa1cd250f265-1.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa32302afc0b-1.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa402baba3a1-1.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa4255fdacb6.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa55c81eac0a.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa55c97f3287.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa70ff399c57.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa580271bea6.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa5809cc7206.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa581709aa82.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa5827c1d42c.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa59c8da4c4b.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa7c84451f81.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa7c828703ab.png"><meta property="og:updated_time" content="2020-03-30T06:59:08.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="R-CNNs详解"><meta name="twitter:description" content="本文主要参考了Faster R-CNN具体实现详解和Object Detection and Classification using R-CNNs。 在这篇文章中，将详细描述最近引入的基于深度学习的对象检测和分类方法，R-CNN（Regions with CNN features）是如何工作的。事实证明，R-CNN在检测和分类自然图像中的物体方面非常有效，其mAP远高于之前的方法。R-CNN方法"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa46e9e0bbd7.png"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>R-CNNs详解 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> R-CNNs详解<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/目标检测/R-CNNs详解.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-03-30 14:59:08" itemprop="dateCreated datePublished" datetime="2020-03-30T14:59:08+08:00">2020-03-30</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/目标检测/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>19k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本文主要参考了<a href="http://activepony.com/shen-du-xue-xi/mu-biao-jian-ce/faster-r-cnn-ju-ti-shi-xian-xiang-jie/" target="_blank" rel="noopener">Faster R-CNN具体实现详解</a>和<a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a>。</p><p>在这篇文章中，将详细描述最近引入的基于深度学习的对象检测和分类方法，R-CNN（Regions with CNN features）是如何工作的。事实证明，R-CNN在检测和分类自然图像中的物体方面非常有效，其mAP远高于之前的方法。R-CNN方法在Ross Girshick等人的以下系列论文中描述。</p><ol><li>R-CNN（<a href="https://arxiv.org/abs/1311.2524）" target="_blank" rel="noopener">https://arxiv.org/abs/1311.2524）</a></li><li>Fast R-CNN（<a href="https://arxiv.org/abs/1504.08083）" target="_blank" rel="noopener">https://arxiv.org/abs/1504.08083）</a></li><li>Faster R-CNN（<a href="https://arxiv.org/abs/1506.01497）" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01497）</a></li></ol><p>这篇文章描述了最后一篇论文中R-CNN方法的最终版本。我首先考虑介绍该方法从第一次引入到最终版本的演变，然而事实表明这工作量太大。所以决定直接详细描述最终版本。</p><p>幸运的是，在TensorFlow，PyTorch和其他机器学习库中，网上有许多R-CNN算法的实现。本文参考了以下实现：<a href="https://github.com/ruotianluo/pytorch-faster-rcnn" target="_blank" rel="noopener">https://github.com/ruotianluo/pytorch-faster-rcnn</a></p><p>本文中使用的大部分术语（例如，不同层的名称）遵循代码中使用的术语。理解本文中提供的信息应该可以更容易地遵循PyTorch实现并进行自己的修改。</p><h2 id="文章组织"><a href="#文章组织" class="headerlink" title="文章组织"></a>文章组织</h2><ul><li><strong>第1部分 - 图像预处理：</strong>在本节中，我们将描述应用于输入图像的预处理步骤。这些步骤包括减去平均像素值和缩放图像。训练和推理之间的预处理步骤必须相同。</li><li><strong>第2节 - 网络组织：</strong>在本节中，我们将描述网络的三个主要组成部分——“head”network，region proposal network(RPN)和分类网络。</li><li><strong>第3节 - 实现细节（训练）：</strong>这是该文章最长的部分，详细描述了训练R-CNN网络所涉及的步骤。</li><li><strong>第4节 - 实现细节（推理）：</strong>在本节中，我们将描述在推理过程涉及的步骤，使用训练好的R-CNN网络来识别有希望的区域并对这些区域中的对象进行分类。</li><li><strong>附录：</strong>这里我们将介绍R-CNN运行过程中一些常用算法的细节，如非极大值抑制和Resnet50架构的细节。</li></ul><h2 id="图像预处理"><a href="#图像预处理" class="headerlink" title="图像预处理"></a>图像预处理</h2><p>在将图片送入网络之前首先执行如下的网络预处理步骤。在训练和推理过程中，以下步骤必须一致。均值向量（大小$1\times3$，每一个图像通道一个均值）并非当前图像像素值的均值，而是针对所有训练和测试图片所设置的一个统一的初始值。图像预处理流程如下：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa46e9e0bbd7.png" alt="img"></p><p>默认的$targetSize$和$maxSize$为600和1000。</p><h2 id="网络组织"><a href="#网络组织" class="headerlink" title="网络组织"></a>网络组织</h2><p>R-CNN使用神经网络主要解决如下两个问题：</p><ul><li>识别输入图片中可能包含前景目标的区域（Region of Interest - RoI）</li><li>计算每一个RoI中的类别概率分布——例如，计算RoI中包含特定类别的目标的概率，在此基础上，可以选择具有最高概率的类别作为分类结果。</li></ul><p>R-CNN主要包含三种类型的神经网络：</p><ul><li>Head</li><li>Region Proposal Network (RPN)</li><li>Classification Network（分类网络）</li></ul><p>R-CNN使用预训练网络（例如ResNet）的前几层从输入图片中提取特征，这一做法由迁移学习理论作为支持（将同一个数据集上训练得到的网络用于不同的问题是可能的）。网络的前几层检测一些通用的特征（如，边、颜色块等在不同的问题中都具有较好的区分性的特征），而后几层学习到的更多是与特定问题相关的高层特征。在我们搭建的网路中，可以直接将后面几层移除或者在反向传播过程中对其参数进行微调。这些从预训练的网络的前几层迁移过来的层构成了”head”网络。</p><p><strong>由”head”网络产生的卷积特征图将被送入RPN网络中，RPN网络使用一系列的卷积层和全连接层产生可能存在前景目标的RoI区域。接着将使用这些RoI区域从“head”网络产生的特征图中裁剪出相应的特征图区域，称为“crop pooling”。由“crop pooling”得到的特征图区域将被送入分类网络，进而经过学习得到该RoI区域所包含的目标种类。</strong></p><p>另一方面，ResNet的权重也可以使用如下方式进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br></pre></td></tr></table></figure><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>下图分别展示了上述几种不同的网络模块，图中给出了每一层网络输出和输入特征的大小，这将有助于理解网络中的特征是如何进行转换的，$w$、$h$表示经过预处理后的输入图片的大小。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5a9ffec911c19.png" alt="img"></p><h2 id="实现细节：训练"><a href="#实现细节：训练" class="headerlink" title="实现细节：训练"></a>实现细节：训练</h2><p>在本节将详细介绍训练R-CNN所涉及到的步骤。一旦理解了训练的流程，理解推理过程将很容易，因为推理只用到了训练过程的一个子集。训练的目标是调整RPN、分类网络的权重以及微调Head（从预训练模型如Resnet中初始化）的权重。RPN网络的任务是产生RoIs区域，分类网络的任务是对每一个RoI给定一个类别分数。为了训练这些网络，我们需要得到相应的ground truths（即图片中所出现的目标的bounding boxes的坐标以及这些目标的类别）。这些信息已经由数据集的标定文件给出，这里有一些常用的通用数据集：</p><p><strong>第一个常用数据集：PASCAL VOC</strong>。VOC2007数据集有20类，有9963张图片，共有24640个标注。</p><ul><li><em>Person:</em> person</li><li><em>Animal:</em> bird, cat, cow, dog, horse, sheep</li><li><em>Vehicle:</em> aeroplane, bicycle, boat, bus, car, motorbike, train</li><li><em>Indoor:</em> bottle, chair, dining table, potted plant, sofa, tv/monitor</li></ul><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/person_06.jpg" alt="img"></p><p><strong>第二个常用数据集：COCO。</strong>该数据集包含超过200K个标记图片，共有90类。</p><p>本文章使用较小的Pascal VOC 2007数据集进行训练。R-CNN能够同时训练RPN网络和分类网络。</p><p>首先介绍两个常见概念：</p><p><strong>概念一：Bounding Boxes Regression Coefficients（也称为regression coefficients或regression targets）</strong>。R-CNN的目标之一就是产生与目标边界尽可能匹配的bounding boxes。R-CNN通过使用一组“regression coefficients”对给定的bounding boxes（给定左上角坐标、宽、高）进行调整来得到匹配的bounding boxes。回归系数由如下方式得出：</p><p>分别将目标（Target，Ground Truth）bounding boxes和原始（Origin，RPN）bounding boxes的左上角坐标表示为：$T_x, T_y, O_x, O_y$，width/height表示为$T_w, T_h, O_w, O_h$。那么“regression targets”（将原始bounding boxes转换为目标bounding boxes的函数系数）计算公式为：</p><script type="math/tex;mode=display">
t _ { x } = \frac { \left( T _ { x } - O _ { x } \right) } { O _ { w } } , t _ { y } = \frac { \left( T _ { y } - O _ { y } \right) } { O _ { h } } , t _ { w } = \log \left( \frac { T _ { w } } { O _ { w } } \right) , t _ { h } = \log \left( \frac { T _ { h } } { O _ { h } } \right)</script><p>上述方程是可逆的，给定“regression coefficients ”以及原始bounding boxes的左上角坐标、宽和高，便可以计算得到目标bounding boxes的左上角坐标、宽和高。<strong>注意，“regression coefficients ”对于没有剪切的仿射变换是不变的。</strong>这一性质在计算分类损失时非常重要。因为目标的“regression coefficients ”在原始长宽比例的图片上计算得到的，而分类网络输出的“regression coefficients ”是在正方形特征图（1：1的长宽比）上经过ROI pooling后计算得到的。当我们在下面讨论分类损失时，这一点将变得更加清楚。</p><p><strong>下面在学习Bounding Boxes Regression Coefficients时，对应的真实标记为上面的$t_x,t_y,t_w,t_h$。所以下面要介绍的回归网络分支的输出就是每个anchor的平移量和变换尺度$t_x,t_y,t_w,t_h$，显然即可用来修正Anchor位置了。简单来说，可以理解Bounding Boxes Regression Coefficients为每个anchor的平移量和变换尺度$t_x,t_y,t_w,t_h$</strong>。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5bac3708478ce.png" alt="img"></p><p>我们记学习到Bounding Boxes Regression Coefficients记作$d_x(A),d_y(A),d_w(A),d_h(A)$。当输入的原始bounding boxes与目标bounding boxes相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当原始bounding boxes与目标bounding boxes比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p><p>线性回归就是给定输入的特征向量$X,$ 学习一组参数$W$, 使得经过线性回归后的值跟真实值$Y$非常接近，即$Y=WX$。对于该问题，输入$X$是cnn feature map，定义为$\phi$，同时输入$t_x,t_y,t_w,t_h$，输出是$d_x(A),d_y(A),d_w(A),d_h(A)$这四种变换。目标函数为：</p><script type="math/tex;mode=display">
d_*(A)=W^T_* \cdot \phi(A)</script><p>其中$\phi(A)$是对应anchor的feature map组成的特征向量，$W_<em>$是需要学习的参数，$d_</em>(A)$是得到的预测值（$<em>$表示$x,y,w,h$，也就是每一个变换对应一个上述目标函数）。为了让预测$d_</em>(A)$和真实值$t_*$差距最小，设计$L1$损失函数：</p><script type="math/tex;mode=display">
Loss = \sum_i^N \mid t_*^i - W_*^T \cdot \phi(A^i) \mid</script><p>函数的优化目标为：</p><script type="math/tex;mode=display">
\hat W_* = argmin_{W_*} \sum_i^n \mid t_*^i - W_*^T \cdot \phi(A^i) \mid + \lambda \mid \mid W_* \mid \mid</script><blockquote><p>为了方便描述，这里以$L1$损失为例介绍，而真实情况中一般使用$smooth_L1$损失。</p></blockquote><p><strong>总结一下：</strong>对于训练bouding box regression网络回归分支，输入是cnn feature $\phi$，监督信号是Anchor与GT的差距$t_x,t_y,t_w,t_h$，即训练目标是：输入$\phi$的情况下使网络输出与监督信号尽可能接近。那么当bouding box regression工作时，当输出$\phi$时，回归网络分支的输出就是每个Anchor的平移量和变换尺度$t_x,t_y,t_w,t_h$，显然即可用来修正Anchor的位置了。</p><p><strong>概念2：Intersection over Union (IoU) Overlap</strong>。我们需要一种度量给定的bounding boxes与另一个bounding boxes的接近程度（与所使用的度量bounding boxes大小的单位无关）。这一度量方法需要满足：当两个bounding boxes完全重合时，结果为1；当两个bounding boxes完全不重合时，结果为0；同时要易于计算。常用的度量方式为交并比，计算方式如下所示：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa6f44a0a9c7-1568628026330.png" alt="img" style="zoom:67%"></p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa6f476535f7.png" alt="img"></p><p>有了这些初步准备，现在让我们深入了解训练R-CNN的实施细节。<strong>在实现中</strong>，R-CNN被分解为几个层，如下所示。一个封装了一系列逻辑步骤（例如，数据的流通）的层以及其他步骤，诸如bounding boxes重合度的比较、non-maxima suppression（非最大值抑制）等其他步骤。</p><p>作者的原图在<a href="R-CNNs详解/img_5aa0053323ac5.png">这里</a>可以找到，但是个人觉得这个图不太正确，并且不够清晰，下面是我绘制的个人理解图，如果有错误，还请大家能够指出来。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/image-20200331201944487.png" alt="image-20200331201944487"></p><ul><li><strong>Anchor Generation Layer：</strong> 这一层生成固定数量的“anchors”（边界框），首先生成9个不同比例和宽高比的anchors，然后在输入图像的均匀间隔网格点上复制这些“anchors”。即这些选中的格子上都包含9个不同尺度和宽高比的anchors。</li><li><strong>Proposal Layer：</strong>根据bounding box regression coefficients对anchors进行变换得到变换后的anchors。然后以每一个anchor属于前景目标的分数为依据，对anchors进行非极大抑制，以对anchors的数目进行进一步的微调。</li><li><strong>Anchor Target Layer：</strong> Anchor Target Layer的目的是产生一些好的anchors以及相对应的前景/背景类标、target regression coefficients，以用于RPN网络的训练。该层的输出仅用于训练RPN网络，不用于分类层的训练。给定由anchors Generation Layer生成的anchors，anchor target layer将对好的前景/背景anchors进行识别。其中与真实box的重合度高于设定阈值的anchors将被识别为好的前景anchors；与真实box的重合度低于某一阈值的anchors将被识别为背景anchors。anchor target layer也产生一系列bounding box的回归数，即每个目标anchor到最近的bounding box的距离。这些回归值只对前景框有意义，因为背景框没有“closest bounding box”的概念。</li><li><strong>RPN Loss：</strong> 在训练RPN网络时，RPN Loss是被最小化的。由以下两部分组成：<ul><li>RPN所产生的bounding boxes中被正确划分为前景/背景的比例</li><li>预测的bounding boxe与目标bounding boxes之间的“regression coefficients”距离</li></ul></li><li><strong>Proposal Target Layer：</strong> Proposal Target Layer的目标是删除一些由proposal layer产生的anchors，生成特定于类别的bounding box regression targets，用于训练分类层产生良好的类标签和回归目标。</li><li><strong>ROI Pooling Layer：</strong> 实现空间转换网络，给定proposal target layer产生的proposal区域的bounding box坐标，对输入特征图进行采样。这些坐标通常不会位于整数边界上，因此需要基于插值的采样。</li><li><strong>Classification Layer：</strong> 分类层以ROI Pooling layer的输出作为输入，将其通过一系列的卷积层后，送入两层全连接层，第一层对于每一个region proposal分别产生类别概率分布；第二层产生一组类别相关的bounding box回归值。</li><li><strong>Classification Loss：</strong> 与RPN Loss相似，在训练分类层时会最小化Classification Loss。在反向传播的过程中，误差的梯度同样会流入RPN网络，所以对分类层进行训练时同样也会修改RPN网络的权重。分类误差由以下部分构成：<ul><li>由RPN产生的bounding boxes被正确分类的比例</li><li>预测与目标regression coefficients之间的距离</li></ul></li></ul><p>下面将详细介绍每一层。</p><h3 id="Anchor-Generation-Layer"><a href="#Anchor-Generation-Layer" class="headerlink" title="Anchor Generation Layer"></a>Anchor Generation Layer</h3><p>针对整幅输入图像，产生一系列的具有不同尺度和比例的bounding boxes，即anchor boxes。对于所有图片来说，产生的anchor boxes是一样的，例如，与图片的内容无关。Anchor boxes中的一些将包含有目标，而大多数不包含目标。RPN的目标则是识别好的anchor boxes（那些更有可能包含前景目标的anchor）以及产生bounding boxes regression coefficients（将anchors转换为更好的bounding boxes，更紧密的贴合前景目标）。</p><p>下图展示了如何生成这些anchor boxes。下图中，输入图像经过resize后，得到的尺寸为800x600，而均匀采样的间隔为16，等于网络总步长（Resnet下采样16倍）。因为Resnet下采样16倍，最终得到的特征图大小为$\frac{800}{16} \times \frac{600}{16}=1900$。所以也可以理解为在最终特征图的每个点上都设置9个anchors。</p><p>注：关于上面的anchors size，其实是根据检测图像设置的。在Python demo中，会把任意大小的输入图像reshape成800x600（记作M=800，N=600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa05d3ecef3e-1568626559915.png" alt="img"></p><p>那么为每个点生成的这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Head网络获得的feature maps，<strong>为每一个点都配备这9种anchors作为初始的检测框</strong>。这样做获得检测框很不准确，不用担心，<strong>后面还有2次bounding box regression可以修正检测框位置。</strong></p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/v2-c93db71cc8f4f4fd8cfb4ef2e2cef4f4_720w.jpg" alt="img"></p><p>解释一下上面这张图的数字。</p><ol><li>在原文中使用的是ZF model中，其Head网络中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li><li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？），同时256-d不变</li><li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anchor要分positive和negative，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4k coordinates</li><li>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中<strong>随机</strong>选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文有解释）</li></ol><p>注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。</p><h3 id="Region-Proposal-Layer"><a href="#Region-Proposal-Layer" class="headerlink" title="Region Proposal Layer"></a>Region Proposal Layer</h3><p>目标检测方法需要产生一组稀疏或稠密特征的“region proposal system”作为输入。R-CNN的第一个版本中使用selective search method产生region proposal。在Faster R-CNN中，基于滑窗的技术被用于产生一系列稠密的候选框。接着RPN依据区域包含目标的概率对region proposal进行评分。Region Proposal Layer有两个目的：</p><ol><li>从一系列的anchor中，识别出前景和背景目标。</li><li>通过使用一组“regression coefficients”，对anchor的坐标和宽高进行调整，以得到更为精确的anchors（使其更加匹配真正的目标）。</li></ol><p>Region Proposal Layer包含Region Proposal Network以及三个Layer：Proposal Layer、Anchor Target Layer 和Proposal Target Layer。具体细节描述如下：</p><h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h4><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa0695484e3e.png" alt="img"></p><p>RPN的输入为head网络产生的特征图，将该特征图经过一层卷积层（rpn_net）进行处理，后接ReLU激活函数。激活后的特征图再分别通过两个并行的大小为1x1的卷积层，分别<strong>产生前景/背景类别分数和相应的boundding boxes regression coefficients。</strong>Head networks的步长与产生anchors时的步长长度相匹配。所以anchor boxes的数量与region proposal network产生的信息是一一对应的（anchor boxes数量=类别得分数量=bounding box regression coefficients数量=$\frac{w}{16}\times\frac{h}{16}\times9$）。</p><h3 id="Proposal-Layer"><a href="#Proposal-Layer" class="headerlink" title="Proposal Layer"></a>Proposal Layer</h3><p>Proposal Layer以anchor generation layer产生的anchors为输入，依据各anchors包含前景目标的概率对anchors进行non-maximum suppression，以达到减少anchors数目的目的。同时，依据由RPN产生的regression coefficients对anchor boxes进行变换得到对应的transformed bounding boxes。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa5766d53b63.png" alt="img"></p><h3 id="Anchor-Target-Layer"><a href="#Anchor-Target-Layer" class="headerlink" title="Anchor Target Layer"></a>Anchor Target Layer</h3><p>anchor target layer的目的是选择能够用来训练RPN网络的好的anchors：</p><ul><li>更好地区别前景和背景区域</li><li>对于前景boxes产生好的bounding box regression coefficients</li></ul><p>在进一步讲解Anchor Target Layer之前，我们将首先了解RPN损失是如何计算的。</p><h4 id="Calculating-RPN-Loss"><a href="#Calculating-RPN-Loss" class="headerlink" title="Calculating RPN Loss"></a>Calculating RPN Loss</h4><p>我们已经知道RPN层的目标是产生好的bounding boxes。为了达到这一目标，RPN必须学会从给定的anchor boxes中区分出前景和背景，并计算regression coefficients以对前景anchor boxes的位置、宽和高进行修正，使其更好地匹配前景目标。RPN损失正是以这种方式使得网络学到更好的行为。</p><p>RPN损失可以看作分类损失和bounding box regression损失之和。分类损失使用交叉熵损失对未被正确分类的boxes进行惩罚，回归损失使用真实边框regression coefficients（使用与前景anchor boxes最为匹配的ground truth boxes计算得到）与预测的regression coefficients（由RPN网络结构中的rpn_bbox_pred_net给出）之间的距离函数计算得出。</p><script type="math/tex;mode=display">
RPN Loss = \text{Classification Loss} + \text{Bounding Box Regression Loss}</script><p><strong>分类损失：</strong></p><script type="math/tex;mode=display">
cross\_entropy(predicted\_class, actual\_class)</script><p><strong>Bounding Box Regression Loss：</strong></p><script type="math/tex;mode=display">
L_{loc} = \sum_{u \in {\text{all foreground anchors}}}l_u</script><p><strong>将所有前景anchor的回归损失相加。计算背景anchor的回归损失没有意义，</strong>因为不存在ground truths与背景anchor匹配。回归损失计算如下：</p><script type="math/tex;mode=display">
l_u = \sum_{i \in {x,y,w,h}}smooth_{L1} \left( u_i \left( predicted \right) - u_i \left( target \right) \right)</script><p>这展示了给定前景anchor如何计算回归损失。我们首先计算由预测（由RPN网络得到）与目标（由离anchor box最近的真实box得到）regression coefficients的差别。这有四个元素：对应左上角的坐标和bounding box的宽度/高度。$L1$平滑函数定义如下：</p><script type="math/tex;mode=display">
smooth_{L1} = \begin{cases} \frac{\sigma^2x^2}{2} & \lVert x \rVert < \frac{1}{\sigma^2} \\ \lVert x \rVert - \frac{0.5}{\sigma^2} & otherwise\end{cases}</script><p>上式中$\sigma$任意选定（代码中设定为3）。注意，在Python实现中，表示前景目标的mask array（bbox_inside_weights）以向量运算的形式来计算损失，以避免for-if循环。</p><p>因而，为了计算损失，我们需要计算如下量：</p><ol><li>真实类标（前景或背景）以及anchor boxes的分数</li><li>前景anchor boxes的target regression coefficients</li></ol><p>下面将<strong>展示anchor target layer是如何计算得出上述量的</strong>。首先选择出在图片内部的anchor boxes；接着，通过计算图像内的所有anchor boxes与所有ground truth boxes的IoU来选出好的前景boxes。基于重合度，以下两类boxes被标记为<strong>前景</strong>：</p><ol><li>typeA：对于每一个ground truth box，与其有着最大IoU重合度的前景boxes（可能有多个）</li><li>typeB：与一些ground truth boxes的最大IoU重合度超过了设定阈值的anchor box</li></ol><p>下图展示了这些boxes：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa13d4d911d3.png" alt="img"></p><p>注意，只有与一些ground truth boxes的重合度超过给定的阈值的anchor boxes才会被选定为前景框。这一做法的目的是为了避免RPN进行一些无谓的学习任务（学习一些与最匹配的ground truth boxes相距较远的anchor boxes）。同样，重合度低于负样本阈值的anchor boxes将被归类为背景框。<strong>并不是所有未被划分为前景框的目标都会被划分为背景框。这些既未被划分为前景框，也未被划分未背景框的anchor boxes是不被关心的。</strong>在计算RPN损失时，不计算这些框。</p><p>有两个参数与我们最终得到的背景目标和前景目标的总数有关，分别是前景目标和背景目标的总数、前景目标占两者的比例。如果通过测试的前景目标的数目超过了阈值，我们便将这些超过部分的前景框随机标记为“don’t care”，同样，背景框中超出的部分将被标记为“don’t care”。</p><p>接着，我们计算前景框和与其相对应有最大重合度的ground truths boxes的bounding box regression coefficients。这一步是很容易的，依据给定公式进行计算即可。</p><p>这就结束了我们对anchor target layer的讨论。总结该层的输入、输出如下：</p><p><strong>参数：</strong></p><ul><li>TRAIN.RPN_POSITIVE_OVERLAP：确定anchor box是否是好的前景框 (Default: 0.7)</li><li>TRAIN.RPN_NEGATIVE_OVERLAP：如果一个anchor与最匹配的ground truth box的重合度小于该阈值，则将其标定为背景。阈值大于RPN_NEGATIVE_OVERLAP但小于RPN_POSITIVE_OVERLAP的框被标记为“don’t care”。(Default: 0.3)</li><li>TRAIN.RPN_BATCHSIZE：前景和背景anchors的总数。 (default: 256)</li><li>TRAIN.RPN_FG_FRACTION：前景框所占比例 (default: 0.5)。如果前景框的数目大于<br>TRAIN.RPN_BATCHSIZE×TRAIN.RPN_FG_FRACTION，超出的部分将被标记为 (随机选择索引) “don’t care”。</li></ul><p><strong>输入：</strong></p><ul><li>RPN Network Outputs (predicted foreground/background class labels, regression coefficients)</li><li>Anchor boxes (由anchor generation layer生成)</li><li>Ground truth boxes</li></ul><p><strong>输出：</strong></p><ul><li>好的foreground/background boxes以及相关类标</li><li>Target regression coefficients</li></ul><p>其它几层，proposal target layer、RoI pooling layer、classfication layer用于产生计算分类损失所需的信息。正如我们介绍anchor target layer那样，将首先介绍计算分类层损失所需的信息。</p><h3 id="Calculating-Classification-Layer-Loss"><a href="#Calculating-Classification-Layer-Loss" class="headerlink" title="Calculating Classification Layer Loss"></a>Calculating Classification Layer Loss</h3><p>与RPN损失类似，分类层损失可以分为两部分——分类损失和bounding box regression loss。</p><script type="math/tex;mode=display">
\text{Classification Layer Loss} = \text{Classification Loss} + \text{Bounding Box Regression Loss}</script><p>RPN层与分类层的主要不同在于：RPN解决两分类问题——前景和背景，分类层需要处理所有的目标类别（外加背景类）。</p><p>分类损失等于真实类别与预测类别之间的交叉熵损失，计算方式如下：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa1cd250f265-1.png" alt="img"></p><p>这里的bounding boxes regression loss的计算方式与RPN中的计算方式类似，除了这里的regression coefficients是与类别相关的。网络针对每一个目标种类计算regression coefficients。很明显，<strong>target regression coefficients只对正确的类别有效，正确的类别即与给定的anchor box有着最大的重合度的ground truth bounding box的类别。</strong>在计算回归系数损失时，使用为每一个anchor box标定正确类别的mask array。不正确的类别的regression coefficients则被忽略。mask矩阵的使用避免了复杂的for循环，而采用矩阵乘法的形式，更为高效。</p><p>在计算classification layer loss需要以下数值：</p><ul><li>预测出的类标及bounding box regression coefficients（由分类网络输出）。</li><li>每一个anchor box的类别</li><li>Target bounding box regression coefficients</li></ul><p>现在让我们看看这些数值如何在proposal target and classification layers中被计算。</p><h3 id="Proposal-Target-Layer"><a href="#Proposal-Target-Layer" class="headerlink" title="Proposal Target Layer"></a>Proposal Target Layer</h3><p>Proposal Target Layer的作用是从proposal layer输出的RoIs中选择出有可能存在目标的RoIs。这些RoIs将被用于对head layer产生的特征图进行裁剪池化(crop pooling)，裁剪得到的小的特征图将被传入网络的剩余部分(head_to_tail) ，进而计算得出预测的类别分数和box regression coefficients。</p><p><strong>与anchor target layer类似，选择好的proposals（与gt boxes有着最大重合度的）传入classification layer是很重要的。否则，classification layer所学习的将是无望的学习任务。</strong></p><p>传入proposal layer target层的是由proposal layer计算得出的RoIs。利用每个ROI与所有ground truth boxes的最大重叠度，将RoI划分为前景或背景ROIs。最大重合度超过给定阈值（TRAIN.FG_THRESH, default: 0.5）的将被设定为前景目标。最大重合度位于阈值区间 TRAIN.BG_THRESH_LO 和 TRAIN.BG_THRESH_HI (default 0.1, 0.5)中的将被设定为背景目标。这是一个称为“hard negative mining”的方法，该方法将识别起来较为困难的背景样本传入分类层。</p><p>该方法的目标是使得前景和背景region的数目保持为常数。为了避免出现背景目标过少的情况，该算法通过随机重复一些背景目标的索引来补足batch中的差额。</p><p>接着， bounding box target regression误差将在每一个RoI及其最匹配的gt之间计算得到（包括背景RoI，因为对于这些背景目标，同样存在与其重叠的gt）。这些regression targets将被扩充至所有的类别，如下图所示：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa32302afc0b-1.png" alt="img"></p><p>图中的bbox_inside_weights是一个掩模矩阵，只有前景ROI所对应的正确的类别处的值为1，其他值为0，背景ROIs所对应的值为0。因为在计算classification layer loss的bounding box regression部分时，只有前景区域的regression coefficients才被包括在内，背景目标的边框回归损失不计。但在计算classification损失部分时，要计算背景目标，因为背景目标所属的类别为0。</p><p><strong>输入：</strong></p><ol><li>由proposal layer产生的RoIs</li><li>ground truth信息</li></ol><p><strong>输出：</strong></p><ol><li>选择出的符合重合度标准的前景和背景ROIs</li><li>RoIs的类别相关的target regression coefficients</li></ol><p><strong>参数：</strong></p><ol><li>TRAIN.FG_THRESH:（default: 0.5）用于选择前景ROIs。与gt的最大重合度高于该阈值的RoI被设定为前景</li><li>TRAIN.BG_THRESH_HI: (default 0.5)</li><li>TRAIN.BG_THRESH_LO: (default 0.1)这俩个参数用于选择背景ROIs，最大重合度位于BG_THRESH_HI 和BG_THRESH_LO 区间内的RoI被设定为背景目标</li><li>TRAIN.BATCH_SIZE: (default 128)前景和被选中的背景boxes的总数</li><li>TRAIN.FG_FRACTION: (default 0.25)前景目标的数目不能超过BATCH_SIZE*FG_FRACTION</li></ol><h3 id="Crop-Pooling"><a href="#Crop-Pooling" class="headerlink" title="Crop Pooling"></a>Crop Pooling</h3><p>Proposal target layer产生可能的ROIs，提供相关目标类别标签和regression coefficients以便训练分类器时使用。下一步是使用这些ROIs从由head network产生的特征图中抽取对应的特征。这些抽取得到的特征图将被用于剩下的网络层，进一步产生每一个ROI的目标类别概率分布和regression coefficients。Crop Pooling的作用就是从卷积特征图中抽取ROIs对应的特征图。</p><p>Crop Pooling的核心内容描述于“Spatial Transformation Networks” <a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/#ITEM-1455-7" target="_blank" rel="noopener">(Anon. 2016)</a><a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">*</a>。<strong>目标是在输入特征图上使用一个扭曲函数(warping function，2x3的仿射变换矩阵）来输出warped 特征图</strong>，如下图所示。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa402baba3a1-1.png" alt="img"></p><p> crop pooling主要有两个步骤：</p><ol><li>对一个集合的目标坐标（target coordinates）应用仿射变换，得到一个方格的源坐标（source coordinates）。公式为：$\left[ \begin{array} { c } { x _ { i } ^ { s } } \\ { y _ { i } ^ { s } } \end{array} \right] = \left[ \begin{array} { l l l } { \theta _ { 11 } } &amp; { \theta _ { 12 } } &amp; { \theta _ { 13 } } \\ { \theta _ { 21 } } &amp; { \theta _ { 22 } } &amp; { \theta _ { 23 } } \end{array} \right] \left[ \begin{array} { l } { x _ { i } ^ { t } } \\ { y _ { i } ^ { t } } \\ { 1 } \end{array} \right]$，其中，$x _ { i } ^ { s } , y _ { i } ^ { s } , x _ { i } ^ { t } , y _ { i } ^ { t }$都是width/height归一化的坐标，因而$- 1 \leq x _ { i } ^ { s } , y _ { i } ^ { s } , x _ { i } ^ { t } , y _ { i } ^ { t } \leq 1$。</li><li>在第二步中，在源坐标处采样输入(源)映射，以生成输出(目标)映射。每一对$\left( x _ { i } ^ { s } , y _ { i } ^ { s } \right)$坐标对应输入特征图中的一个空间位置，接着使用采样核（如双线性采样核）对该位置进行采样，最终得到输出图像上相对应的特定位置的值。</li></ol><p>Spatial transformation中所描述的采样方法是可微的，因而损失的梯度可以直接反向传播回输入特征图和采样的方格坐标。</p><p>幸运的是，pytroch提供了裁剪池化对应的API，API中的两个函数分别对应上述两个步骤。<code>torch.nn.functional.affine_grid</code>以仿射变换矩阵为输入，输出一个集合的采样坐标，<code>torch.nn.functional.grid_sample</code>对这些坐标处的格子进行采样。Pytorch自动进行误差梯度的反向传播。</p><p>为了使用裁剪池化，我们需要进行如下操作：</p><ol><li>将RoI的坐标除以head network的stride长度。Proposal target layer产生的是原始输入图像上ROIs的坐标（800x600）。为了将这些坐标转换至”head”网络所输出的特征图上，我们必须将这些坐标除以stride（本次实现中为16）。</li><li>为了使用API，我们需要提供仿射变换矩阵，仿射变换矩阵的计算方法如下所示。</li><li>我们同样需要知道目标特征图中$x、y$两个维度上的点的个数。这一参数由<code>cfg.POOLING_SIZE</code>(default 7）提供。因而，在进行裁剪池化时，非正方形RoI将被用于从卷积特征图上裁剪出扭曲为大小恒定的正方形特征图。所以必须执行裁剪池化操作（实现上述扭曲），因为接下来的卷积和全连接层要求输入的特征图大小是固定的。</li></ol><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa4255fdacb6.png" alt="img"></p><blockquote><p>我们所需要的是未经过扭曲的变换，由于我们已经知道了源坐标（对预测得出的RoI的坐标进行归一化得到）和目标坐标的值（池化得到的特征图的对角坐标是固定的，如上图所示），使用简单的矩阵运算即可得出仿射变换矩阵。由于每一个RoI的坐标都是不同的，所以对于每一个RoI都需要单独计算一个仿射变换矩阵。</p></blockquote><h3 id="Classification-Layer"><a href="#Classification-Layer" class="headerlink" title="Classification Layer"></a>Classification Layer</h3><p>crop pooling layer以proposal target layer输出的RoIs和head network输出的特征图为输入，输出固定大小的输出特征图。该输出特征图将被传入后接最大池化（用于改变特征图的空间大小）的四层ResNet。结果（代码中为”fc7”）是，对于每一个RoI将得到一个一维的特征向量。流程如下，下面$n$表示从Proposal target layer得到的ROIs的数量：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa55c81eac0a.png" alt="img"></p><p>所产生的一维特征将被传入另个全连接层bbox_pred_net和cls_score_net。针对每一个bounding boxes，cls_score_net layer将产生类别分数（可以使用softmax将其转换为概率）。bbox_pred_net layer将产生类别相关的bounding box regression coefficients，该regression coefficients将和由proposal target layer产生的原始的bounding box系数一起产生最后的bounding boxes。流程如下：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa55c97f3287.png" alt="img"></p><p>有必要回顾一下两个bounding box regression coefficients的差别。第一个由RPN网络产生，第二个由分类网络产生。</p><p>第一个用于引导RPN网络产生好的前景bounding boxes（与目标边界贴合地更加紧）。target regression coefficients（将ROI框与其最接近匹配的真实bounding box对齐所需的coefficients）由anchor target layer生成。很难清楚地解释学习过程是如何发生地，但我可以假设，RPN卷积网络和全连接网络会将由神经网络产生的不同的图像特征图转换为尽可能好的目标边界框。我们将会在前向推理章节介绍regression coefficients的使用方法。</p><p>第二个bounding box coefficients由classification layer层产生。这些回归系数是类别相关的，即，对于每一个RoI，会针对每一个类别分别产生一个边框回归系数。这些regression coefficients的target regression coefficients是由proposal target layer产生的。要注意到，classification layer在由仿射变化作用于head网络输出所产生的正方形特征图上操作。然而，因为回归系数对于无裁剪的仿射变换具有不变性，由proposal target layer产生的target regression coefficients才可以和classification layer产生的regression coefficients进行比较，并作为一个有效的学习信号。</p><p>要注意的是，在训练classification layer时，误差的梯度同样反向传播至RPN层。这时由于在进行crop pooling时所用的RoI坐标正是网络的输出本身，因为他们是将RPN网络生成regression coefficient施加在anchor boxes上的结果。在反向传播过程中，误差的梯度将通过crop pooling传播至RPN。计算和实现这些梯度运算是存在一定的难度的，庆幸的是这些运算pytorch中已经给出了实现。</p><h2 id="实现细节：推理"><a href="#实现细节：推理" class="headerlink" title="实现细节：推理"></a>实现细节：推理</h2><p>推理过程如下：</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa70ff399c57.png" alt="img"></p><p>anchor target and proposal target layers没有使用到。RPN网络被假定已经学习了如何将anchor boxes分为前景和背景boxes，以及产生好的bounding box coefficients。 proposal layer将bounding box coefficients用于最好的前top个anchor boxes，同时使用NMS避免大量的重叠。为了更加清晰，这些步骤的输出如下所示。结果框被发送到分类层，在那里生成类分数和类特定的bounding box regression coefficients。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa580271bea6.png" alt="img"></p><p>红色方框显示的是得分最高的6个anchors。绿框表示应用RPN网络计算的regression parameters后的anchor boxes。绿色方框似乎更贴合底层对象。注意，应用regression parameters后，矩形仍然是矩形，即，没有剪切。还要注意矩形之间的明显重叠。这种冗余通过应用非极大值抑制来解决。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa5809cc7206.png" alt="img"></p><p>红色方框显示NMS之前的前5个边框，绿色方框显示NMS之后的前5个边框。通过抑制重叠的方框，其他方框(得分列表中较低的方框)有机会显示出来。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa581709aa82.png" alt="img"></p><p>从最终的分类分数数组(dim: n, 21；n表示有n个前景目标)中，我们选择一个前景目标对应的列，比如car。然后，我们选择该数组中与最大得分对应的行。这一行对应的建议最有可能是一辆车。现在让这一行的索引为car_score_max_idx。最后的边界框坐标数组（应用回归系数后）为bboxes (dim: n,21*4)。从这个数组中，我们选择与car_score_max_idx对应的行。我们期望在该行中，与car列对应的bounding boxes应该比其他bounding boxes（对应错误的目标类）更适合测试图像中的car。情况确实如此。红色框对应于original proposal box，蓝色框是car类计算出来的bounding box，白色框对应于其他（不正确的）前景类。可以看出，蓝色的盒子比其他盒子更适合实际的车。</p><p>为了显示最终的分类结果，我们应用了另一轮的NMS，<strong>并对类分数应用了目标检测阈值</strong>。然后，我们绘制所有满足检测阈值的ROIs对应的转换边界框。结果如下所示。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa5827c1d42c.png" alt="img"></p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="ResNet50-Network结构"><a href="#ResNet50-Network结构" class="headerlink" title="ResNet50 Network结构"></a>ResNet50 Network结构</h3><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa59c8da4c4b.png" alt="img"></p><h3 id="Non-Maximum-Suppression-NMS"><a href="#Non-Maximum-Suppression-NMS" class="headerlink" title="Non-Maximum Suppression (NMS)"></a>Non-Maximum Suppression (NMS)</h3><p>非最大抑制是一种技术，用于通过消除重叠超过阈值的方框来减少候选方框的数量。首先根据一些标准 (通常是右下角的y坐标) 对这些框进行排序。然后我们遍历方框列表，并抑制那些与考虑中的方框IoU重叠度超过阈值的方框。根据y坐标对这些盒子进行排序，结果会在一组重叠的盒子中保留最低的那个盒子。这可能并不总是理想的结果。在R-CNN中使用的NMS是根据前景得分来排序的。这将导致在一组重叠的框中保留得分最高的框。</p><p>下图显示了这两种方法的区别。黑色的数字是每个框的前景得分。右边的图像显示了对左边的图像应用NMS的结果。第一个图使用标准的NMS(方框按右下角的y坐标排列)。这将导致框中保留较低的分数。第二个图使用了修改过的NMS(按前景得分对框进行排序)。这将导致保留前景得分最高的框，这是更可取的。在这两种情况下，盒之间的重叠被认为高于NMS重叠阈值的。</p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa7c84451f81.png" alt="img"></p><p><img src="/DeepLearningApplications/目标检测/R-CNNs详解/img_5aa7c828703ab.png" alt="img"></p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><p>本文的VISIO图对应的原件可以在<a href="R-CNNs详解_Attachments/R-CNN示意图.vsdx">这里</a>找到。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a><br><a href="http://activepony.com/shen-du-xue-xi/mu-biao-jian-ce/faster-r-cnn-ju-ti-shi-xian-xiang-jie/" target="_blank" rel="noopener">Faster R-CNN具体实现详解</a><br><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">一文读懂Faster RCNN</a><br><a href="https://www.cnblogs.com/vincent1997/p/10889171.html" target="_blank" rel="noopener">物体检测丨Faster R-CNN详解</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/" title="R-CNNs详解">https://www.zdaiot.com/DeepLearningApplications/目标检测/R-CNNs详解/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/目标检测/" rel="tag"><i class="fa fa-tag"></i> 目标检测</a><a href="/tags/R-CNN/" rel="tag"><i class="fa fa-tag"></i> R-CNN</a></div><div class="post-nav"><div class="post-nav-item"><a href="/ImageProcessing/UV的概念及作用/" rel="prev" title="UV的概念及作用"><i class="fa fa-chevron-left"></i> UV的概念及作用</a></div><div class="post-nav-item"> <a href="/DeepLearningApplications/目标检测/实时目标检测：YOLO、YOLOv2以及YOLOv3/" rel="next" title="实时目标检测：YOLO、YOLOv2以及YOLOv3">实时目标检测：YOLO、YOLOv2以及YOLOv3<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#文章组织"><span class="nav-number">1.</span> <span class="nav-text">文章组织</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像预处理"><span class="nav-number">2.</span> <span class="nav-text">图像预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#网络组织"><span class="nav-number">3.</span> <span class="nav-text">网络组织</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#网络结构"><span class="nav-number">3.1.</span> <span class="nav-text">网络结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现细节：训练"><span class="nav-number">4.</span> <span class="nav-text">实现细节：训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anchor-Generation-Layer"><span class="nav-number">4.1.</span> <span class="nav-text">Anchor Generation Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Region-Proposal-Layer"><span class="nav-number">4.2.</span> <span class="nav-text">Region Proposal Layer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Region-Proposal-Network"><span class="nav-number">4.2.1.</span> <span class="nav-text">Region Proposal Network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Proposal-Layer"><span class="nav-number">4.3.</span> <span class="nav-text">Proposal Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Anchor-Target-Layer"><span class="nav-number">4.4.</span> <span class="nav-text">Anchor Target Layer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Calculating-RPN-Loss"><span class="nav-number">4.4.1.</span> <span class="nav-text">Calculating RPN Loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Calculating-Classification-Layer-Loss"><span class="nav-number">4.5.</span> <span class="nav-text">Calculating Classification Layer Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Proposal-Target-Layer"><span class="nav-number">4.6.</span> <span class="nav-text">Proposal Target Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Crop-Pooling"><span class="nav-number">4.7.</span> <span class="nav-text">Crop Pooling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification-Layer"><span class="nav-number">4.8.</span> <span class="nav-text">Classification Layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现细节：推理"><span class="nav-number">5.</span> <span class="nav-text">实现细节：推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#附录"><span class="nav-number">6.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet50-Network结构"><span class="nav-number">6.1.</span> <span class="nav-text">ResNet50 Network结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-Maximum-Suppression-NMS"><span class="nav-number">6.2.</span> <span class="nav-text">Non-Maximum Suppression (NMS)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#附件"><span class="nav-number">7.</span> <span class="nav-text">附件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">8.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">324</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">56</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">381</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.2m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">34:04</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '97cb4b8b0edf88d1b6fc5895fae76fb3',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>