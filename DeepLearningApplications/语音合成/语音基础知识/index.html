<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="开新坑了，最近在攻击语音合成系统，现在学习一下语音基础知识。 辅音和元音的区别 辅音发音时，气流在通过咽头、口腔的过程中， 要受到某部位的阻碍；元音发音时，气流在咽头、 口腔不受阻碍。这是元音和辅音最主要的区别。 辅音发音时，发音器官成阻的部位特别紧张； 元音发音时发音器官各部位保持均衡的紧张状态。 辅音发音时，气流较强；元音发音时，气流较 弱。 辅音发音时，声带不一定振动，声音一般不响亮；元音发"><meta name="keywords" content="语音合成,语音基础知识"><meta property="og:type" content="article"><meta property="og:title" content="语音基础知识"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="开新坑了，最近在攻击语音合成系统，现在学习一下语音基础知识。 辅音和元音的区别 辅音发音时，气流在通过咽头、口腔的过程中， 要受到某部位的阻碍；元音发音时，气流在咽头、 口腔不受阻碍。这是元音和辅音最主要的区别。 辅音发音时，发音器官成阻的部位特别紧张； 元音发音时发音器官各部位保持均衡的紧张状态。 辅音发音时，气流较强；元音发音时，气流较 弱。 辅音发音时，声带不一定振动，声音一般不响亮；元音发"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/69e0842536f5cb23b16e05e2cc8402f5_720w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/7301a5be8a003cdaa272a5eba362d43a_1440w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/fd6436356cdd4647bfabff165d24df1c_1440w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/5c20e251d868c7d535ab9c4e4e8a5a16_1440w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/791ea703a54bac444deacdce9f039cb1_1440w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/517c32cad639c6f66721060bf2f3aa9e_1440w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/25ccd5ecb60a5dcd0acf414e67b12594_1440w.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/6169076_1625140337.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/v2-0b6dcfd913268a433bf3abaee8948cd1_b.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/v2-624ccef5c445fe5b2502d65de1ee38bd_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/v2-4cb311bf47a663f26f2f78ff4b59cb78_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/v2-8aaf99cbab0e57fb43597277da4f716f_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/v2-ab5db002d1d5126c1b5bf3e0a20b7af8_r.jpg"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224530922-479480973.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224541923-1780314401.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224554397-162344585.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224622095-1486888859.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224736710-1682561520.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224852822-1848315135.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224902529-1572460338.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224927630-2116898656.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224944430-837617222.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221225011597-1988899505.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/watermark.png"><meta property="og:updated_time" content="2022-06-04T07:22:03.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="语音基础知识"><meta name="twitter:description" content="开新坑了，最近在攻击语音合成系统，现在学习一下语音基础知识。 辅音和元音的区别 辅音发音时，气流在通过咽头、口腔的过程中， 要受到某部位的阻碍；元音发音时，气流在咽头、 口腔不受阻碍。这是元音和辅音最主要的区别。 辅音发音时，发音器官成阻的部位特别紧张； 元音发音时发音器官各部位保持均衡的紧张状态。 辅音发音时，气流较强；元音发音时，气流较 弱。 辅音发音时，声带不一定振动，声音一般不响亮；元音发"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/69e0842536f5cb23b16e05e2cc8402f5_720w.jpg"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>语音基础知识 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 语音基础知识<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/语音合成/语音基础知识.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-06-04 15:22:03" itemprop="dateCreated datePublished" datetime="2022-06-04T15:22:03+08:00">2022-06-04</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/语音合成/" itemprop="url" rel="index"><span itemprop="name">语音合成</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>8.2k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>开新坑了，最近在攻击语音合成系统，现在学习一下语音基础知识。</p><h2 id="辅音和元音的区别"><a href="#辅音和元音的区别" class="headerlink" title="辅音和元音的区别"></a>辅音和元音的区别</h2><ul><li>辅音发音时，气流在通过咽头、口腔的过程中， 要受到某部位的阻碍；元音发音时，气流在咽头、 口腔不受阻碍。这是元音和辅音最主要的区别。</li><li>辅音发音时，发音器官成阻的部位特别紧张； 元音发音时发音器官各部位保持均衡的紧张状态。</li><li>辅音发音时，气流较强；元音发音时，气流较 弱。</li><li>辅音发音时，声带不一定振动，声音一般不响亮；元音发音时，声带振动，声音比辅音响亮。</li></ul><blockquote><p>一般只有元音（一些介于元音辅音中间分类不明的音暂不讨论）才会有共振峰，而元音的音质由声道的形状决定，而声道的形状又通过发音的动作来塑造（articulatory+movements）。</p></blockquote><h2 id="清音和浊音"><a href="#清音和浊音" class="headerlink" title="清音和浊音"></a>清音和浊音</h2><ul><li>清音：声带不振动</li><li>浊音：声带振动而发音</li><li>元音都是浊音、辅音有清音也有浊音。</li></ul><h2 id="波形、频谱和语谱（声谱）"><a href="#波形、频谱和语谱（声谱）" class="headerlink" title="波形、频谱和语谱（声谱）"></a>波形、频谱和语谱（声谱）</h2><p>以下内容主要来源于<a href="https://www.zhihu.com/question/27126800/answer/35376174" target="_blank" rel="noopener">不同元音辅音在声音频谱的表现是什么样子？ - 王赟 Maigo的回答 - 知乎</a>。</p><h3 id="波形"><a href="#波形" class="headerlink" title="波形"></a>波形</h3><p>声音最直接的表示方式是波形，英文叫waveform。另外两种表示方式（频谱和语谱图）下文再说。波形的横轴是时间（所以波形也叫声音的时域表示），纵轴的含义并不重要，可以理解成位移（声带或者耳机膜的位置）或者压强。</p><p>当横轴的分辨率不高的时候，语音的波形看起来就是像你贴的图中一样，呈现一个个的三角形。这些三角形的轮廓叫作波形的<strong>包络（envelope）</strong>。包络的大小代表了声音的响度。一般来说，每一个音节会对应着一个三角形，因为一般地每个音节含有一个元音，而元音比辅音听起来响亮。但例外也是有的，比如：1) 像/s/这样的音，持续时间比较长，也会形成一个三角形；2) 爆破音（尤其是送气爆破音，如/p/）可能会在瞬时聚集大量能量，在波形的包络上就体现为一个脉冲。</p><p>下面这张图中上方的子图，是读单词pass /pæs/的录音。它的横坐标已经被拉开了一些，但其实这个波形是由两个“三角形”组成的。0.05秒处那个小突起是爆破音/p/，0.05秒到0.3秒是元音/æ/，0.3到0.58秒是辅音/s/。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/69e0842536f5cb23b16e05e2cc8402f5_720w.jpg" data-rawwidth="675" data-rawheight="328" class="origin_image zh-lightbox-thumb" width="675" data-original="https://pica.zhimg.com/69e0842536f5cb23b16e05e2cc8402f5_r.jpg?source=1940ef5c"></p><p>如果你把横轴的分辨率调高，比如只观察0.02s秒甚至更短时间内的波形，你就可以看到波形的精细结构（fine structure），像上图的下面两个子图。波形的精细结构可能呈现两种情况：一种是有周期性的，比如左边那段波形（图中显示了两个周期多一点），这种波形一般是元音或者辅音中的鼻音、浊擦音以及/l/、/r/等；另一种是乱的，比如右边那段波形，这种波形一般是辅音中的清擦音。辅音中的爆破音，则往往表现为一小段静音加一个脉冲（如pass开头的/p/）。</p><h3 id="频谱"><a href="#频谱" class="headerlink" title="频谱"></a>频谱</h3><p>看完了声音的<strong>时域</strong>表示，我们再来看它的<strong>频域</strong>表示——<strong>频谱（spectrum）</strong>。它是由一小段波形做傅里叶变换（Fourier transform）之后取模得到的。注意，必须是一小段波形，太长了弄出来的东西（比如你贴的右边的图）就没意义了！这样的一小段波形（通常在0.02~0.05s这样的数量级）称为<strong>一帧（frame）</strong>。下面是读的pass的波形中，以0.17s和0.4s为中心截取0.04s波形经傅里叶变换得到的频谱。频谱的横轴是频率；录音的采样率用的是16000 Hz，频谱的频率范围也是0 ~ 16000 Hz。但由于0 ~ 8000 Hz和8000 ~ 16000 Hz的频谱是对称的，所以一般只画0 ~ 8000 Hz的部分。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/7301a5be8a003cdaa272a5eba362d43a_1440w.jpg" alt="img"></p><p>频谱跟波形一样，也有包络和精细结构。你把横轴压缩，看到的就是包络；把横轴拉开，看到的就是精细结构。我上面这两张图使得二者都能看到。</p><p>第一个频谱是元音/æ/的频谱，可以看到它的精细结构是有周期性的，每隔108 Hz出现一个峰。从这儿也可以看出来，语音不是一个单独的频率，而是由许多频率的简谐振动叠加而成的。第一个峰叫<strong>基音</strong>，其余的峰叫<strong>泛音</strong>。第一个峰的频率（也是相邻峰的间隔）叫作<strong>基频（fundamental frequency），也叫音高（pitch）</strong>，常记作$f_0$。有时说“一个音的频率”，就是特指基频。基频的倒数叫<strong>基音周期</strong>。你再看看上面元音/æ/的波形的周期，大约是0.009 s，跟基频108 Hz吻合。频谱上每个峰的高度是不一样的，这些峰的高度之比决定了<strong>音色（timbre）</strong>。不过对于语音来说，一般没有必要精确地描写每个峰的高度，而是用<strong>“共振峰”（formant）</strong>来描述音色。共振峰指的是包络的峰。在我这个图中，忽略精细结构，可以看到0~1000 Hz形成一个比较宽的峰，1800 Hz附近形成一个比较窄的峰。共振峰的频率一般用$f_1$、$f_2$等等来表示。上图中，$f_1$是多少很难精确地读出来，但$f_2 \approx 1800Hz$。当然，在2800 Hz、3800 Hz、5000 Hz处还有第三、四、五共振峰，但它们与第一、二共振峰相比就弱了许多。除了元音以外，辅音中的鼻音、浊擦音以及/l/、/r/等也具有这种频谱，可以讨论基频和共振峰频率（不过浊擦音一般不讨论共振峰频率）。</p><p>第二个频谱是辅音/s/的频谱。可以看出它的精细结构是没有周期性的，所以就无所谓基频。一般也不提这种频谱的共振峰。清擦音的频谱一般都是这样。</p><h3 id="语谱（声谱）"><a href="#语谱（声谱）" class="headerlink" title="语谱（声谱）"></a>语谱（声谱）</h3><p>我们最后来看一下声音的第三种表示方式——<strong>语谱图</strong>（<strong>spectrogram</strong>）。上面说过，频谱只能表示一小段声音。那么，如果我想观察一整段语音信号的频域特性，要怎么办呢？我们可以把一整段语音信号截成许多帧，把它们各自的频谱“竖”起来（即用纵轴表示频率），用颜色的深浅来代替频谱强度，再把所有帧的频谱横向并排起来（即用横轴表示时间），就得到了<strong>语谱图</strong>，它可以称为声音的<strong>时频域</strong>表示。下面我就偷懒，不用Matlab自己画语谱图，而用Cool Edit绘制上面“pass”的语谱图，如下：</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/fd6436356cdd4647bfabff165d24df1c_1440w.jpg" alt="img"></p><p>注意横轴是时间，纵轴是频率，颜色越亮代表强度越大。可以观察一下0.17s和0.4s处，是不是跟我上面画的频谱相似？然后再试着从这张语谱图上读出元音/æ/的第二共振峰频率。</p><p>语谱图的好处是可以直观地看出共振峰频率的变化。我上面读的“pass”中只有一个单元音，如果有双元音就会非常明显了。比如下面这张我读的“eye” /aɪ/，可以非常明显地看出在元音从/a/向/ɪ/过渡的阶段（0.2 ~ 0.25s），$f_1$在降低，而$f_2$在升高。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/5c20e251d868c7d535ab9c4e4e8a5a16_1440w.jpg" alt="img"></p><p>元音与共振峰的关系已经研究得比较透彻了，简单地说：</p><p>1) 开口度越大, $ f_{1} $ 越高;<br>2) 舌位越靠前, $ f_{2} $ 越高;<br>3) 不圆唇元音的 $ f_{3} $ 比圆唇元音高。例如, $ / \mathrm{a} / $ 是开、后、不圆唇元音, 所以 $ f_{1} $ 高, $ f_{2} $ 低, $ f_{3} $ 高；/y/（即汉语拼音的ü）是闭、前、圆 唇元音, 所以 $ f_{1} $ 低, $ f_{2} $ 高, $ f_{3} $ 低。也许大家见过下图那样的元音图Q (vowel chart) , 我把 $ f_{1} $ 和 $ f_{2} $ 的变化方向标 $ Q $ 上去。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/791ea703a54bac444deacdce9f039cb1_1440w.jpg" alt="img" style="zoom:75%"></p><p>$f_3$最明显的体现其实是在英语的辅音/r/中，例如下面我读的erase /ɪ’reɪz/的语谱图，可以看到辅音/r/处（0.19s左右）$f_3$明显低，把$f_2$也压下去了。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/517c32cad639c6f66721060bf2f3aa9e_1440w.jpg" alt="img"></p><p>清擦音可以根据能量集中的频段来分辨。下面是我读的/f/, /θ/, /s/, /ʃ/的语谱图。浊擦音会在清擦音的基础上有周期性的精细结构。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/25ccd5ecb60a5dcd0acf414e67b12594_1440w.jpg" alt="img"></p><p>爆破音的爆破时间很短，在语谱图上一般较难分辨。</p><p>“两个音之间的音是什么样子”，就要分情况讨论了。</p><p>1) 如果是两个元音，那么可以在元音图上找到两个元音，取它们连线的中点。这对应着把$f_1$、$f_2$分别取平均。<br>2) 如果是两个清擦音，那么可以把它们的频谱取平均，这样听起来应该是个四不像（后来我做了实验，结果见这里：<a href="http://maigoakisame.github.io/fricative-mix/" target="_blank" rel="noopener">Mixture of Unvoiced Fricatives</a>）。<br>3) /t/和/ʃ/属于不同类型的辅音，很难定义它们“之间”是什么东西。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/DeepLearningApplications/语音合成/语音基础知识/6169076_1625140337.jpg" alt="img" style="zoom:33%"></p><h2 id="语音基本概念"><a href="#语音基本概念" class="headerlink" title="语音基本概念"></a>语音基本概念</h2><p>以下内容主要来源于<a href="https://zhuanlan.zhihu.com/p/510550742" target="_blank" rel="noopener">语音基础知识（附相关实现代码）</a>。在不理解的地方我会加上自己的注释。</p><p>声波通过空气传播，被麦克风接收，通过<strong>采样、量化、编码</strong>转换为离散的数字信号，即波形文件。<strong>音量、音高和音色是声音的基本属性。</strong></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-0b6dcfd913268a433bf3abaee8948cd1_b.jpg" alt></p><p><strong>1）采样</strong>：原始的语音信号是连续的模拟信号，需要对语音进行采样，转化为时间轴上离散的数据。</p><p><strong>采样后</strong>，模拟信号被等间隔地取样，这时信号在时间上就不再连续了，但在幅度上还是连续的。经过采样处理之后，模拟信号变成了离散时间信号。</p><p><strong>采样频率</strong>是指一秒钟内对声音信号的采样次数，采样频率越高声音的还原就越真实越自然。</p><p>在当今的主流采集卡上，采样频率一般共分为 22.05KHz、44.1KHz、48KHz 三个等级，22.05KHz 只能达到 FM 广播的声音品质，44.1KHz 则是理论上的 CD 音质界限（人耳一般可以感觉到 20-20K Hz 的声音，根据香农采样定理，采样频率应该不小于最高频率的两倍，所以 40KHz 是能够将人耳听见的声音进行很好的还原的一个数值，于是 CD 公司把采样率定为 44.1KHz），48KHz 则更加精确一些。</p><p>对于高于 48KHz 的采样频率人耳已无法辨别出来了，所以在电脑上没有多少使用价值。</p><p><strong>2）量化</strong>：进行分级量化，将信号采样的幅度划分成几个区段，把落在某区段的采样到的样品值归成一类，并给出相应的量化值。根据量化间隔是否均匀划分，又分为均匀量化和非均匀量化。</p><p><strong>均匀量化</strong>的特点为 “大信号的信噪比大，小信号的信噪比小”。缺点为 “为了保证信噪比要求，编码位数必须足够大，但是这样导致了信道利用率低，如果减少编码位数又不能满足信噪比的要求”（根据信噪比公式，编码位数越大，信噪比越大，通信质量越好）。</p><p>通常对语音信号采用<strong>非均匀量化</strong>，基本方法是对大信号使用大的量化间隔，对小信号使用小的量化间隔。由于小信号时量化间隔变小，其相应的量化噪声功率也减小（根据量化噪声功率公式），从而使小信号时的量化信噪比增大，改善了小信号时的信噪比。</p><p><strong>量化后</strong>，信号不仅在时间上不再连续，在幅度上也不连续了。经过量化处理之后，离散时间信号变成了数字信号。</p><p><strong>3）编码</strong>：在量化之后信号已经变成了数字信号，需要将数字信号编码成二进制。<strong>“</strong>CD 质量<strong>”</strong> 的语音采用 44100 个样本每秒的采样率，每个样本 16 比特，这个 16 比特就是编码的位数。</p><p>采样，量化，编码的过程称为 A/D（从模拟信号到数字信号）转换，如上图 1 所示。</p><p>补充<strong>比特率</strong>的概念：比特率是指每秒传送的比特(bit)数。单位为 bps(Bit Per Second)，比特率越高，传送的数据越大，音质越好。以电话为例，每秒3000点取样，每个样本是7比特，那么电话的比特率是21000。而CD是每秒44100点取样，两个声道，每个取样是13位PCM编码，所以CD的比特率是$44100<em>2</em>13=1146600$，也就是说CD每秒的数据量大约是144KB，而一张CD的容量是74分等于4440秒，就是639360KB＝640MB。</p><h3 id="能量"><a href="#能量" class="headerlink" title="能量"></a>能量</h3><p><strong>音频的能量通常指的是时域上每帧的能量，幅度的平方。</strong>在简单的语音活动检测（Voice Activity Detection，VAD）中，直接利用能量特征：能量大的音频片段是语音，能量小的音频片段是非语音（包括噪音、静音段等）。这种 VAD 的局限性比较大，正确率也不高，对噪音非常敏感。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_file, sr=None, frame_len=<span class="number">512</span>, n_fft=None, win_step=<span class="number">2</span> / <span class="number">3</span>, window=<span class="string">"hamming"</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化</span></span><br><span class="line"><span class="string">        :param input_file: 输入音频文件</span></span><br><span class="line"><span class="string">        :param sr: 所输入音频文件的采样率，默认为None</span></span><br><span class="line"><span class="string">        :param frame_len: 帧长，默认512个采样点(32ms,16kHz),与窗长相同</span></span><br><span class="line"><span class="string">        :param n_fft: FFT窗口的长度，默认与窗长相同</span></span><br><span class="line"><span class="string">        :param win_step: 窗移，默认移动2/3，512*2/3=341个采样点(21ms,16kHz)</span></span><br><span class="line"><span class="string">        :param window: 窗类型，默认汉明窗</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.input_file = input_file</span><br><span class="line">        self.frame_len = frame_len  <span class="comment"># 帧长，单位采样点数</span></span><br><span class="line">        self.wave_data, self.sr = librosa.load(self.input_file, sr=sr)</span><br><span class="line">        self.window_len = frame_len  <span class="comment"># 窗长512</span></span><br><span class="line">        <span class="keyword">if</span> n_fft <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.fft_num = self.window_len  <span class="comment"># 设置NFFT点数与窗长相等</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fft_num = n_fft</span><br><span class="line">        self.win_step = win_step</span><br><span class="line">        self.hop_length = round(self.window_len * win_step)  <span class="comment"># 重叠部分采样点数设置为窗长的1/3（1/3~1/2）,即帧移(窗移)2/3</span></span><br><span class="line">        self.window = window</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">energy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        每帧内所有采样点的幅值平方和作为能量值</span></span><br><span class="line"><span class="string">        :return: 每帧能量值，np.ndarray[shape=(1，n_frames), dtype=float64]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mag_spec = np.abs(librosa.stft(self.wave_data, n_fft=self.fft_num, hop_length=self.hop_length,</span><br><span class="line">                                       win_length=self.frame_len, window=self.window))</span><br><span class="line">        pow_spec = np.square(mag_spec) <span class="comment"># [frequency, time (n_frames)]</span></span><br><span class="line">        energy = np.sum(pow_spec, axis=<span class="number">0</span>) <span class="comment"># [n_frames]</span></span><br><span class="line">        energy = np.where(energy == <span class="number">0</span>, np.finfo(np.float64).eps, energy)  <span class="comment"># 避免能量值为0，防止后续取log出错(eps是取非负的最小值), 即np.finfo(np.float64).eps = 2.220446049250313e-16</span></span><br><span class="line">        <span class="keyword">return</span> energy</span><br></pre></td></tr></table></figure><h3 id="短时能量"><a href="#短时能量" class="headerlink" title="短时能量"></a>短时能量</h3><p>短时能量体现的是信号在不同时刻的强弱程度。设第 n 帧语音信号的短时能量用$E_n$表示，则其计算公式为：</p><script type="math/tex;mode=display">
E_n = \sum_{m=0}^{M-1}x_n^2(m)</script><p>上式中，M 为帧长，$x_n(m)$为该帧中的样本点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">short_time_energy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算语音短时能量：每一帧中所有语音信号的平方和</span></span><br><span class="line"><span class="string">        :return: 语音短时能量列表(值范围0-每帧归一化后能量平方和，这里帧长512，则最大值为512)，</span></span><br><span class="line"><span class="string">        np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=float64]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        energy = []  <span class="comment"># 语音短时能量列表</span></span><br><span class="line">        energy_sum_per_frame = <span class="number">0</span>  <span class="comment"># 每一帧短时能量累加和</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.wave_data)):  <span class="comment"># 遍历每一个采样点数据</span></span><br><span class="line">            energy_sum_per_frame += self.wave_data[i] ** <span class="number">2</span>  <span class="comment"># 求语音信号能量的平方和</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % self.frame_len == <span class="number">0</span>:  <span class="comment"># 一帧所有采样点遍历结束</span></span><br><span class="line">                energy.append(energy_sum_per_frame)  <span class="comment"># 加入短时能量列表</span></span><br><span class="line">                energy_sum_per_frame = <span class="number">0</span>  <span class="comment"># 清空和</span></span><br><span class="line">            <span class="keyword">elif</span> i == len(self.wave_data) - <span class="number">1</span>:  <span class="comment"># 不满一帧，最后一个采样点</span></span><br><span class="line">                energy.append(energy_sum_per_frame)  <span class="comment"># 将最后一帧短时能量加入列表</span></span><br><span class="line">        energy = np.array(energy)</span><br><span class="line">        energy = np.where(energy == <span class="number">0</span>, np.finfo(np.float64).eps, energy)  <span class="comment"># 避免能量值为0，防止后续取log出错(eps是取非负的最小值)</span></span><br><span class="line">        <span class="keyword">return</span> energy</span><br></pre></td></tr></table></figure><h3 id="声强和声强级（声压和声压级）"><a href="#声强和声强级（声压和声压级）" class="headerlink" title="声强和声强级（声压和声压级）"></a>声强和声强级（声压和声压级）</h3><p>单位时间内通过垂直于声波传播方向的单位面积的平均声能，称作声强，声强用 P 表示，单位为 “瓦 / 平米”。实验研究表明，人对声音的强弱感觉并不是与声强成正比，而是与其对数成正比，所以一般<strong>声强用声强级来表示</strong>：</p><script type="math/tex;mode=display">
L = 10 \text{log}\left(\frac{P}{P'} \right)</script><p>其中，P 为声强， $P’=10e^{-12}$单位（$w/m^2$）称为基本声强，声强级的常用单位是分贝 (dB)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intensity</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算声音强度，用声压级表示：每帧语音在空气中的声压级Sound Pressure Level(SPL)，单位dB</span></span><br><span class="line"><span class="string">        公式：20*lg(P/Pref)，P为声压（Pa），Pref为参考压力(听力阈值压力)，一般为1.0*10-6 Pa</span></span><br><span class="line"><span class="string">        这里P认定为声音的幅值：求得每帧所有幅值平方和均值，除以Pref平方，再取10倍lg</span></span><br><span class="line"><span class="string">        :return: 每帧声压级，dB，np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=float64]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p0 = <span class="number">1.0e-6</span>  <span class="comment"># 听觉阈限压力auditory threshold pressure: 2.0*10-5 Pa</span></span><br><span class="line">        e = self.short_time_energy()</span><br><span class="line">        spl = <span class="number">10</span> * np.log10(<span class="number">1</span> / (np.power(p0, <span class="number">2</span>) * self.frame_len) * e)</span><br><span class="line">        <span class="keyword">return</span> spl</span><br></pre></td></tr></table></figure><h3 id="过零率"><a href="#过零率" class="headerlink" title="过零率"></a>过零率</h3><p>过零率体现的是信号过零点的次数，体现的是频率特性。</p><script type="math/tex;mode=display">
Z_{n}=\sum_{n=0}^{N-1} \sum_{m=0}^{M-1}\left|\operatorname{sgn}\left(x_{n}(m)\right)-\operatorname{sgn}\left(x_{n}(m-1)\right)\right|</script><p>其中，N 表示帧数，M 表示每一帧中的样本点个数，sgn 为符号函数，即：</p><script type="math/tex;mode=display">
\operatorname{sgn}=\left\{\begin{array}{c}
1, x \geq 0 \\
-1, x<0
\end{array}\right.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_crossing_rate</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算语音短时过零率：单位时间(每帧)穿过横轴（过零）的次数</span></span><br><span class="line"><span class="string">        :return: 每帧过零率次数列表，np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=uint32]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        zcr = []  <span class="comment"># 语音短时过零率列表</span></span><br><span class="line">        counting_sum_per_frame = <span class="number">0</span>  <span class="comment"># 每一帧过零次数累加和，即过零率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.wave_data)):  <span class="comment"># 遍历每一个采样点数据</span></span><br><span class="line">            <span class="keyword">if</span> i % self.frame_len == <span class="number">0</span>:  <span class="comment"># 开头采样点无过零，因此每一帧的第一个采样点跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> self.wave_data[i] * self.wave_data[i - <span class="number">1</span>] &lt; <span class="number">0</span>:  <span class="comment"># 相邻两个采样点乘积小于0，则说明穿过横轴</span></span><br><span class="line">                counting_sum_per_frame += <span class="number">1</span>  <span class="comment"># 过零次数加一</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % self.frame_len == <span class="number">0</span>:  <span class="comment"># 一帧所有采样点遍历结束</span></span><br><span class="line">                zcr.append(counting_sum_per_frame)  <span class="comment"># 加入短时过零率列表</span></span><br><span class="line">                counting_sum_per_frame = <span class="number">0</span>  <span class="comment"># 清空和</span></span><br><span class="line">            <span class="keyword">elif</span> i == len(self.wave_data) - <span class="number">1</span>:  <span class="comment"># 不满一帧，最后一个采样点</span></span><br><span class="line">                zcr.append(counting_sum_per_frame)  <span class="comment"># 将最后一帧短时过零率加入列表</span></span><br><span class="line">        <span class="keyword">return</span> np.array(zcr, dtype=np.uint32)</span><br></pre></td></tr></table></figure><h3 id="基频和基音周期"><a href="#基频和基音周期" class="headerlink" title="基频和基音周期"></a>基频和基音周期</h3><p>基音周期反映了声门相邻两次开闭之间的时间间隔，基频（fundamental frequency， F0）则是基音周期的倒数，对应着声带振动的频率，代表声音的音高，声带振动越快，基频越高。如图 2 所示，蓝色箭头指向的就是基频的位置，决定音高。它是语音激励源的一个重要特征，比如可以通过基频区分性别。一般来说，成年男性基频在 100-250Hz 左右，成年女性基频在 150-350Hz 左右，女声的音高一般比男声稍高。 人类可感知声音的频率大致在 20-20000Hz 之间，人类对于基频的感知遵循对数律，也就是说，人们会感觉 100Hz 到 200Hz 的差距，与 200Hz 到 400Hz 的差距相同。因此，<strong>音高常常用基频的对数来表示。</strong></p><blockquote><p>这部分的详细介绍可以看前面的<code>波形、频谱和语谱（声谱）</code>小节。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-624ccef5c445fe5b2502d65de1ee38bd_r.jpg" alt></p><h3 id="音高"><a href="#音高" class="headerlink" title="音高"></a>音高</h3><p>音高（pitch）是由声音的基频决定的，音高和基频常常混用。可以这样认为，<strong>音高（pitch）是稀疏离散化的基频（F0）</strong>。由规律振动产生的声音一般都会有基频，比如语音中的元音和浊辅音；也有些声音没有基频，比如人类通过口腔挤压气流的清辅音。在汉语中，元音有 a/e/i/o/u，浊辅音有 y/w/v，其余音素比如 b/p/q/x 等均为清辅音，在发音时，可以通过触摸喉咙感受和判断发音所属音素的种类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pitch</span><span class="params">(self, ts_mag=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取每帧音高，即基频，这里应该包括基频和各次谐波，最小的为基频（一次谐波），其他的依次为二次、三次...谐波</span></span><br><span class="line"><span class="string">        各次谐波等于基频的对应倍数，因此基频也等于各次谐波除以对应的次数，精确些等于所有谐波之和除以谐波次数之和</span></span><br><span class="line"><span class="string">        :param ts_mag: 幅值倍乘因子阈值，&gt;0，大于np.average(np.nonzero(magnitudes)) * ts_mag则认为对应的音高有效,默认0.25</span></span><br><span class="line"><span class="string">        :return: 每帧基频及其对应峰的幅值(&gt;0)，</span></span><br><span class="line"><span class="string">                 np.ndarray[shape=(1 + n_fft/2，n_frames), dtype=float32]，（257，全部采样点数/(512*2/3)+1）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mag_spec = np.abs(librosa.stft(self.wave_data, n_fft=self.fft_num, hop_length=self.hop_length,</span><br><span class="line">                                       win_length=self.frame_len, window=self.window))</span><br><span class="line">        pitches, magnitudes = librosa.piptrack(S=mag_spec, sr=self.sr, threshold=<span class="number">1.0</span>, ref=np.mean,</span><br><span class="line">                                               fmin=<span class="number">50</span>, fmax=<span class="number">500</span>)  <span class="comment"># 人类正常说话基频最大可能范围50-500Hz</span></span><br><span class="line">        ts = np.average(magnitudes[np.nonzero(magnitudes)]) * ts_mag</span><br><span class="line">        pit_likely = pitches</span><br><span class="line">        mag_likely = magnitudes</span><br><span class="line">        pit_likely[magnitudes &lt; ts] = <span class="number">0</span></span><br><span class="line">        mag_likely[magnitudes &lt; ts] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> pit_likely, mag_likely</span><br><span class="line">pitches, mags = self.pitch()  <span class="comment"># 获取每帧基频</span></span><br><span class="line">f0_likely = []  <span class="comment"># 可能的基频F0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(pitches.shape[<span class="number">1</span>]):  <span class="comment"># 按列遍历非0最小值，作为每帧可能的F0</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        f0_likely.append(np.min(pitches[np.nonzero(pitches[:, i]), i]))</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        f0_likely.append(np.nan)  <span class="comment"># 当一列，即一帧全为0时，赋值最小值为nan</span></span><br><span class="line">f0_all = np.array(f0_likely)</span><br></pre></td></tr></table></figure><h3 id="共振峰"><a href="#共振峰" class="headerlink" title="共振峰"></a>共振峰</h3><p>声门处的准周期激励进入声道时会引起共振特性，产生一组共振频率，这一组共振频率称为共振峰频率或简称共振峰。共振峰包含在语音的频谱包络中，频谱极大值就是共振峰。<strong>频率最低的共振峰称为第一共振峰，对应的频率也称作基频，决定语音的 F0，其它的共振峰统称为谐波</strong>，如上图 2 所示，蓝色箭头指向频谱的第一共振峰，也就是基频的位置，决定音高；而绿框则是其它共振峰，统称为谐波。谐波是基频对应的整数次频率成分，由声带发声带动空气共振形成的，对应着声音三要素的音色。谐波的位置，相邻的距离共同形成了音色特征。谐波之间距离近听起来则偏厚粗，之间距离远听起来偏清澈。在男声变女声的时候，除了基频的移动，还需要调整谐波间的包络，距离等，否则将会丢失音色信息。</p><h3 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h3><p>为了有一个直观的图来解释上述的理论，可以把语音波形、短时能量、声强级、过零率、音高绘制在一张图上，如下图 3 所示：</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-4cb311bf47a663f26f2f78ff4b59cb78_r.jpg" style="zoom:67%"></p><h2 id="语音信号的预处理操作"><a href="#语音信号的预处理操作" class="headerlink" title="语音信号的预处理操作"></a>语音信号的预处理操作</h2><p>以下内容主要来源于<a href="https://zhuanlan.zhihu.com/p/510550742" target="_blank" rel="noopener">语音基础知识（附相关实现代码）</a>。在不理解的地方我会加上自己的注释。</p><p>在进行语音特征（如 MFCC、频谱图、声谱图等）提取之前一般要进行语音信号的预处理操作，主要包括：预加重、分帧、加窗。</p><h3 id="预加重"><a href="#预加重" class="headerlink" title="预加重"></a>预加重</h3><p>语音经过说话人的口唇辐射发出，受到唇端辐射抑制，高频能量明显降低。一般来说，当语音信号的频率提高两倍时，其功率谱的幅度下降约 6dB，即语音信号的高频部分受到的抑制影响较大。比如像元音等一些因素的发音包含了较多的高频信号的成分，高频信号的丢失，可能会导致音素的共振峰并不明显，使得声学模型对这些音素的建模能力不强。预加重（pre-emphasis）是个一阶高通滤波器，可以提高信号高频部分的能量，给定时域输入信号$x[n]$，预加重之后信号为：</p><script type="math/tex;mode=display">
x'[n]=x[n]-a\times x[n-1]</script><p>其中，a 是预加重系数，一般取 0.97 或 0.95。如下图 4 所示，元音音素 /aa/ 原始的频谱图（左）和经过预加重之后的频谱图（右）。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-8aaf99cbab0e57fb43597277da4f716f_r.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preemphasis</span><span class="params">(y, coef=<span class="number">0.97</span>, zi=None, return_zf=False)</span>:</span></span><br><span class="line">    <span class="string">"""Pre-emphasize an audio signal with a first-order auto-regressive filter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y[n] -&gt; y[n] - coef * y[n-1]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y : np.ndarray</span></span><br><span class="line"><span class="string">        Audio signal</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    coef : positive number</span></span><br><span class="line"><span class="string">        Pre-emphasis coefficient.  Typical values of ``coef`` are between 0 and 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        At the limit ``coef=0``, the signal is unchanged.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        At ``coef=1``, the result is the first-order difference of the signal.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The default (0.97) matches the pre-emphasis filter used in the HTK</span></span><br><span class="line"><span class="string">        implementation of MFCCs [#]_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. [#] http://htk.eng.cam.ac.uk/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    zi : number</span></span><br><span class="line"><span class="string">        Initial filter state.  When making successive calls to non-overlapping</span></span><br><span class="line"><span class="string">        frames, this can be set to the ``zf`` returned from the previous call.</span></span><br><span class="line"><span class="string">        (See example below.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        By default ``zi`` is initialized as ``2*y[0] - y[1]``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return_zf : boolean</span></span><br><span class="line"><span class="string">        If ``True``, return the final filter state.</span></span><br><span class="line"><span class="string">        If ``False``, only return the pre-emphasized signal.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    y_out : np.ndarray</span></span><br><span class="line"><span class="string">        pre-emphasized signal</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    zf : number</span></span><br><span class="line"><span class="string">        if ``return_zf=True``, the final filter state is also returned</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    b = np.asarray([<span class="number">1.0</span>, -coef], dtype=y.dtype)</span><br><span class="line">    a = np.asarray([<span class="number">1.0</span>], dtype=y.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> zi <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Initialize the filter to implement linear extrapolation</span></span><br><span class="line">        zi = <span class="number">2</span> * y[..., <span class="number">0</span>] - y[..., <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    zi = np.atleast_1d(zi)</span><br><span class="line"></span><br><span class="line">    y_out, z_f = scipy.signal.lfilter(b, a, y, zi=np.asarray(zi, dtype=y.dtype))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> return_zf:</span><br><span class="line">        <span class="keyword">return</span> y_out, z_f</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wave_data, self.sr = librosa.load(input_file, sr=sr)  <span class="comment"># 音频全部采样点的归一化数组形式数据</span></span><br><span class="line">wave_data = preemphasis(wave_data, coef=preemph)  <span class="comment"># 预加重，系数0.97</span></span><br></pre></td></tr></table></figure><h3 id="分帧"><a href="#分帧" class="headerlink" title="分帧"></a>分帧</h3><p>语音信号是非平稳信号，考虑到发浊音时声带有规律振动，即基音频率在短时范围内时相对固定的，因此可以认为语音信号具有短时平稳特性，一般认为 10ms~50ms 的语音信号片段是一个准稳态过程。_短时分析_采用分帧方式，一般每帧帧长为 20ms 或 50ms。假设语音采样率为 16kHz，帧长为 20ms，则一帧有 16000×0.02=320 个样本点。</p><p>相邻两帧之间的基音有可能发生变化，如两个音节之间，或者声母向韵母过渡。为确保声学特征参数的平滑性，一般采用重叠取帧的方式，即相邻帧之间存在重叠部分。一般来说，帧长和帧移的比例为 1:4 或 1:5。</p><blockquote><p>短时分析：虽然语音信号具有时变特性，但是在一个短时间范围内（一般认为在 10-30ms）其特性基本保持相对稳定，即语音具有短时平稳性。所以任何语音信号的分析和处理必须建立在 “短时” 的基础上，即进行“短时分析”。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">framesig</span><span class="params">(sig,frame_len,frame_step)</span>:</span></span><br><span class="line">    <span class="string">"""Frame a signal into overlapping frames.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sig: the audio signal to frame.</span></span><br><span class="line"><span class="string">    :param frame_len: length of each frame measured in samples.</span></span><br><span class="line"><span class="string">    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.</span></span><br><span class="line"><span class="string">    :returns: an array of frames. Size is NUMFRAMES by frame_len.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    slen = len(sig)</span><br><span class="line">    frame_len = int(round_half_up(frame_len))</span><br><span class="line">    frame_step = int(round_half_up(frame_step))</span><br><span class="line">    <span class="keyword">if</span> slen &lt;= frame_len:</span><br><span class="line">        numframes = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        numframes = <span class="number">1</span> + int(math.ceil((<span class="number">1.0</span>*slen - frame_len)/frame_step))</span><br><span class="line"></span><br><span class="line">    padlen = int((numframes<span class="number">-1</span>)*frame_step + frame_len)</span><br><span class="line"></span><br><span class="line">    zeros = numpy.zeros((padlen - slen,))</span><br><span class="line">    padsignal = numpy.concatenate((sig,zeros))</span><br><span class="line"></span><br><span class="line">    indices = numpy.tile(numpy.arange(<span class="number">0</span>,frame_len),(numframes,<span class="number">1</span>)) + numpy.tile(numpy.arange(<span class="number">0</span>,numframes*frame_step,frame_step),(frame_len,<span class="number">1</span>)).T</span><br><span class="line">    indices = numpy.array(indices,dtype=numpy.int32)</span><br><span class="line">    frames = padsignal[indices]</span><br><span class="line">    <span class="keyword">return</span> frames</span><br><span class="line"></span><br><span class="line">frames = framesig(sig=sig, frame_len=<span class="number">0.030</span> * sample_rate, <span class="comment"># 取帧长为30ms</span></span><br><span class="line">                                          frame_step=<span class="number">0.006</span> * sample_rate, <span class="comment"># 取帧移为6ms</span></span><br><span class="line">                                          )</span><br></pre></td></tr></table></figure><h3 id="加窗"><a href="#加窗" class="headerlink" title="加窗"></a>加窗</h3><p>分帧相当于对语音信号加矩形窗（用矩形窗其实就是不加窗），矩形窗在时域上对信号进行截断，在边界处存在多个旁瓣，会发生频谱泄露。为了减少频谱泄露，通常对分帧之后的信号进行其它形式的加窗操作。常用的窗函数有：汉明（Hamming）窗、汉宁（Hanning）窗和布莱克曼（Blackman）窗等。 <strong>加窗主要是为了使时域信号似乎更好地满足 FFT 处理的周期性要求，减少泄漏（加窗不能消除泄漏，只能减少， 如下图 5 所示）。</strong></p><blockquote><p>什么是频谱泄露？</p><p>音频处理中，经常需要利用傅里叶变换将时域信号转换到频域，而一次快速傅里叶变换（FFT）只能处理有限长的时域信号，但语音信号通常是长的，所以需要将原始语音截断成一帧一帧长度的数据块。这个过程叫信号截断，也叫分帧。分完帧后再对每帧做 FFT，得到对应的频域信号。FFT 是离散傅里叶变换（DFT）的快速计算方式，而<strong>做 DFT 有一个先验条件：分帧得到的数据块必须是整数周期的信号，也即是每次截断得到的信号要求是周期主值序列。</strong>但做分帧时，很难满足周期截断，因此就会导致频谱泄露。一句话，频谱泄露就是分析结果中，出现了本来没有的频率分量。比如说，50Hz 的纯正弦波，本来只有一种频率分量，分析结果却包含了与 50Hz 频率相近的其它频率分量。</p><p>非周期的无限长序列，任意截取一段有限长的序列，都不能代表实际信号，分析结果当然与实际信号不一致！也就是会造成频谱泄露。而周期的无限长序列，假设截取的是正好一个或整数个信号周期的序列，这个有限长序列就可以代表原无限长序列，如果分析的方法得当的话，分析结果应该与实际信号一致！因此也就不会造成频谱泄露。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-ab5db002d1d5126c1b5bf3e0a20b7af8_r.jpg" alt></p><p>汉明窗的窗函数为: $ W_{\mathrm{ham}}[n]=0.54-0.46 \cos \left(\frac{2 \pi n}{N}-1\right) $；汉宁窗的窗函数为: $ W_{h a n}[n]=0.5\left[1-\cos \left(\frac{2 \pi n}{N}-1\right)\right] $ ，其中$n$介于0到$ \mathrm{N}-1 $ 之间，$ \mathrm{N} $ 是窗的长度。</p><p>加窗就是用一定的窗函数$ w(n) $来乘$ s(n) $， 从而形成加窗语音信号$s_{w}(n)=\mathrm{s}(\mathrm{n}) * w(\mathrm{n}) $。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">framesig</span><span class="params">(sig,frame_len,frame_step,winfunc=lambda x:numpy.ones<span class="params">(<span class="params">(x,)</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Frame a signal into overlapping frames.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sig: the audio signal to frame.</span></span><br><span class="line"><span class="string">    :param frame_len: length of each frame measured in samples.</span></span><br><span class="line"><span class="string">    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.</span></span><br><span class="line"><span class="string">    :param winfunc: the analysis window to apply to each frame. By default no window is applied.</span></span><br><span class="line"><span class="string">    :returns: an array of frames. Size is NUMFRAMES by frame_len.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    slen = len(sig)</span><br><span class="line">    frame_len = int(round_half_up(frame_len))</span><br><span class="line">    frame_step = int(round_half_up(frame_step))</span><br><span class="line">    <span class="keyword">if</span> slen &lt;= frame_len:</span><br><span class="line">        numframes = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        numframes = <span class="number">1</span> + int(math.ceil((<span class="number">1.0</span>*slen - frame_len)/frame_step))</span><br><span class="line"></span><br><span class="line">    padlen = int((numframes<span class="number">-1</span>)*frame_step + frame_len)</span><br><span class="line"></span><br><span class="line">    zeros = numpy.zeros((padlen - slen,))</span><br><span class="line">    padsignal = numpy.concatenate((sig,zeros))</span><br><span class="line"></span><br><span class="line">    indices = numpy.tile(numpy.arange(<span class="number">0</span>,frame_len),(numframes,<span class="number">1</span>)) + numpy.tile(numpy.arange(<span class="number">0</span>,numframes*frame_step,frame_step),(frame_len,<span class="number">1</span>)).T</span><br><span class="line">    indices = numpy.array(indices,dtype=numpy.int32)</span><br><span class="line">    frames = padsignal[indices]</span><br><span class="line">    <span class="comment"># 加窗操作</span></span><br><span class="line">    win = numpy.tile(winfunc(frame_len),(numframes,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> frames*win</span><br><span class="line"></span><br><span class="line">frames = framesig(sig=sig, frame_len=<span class="number">0.030</span> * sample_rate, <span class="comment"># 取帧长为30ms</span></span><br><span class="line">                  frame_step=<span class="number">0.006</span> * sample_rate, <span class="comment"># 取帧移为6ms</span></span><br><span class="line">                  winfunc=np.hamming</span><br><span class="line">                  )</span><br></pre></td></tr></table></figure><h2 id="语音声学特征介绍"><a href="#语音声学特征介绍" class="headerlink" title="语音声学特征介绍"></a>语音声学特征介绍</h2><p>以下内容主要来源于<a href="https://www.cnblogs.com/liaohuiqiang/p/10159429.html" target="_blank" rel="noopener">论文笔记：语音情感识别（四）语音特征之声谱图，log梅尔谱，MFCC，deltas</a></p><p>声音信号本是一维的时域信号，直观上很难看出频率变化规律。傅里叶变换可把它变到频域上，虽然可看出信号的频率分布，但是丢失了时域信息，无法看出频率分布随时间的变化。为了解决这个问题，很多时频分析手段应运而生，如短时傅里叶，小波，Wigner分布等都是常用的<strong>时频域分析方法</strong>。</p><h3 id="原始信号"><a href="#原始信号" class="headerlink" title="原始信号"></a>原始信号</h3><p>从音频文件中读取出来的原始语音信号通常称为 raw waveform，是一个一维数组，长度是由音频长度和采样率决定，比如采样率 Fs 为 16KHz，表示一秒钟内采样 16000 个点，这个时候如果音频长度是 10 秒，那么 raw waveform 中就有 160000 个值，<strong>值的大小通常表示的是振幅。</strong></p><h3 id="（线性）声谱图"><a href="#（线性）声谱图" class="headerlink" title="（线性）声谱图"></a>（线性）声谱图</h3><p>（1）对原始信号进行分帧加窗后，可以得到很多帧，对每一帧做 FFT（快速傅里叶变换），傅里叶变换的作用是把时域信号转为频域信号，把每一帧 FFT 后的频域信号（频谱图）在时间上堆叠起来就可以得到声谱图，其直观理解可以形象地表示为以下几个图，图源见<a href="http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf" target="_blank" rel="noopener">CMU 语音课程 slides</a>。</p><p>（2）有些论文提到的 DCT（离散傅里叶变换）和 STFT（短时傅里叶变换）其实是差不多的东西。STFT 就是对一系列加窗数据做 FFT。而 DCT 跟 FFT 的关系就是：FFT 是实现 DCT 的一种快速算法。</p><p>（3）FFT 有个参数 N，表示对多少个点做 FFT，如果一帧里面的点的个数小于 N 就会 zero-padding 到 N 的长度。对一帧信号做 FFT 后会得到 N 点的复数，这个点的模值就是该频率值下的幅度特性。每个点对应一个频率点，某一点 n（n 从 1 开始）表示的频率为$F_n = (n-1)*Fs/N$，第一个点（n=1，Fn 等于 0）表示直流信号，最后一个点 N 的下一个点（n=N+1，Fn=Fs 时，实际上这个点是不存在的）表示采样频率 Fs。</p><p>（4）FFT 后我们可以得到 N 个频点，频率间隔（也叫频率分辨率或）为 Fs / N，比如，采样频率为 16000，N 为 1600，那么 FFT 后就会得到 1600 个点，频率间隔为 10Hz，FFT 得到的 1600 个值的模可以表示 1600 个频点对应的振幅。因为 FFT 具有对称性，当 N 为偶数时取 N/2+1 个点，当 N 为奇数时，取 (N+1)/2 个点，比如 N 为 512 时最后会得到 257 个值。</p><p>（5）用 python_speech_feature 库时可以看到有三种声谱图，包括振幅谱，功率谱（有些资料称为能量谱，是一个意思，功率就是单位时间的能量），log 功率谱。振幅谱就是 fft 后取绝对值。功率谱就是在振幅谱的基础上平方然后除以 N。log 功率谱就是在功率谱的基础上取 10 倍 lg，然后减去最大值。得到声谱图矩阵后可以通过 matplotlib 来画图。</p><p>（6）常用的声谱图都是 STFT 得到的，另外也有用 CQT（constant-Q transform）得到的，为了区分，将它们分别称为 STFT 声谱图和 CQT 声谱图。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224530922-479480973.png" style="zoom:67%"></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224541923-1780314401.png" style="zoom:67%"></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224554397-162344585.png" style="zoom:67%"></p><h3 id="梅尔声谱图"><a href="#梅尔声谱图" class="headerlink" title="梅尔声谱图"></a>梅尔声谱图</h3><p>梅尔频谱的英文为Mel-spectrogram。</p><p>（1）人耳听到的声音高低和实际（Hz）频率不呈线性关系，用 Mel 频率更符合人耳的听觉特性（这正是用 Mel 声谱图的一个动机，由人耳听力系统启发），即在 1000Hz 以下呈线性分布，1000Hz 以上呈对数增长，Mel 频率与 Hz 频率的关系为$f_{mel} = 2595 \cdot lg(1+\frac{f}{700Hz})$，如下图所示，图源见<a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" target="_blank" rel="noopener">一个 MFCC 的介绍教程</a>。有另一种计算方式为$f_{mel} = 1125 \cdot ln(1+\frac{f}{700Hz})$。下面给出一个计算 Mel 声谱图的例子。另，python 中可以用 librosa 调包得到梅尔声谱图。</p><blockquote><p>通过实际的主观实验，科学家发现人耳对低频信号的区别更加敏感，而对高频信号的区别则不那么敏感。也就是说低频段上的两个频度和高频段上的两个频度，人们会更容易区分前者。因此我们就明白了，频域上相等距离的两对频度，对于人耳来说他们的距离不一定相等。那么，能不能调整频域的刻度，使得这个新的刻度上相等距离的两对频度，对于人耳来说也相等呢？答案是可以的，这就是梅尔刻度。</p><p>下图展示了梅尔频度-正常频度的对应关系，正如之前所说明的，低频段的部分，梅尔刻度和正常频度几乎呈线性关系，而在高频段，因为人耳的感知变弱，因此两者呈对数关系。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224622095-1486888859.png" style="zoom:67%"></p><p>（2）假设现在用 10 个 Mel filterbank（一些论文会用 40 个，如果求 MFCC 一般是用 26 个然后在最后取前 13 个），为了获得 filterbanks 需要选择一个 lower 频率和 upper 频率，用 300 作为 lower，8000 作为 upper 是不错的选择。如果采样率是 8000Hz 那么 upper 频率应该限制为 4000。然后用公式把 lower 和 upper 转为 Mel 频率，我们使用上述第二个公式（ln 那条），可以得到 401.25Mel 和 2834.99Mel。</p><p>（3）因为用 10 个滤波器，所以需要 12 个点来划分出 10 个区间，在 401.25Mel 和 2834.99Mel 之间划分出 12 个点，m(i) = (401.25, 622.50, 843.75, 1065.00, 1286.25, 1507.50, 1728.74, 1949.99, 2171.24, 2392.49, 2613.74, 2834.99)。</p><p>（4）然后把这些点转回 Hz 频率，h(i) = (300, 517.33, 781.90, 1103.97, 1496.04, 1973.32, 2554.33, 3261.62, 4122.63, 5170.76, 6446.70, 8000)。</p><p>（5）把这些频率转为 fft bin，f(i) = floor( (N+1)*h(i)/Fs)，N 为 FFT 长度，默认为 512，Fs 为采样频率，默认为 16000Hz，则 f(i) = (9, 16, 25, 35, 47, 63, 81, 104, 132, 165, 206, 256)。这里 256 刚好对应 512 点 FFT 的 8000Hz。</p><p>（6）然后创建滤波器，第一个滤波器从第一个点开始，在第二个点到达最高峰，第三个点跌回零。第二个滤波器从第二个点开始，在第三个点到达最大值，在第四个点跌回零。以此类推。滤波器的示意图如下图所示，图源见<a href="https://blog.csdn.net/xiaoding133/article/details/8106672" target="_blank" rel="noopener">csdn-MFCC 计算过程</a>。可以看到随着频率的增加，滤波器的宽度也增加。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224736710-1682561520.png" alt></p><p>（7）接下来给出滤波器输出的计算公式，如下所示，其中 m 从 1 到 M，M 表示滤波器数量，这里是 10。k 表示点的编号，一个 fft 内 256 个点，k 从 1 到 256，表示了 fft 中的 256 个频点（k=0 表示直流信号，算进来就是 257 个频点，为了简单起见这里省略 k=0 的情况）。</p><script type="math/tex;mode=display">
H_m(k) = \left\{\begin{matrix} \frac{k-f(m-1)}{f(m)-f(m-1)} & f(m-1) \leq k \leq f(m)\\ \frac{f(m+1)-k}{f(m+1)-f(m)} & f(m) \leq k \leq f(m+1) \\ 0 & others \\ \end{matrix}\right.</script><p>（8）最后还要乘上 fft 计算出来的能量谱，关于能量谱在前一节（线性）声谱图中已经讲过了。将滤波器的输出应用到能量谱后得到的就是梅尔谱，具体应用公式如下，其中 $|X(k)|^2$表示能量谱中第 k 个点的能量。以每个滤波器的频率范围内的输出作为权重，乘以能量谱中对应频率的对应能量，然后把这个滤波器范围内的能量加起来。举个例子，比如第一个滤波器负责的是 9 和 16 之间的那些点（在其它范围的点滤波器的输出为 0），那么只对这些点对应的频率对应的能量做加权和。</p><script type="math/tex;mode=display">
MelSpec(m) = \sum_{k=f(m-1)}^{f(m+1)} H_m(k) * |X(k)|^2</script><p>（9）这样计算后，对于一帧会得到 M 个输出。经常会在论文中看到说 40 个梅尔滤波器输出，指的就是这个（实际上前面说的梅尔滤波器输出是权重 H，但是这里的意思应该是将滤波器输出应用到声谱后得到的结果，根据上下文可以加以区分）。然后在时间上堆叠多个 “40 个梅尔滤波器输出” 就得到了梅尔尺度的声谱（梅尔谱），如果再取个 log，就是 log 梅尔谱，log-Mels。</p><p>（10）把滤波器范围内的能量加起来，可以解决一个问题，这个问题就是人耳是很难理解两个靠的很近的线性频率（就是和梅尔频率相对应的赫兹频率）之间不同。如果把一个频率区域的能量加起来，只关心在每个频率区域有多少能量，这样人耳就比较能区分，我们希望这种方式得到的（Mel）声谱图可以更加具有辨识度。最后取 log 的 motivation 也是源于人耳的听力系统，人对声音强度的感知也不是线性的，一般来说，要使声音的音量翻倍，我们需要投入 8 倍的能量，为了把能量进行压缩，所以取了 log，这样，当 x 的 log 要翻倍的话，就需要增加很多的 x。另外一个取 log 的原因是为了做倒谱分析得到 MFCC，具体细节见下面 MFCC 的介绍。</p><h3 id="MFCC"><a href="#MFCC" class="headerlink" title="MFCC"></a>MFCC</h3><p>（1）MFCC，梅尔频率的倒谱系数（Mel Frequency Cepstral Coefficents），是广泛应用于语音领域的特征，在这之前常用的是线性预测系数 Linear Prediction Coefficients（LPCs）和线性预测倒谱系数（LPCCs），特别是用在 HMM 上。</p><p>（2）先说一下获得 MFCC 的步骤，首先分帧加窗，然后对每一帧做 FFT 后得到（单帧）能量谱（具体步骤见上面线性声谱图的介绍），对线性声谱图应用梅尔滤波器后然后取 log 得到 log 梅尔声谱图（具体步骤见上面梅尔声谱图的介绍），然后对 log 滤波能量（log 梅尔声谱）做 DCT，离散余弦变换（傅里叶变换的一种），然后保留第二个到第 13 个系数，得到的这 12 个系数就是 MFCC。</p><p>（3）然后再大致说说 MFCC 的含义，下图第一个图（图源见参考资料 [1]）是语音的频谱图，峰值是语音的主要频率成分，这些峰值称为共振峰，共振峰携带了声音的辨识（相当于人的身份证）。把这些峰值平滑地连起来得到的曲线称为频谱包络，包络描述了携带声音辨识信息的共振峰，所以我们希望能够得到这个包络来作为语音特征。频谱由频谱包络和频谱细节组成，如下第二个图（图源见参考资料[1]）所示，其中 log X[k] 代表频谱（注意图中给出的例子是赫兹谱，这里只是举例子，实际我们做的时候通常都是用梅尔谱），log H[k]代表频谱包络，log E[k]代表频谱细节。我们要做的就是从频谱中分离得到包络，这个过程也称为倒谱分析，下面就说说倒谱分析是怎么做的。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224852822-1848315135.png" style="zoom:67%"></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224902529-1572460338.png" style="zoom:67%"></p><p>（4）要做的其实就是对频谱做 FFT，在频谱上做 FFT 这个操作称为逆 FFT，需要注意的是我们是在频谱的 log 上做的，因为这样做 FFT 后的结果 x[k]可以分解成 h[k]和 e[k]的和。我们先看下图（图源见参考资料 [1]），对包络 log H[k] 做 IFFT 的结果，可以看成 “每秒 4 个周期的正弦波”，于是我们在伪频率轴上的 4Hz 上给一个峰值，记作 h[k]。对细节 log E[k] 做 IFFT 的结果，可以看成 “每秒 100 个周期的正弦波”，于是我们在伪频率轴上的 100Hz 上给一个峰值，记作 e[k]。对频谱 log X[k] 做 IFFT 后的结果记作 x[k]，这就是我们说的倒谱，它会等于 h[k]和 e[k]的叠加，如下第二个图所示。我们想要得到的就是包络对应的 h[k]，而 h[k]是 x[k]的低频部分，只需要对 x[k]取低频部分就可以得到了。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224927630-2116898656.png" style="zoom:67%"></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224944430-837617222.png" style="zoom:67%"></p><p>（5）最后再总结一下得到 MFCC 的步骤，求线性声谱图，做梅尔滤波得到梅尔声谱图，求个 log 得到 log 梅尔谱，做倒谱分析也就是对 log X[k] 做 DCT 得到 x[k]，取低频部分就可以得到倒谱向量，通常会保留第 2 个到第 13 个系数，得到 12 个系数，这 12 个系数就是常用的 MFCC。图源见参考资料 [1]。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221225011597-1988899505.png" style="zoom:67%"></p><h3 id="deltas，deltas-deltas"><a href="#deltas，deltas-deltas" class="headerlink" title="deltas，deltas-deltas"></a>deltas，deltas-deltas</h3><p>（1）deltas 和 deltas-deltas，看到很多人翻译成一阶差分和二阶差分，也被称为微分系数和加速度系数。使用它们的原因是，MFCC 只是描述了一帧语音上的能量谱包络，但是语音信号似乎有一些动态上的信息，也就是 MFCC 随着时间的改变而改变的轨迹。有证明说计算 MFCC 轨迹并把它们加到原始特征中可以提高语音识别的表现。</p><p>（2）以下是 deltas 的一个计算公式，其中 t 表示第几帧，N 通常取 2，c 指的就是 MFCC 中的某个系数。deltas-deltas 就是在 deltas 上再计算以此 deltas。</p><script type="math/tex;mode=display">
d_t = \frac{\sum_{n=1}^{N} n(c_{t+n}-c_{t-n})}{2 \sum_{n=1}^{N} n^2}</script><p>（3）对 MFCC 中每个系数都做这样的计算，最后会得到 12 个一阶差分和 12 个二阶差分，我们通常在论文中看到的 “MFCC 以及它们的一阶差分和二阶差分” 指的就是这个。</p><p>（4）值得一提的是 deltas 和 deltas-deltas 也可以用在别的参数上来表述动态特性，有论文中是直接在 log Mels 上做一阶差分和二阶差分的，<a href="https://www.cnblogs.com/liaohuiqiang/p/10128835.html" target="_blank" rel="noopener">论文笔记：语音情感识别（二）声谱图 + CRNN</a> 中 3-D Convolutional Recurrent Neural Networks with Attention Model for Speech Emotion Recognition 这篇论文就是这么做的。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><strong>1.频谱</strong>：时域信号（一维）短时傅里叶变换后的频域信号（一维）。</p><p><strong>2.声谱图/语谱图</strong>：把一整段语音信号截成许多帧，把它们各自的频谱“竖”起来（即用纵轴表示频率），用颜色的深浅来代替频谱强度，再把所有帧的频谱横向并排起来（即用横轴表示时间），就得到了<strong>语谱图</strong>，它可以称为声音的<strong>时频域</strong>表示。</p><p><strong>3.倒谱</strong>：也叫做倒频谱，二次谱，对数功率谱等。对声谱图取对数后，再DFT变回时域，此时不是完全意义上的时域，应叫做倒谱域。</p><p><strong>4.MFCC</strong>：对线性声谱图应用mel滤波器后，取log，得到log梅尔声谱图，然后对log滤波能量（log梅尔声谱）做DCT离散余弦变换（傅里叶变换的一种），然后保留第2到第13个系数，得到的这12个系数就是MFCC。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/watermark.png" alt="在这里插入图片描述"></p><p><strong>附加：</strong></p><p><strong>1.能量谱</strong>：也叫做能量密度谱。是原信号傅里叶变化的平方。用于描述时间序列的能量随频率的分布。</p><p><strong>2.功率谱</strong>：将频谱或时频谱（语谱）中的幅值进行平方，得到功率谱。</p><p><strong>3.功率谱密度</strong>：定义为单位频带内的吸纳后功率。其推导公式较为复杂，但维纳-辛欣定理证明了：一段信号的功率谱等于这段信号自相关函数的傅里叶变换。</p><p>注：信号分为确定和随机，确定信号又分为能量和功率，随机信号一定是功率信号。语音信号是随机信号。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[1] <a href="http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf" target="_blank" rel="noopener">CMU 语音课程 slides</a></p><p>[2] <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" target="_blank" rel="noopener">一个 MFCC 的介绍教程</a></p><p>[3] <a href="https://blog.csdn.net/xiaoding133/article/details/8106672" target="_blank" rel="noopener">csdn-MFCC 计算过程</a></p><p>[4] <a href="https://www.cnblogs.com/BaroC/p/4283380.html" target="_blank" rel="noopener">博客园 - MFCC 学习笔记</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/cnlinxi/book-text-to-speech" target="_blank" rel="noopener">cnlinxi/book-text-to-speech: A book about Text-to-Speech (TTS) in Chinese. (github.com)</a><br><a href="https://blog.csdn.net/qq_36002089/article/details/108378796" target="_blank" rel="noopener">声谱图，梅尔语谱，倒谱，梅尔倒谱系数</a><br><a href="https://www.cnblogs.com/liaohuiqiang/p/10159429.html" target="_blank" rel="noopener">论文笔记：语音情感识别（四）语音特征之声谱图，log梅尔谱，MFCC，deltas</a><br><a href="https://zhuanlan.zhihu.com/p/510550742" target="_blank" rel="noopener">语音基础知识（附相关实现代码）</a><br><a href="https://www.zhihu.com/question/27126800/answer/35376174" target="_blank" rel="noopener">不同元音辅音在声音频谱的表现是什么样子？ - 王赟 Maigo的回答 - 知乎</a><br><a href="https://audiosns.com/1571.html" target="_blank" rel="noopener">搬运工：波形、频谱和声谱的关系</a><br><a href="https://zhuanlan.zhihu.com/p/421460202" target="_blank" rel="noopener">语音合成基础(3)——关于梅尔频谱你想知道的都在这里</a><br><a href="https://zhuanlan.zhihu.com/p/99122527" target="_blank" rel="noopener">语音合成基础(1)——语音和TTS</a><br><a href="https://www.jianshu.com/p/2b83e68a055b" target="_blank" rel="noopener">《语音信号处理》整理</a><br><a href="https://www.cnblogs.com/guanghe/p/10020120.html" target="_blank" rel="noopener">MP3的采样率和比特率</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/" title="语音基础知识">https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/语音合成/" rel="tag"><i class="fa fa-tag"></i> 语音合成</a><a href="/tags/语音基础知识/" rel="tag"><i class="fa fa-tag"></i> 语音基础知识</a></div><div class="post-nav"><div class="post-nav-item"><a href="/DeepLearningApplications/自然语言处理/bert模型详解/" rel="prev" title="bert模型详解"><i class="fa fa-chevron-left"></i> bert模型详解</a></div><div class="post-nav-item"> <a href="/DeepLearningApplications/语音合成/语音合成模型/" rel="next" title="语音合成模型">语音合成模型<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#辅音和元音的区别"><span class="nav-number">1.</span> <span class="nav-text">辅音和元音的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#清音和浊音"><span class="nav-number">2.</span> <span class="nav-text">清音和浊音</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#波形、频谱和语谱（声谱）"><span class="nav-number">3.</span> <span class="nav-text">波形、频谱和语谱（声谱）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#波形"><span class="nav-number">3.1.</span> <span class="nav-text">波形</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#频谱"><span class="nav-number">3.2.</span> <span class="nav-text">频谱</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语谱（声谱）"><span class="nav-number">3.3.</span> <span class="nav-text">语谱（声谱）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">3.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语音基本概念"><span class="nav-number">4.</span> <span class="nav-text">语音基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#能量"><span class="nav-number">4.1.</span> <span class="nav-text">能量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#短时能量"><span class="nav-number">4.2.</span> <span class="nav-text">短时能量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#声强和声强级（声压和声压级）"><span class="nav-number">4.3.</span> <span class="nav-text">声强和声强级（声压和声压级）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过零率"><span class="nav-number">4.4.</span> <span class="nav-text">过零率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基频和基音周期"><span class="nav-number">4.5.</span> <span class="nav-text">基频和基音周期</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#音高"><span class="nav-number">4.6.</span> <span class="nav-text">音高</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共振峰"><span class="nav-number">4.7.</span> <span class="nav-text">共振峰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#汇总"><span class="nav-number">4.8.</span> <span class="nav-text">汇总</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语音信号的预处理操作"><span class="nav-number">5.</span> <span class="nav-text">语音信号的预处理操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#预加重"><span class="nav-number">5.1.</span> <span class="nav-text">预加重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分帧"><span class="nav-number">5.2.</span> <span class="nav-text">分帧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#加窗"><span class="nav-number">5.3.</span> <span class="nav-text">加窗</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语音声学特征介绍"><span class="nav-number">6.</span> <span class="nav-text">语音声学特征介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原始信号"><span class="nav-number">6.1.</span> <span class="nav-text">原始信号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（线性）声谱图"><span class="nav-number">6.2.</span> <span class="nav-text">（线性）声谱图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梅尔声谱图"><span class="nav-number">6.3.</span> <span class="nav-text">梅尔声谱图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MFCC"><span class="nav-number">6.4.</span> <span class="nav-text">MFCC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deltas，deltas-deltas"><span class="nav-number">6.5.</span> <span class="nav-text">deltas，deltas-deltas</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结-1"><span class="nav-number">6.6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考资料"><span class="nav-number">6.7.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">317</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">54</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">372</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://kalacloud.com" title="https://kalacloud.com" rel="noopener" target="_blank">卡拉云低代码工具</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2.3m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">35:08</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : '551898f7e33e4cf3faf7d3c9e0a522f7',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>