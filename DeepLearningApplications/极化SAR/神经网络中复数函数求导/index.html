<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.zdaiot.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="最近接触到了对复数函数的求导，之前已知没有研究过。在看教程的时候，发现自己微积分忘得是一干二净。真的是，233333。算了不多说了，下面对神经网络中的复数函数求导进行记录。 复数神经网络有以下几个特点：  网络的输入与权重均为复数 损失值为实数  本文的内容主要包含一下几个部分：  推导复数神经网络的反向传播公式 为复数神经网络的反向传播完成通用代码"><meta name="keywords" content="函数导数,复数函数,反向传播"><meta property="og:type" content="article"><meta property="og:title" content="神经网络中复数函数求导"><meta property="og:url" content="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/index.html"><meta property="og:site_name" content="zdaiot"><meta property="og:description" content="最近接触到了对复数函数的求导，之前已知没有研究过。在看教程的时候，发现自己微积分忘得是一干二净。真的是，233333。算了不多说了，下面对神经网络中的复数函数求导进行记录。 复数神经网络有以下几个特点：  网络的输入与权重均为复数 损失值为实数  本文的内容主要包含一下几个部分：  推导复数神经网络的反向传播公式 为复数神经网络的反向传播完成通用代码"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/cbp.png"><meta property="og:image" content="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/complexconv.png"><meta property="og:updated_time" content="2019-12-02T09:35:18.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="神经网络中复数函数求导"><meta name="twitter:description" content="最近接触到了对复数函数的求导，之前已知没有研究过。在看教程的时候，发现自己微积分忘得是一干二净。真的是，233333。算了不多说了，下面对神经网络中的复数函数求导进行记录。 复数神经网络有以下几个特点：  网络的输入与权重均为复数 损失值为实数  本文的内容主要包含一下几个部分：  推导复数神经网络的反向传播公式 为复数神经网络的反向传播完成通用代码"><meta name="twitter:image" content="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/cbp.png"><link rel="canonical" href="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>神经网络中复数函数求导 | zdaiot</title><script data-pjax>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?0b0b58037319da4959d5a3c014160ccd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">zdaiot</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">404NotFound</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/uploads/avatar.png"><meta itemprop="name" content="zdaiot"><meta itemprop="description" content="404NotFound"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="zdaiot"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络中复数函数求导<a href="https://github.com/zdaiot/zdaiot.github.io/tree/hexo/source/_posts/DeepLearningApplications/极化SAR/神经网络中复数函数求导.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-11-20 17:35:02" itemprop="dateCreated datePublished" datetime="2019-11-20T17:35:02+08:00">2019-11-20</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-12-02 17:35:18" itemprop="dateModified" datetime="2019-12-02T17:35:18+08:00">2019-12-02</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/" itemprop="url" rel="index"><span itemprop="name">DeepLearningApplications</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearningApplications/极化SAR/" itemprop="url" rel="index"><span itemprop="name">极化SAR</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>8.3k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>8 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>最近接触到了对复数函数的求导，之前已知没有研究过。在看教程的时候，发现自己微积分忘得是一干二净。真的是，233333。算了不多说了，下面对神经网络中的复数函数求导进行记录。</p><p>复数神经网络有以下几个特点：</p><ul><li>网络的输入与权重均为复数</li><li>损失值为实数</li></ul><p>本文的内容主要包含一下几个部分：</p><ul><li>推导复数神经网络的反向传播公式</li><li>为复数神经网络的反向传播完成通用代码</li></ul><h2 id="Holomorphism与Cauchy-Riemann方程"><a href="#Holomorphism与Cauchy-Riemann方程" class="headerlink" title="Holomorphism与Cauchy-Riemann方程"></a>Holomorphism与Cauchy-Riemann方程</h2><p>Holomorphism（全纯）也称为analyticity，维基百科的定义为：</p><p>若$U$为$\mathbb C$的开子集（开集是指不包含任何自己边界点的集合。或者说，开集包含的任意一点的充分小的邻域都包含在其自身中），且$f:U\rightarrow \mathbb C$为一个函数。</p><p>我们称$f$是在$U$中<strong>一点$z_0$是复可微（complex differentiable）或全纯的</strong>，当且仅当该极限存在：</p><script type="math/tex;mode=display">
\begin{eqnarray} f'(z_0)=lim_{z \rightarrow z_0} \frac{f(z)-f(z_0)}{z-z_0} \end{eqnarray}</script><p>若$f$在$U$上<strong>任取一点均全纯，则称$f$在$U$上全纯</strong>。特别地，若函数在整个复平面全纯，我们称这个函数为整函数。</p><p>在论文《Deep complex networks》中，是这样定义的（其实和维基百科上定义一致，但是一般英文表述比较严谨，所以放在这里）</p><p>Holomorphism, also called analyticity, ensures that a complex-valued function is <strong>complex differentiable in the neighborhood of every point in its domain</strong>. This means that the derivative, $f’(z_0)\equiv lim_{\Delta z \rightarrow 0} \frac{f(z_0+\Delta z)-f(z_0)}{\Delta z}$of $f$, exists at every point $z_0$ in the domain of $f$ where $f$ is a complex-valued<br>function of a complex variable $z = x + i y$ such that $f(z) = u(x, y) + i v(x, y)$。$u$和$v$均为实值函数，所以$\Delta z$可以使用$\Delta z=\Delta x + i \Delta y$ 表达。$\Delta z$可以从不同的方向逼近0（实轴、虚轴、实轴与虚轴之间）。为了复数可微分，$f’(z_0)$不管从哪个方向逼近0，值应该都相同。当$\Delta z$从实轴逼近0的时候，$f’(z_0)$可以写成：</p><script type="math/tex;mode=display">
\begin{eqnarray} \begin{split} f'(z_0) & \equiv & lim_{\Delta z \rightarrow 0} \frac{f(z_0+\Delta z)-f(z_0)}{\Delta z} \\
& =&  \lim_{\Delta x \rightarrow 0} \lim_{\Delta y \rightarrow 0} \left[ \frac{\Delta u(x_0, y_0) + i \Delta v(x_0, y_o)}{\Delta x + i \Delta y} \right] \\ 
& =&  \lim_{\Delta x \rightarrow 0} \left[ \frac{\Delta u(x_0, y_0) + i \Delta v(x_0, y_o)}{\Delta x + i 0} \right] \end{split} \end{eqnarray}</script><p>当$\Delta z$从虚轴逼近0的时候，$f’(z_0)$可以写成：</p><script type="math/tex;mode=display">
\begin{eqnarray} \begin{split} f'(z_0) & \equiv & lim_{\Delta z \rightarrow 0} \frac{f(z_0+\Delta z)-f(z_0)}{\Delta z} \\
& =&  \lim_{\Delta x \rightarrow 0} \lim_{\Delta y \rightarrow 0} \left[ \frac{\Delta u(x_0, y_0) + i \Delta v(x_0, y_o)}{\Delta x + i \Delta y} \right] \\ 
& =&  \lim_{\Delta y \rightarrow 0} \left[ \frac{\Delta u(x_0, y_0) + i \Delta v(x_0, y_o)}{0 + i \Delta y} \right] \end{split} \end{eqnarray}</script><p>要想使得公式$({2})$和公式$({3})$相等，则需要满足下式：</p><script type="math/tex;mode=display">
\begin{eqnarray} \frac{\delta f}{\delta z}=\frac{\delta u}{\delta x}+i\frac{\delta v}{\delta x} = -i \frac{\delta u}{\delta y}+\frac{\delta v}{\delta y} \end{eqnarray}</script><p>为了复数可微分，$f$应该满足：</p><script type="math/tex;mode=display">
\begin{eqnarray} \begin{split}
\frac{\delta u}{\delta x}& = &\frac{\delta v}{\delta y} \\
\frac{\delta u}{\delta y}& = &-\frac{\delta v}{\delta x}
 \end{split} \end{eqnarray}</script><p>这被称为Cauchy-Riemann方程（黎曼方程），它是$f$复微分的必要条件。假设$u$和$v$具有连续的一阶偏导数，则Cauchy-Riemann方程成为$f$为Holomorphism的充分条件。</p><p>另外，论文中提到Hirose and Yoshida (2012) [1] 证明了<strong>对于反向传播，不一定需要函数为Holomorphism，主需要函数关于实部和虚部分别求导即可。</strong></p><h2 id="Wirtinger算子"><a href="#Wirtinger算子" class="headerlink" title="Wirtinger算子"></a>Wirtinger算子</h2><p>Wirtinger算子的思路是，将任何复变函数$f$，看做$f(z,z^<em>)$，求导数就是对$z$和共轭$z^</em>$分别求导：</p><script type="math/tex;mode=display">
\begin{eqnarray}  df = \frac{\delta f}{\delta z}dz +  \frac{\delta f}{\delta z^*}dz^* \end{eqnarray}</script><p>其中：</p><script type="math/tex;mode=display">
\begin{eqnarray} \frac{\delta f}{\delta z} = \frac{\delta f}{\delta x} -j \frac{\delta f}{\delta y} \end{eqnarray}</script><script type="math/tex;mode=display">
\begin{eqnarray}  \frac{\delta f}{\delta z^*} = \frac{\delta f}{\delta x} + j \frac{\delta f}{\delta y} \end{eqnarray}</script><h2 id="复数变量的反向传播"><a href="#复数变量的反向传播" class="headerlink" title="复数变量的反向传播"></a>复数变量的反向传播</h2><p>设$J(z)$是定义在复平面$z=x+iy$上的实值损失函数，则根据Wirtinger算子，$J(z)$关于$z$的梯度为：</p><script type="math/tex;mode=display">
\begin{align}\begin{split}\nabla J(z) &= \frac{\partial J}{\partial x} + i\frac{\partial J}{\partial y}\\&= \frac{\partial J}{\partial z}\frac{\partial z}{\partial x}+\frac{\partial J}{\partial z^*}\frac{\partial z^*}{\partial x}+ i\left[\frac{\partial J}{\partial z}\frac{\partial z}{\partial y}+\frac{\partial J}{\partial z^*}\frac{\partial z^*}{\partial y}\right]\\&=2\frac{\partial J}{\partial z^*}=2\left(\frac{\partial J}{\partial z}\right)^*\end{split}\end{align}</script><p>在最后一层中，我们使用了实数$J$，对于前面的层只需要一层一层的向前传播即可。</p><script type="math/tex;mode=display">
\frac{\partial J}{\partial y_l}=\sum\limits_{y_{l+1}}\frac{\partial J}{\partial y_{l+1}}\frac{\partial y_{l+1}}{\partial y_l}+\frac{\partial J}{\partial y^*_{l+1}}\frac{\partial y_{l+1}^*}{\partial y_l}.</script><p>其中$y_l$和$y_{l+1}$分别是第$l$层和第$l+1$的变量；$y_{l+1}=f_l(y_l)$。</p><p>复变函数按照是否可导，分为全纯函数holomothic和nonholomophic，判断条件为Cauchy-Riemann方程。</p><ul><li><p>如果$f_l$是<strong>holomophic</strong>（全纯函数），那么上面的第二项消失，变成了：</p><script type="math/tex;mode=display">
\frac{\partial J}{\partial y_l}=\sum\limits_{y_{l+1}}\frac{\partial J}{\partial y_{l+1}}\frac{\partial y_{l+1}}{\partial y_l},</script></li><li><p>如果$f_l$为nonholomophic</p><script type="math/tex;mode=display">
\frac{\partial J}{\partial y_l}=\sum\limits_{y_{l+1}}\frac{\partial J}{\partial y_{l+1}}\frac{\partial y_{l+1}}{\partial y_l}+\left(\frac{\partial J}{\partial y_{l+1}}\frac{\partial y_{l+1}}{\partial y_l^*}\right)^*.</script></li></ul><blockquote><p>这种思路不同与论文《Deep complex networks》中求导方式，在这种思路中，权重和输入均为复数矩阵，求导时要损失函数对于整个复数求导；而在上述论文中，输入和权重必须可以均为实数（即使用双倍的实数网络代替复数网络），求导时可以分别求实部和虚部的偏导数。</p></blockquote><h2 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h2><p>给定一个长度为10的向量$x$，网络的损失函数为$J=f_2\left( f_1 \left( f_x(x) \right) \right)$。其中$f_1(z)=z^*$，$f_2 (z)=-e ^{-|z|^2}$。这是一个简单的方程，naive BP像实数网络会失败，代码在最下面给出：</p><p><img src="/DeepLearningApplications/极化SAR/神经网络中复数函数求导/cbp.png" alt="img"></p><p>而正确的方程正确的收敛到了-10.</p><h2 id="关于复数神经网络的思考"><a href="#关于复数神经网络的思考" class="headerlink" title="关于复数神经网络的思考"></a>关于复数神经网络的思考</h2><p>许多计算机科学的人强调复数函数可以使用双倍尺寸的实数网络代替，这是不正确的。这让我们想到一个问题：为什么需要复数值。若没有复数值，那么</p><ul><li>unitary矩阵（酉矩阵）不能很容易的实现</li><li>相位性质不能很好的表达，光和全息图，声音，量子波函数等</li></ul><p><strong>虽然一个复数神经网络必须包含至少一个nonholomophic函数（让损失值为正值）</strong>。我认为复数函数的价值在于holomothic，如果一个函数是nonholomophic，那么它和双倍尺寸的复数值网络没有太大区别。</p><p>复数神经网络倾向于blow up，那也意味着，不能定义soft函数，像sigmoid和tanh。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Test complex back propagation.</span></span><br><span class="line"><span class="string">The theory could be found in Akira's book "Complex Valued Neural Networks".</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># define two useful functions and their derivatives.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1_forward</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> x.conj()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df1_z</span><span class="params">(x, y)</span>:</span> <span class="keyword">return</span> np.zeros_like(x, dtype=<span class="string">'complex128'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df1_zc</span><span class="params">(x, y)</span>:</span> <span class="keyword">return</span> np.ones_like(x, dtype=<span class="string">'complex128'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2_forward</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> -np.exp(-x * x.conj())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df2_z</span><span class="params">(x, y)</span>:</span> <span class="keyword">return</span> -y * x.conj()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df2_zc</span><span class="params">(x, y)</span>:</span> <span class="keyword">return</span> -y * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># we compare the correct and incorrect back propagation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_backward</span><span class="params">(df_z, df_zc)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    naive back propagation meta formula,</span></span><br><span class="line"><span class="string">    df_z and df_zc are dirivatives about variables and variables' conjugate.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> x, y, dy: df_z(x, y) * dy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correct_backward</span><span class="params">(df_z, df_zc)</span>:</span></span><br><span class="line">    <span class="string">'''the correct version.'''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> x, y, dy: df_z(x, y) * dy +\</span><br><span class="line">                    df_zc(x, y).conj() * dy.conj()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the version in naive bp</span></span><br><span class="line">f1_backward_naive = naive_backward(df1_z, df1_zc)</span><br><span class="line">f2_backward_naive = naive_backward(df2_z, df2_zc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the correct backward propagation</span></span><br><span class="line">f1_backward_correct = correct_backward(df1_z, df1_zc)</span><br><span class="line">f2_backward_correct = correct_backward(df2_z, df2_zc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initial parameters, and network parameters</span></span><br><span class="line">num_input = <span class="number">10</span></span><br><span class="line">a0 = np.random.randn(num_input) + <span class="number">1j</span> * np.random.randn(num_input)</span><br><span class="line">num_layers = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''forward pass'''</span></span><br><span class="line">    yl = [x]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">        <span class="keyword">if</span> i == num_layers - <span class="number">1</span>:</span><br><span class="line">            x = f2_forward(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = f1_forward(x)</span><br><span class="line">        yl.append(x)</span><br><span class="line">    <span class="keyword">return</span> yl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(yl, version)</span>:</span>  <span class="comment"># version = 'correct' or 'naive'</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    back propagation, yl is a list of outputs.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dy = <span class="number">1</span> * np.ones(num_input, dtype=<span class="string">'complex128'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">        y = yl[num_layers - i]</span><br><span class="line">        x = yl[num_layers - i - <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            dy = eval(<span class="string">'f2_backward_%s'</span> % version)(x, y, dy)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dy = eval(<span class="string">'f1_backward_%s'</span> % version)(x, y, dy)</span><br><span class="line">    <span class="keyword">return</span> dy.conj() <span class="keyword">if</span> version == <span class="string">'correct'</span> <span class="keyword">else</span> dy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_run</span><span class="params">(version, alpha=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">'''simple optimization for target loss function.'''</span></span><br><span class="line">    cost_histo = []</span><br><span class="line">    x = a0.copy()</span><br><span class="line">    num_run = <span class="number">2000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_run):</span><br><span class="line">        yl = forward(x)</span><br><span class="line">        g_a = backward(yl, version)</span><br><span class="line">        x[:num_input] = (x - alpha * g_a)[:num_input]</span><br><span class="line">        cost_histo.append(yl[<span class="number">-1</span>].sum().real)</span><br><span class="line">    <span class="keyword">return</span> np.array(cost_histo)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    lr = <span class="number">0.01</span></span><br><span class="line">    cost_r = optimize_run(<span class="string">'naive'</span>, lr)</span><br><span class="line">    cost_a = optimize_run(<span class="string">'correct'</span>, lr)</span><br><span class="line">    figure(figsize=(<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">    plot(cost_r, lw=<span class="number">2</span>)</span><br><span class="line">    plot(cost_a, lw=<span class="number">2</span>)</span><br><span class="line">    legend([<span class="string">'Naive'</span>, <span class="string">'Correct'</span>])</span><br><span class="line">    ylabel(<span class="string">r'$e^&#123;-|(x^*)^*|^2&#125;$'</span>, fontsize = <span class="number">18</span>)</span><br><span class="line">    xlabel(<span class="string">'step'</span>, fontsize = <span class="number">18</span>)</span><br><span class="line">    tight_layout()</span><br><span class="line">    show()</span><br></pre></td></tr></table></figure><h2 id="复数神经网络"><a href="#复数神经网络" class="headerlink" title="复数神经网络"></a>复数神经网络</h2><p>根据我查的资料，这里的复数神经网络并没有用到反向传播中的复数求导机制，而是将复数的实数部分和虚数部分分开拆成两部分进行运算，个人感觉就是上面博客中所说的使用双倍尺寸的实数网络代替复数神经网络，通过下面这一张图可以知道大概原理：</p><p><img src="/DeepLearningApplications/极化SAR/神经网络中复数函数求导/complexconv.png" alt="img"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Akira Hirose and Shotaro Yoshida. Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence. IEEE Transactions on Neural Networks and learning systems, 23(4):<br>541–551, 2012.</p><p><a href="https://giggleliu.github.io/2018/02/01/complex_bp.html" target="_blank" rel="noopener">Back Propagation for Complex Valued Neural Networks</a><br><a href="https://zhuanlan.zhihu.com/p/86280502" target="_blank" rel="noopener">【原创】复数神经网络的反向传播算法，及pytorch实现方法</a><br><a href="https://zh.wikipedia.org/wiki/%E5%85%A8%E7%BA%AF%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">全纯函数</a></p></div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/uploads/wechat.png" alt="zdaiot 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/uploads/aipay.jpg" alt="zdaiot 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zdaiot</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/" title="神经网络中复数函数求导">https://www.zdaiot.com/DeepLearningApplications/极化SAR/神经网络中复数函数求导/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/uploads/wechat-qcode.jpg"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/函数导数/" rel="tag"><i class="fa fa-tag"></i> 函数导数</a><a href="/tags/复数函数/" rel="tag"><i class="fa fa-tag"></i> 复数函数</a><a href="/tags/反向传播/" rel="tag"><i class="fa fa-tag"></i> 反向传播</a></div><div class="post-nav"><div class="post-nav-item"><a href="/DataStructureAlgorithm/03复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度/" rel="prev" title="03复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度"><i class="fa fa-chevron-left"></i> 03复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度</a></div><div class="post-nav-item"> <a href="/DataStructureAlgorithm/04数组：为什么很多编程语言中数组都从0开始编号？/" rel="next" title="04数组：为什么很多编程语言中数组都从0开始编号？">04数组：为什么很多编程语言中数组都从0开始编号？<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Holomorphism与Cauchy-Riemann方程"><span class="nav-number">1.</span> <span class="nav-text">Holomorphism与Cauchy-Riemann方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wirtinger算子"><span class="nav-number">2.</span> <span class="nav-text">Wirtinger算子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#复数变量的反向传播"><span class="nav-number">3.</span> <span class="nav-text">复数变量的反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简单测试"><span class="nav-number">4.</span> <span class="nav-text">简单测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#关于复数神经网络的思考"><span class="nav-number">5.</span> <span class="nav-text">关于复数神经网络的思考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码"><span class="nav-number">6.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#复数神经网络"><span class="nav-number">7.</span> <span class="nav-text">复数神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">8.</span> <span class="nav-text">参考</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zdaiot" src="/uploads/avatar.png"><p class="site-author-name" itemprop="name">zdaiot</p><div class="site-description" itemprop="description">404NotFound</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">305</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">52</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">364</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zdaiot" title="GitHub → https://github.com/zdaiot" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:zdaiot@163.com" title="E-Mail → mailto:zdaiot@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/" title="知乎 → https://www.zhihu.com/people/" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/zdaiot" title="CSDN → https://blog.csdn.net/zdaiot" rel="noopener" target="_blank"><i class="fa fa-copyright fa-fw"></i> CSDN</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021031914号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">zdaiot</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">2m</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">30:07</span></div><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b5e7e498f94b7ad" async="async"></script></div> <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("08/01/2018 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Run for "+dnum+" Days ",document.getElementById("times").innerHTML=hnum+" Hours "+mnum+" m "+snum+" s"}setInterval("createtime()",250)</script><div class="busuanzi-count"><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script data-pjax>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="/js/local-search.js"></script><div id="pjax"><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4800250e682fe873198b',
      clientSecret: '80f7f33f5f8ddfd3944f455cabadda4ff3299147',
      repo        : 'zdaiot.github.io',
      owner       : 'zdaiot',
      admin       : ['zdaiot'],
      id          : 'c40248051bea98a366a36af3d2ba46ef',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0}})</script></body></html>