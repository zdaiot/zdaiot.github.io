<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zdaiot</title>
  
  <subtitle>404NotFound</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.zdaiot.com/"/>
  <updated>2023-07-03T15:17:23.000Z</updated>
  <id>https://www.zdaiot.com/</id>
  
  <author>
    <name>zdaiot</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python多进程、多线程、协程</title>
    <link href="https://www.zdaiot.com/Python/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/Python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E3%80%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E3%80%81%E5%8D%8F%E7%A8%8B/"/>
    <id>https://www.zdaiot.com/Python/基础语法/Python多进程、多线程、协程/</id>
    <published>2023-07-03T15:17:23.000Z</published>
    <updated>2023-07-03T15:17:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>以下内容主要参考了廖雪峰老师的Python教程，在不懂的地方添加了自己的注释。</p><h2 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h2><p>Unix/Linux操作系统提供了一个<code>fork()</code>系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是<code>fork()</code>调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。</p><p>子进程永远返回<code>0</code>，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用<code>getppid()</code>就可以拿到父进程的ID。</p><p>Python的<code>os</code>模块封装了常见的系统调用，其中就包括<code>fork</code>，可以在Python程序中轻松创建子进程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Process (%s) start...'</span> % os.getpid())</span><br><span class="line"><span class="comment"># Only works on Unix/Linux/Mac:</span></span><br><span class="line">pid = os.fork()</span><br><span class="line"><span class="keyword">if</span> pid == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">'I am child process (%s) and my parent is %s.'</span> % (os.getpid(), os.getppid()))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'I (%s) just created a child process (%s).'</span> % (os.getpid(), pid))</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Process (876) start...</span><br><span class="line">I (876) just created a child process (877).</span><br><span class="line">I am child process (877) and my parent is 876.</span><br></pre></td></tr></table></figure><p>由于Windows没有<code>fork</code>调用，上面的代码在Windows上无法运行。而Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的，推荐大家用Mac学Python！</p><p>有了<code>fork</code>调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。</p><h3 id="multiprocessing"><a href="#multiprocessing" class="headerlink" title="multiprocessing"></a>multiprocessing</h3><p>如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有<code>fork</code>调用，难道在Windows上无法用Python编写多进程的程序？</p><p>由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。<code>multiprocessing</code>模块就是跨平台版本的多进程模块。</p><p><code>multiprocessing</code>模块提供了一个<code>Process</code>类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子进程要执行的代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_proc</span><span class="params">(name)</span>:</span></span><br><span class="line">    print(<span class="string">'Run child process %s (%s)...'</span> % (name, os.getpid()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">'Parent process %s.'</span> % os.getpid())</span><br><span class="line">    p = Process(target=run_proc, args=(<span class="string">'test'</span>,))</span><br><span class="line">    print(<span class="string">'Child process will start.'</span>)</span><br><span class="line">    p.start()</span><br><span class="line">    p.join()</span><br><span class="line">    print(<span class="string">'Child process end.'</span>)</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Parent process 928.</span><br><span class="line">Child process will start.</span><br><span class="line">Run child process test (929)...</span><br><span class="line">Process end.</span><br></pre></td></tr></table></figure><p>创建子进程时，只需要传入一个执行函数和函数的参数，创建一个<code>Process</code>实例，用<code>start()</code>方法启动，这样创建进程比<code>fork()</code>还要简单。</p><p><code>join()</code>方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。</p><h3 id="Pool"><a href="#Pool" class="headerlink" title="Pool"></a>Pool</h3><p>如果要启动大量的子进程，可以用进程池的方式批量创建子进程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span><span class="params">(name)</span>:</span></span><br><span class="line">    print(<span class="string">'Run task %s (%s)...'</span> % (name, os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * <span class="number">3</span>)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'Task %s runs %0.2f seconds.'</span> % (name, (end - start)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">'Parent process %s.'</span> % os.getpid())</span><br><span class="line">    p = Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p.apply_async(long_time_task, args=(i,))</span><br><span class="line">    print(<span class="string">'Waiting for all subprocesses done...'</span>)</span><br><span class="line">    p.close()</span><br><span class="line">    p.join()</span><br><span class="line">    print(<span class="string">'All subprocesses done.'</span>)</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Parent process 505792.</span><br><span class="line">Waiting for all subprocesses done...</span><br><span class="line">Run task 0 (505324)...</span><br><span class="line">Run task 1 (501888)...</span><br><span class="line">Run task 2 (500856)...</span><br><span class="line">Run task 3 (503812)...</span><br><span class="line">Task 0 runs 0.92 seconds.</span><br><span class="line">Run task 4 (505324)...</span><br><span class="line">Task 3 runs 1.75 seconds.</span><br><span class="line">Task 2 runs 2.26 seconds.</span><br><span class="line">Task 1 runs 2.53 seconds.</span><br><span class="line">Task 4 runs 2.79 seconds.</span><br><span class="line">All subprocesses done.</span><br></pre></td></tr></table></figure><p>代码解读：</p><p>apply_async(func[, args[, kwds[, callback]]])：它是<strong>非阻塞</strong>，apply(func[, args[, kwds]])是<strong>阻塞</strong>的。如果是后者的话，执行如果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Parent process 502288.</span><br><span class="line">Run task 0 (503980)...</span><br><span class="line">Task 0 runs 2.45 seconds.</span><br><span class="line">Run task 1 (500276)...</span><br><span class="line">Task 1 runs 0.97 seconds.</span><br><span class="line">Run task 2 (490944)...</span><br><span class="line">Task 2 runs 0.17 seconds.</span><br><span class="line">Run task 3 (504292)...</span><br><span class="line">Task 3 runs 2.23 seconds.</span><br><span class="line">Run task 4 (503980)...</span><br><span class="line">Task 4 runs 0.83 seconds.</span><br><span class="line">Waiting for all subprocesses done...</span><br><span class="line">All subprocesses done.</span><br></pre></td></tr></table></figure><p>对<code>Pool</code>对象调用<code>join()</code>方法会等待所有子进程执行完毕，调用<code>join()</code>之前必须先调用<code>close()</code>，调用<code>close()</code>之后就不能继续添加新的<code>Process</code>了。</p><p>请注意输出的结果，task <code>0</code>，<code>1</code>，<code>2</code>，<code>3</code>是立刻执行的，而task <code>4</code>要等待前面某个task完成后才执行，这是因为<code>Pool</code>的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是<code>Pool</code>有意设计的限制，并不是操作系统的限制。如果改成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p = Pool(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>就可以同时跑5个进程。</p><p>由于<code>Pool</code>的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。</p><h3 id="子进程"><a href="#子进程" class="headerlink" title="子进程"></a>子进程</h3><p>很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。</p><p><strong><code>subprocess</code>模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。</strong></p><p>下面的例子演示了如何在Python代码中运行命令<code>nslookup www.python.org</code>，这和命令行直接运行的效果是一样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line">print(<span class="string">'$ nslookup www.python.org'</span>)</span><br><span class="line">r = subprocess.call([<span class="string">'nslookup'</span>, <span class="string">'www.python.org'</span>])</span><br><span class="line">print(<span class="string">'Exit code:'</span>, r)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ nslookup www.python.org</span><br><span class="line">Server:192.168.19.4</span><br><span class="line">Address:192.168.19.4#53</span><br><span class="line"></span><br><span class="line">Non-authoritative answer:</span><br><span class="line">www.python.orgcanonical name = python.map.fastly.net.</span><br><span class="line">Name:python.map.fastly.net</span><br><span class="line">Address: 199.27.79.223</span><br><span class="line"></span><br><span class="line">Exit code: 0</span><br></pre></td></tr></table></figure><p>如果子进程还需要输入，则可以通过<code>communicate()</code>方法输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line">print(<span class="string">'$ nslookup'</span>)</span><br><span class="line">p = subprocess.Popen([<span class="string">'nslookup'</span>], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)</span><br><span class="line">output, err = p.communicate(<span class="string">b'set q=mx\npython.org\nexit\n'</span>)</span><br><span class="line">print(output.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">print(<span class="string">'Exit code:'</span>, p.returncode)</span><br></pre></td></tr></table></figure><p>上面的代码相当于在命令行执行命令<code>nslookup</code>，然后手动输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set q=mx</span><br><span class="line">python.org</span><br><span class="line">exit</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ nslookup</span><br><span class="line">Server:192.168.19.4</span><br><span class="line">Address:192.168.19.4#53</span><br><span class="line"></span><br><span class="line">Non-authoritative answer:</span><br><span class="line">python.orgmail exchanger = 50 mail.python.org.</span><br><span class="line"></span><br><span class="line">Authoritative answers can be found from:</span><br><span class="line">mail.python.orginternet address = 82.94.164.166</span><br><span class="line">mail.python.orghas AAAA address 2001:888:2000:d::a6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Exit code: 0</span><br></pre></td></tr></table></figure><h3 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h3><p><code>Process</code>之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的<code>multiprocessing</code>模块包装了底层的机制，提供了<code>Queue</code>、<code>Pipes</code>等多种方式来交换数据。</p><p>我们以<code>Queue</code>为例，在父进程中创建两个子进程，一个往<code>Queue</code>里写数据，一个从<code>Queue</code>里读数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写数据进程执行的代码:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(q)</span>:</span></span><br><span class="line">    print(<span class="string">'Process to write: %s'</span> % os.getpid())</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>]:</span><br><span class="line">        print(<span class="string">'Put %s to queue...'</span> % value)</span><br><span class="line">        q.put(value)</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读数据进程执行的代码:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(q)</span>:</span></span><br><span class="line">    print(<span class="string">'Process to read: %s'</span> % os.getpid())</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        value = q.get(<span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">'Get %s from queue.'</span> % value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 父进程创建Queue，并传给各个子进程：</span></span><br><span class="line">    q = Queue()</span><br><span class="line">    pw = Process(target=write, args=(q,))</span><br><span class="line">    pr = Process(target=read, args=(q,))</span><br><span class="line">    <span class="comment"># 启动子进程pw，写入:</span></span><br><span class="line">    pw.start()</span><br><span class="line">    <span class="comment"># 启动子进程pr，读取:</span></span><br><span class="line">    pr.start()</span><br><span class="line">    <span class="comment"># 等待pw结束:</span></span><br><span class="line">    pw.join()</span><br><span class="line">    <span class="comment"># pr进程里是死循环，无法等待其结束，只能强行终止:</span></span><br><span class="line">    pr.terminate()</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Process to write: 50563</span><br><span class="line">Put A to queue...</span><br><span class="line">Process to read: 50564</span><br><span class="line">Get A from queue.</span><br><span class="line">Put B to queue...</span><br><span class="line">Get B from queue.</span><br><span class="line">Put C to queue...</span><br><span class="line">Get C from queue.</span><br></pre></td></tr></table></figure><p>在Unix/Linux下，<code>multiprocessing</code>模块封装了<code>fork()</code>调用，使我们不需要关注<code>fork()</code>的细节。由于Windows没有<code>fork</code>调用，因此，<code>multiprocessing</code>需要“模拟”出<code>fork</code>的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所以，如果<code>multiprocessing</code>在Windows下调用失败了，要先考虑是不是pickle失败了。</p><h3 id="实战：进程池中使用不同子函数"><a href="#实战：进程池中使用不同子函数" class="headerlink" title="实战：进程池中使用不同子函数"></a>实战：进程池中使用不同子函数</h3><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Lee</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"\nRun task Lee-%s"</span> %(os.getpid())) <span class="comment">#os.getpid()获取当前的进程的ID</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * <span class="number">10</span>) <span class="comment">#random.random()随机生成0-1之间的小数</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'Task Lee, runs %0.2f seconds.'</span> %(end - start))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Marlon</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"\nRun task Marlon-%s"</span> %(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * <span class="number">40</span>)</span><br><span class="line">    end=time.time()</span><br><span class="line">    print(<span class="string">'Task Marlon runs %0.2f seconds.'</span> %(end - start))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Allen</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"\nRun task Allen-%s"</span> %(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * <span class="number">30</span>)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'Task Allen runs %0.2f seconds.'</span> %(end - start))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Frank</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"\nRun task Frank-%s"</span> %(os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * <span class="number">20</span>)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'Task Frank runs %0.2f seconds.'</span> %(end - start))</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    function_list=  [Lee, Marlon, Allen, Frank] </span><br><span class="line">    print(<span class="string">"parent process %s"</span> %(os.getpid()))</span><br><span class="line"></span><br><span class="line">    pool=multiprocessing.Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> func <span class="keyword">in</span> function_list:</span><br><span class="line">        pool.apply_async(func)     <span class="comment">#Pool执行函数，apply执行函数,当有一个进程执行完毕后，会添加一个新的进程到pool中</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Waiting for all subprocesses done...'</span>)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()    <span class="comment">#调用join之前，一定要先调用close() 函数，否则会出错, close()执行后不会有新的进程加入到pool,join函数等待素有子进程结束</span></span><br><span class="line">    print(<span class="string">'All subprocesses done.'</span>)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">parent process <span class="number">24276</span></span><br><span class="line">Waiting <span class="keyword">for</span> all subprocesses done...</span><br><span class="line"></span><br><span class="line">Run task Lee<span class="number">-16688</span></span><br><span class="line">Run task Marlon<span class="number">-27824</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Run task Allen<span class="number">-2188</span></span><br><span class="line"></span><br><span class="line">Run task Frank<span class="number">-27380</span></span><br><span class="line">Task Marlon runs <span class="number">2.47</span> seconds.</span><br><span class="line">Task Frank runs <span class="number">5.59</span> seconds.</span><br><span class="line">Task Lee, runs <span class="number">6.12</span> seconds.</span><br><span class="line">Task Allen runs <span class="number">19.02</span> seconds.</span><br><span class="line">All subprocesses done.</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在Unix/Linux下，可以使用<code>fork()</code>调用实现多进程。</p><p>要实现跨平台的多进程，可以使用<code>multiprocessing</code>模块。</p><p>进程间通信是通过<code>Queue</code>、<code>Pipes</code>等实现的。</p><p>要想实现分布式进程，可以考虑multiprocessing的managers子模块，把多进程分布到多台机器上。详细请参考<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017631559645600" target="_blank" rel="noopener">分布式进程</a>。</p><p>关于<code>Pool.apply</code>, <code>Pool.apply_async</code>, <code>Pool.map</code> and <code>Pool.map_async</code>的区别请参考<a href="https://stackoverflow.com/a/59663852/15304315" target="_blank" rel="noopener">multiprocessing.Pool: When to use apply, apply_async or map?</a></p><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>多任务可以由多进程完成，也可以由一个进程内的多线程完成。</p><p>我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。</p><p>由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。</p><p>Python的标准库提供了两个模块：<code>_thread</code>和<code>threading</code>，<code>_thread</code>是低级模块，<code>threading</code>是高级模块，对<code>_thread</code>进行了封装。绝大多数情况下，我们只需要使用<code>threading</code>这个高级模块。</p><p>启动一个线程就是把一个函数传入并创建<code>Thread</code>实例，然后调用<code>start()</code>开始执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time, threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新线程执行的代码:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'thread %s is running...'</span> % threading.current_thread().name)</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; <span class="number">5</span>:</span><br><span class="line">        n = n + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'thread %s &gt;&gt;&gt; %s'</span> % (threading.current_thread().name, n))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'thread %s ended.'</span> % threading.current_thread().name)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'thread %s is running...'</span> % threading.current_thread().name)</span><br><span class="line">t = threading.Thread(target=loop, name=<span class="string">'LoopThread'</span>)</span><br><span class="line">t.start()</span><br><span class="line">t.join()</span><br><span class="line">print(<span class="string">'thread %s ended.'</span> % threading.current_thread().name)</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">thread MainThread is running...</span><br><span class="line">thread LoopThread is running...</span><br><span class="line">thread LoopThread &gt;&gt;&gt; 1</span><br><span class="line">thread LoopThread &gt;&gt;&gt; 2</span><br><span class="line">thread LoopThread &gt;&gt;&gt; 3</span><br><span class="line">thread LoopThread &gt;&gt;&gt; 4</span><br><span class="line">thread LoopThread &gt;&gt;&gt; 5</span><br><span class="line">thread LoopThread ended.</span><br><span class="line">thread MainThread ended.</span><br></pre></td></tr></table></figure><p>由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的<code>threading</code>模块有个<code>current_thread()</code>函数，它永远返回当前线程的实例。主线程实例的名字叫<code>MainThread</code>，子线程的名字在创建时指定，我们用<code>LoopThread</code>命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为<code>Thread-1</code>，<code>Thread-2</code>……</p><h3 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h3><p>多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。</p><p>来看看多个线程同时操作一个变量怎么把内容给改乱了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time, threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定这是你的银行存款:</span></span><br><span class="line">balance = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_it</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="comment"># 先存后取，结果应该为0:</span></span><br><span class="line">    <span class="keyword">global</span> balance</span><br><span class="line">    balance = balance + n</span><br><span class="line">    balance = balance - n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_thread</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000000</span>):</span><br><span class="line">        change_it(n)</span><br><span class="line"></span><br><span class="line">t1 = threading.Thread(target=run_thread, args=(<span class="number">5</span>,))</span><br><span class="line">t2 = threading.Thread(target=run_thread, args=(<span class="number">8</span>,))</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br><span class="line">print(balance)</span><br></pre></td></tr></table></figure><p> 我们定义了一个共享变量<code>balance</code>，初始值为<code>0</code>，并且启动两个线程，先存后取，理论上结果应该为<code>0</code>，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，<code>balance</code>的结果就不一定是<code>0</code>了。</p><p>原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">balance = balance + n</span><br></pre></td></tr></table></figure><p>也分两步：</p><ol><li>计算<code>balance + n</code>，存入临时变量中；</li><li>将临时变量的值赋给<code>balance</code>。</li></ol><p>也就是可以看成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = balance + n</span><br><span class="line">balance = x</span><br></pre></td></tr></table></figure><p>由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">初始值 balance = 0</span><br><span class="line"></span><br><span class="line">t1: x1 = balance + 5 # x1 = 0 + 5 = 5</span><br><span class="line">t1: balance = x1     # balance = 5</span><br><span class="line">t1: x1 = balance - 5 # x1 = 5 - 5 = 0</span><br><span class="line">t1: balance = x1     # balance = 0</span><br><span class="line"></span><br><span class="line">t2: x2 = balance + 8 # x2 = 0 + 8 = 8</span><br><span class="line">t2: balance = x2     # balance = 8</span><br><span class="line">t2: x2 = balance - 8 # x2 = 8 - 8 = 0</span><br><span class="line">t2: balance = x2     # balance = 0</span><br><span class="line">    </span><br><span class="line">结果 balance = 0</span><br></pre></td></tr></table></figure><p>但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">初始值 balance = 0</span><br><span class="line"></span><br><span class="line">t1: x1 = balance + 5  # x1 = 0 + 5 = 5</span><br><span class="line"></span><br><span class="line">t2: x2 = balance + 8  # x2 = 0 + 8 = 8</span><br><span class="line">t2: balance = x2      # balance = 8</span><br><span class="line"></span><br><span class="line">t1: balance = x1      # balance = 5</span><br><span class="line">t1: x1 = balance - 5  # x1 = 5 - 5 = 0</span><br><span class="line">t1: balance = x1      # balance = 0</span><br><span class="line"></span><br><span class="line">t2: x2 = balance - 8  # x2 = 0 - 8 = -8</span><br><span class="line">t2: balance = x2      # balance = -8</span><br><span class="line"></span><br><span class="line">结果 balance = -8</span><br></pre></td></tr></table></figure><p>究其原因，是因为修改<code>balance</code>需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。</p><p>两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改<code>balance</code>的时候，别的线程一定不能改。</p><p>如果我们要确保<code>balance</code>计算正确，就要给<code>change_it()</code>上一把锁，当某个线程开始执行<code>change_it()</code>时，我们说，该线程因为获得了锁，因此其他线程不能同时执行<code>change_it()</code>，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过<code>threading.Lock()</code>来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">balance = <span class="number">0</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_thread</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100000</span>):</span><br><span class="line">        <span class="comment"># 先要获取锁:</span></span><br><span class="line">        lock.acquire()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 放心地改吧:</span></span><br><span class="line">            change_it(n)</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            <span class="comment"># 改完了一定要释放锁:</span></span><br><span class="line">            lock.release()</span><br></pre></td></tr></table></figure><p>当多个线程同时执行<code>lock.acquire()</code>时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。</p><p>获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用<code>try...finally</code>来确保锁一定会被释放。</p><p>锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。</p><h3 id="多核CPU"><a href="#多核CPU" class="headerlink" title="多核CPU"></a>多核CPU</h3><p>如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。</p><p>如果写一个死循环的话，会出现什么情况呢？打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。我们可以监控到一个死循环线程会100%占用一个CPU。如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。</p><p>试试用Python写个死循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading, multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x = x ^ <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(multiprocessing.cpu_count()):</span><br><span class="line">    t = threading.Thread(target=loop)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure><p>启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。</p><p>但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？</p><p>因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个<strong>GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行</strong>。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。</p><p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p><p>所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。</p><p>不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。</p><p>Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦</p><h2 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h2><p>在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。</p><p>但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦。如下代码所示，假设这份代码需要多线程调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_student</span><span class="params">(name)</span>:</span></span><br><span class="line">    std = Student(name)</span><br><span class="line">    <span class="comment"># std是局部变量，但是每个函数都要用它，因此必须传进去：</span></span><br><span class="line">    do_task_1(std)</span><br><span class="line">    do_task_2(std)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_1</span><span class="params">(std)</span>:</span></span><br><span class="line">    do_subtask_1(std)</span><br><span class="line">    do_subtask_2(std)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_2</span><span class="params">(std)</span>:</span></span><br><span class="line">    do_subtask_2(std)</span><br><span class="line">    do_subtask_2(std)</span><br></pre></td></tr></table></figure><p>每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的<code>Student</code>对象，不能共享。</p><p>如果用一个全局<code>dict</code>存放所有的<code>Student</code>对象，然后以<code>thread</code>自身作为<code>key</code>获得线程对应的<code>Student</code>对象如何？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">global_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">std_thread</span><span class="params">(name)</span>:</span></span><br><span class="line">    std = Student(name)</span><br><span class="line">    <span class="comment"># 把std放到全局变量global_dict中：</span></span><br><span class="line">    global_dict[threading.current_thread()] = std</span><br><span class="line">    do_task_1()</span><br><span class="line">    do_task_2()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 不传入std，而是根据当前线程查找：</span></span><br><span class="line">    std = global_dict[threading.current_thread()]</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 任何函数都可以查找出当前线程的std变量：</span></span><br><span class="line">    std = global_dict[threading.current_thread()]</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>这种方式理论上是可行的，它最大的优点是消除了<code>std</code>对象在每层函数中的传递问题，但是，每个函数获取<code>std</code>的代码有点丑。</p><p>有没有更简单的方式？</p><p><code>ThreadLocal</code>应运而生，不用查找<code>dict</code>，<code>ThreadLocal</code>帮你自动做这件事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 创建全局ThreadLocal对象:</span></span><br><span class="line">local_school = threading.local()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_student</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 获取当前线程关联的student:</span></span><br><span class="line">    std = local_school.student</span><br><span class="line">    print(<span class="string">'Hello, %s (in %s)'</span> % (std, threading.current_thread().name))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_thread</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="comment"># 绑定ThreadLocal的student:</span></span><br><span class="line">    local_school.student = name</span><br><span class="line">    process_student()</span><br><span class="line"></span><br><span class="line">t1 = threading.Thread(target= process_thread, args=(<span class="string">'Alice'</span>,), name=<span class="string">'Thread-A'</span>)</span><br><span class="line">t2 = threading.Thread(target= process_thread, args=(<span class="string">'Bob'</span>,), name=<span class="string">'Thread-B'</span>)</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br></pre></td></tr></table></figure><p>执行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hello, Alice (in Thread-A)</span><br><span class="line">Hello, Bob (in Thread-B)</span><br></pre></td></tr></table></figure><p>全局变量<code>local_school</code>就是一个<code>ThreadLocal</code>对象，每个<code>Thread</code>对它都可以读写<code>student</code>属性，但互不影响。你可以把<code>local_school</code>看成全局变量，但每个属性如<code>local_school.student</code>都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，<code>ThreadLocal</code>内部会处理。</p><p>可以理解为全局变量<code>local_school</code>是一个<code>dict</code>，不但可以用<code>local_school.student</code>，还可以绑定其他变量，如<code>local_school.teacher</code>等等。</p><p><code>ThreadLocal</code>最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。</p><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>一个<code>ThreadLocal</code>变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。<code>ThreadLocal</code>解决了参数在一个线程中各个函数之间互相传递的问题。</p><h2 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h2><p>协程，又称微线程，纤程。英文名 Coroutine。协程的概念很早就提出来了，但直到最近几年才在某些语言（如 Lua）中得到广泛应用。</p><p>子程序，或者称为函数，在所有语言中都是层级调用，比如 A 调用 B，B 在执行过程中又调用了 C，C 执行完毕返回，B 执行完毕返回，最后是 A 执行完毕。所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。</p><p>子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。协程看上去也是子程序，但执行过程中，<strong>在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。</strong></p><p>注意，在一个子程序中中断，去执行其他子程序，不是函数调用，有点类似 CPU 的中断。比如子程序 A、B：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">A</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'1'</span>)</span><br><span class="line">    print(<span class="string">'2'</span>)</span><br><span class="line">    print(<span class="string">'3'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">B</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'x'</span>)</span><br><span class="line">    print(<span class="string">'y'</span>)</span><br><span class="line">    print(<span class="string">'z'</span>)</span><br></pre></td></tr></table></figure><p>假设由协程执行，在执行 A 的过程中，可以随时中断，去执行 B，B 也可能在执行过程中中断再去执行 A，结果可能是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">x</span><br><span class="line">y</span><br><span class="line">3</span><br><span class="line">z</span><br></pre></td></tr></table></figure><p>但是在 A 中是没有调用 B 的，所以协程的调用比函数调用理解起来要难一些。</p><p><strong>看起来 A、B 的执行有点像多线程，但协程的特点在于是一个线程执行</strong>，那和多线程比，协程有何优势？</p><ul><li><p>最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。</p></li><li><p>第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。</p></li></ul><p>因为协程是一个线程执行，那怎么利用多核 CPU 呢？最简单的方法是<strong>多进程 + 协程，既充分利用多核，又充分发挥协程的高效率</strong>，可获得极高的性能。</p><p>Python 对协程的支持是通过 generator 实现的。在 generator 中，我们不但可以通过<code>for</code>循环来迭代，还可以不断调用<code>next()</code>函数获取由<code>yield</code>语句返回的下一个值。但是 Python 的<code>yield</code>不但可以返回一个值，它还可以接收调用者发出的参数。</p><p>来看例子：</p><p>传统的生产者 - 消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。如果改用协程，生产者生产消息后，直接通过<code>yield</code>跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span><span class="params">()</span>:</span></span><br><span class="line">    r = <span class="string">''</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = <span class="keyword">yield</span> r</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> n:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        print(<span class="string">'[CONSUMER] Consuming %s...'</span> % n)</span><br><span class="line">        r = <span class="string">'200 OK'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">produce</span><span class="params">(c)</span>:</span></span><br><span class="line">    c.send(<span class="literal">None</span>)</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; <span class="number">5</span>:</span><br><span class="line">        n = n + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'[PRODUCER] Producing %s...'</span> % n)</span><br><span class="line">        r = c.send(n)</span><br><span class="line">        print(<span class="string">'[PRODUCER] Consumer return: %s'</span> % r)</span><br><span class="line">    c.close()</span><br><span class="line"></span><br><span class="line">c = consumer()</span><br><span class="line">produce(c)</span><br></pre></td></tr></table></figure><p>执行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[PRODUCER] Producing <span class="number">1.</span>..</span><br><span class="line">[CONSUMER] Consuming <span class="number">1.</span>..</span><br><span class="line">[PRODUCER] Consumer <span class="keyword">return</span>: <span class="number">200</span> OK</span><br><span class="line">[PRODUCER] Producing <span class="number">2.</span>..</span><br><span class="line">[CONSUMER] Consuming <span class="number">2.</span>..</span><br><span class="line">[PRODUCER] Consumer <span class="keyword">return</span>: <span class="number">200</span> OK</span><br><span class="line">[PRODUCER] Producing <span class="number">3.</span>..</span><br><span class="line">[CONSUMER] Consuming <span class="number">3.</span>..</span><br><span class="line">[PRODUCER] Consumer <span class="keyword">return</span>: <span class="number">200</span> OK</span><br><span class="line">[PRODUCER] Producing <span class="number">4.</span>..</span><br><span class="line">[CONSUMER] Consuming <span class="number">4.</span>..</span><br><span class="line">[PRODUCER] Consumer <span class="keyword">return</span>: <span class="number">200</span> OK</span><br><span class="line">[PRODUCER] Producing <span class="number">5.</span>..</span><br><span class="line">[CONSUMER] Consuming <span class="number">5.</span>..</span><br><span class="line">[PRODUCER] Consumer <span class="keyword">return</span>: <span class="number">200</span> OK</span><br></pre></td></tr></table></figure><p>注意到<code>consumer</code>函数是一个<code>generator</code>，把一个<code>consumer</code>传入<code>produce</code>后：</p><ol><li><p>首先调用<code>c.send(None)</code>启动生成器；</p></li><li><p>然后，一旦生产了东西，通过<code>c.send(n)</code>切换到<code>consumer</code>执行；</p></li><li><p><code>consumer</code>通过<code>yield</code>拿到消息，处理，又通过<code>yield</code>把结果传回；</p></li><li><p><code>produce</code>拿到<code>consumer</code>处理的结果，继续生产下一条消息；</p></li><li><p><code>produce</code>决定不生产了，通过<code>c.close()</code>关闭<code>consumer</code>，整个过程结束。</p></li></ol><p>整个流程无锁，由一个线程执行，<code>produce</code>和<code>consumer</code>协作完成任务，所以称为 “协程”，而非线程的抢占式多任务。</p><p>看完之后还是一脸懵逼，要看懂上面的例子，关键在于要理解下面几点：</p><p>1、例子中的<code>c.send(None)</code>，其功能类似于<code>next(c)</code>，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">num</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = num()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.send(<span class="literal">None</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.send(<span class="literal">None</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.send(<span class="literal">None</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;pyshell#9&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    c.send(<span class="literal">None</span>)</span><br><span class="line">StopIteration</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>2、<code>n = yield r</code>，这里是一条语句，但要理解两个知识点，赋值语句先计算<code>=</code> 右边，由于右边是 <code>yield</code> 语句，所以<code>yield</code>语句执行完以后，进入暂停，而赋值语句在下一次启动生成器的时候首先被执行；</p><p>3、<code>send</code> 在接受<code>None</code>参数的情况下，等同于<code>next(generator)</code>的功能，但<code>send</code>同时也可接收其他参数，比如例子中的<code>c.send(n)</code>，要理解这种用法，先看一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">num</span><span class="params">()</span>:</span></span><br><span class="line">        a = <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            a = <span class="keyword">yield</span> a</span><br><span class="line">       </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = num()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.send(<span class="literal">None</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.send(<span class="number">5</span>)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.send(<span class="number">100</span>)</span><br><span class="line"><span class="number">100</span></span><br></pre></td></tr></table></figure><p>在上面的例子中，首先使用 <code>c.send(None)</code>，返回生成器的第一个值，<code>a = yield 1</code> ，也就是<code>1</code>（但此时，并未执行赋值语句），</p><p>接着我们使用了<code>c.send(5)</code>，再次启动生成器，并同时传入了一个参数<code>5</code>，再次启动生成的时候，从上次<code>yield</code>语句断掉的地方开始执行，即 <code>a</code> 的赋值语句，由于我们传入了一个参数<code>5</code>，所以<code>a</code>被赋值为5，接着程序进入<code>whlie</code>循环，当程序执行到 <code>a = yield a</code>，同理，先返回生成器的值 <code>5</code>，下次启动生成器的时候，再执行赋值语句，以此类推…</p><p>所以<code>c.send(n)</code>的用法就是老师上文中所说的 ，” Python的<code>yield</code>不但可以返回一个值，它还可以接收调用者发出的参数。”</p><p><strong>但注意，在一个生成器函数未启动之前，是不能传递值进去。也就是说在使用<code>c.send(n)</code>之前，必须先使用<code>c.send(None)</code>或者<code>next(c)</code>来返回生成器的第一个值。</strong></p><p>最后我们来看上文中的例子，梳理下执行过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span><span class="params">()</span>:</span></span><br><span class="line">    r = <span class="string">''</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = <span class="keyword">yield</span> r</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> n:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        print(<span class="string">'[CONSUMER] Consuming %s...'</span> % n)</span><br><span class="line">        r = <span class="string">'200 OK'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">produce</span><span class="params">(c)</span>:</span></span><br><span class="line">    c.send(<span class="literal">None</span>)</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; <span class="number">5</span>:</span><br><span class="line">        n = n + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'[PRODUCER] Producing %s...'</span> % n)</span><br><span class="line">        r = c.send(n)</span><br><span class="line">        print(<span class="string">'[PRODUCER] Consumer return: %s'</span> % r)</span><br><span class="line">    c.close()</span><br><span class="line"></span><br><span class="line">c = consumer()</span><br><span class="line">produce(c)</span><br></pre></td></tr></table></figure><p>第一步：执行 <code>c.send(None)</code>，启动生成器返回第一个值，<code>n = yield r</code>，此时 <code>r</code> 为空，<code>n</code> 还未赋值，然后生成器暂停，等待下一次启动。</p><p>第二步：生成器返回空值后进入暂停，<code>produce(c)</code> 接着往下运行，进入<code>While</code>循环，此时 <code>n</code> 为<code>1</code>，所以打印：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[PRODUCER] Producing 1...</span><br></pre></td></tr></table></figure><p>第三步：<code>produce(c)</code> 往下运行到 <code>r = c.send(1)</code>，再次启动生成器，并传入了参数<code>1</code>，而生成器从上次<code>n</code>的赋值语句开始执行，<code>n</code> 被赋值为<code>1</code>，<code>n</code>存在，<code>if</code> 语句不执行，然后打印：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CONSUMER] Consuming 1...</span><br></pre></td></tr></table></figure><p>接着<code>r</code>被赋值为<code>&#39;200 OK&#39;</code>，然后又进入循环，执行<code>n = yield r</code>，返回生成器的第二个值，<code>&#39;200 OK&#39;</code>，然后生成器进入暂停，等待下一次启动。</p><p>第四步：生成器返回<code>&#39;200 OK&#39;</code>进入暂停后，<code>produce(c)</code>往下运行，进入<code>r</code>的赋值语句，<code>r</code>被赋值为<code>&#39;200 OK&#39;</code>，接着往下运行，打印：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[PRODUCER] Consumer return: 200 OK</span><br></pre></td></tr></table></figure><p>以此类推…</p><p>当<code>n</code>为<code>5</code>跳出循环后，使用<code>c.close()</code> 结束生成器的生命周期，然后程序运行结束。</p><p>最后套用 Donald Knuth 的一句话总结协程的特点：“子程序就是协程的一种特例。”</p><h2 id="asyncio实现异步IO"><a href="#asyncio实现异步IO" class="headerlink" title="asyncio实现异步IO"></a>asyncio实现异步IO</h2><p><code>asyncio</code>是Python 3.4版本引入的标准库，直接内置了对异步IO的支持。具体来说，协程都是通过使用<code>yield from</code>和<code>asyncio模块</code>中的<code>@asyncio.coroutine</code>来实现的。<code>asyncio</code>专门被用来实现异步IO操作。</p><h3 id="为什么需要协程"><a href="#为什么需要协程" class="headerlink" title="为什么需要协程"></a>为什么需要协程</h3><p>下面我们先来看一个用普通同步代码实现多个 IO 任务的案例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通同步代码实现多个IO任务</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">taskIO_1</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'开始运行IO任务1...'</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)  <span class="comment"># 假设该任务耗时2s</span></span><br><span class="line">    print(<span class="string">'IO任务1已完成，耗时2s'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">taskIO_2</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'开始运行IO任务2...'</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)  <span class="comment"># 假设该任务耗时3s</span></span><br><span class="line">    print(<span class="string">'IO任务2已完成，耗时3s'</span>)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">taskIO_1()</span><br><span class="line">taskIO_2()</span><br><span class="line">print(<span class="string">'所有IO任务总耗时%.5f秒'</span> % float(time.time()-start))</span><br></pre></td></tr></table></figure><p>执行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">开始运行IO任务1...</span><br><span class="line">IO任务1已完成，耗时2s</span><br><span class="line">开始运行IO任务2...</span><br><span class="line">IO任务2已完成，耗时3s</span><br><span class="line">所有IO任务总耗时5.00604秒</span><br></pre></td></tr></table></figure><p>上面，我们顺序实现了两个同步 IO 任务<code>taskIO_1()</code>和<code>taskIO_2()</code>，则最后总耗时就是 <strong>5 秒</strong>。我们都知道，在计算机中 CPU 的运算速率要远远大于 IO 速率，而当 CPU 运算完毕后，如果再要闲置很长时间去等待 IO 任务完成才能进行下一个任务的计算，这样的任务执行效率很低。</p><p>所以我们需要有一种异步的方式来处理类似上述任务，会极大增加效率 (当然就是协程啦～)。而我们最初很容易想到的，<strong>是能否在上述 IO 任务执行前中断当前 IO 任务</strong> (对应于上述代码<code>time.sleep(2)</code>)，<strong>进行下一个任务，当该 IO 任务完成后再唤醒该任务。</strong></p><p>而在 Python 中生成器中的关键字<code>yield</code>可以实现<strong>中断功能</strong>。所以起初，协程是基于生成器的变形进行实现的，之后虽然编码形式有变化，但基本原理还是一样的。</p><h3 id="yield与yield-from"><a href="#yield与yield-from" class="headerlink" title="yield与yield from"></a>yield与yield from</h3><p><code>yield</code>在生成器中有中断的功能，可以传出值，也可以从函数外部接收值，而<code>yield from</code>的实现就是简化了<code>yield</code>操作。看下面案例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_1</span><span class="params">(titles)</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> titles</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_2</span><span class="params">(titles)</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> titles</span><br><span class="line"></span><br><span class="line">titles = [<span class="string">'Python'</span>,<span class="string">'Java'</span>,<span class="string">'C++'</span>]</span><br><span class="line"><span class="keyword">for</span> title <span class="keyword">in</span> generator_1(titles):</span><br><span class="line">    print(<span class="string">'生成器1:'</span>,title)</span><br><span class="line"><span class="keyword">for</span> title <span class="keyword">in</span> generator_2(titles):</span><br><span class="line">    print(<span class="string">'生成器2:'</span>,title)</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">生成器1: [&apos;Python&apos;, &apos;Java&apos;, &apos;C++&apos;]</span><br><span class="line">生成器2: Python</span><br><span class="line">生成器2: Java</span><br><span class="line">生成器2: C++</span><br></pre></td></tr></table></figure><p>在这个例子中<code>yield titles</code>返回了<code>titles</code>完整列表，而<code>yield from titles</code>实际等价于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> title <span class="keyword">in</span> titles:　<span class="comment"># 等价于yield from titles</span></span><br><span class="line">    <span class="keyword">yield</span> title</span><br></pre></td></tr></table></figure><p>而yield from功能还不止于此，它还有一个主要的功能是省去了很多异常的处理，不再需要我们手动编写，其内部已经实现大部分异常处理。</p><p>举个例子，下面通过生成器来实现一个整数加和的程序，通过send()函数向生成器中传入要加和的数字，然后最后以返回None结束，total保存最后加和的总数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_1</span><span class="params">()</span>:</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x = <span class="keyword">yield</span> </span><br><span class="line">        print(<span class="string">'加'</span>,x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> x:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        total += x</span><br><span class="line">    <span class="keyword">return</span> total</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_2</span><span class="params">()</span>:</span> <span class="comment"># 委托生成器</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        total = <span class="keyword">yield</span> <span class="keyword">from</span> generator_1() <span class="comment"># 子生成器</span></span><br><span class="line">        print(<span class="string">'加和总数是:'</span>,total)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span> <span class="comment"># 调用方</span></span><br><span class="line">    g1 = generator_1()</span><br><span class="line">    g1.send(<span class="literal">None</span>)</span><br><span class="line">    g1.send(<span class="number">2</span>)</span><br><span class="line">    g1.send(<span class="number">3</span>)</span><br><span class="line">    g1.send(<span class="literal">None</span>)</span><br><span class="line">    <span class="comment"># g2 = generator_2()</span></span><br><span class="line">    <span class="comment"># g2.send(None)</span></span><br><span class="line">    <span class="comment"># g2.send(2)</span></span><br><span class="line">    <span class="comment"># g2.send(3)</span></span><br><span class="line">    <span class="comment"># g2.send(None)</span></span><br><span class="line">    </span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>执行结果如下。可见对于生成器<code>g1</code>，在最后传入<code>None</code>后，程序退出，报<code>StopIteration</code>异常并返回了最后<code>total</code>值是５。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">加 <span class="number">2</span></span><br><span class="line">加 <span class="number">3</span></span><br><span class="line">加 <span class="literal">None</span></span><br><span class="line">------------------------------------------</span><br><span class="line">StopIteration       </span><br><span class="line">&lt;ipython-input<span class="number">-37</span>-cf298490352b&gt; <span class="keyword">in</span> main()</span><br><span class="line">---&gt; 19     g1.send(None)</span><br><span class="line">StopIteration: <span class="number">5</span></span><br></pre></td></tr></table></figure><p>如果把<code>g1.send()</code>那５行注释掉，解注下面的<code>g2.send()</code>代码，则结果如下。可见<code>yield from</code>封装了处理常见异常的代码。对于g2即便传入None也不报异常，其中<code>total = yield from generator_1()</code>返回给total的值是<code>generator_1()</code>最终的<code>return total</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">加 2</span><br><span class="line">加 3</span><br><span class="line">加 None</span><br><span class="line">加和总数是: 5</span><br></pre></td></tr></table></figure><p>借用上述例子，这里有几个概念需要理一下：</p><ul><li>子生成器：<code>yield from</code>后的<code>generator_1()</code>生成器函数是子生成器</li><li>委托生成器：<code>generator_2()</code>是程序中的委托生成器，它负责委托子生成器完成具体任务。</li><li>调用方：<code>main()</code>是程序中的调用方，负责调用委托生成器。</li></ul><p><strong><code>yield from</code>在其中还有一个关键的作用是：建立调用方和子生成器的通道</strong>，</p><ul><li>在上述代码中<code>main()</code>每一次在调用<code>send(value)</code>时，<code>value</code>不是传递给了<strong>委托生成器</strong>generator_2()，而是借助<code>yield from</code>传递给了<strong>子生成器</strong>generator_1()中的<code>yield</code></li><li>同理，<strong>子生成器</strong>中的数据也是通过<code>yield</code>直接发送到<strong>调用方</strong>main()中。</li></ul><h3 id="asyncio-coroutine实现协程"><a href="#asyncio-coroutine实现协程" class="headerlink" title="@asyncio.coroutine实现协程"></a>@asyncio.coroutine实现协程</h3><p>那<code>yield from</code>通常用在什么地方呢？在协程中，<strong>只要是和IO任务类似的、耗费时间的任务都需要使用<code>yield from</code>来进行中断，达到异步功能！</strong></p><p>我们在上面那个同步IO任务的代码中修改成协程的用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用同步方式编写异步功能</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="meta">@asyncio.coroutine # 标志协程的装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">taskIO_1</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'开始运行IO任务1...'</span>)</span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">2</span>)  <span class="comment"># 假设该任务耗时2s</span></span><br><span class="line">    print(<span class="string">'IO任务1已完成，耗时2s'</span>)</span><br><span class="line">    <span class="keyword">return</span> taskIO_1.__name__</span><br><span class="line"></span><br><span class="line"><span class="meta">@asyncio.coroutine # 标志协程的装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">taskIO_2</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'开始运行IO任务2...'</span>)</span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">3</span>)  <span class="comment"># 假设该任务耗时3s</span></span><br><span class="line">    print(<span class="string">'IO任务2已完成，耗时3s'</span>)</span><br><span class="line">    <span class="keyword">return</span> taskIO_2.__name__</span><br><span class="line"></span><br><span class="line"><span class="meta">@asyncio.coroutine # 标志协程的装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span> <span class="comment"># 调用方</span></span><br><span class="line">    tasks = [taskIO_1(), taskIO_2()]  <span class="comment"># 把所有任务添加到task中</span></span><br><span class="line">    done,pending = <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.wait(tasks) <span class="comment"># 子生成器</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> done: <span class="comment"># done和pending都是一个任务，所以返回结果需要逐个调用result()</span></span><br><span class="line">        print(<span class="string">'协程无序返回值：'</span>+r.result())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    loop = asyncio.get_event_loop() <span class="comment"># 创建一个事件循环对象loop</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        loop.run_until_complete(main()) <span class="comment"># 完成事件循环，直到最后一个任务结束</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        loop.close() <span class="comment"># 结束事件循环</span></span><br><span class="line">    print(<span class="string">'所有IO任务总耗时%.5f秒'</span> % float(time.time()-start))</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">开始运行IO任务1...</span><br><span class="line">开始运行IO任务2...</span><br><span class="line">IO任务1已完成，耗时2s</span><br><span class="line">IO任务2已完成，耗时3s</span><br><span class="line">协程无序返回值：taskIO_2</span><br><span class="line">协程无序返回值：taskIO_1</span><br><span class="line">所有IO任务总耗时3.00209秒</span><br></pre></td></tr></table></figure><p>【使用方法】： <code>@asyncio.coroutine</code>装饰器是协程函数的标志，我们需要在每一个任务函数前加这个装饰器，并在函数中使用<code>yield from</code>。在同步 IO 任务的代码中使用的<code>time.sleep(2)</code>来假设任务执行了 2 秒。但在<strong>协程中<code>yield from</code>后面必须是子生成器函数</strong>，而<code>time.sleep()</code>并不是生成器，所以这里需要使用内置模块提供的生成器函数<code>asyncio.sleep()</code>。 </p><p>【功能】：通过使用协程，极大增加了多任务执行效率，最后消耗的时间是任务队列中耗时最多的时间。上述例子中的总耗时 3 秒就是<code>taskIO_2()</code>的耗时时间。  </p><p>【执行过程】：</p><ol><li>上面代码先通过<code>get_event_loop()</code>获取了一个标准事件循环 loop(因为是一个，所以协程是单线程)</li><li>然后，我们通过<code>run_until_complete(main())</code>来运行协程 (此处把调用方协程 main() 作为参数，调用方负责调用其他委托生成器)，<code>run_until_complete</code>的特点就像该函数的名字，直到循环事件的所有事件都处理完才能完整结束。</li><li>进入调用方协程，我们把多个任务 [<code>taskIO_1()</code>和<code>taskIO_2()</code>] 放到一个<code>task</code>列表中，可理解为打包任务。</li><li>现在，我们使用<code>asyncio.wait(tasks)</code>来获取一个 <strong>awaitable objects 即可等待对象的集合</strong> (此处的 aws 是协程的列表)，<strong>并发运行传入的 aws</strong>，同时通过<code>yield from</code>返回一个包含<code>(done, pending)</code>的元组，<strong>done 表示已完成的任务列表，pending 表示未完成的任务列表</strong>；如果使用<code>asyncio.as_completed(tasks)</code>则会按完成顺序生成协程的<strong>迭代器</strong> (常用于 for 循环中)，因此当你用它迭代时，会尽快得到每个可用的结果。【此外，当轮询到某个事件时 (如 taskIO_1())，直到遇到该<strong>任务中的<code>yield from</code>中断</strong>，开始<strong>处理下一个事件</strong> (如 taskIO_2()))，当<code>yield from</code>后面的子生成器<strong>完成任务</strong>时，该事件才再次<strong>被唤醒</strong>】</li><li>因为<code>done</code>里面有我们需要的返回结果，但它目前还是个任务列表，所以要取出返回的结果值，我们遍历它并逐个调用<code>result()</code>取出结果即可。(注：对于<code>asyncio.wait()</code>和<code>asyncio.as_completed()</code>返回的结果均是先完成的任务结果排在前面，所以此时打印出的结果不一定和原始顺序相同，但使用<code>gather()</code>的话可以得到原始顺序的结果集，<a href="https://blog.csdn.net/SL_World/article/details/86691747" target="_blank" rel="noopener">两者更详细的案例说明见此</a>)</li><li>最后我们通过<code>loop.close()</code>关闭事件循环。</li></ol><p>综上所述：异步IO的完整实现是靠<strong>①事件循环＋②协程</strong>。有关更底层的原理可以参考<a href="https://stackoverflow.com/a/51116910/15304315" target="_blank" rel="noopener">How does asyncio work?</a>。</p><h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>我们用<code>asyncio</code>的异步网络连接来获取sina、sohu和163的网站首页：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wget</span><span class="params">(host)</span>:</span></span><br><span class="line">    print(<span class="string">'wget %s...'</span> % host)</span><br><span class="line">    connect = asyncio.open_connection(host, <span class="number">80</span>)</span><br><span class="line">    reader, writer = <span class="keyword">yield</span> <span class="keyword">from</span> connect</span><br><span class="line">    header = <span class="string">'GET / HTTP/1.0\r\nHost: %s\r\n\r\n'</span> % host</span><br><span class="line">    writer.write(header.encode(<span class="string">'utf-8'</span>))</span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> writer.drain()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = <span class="keyword">yield</span> <span class="keyword">from</span> reader.readline()</span><br><span class="line">        <span class="keyword">if</span> line == <span class="string">b'\r\n'</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        print(<span class="string">'%s header &gt; %s'</span> % (host, line.decode(<span class="string">'utf-8'</span>).rstrip()))</span><br><span class="line">    <span class="comment"># Ignore the body, close the socket</span></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks = [wget(host) <span class="keyword">for</span> host <span class="keyword">in</span> [<span class="string">'www.sina.com.cn'</span>, <span class="string">'www.sohu.com'</span>, <span class="string">'www.163.com'</span>]]</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">wget www.sohu.com...</span><br><span class="line">wget www.sina.com.cn...</span><br><span class="line">wget www.163.com...</span><br><span class="line">(等待一段时间)</span><br><span class="line">(打印出sohu的header)</span><br><span class="line">www.sohu.com header &gt; HTTP/1.1 200 OK</span><br><span class="line">www.sohu.com header &gt; Content-Type: text/html</span><br><span class="line">...</span><br><span class="line">(打印出sina的header)</span><br><span class="line">www.sina.com.cn header &gt; HTTP/1.1 200 OK</span><br><span class="line">www.sina.com.cn header &gt; Date: Wed, 20 May 2015 04:56:33 GMT</span><br><span class="line">...</span><br><span class="line">(打印出163的header)</span><br><span class="line">www.163.com header &gt; HTTP/1.0 302 Moved Temporarily</span><br><span class="line">www.163.com header &gt; Server: Cdn Cache Server V2.0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可见3个连接由一个线程通过<code>coroutine</code>并发完成。</p><h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><p><code>asyncio</code>提供了完善的异步IO支持；</p><p>异步操作需要在<code>coroutine</code>中通过<code>yield from</code>完成；</p><p>多个<code>coroutine</code>可以封装成一组Task然后并发执行。</p><p>用<code>asyncio</code>提供的<code>@asyncio.coroutine</code>可以把一个generator标记为coroutine类型，然后在coroutine内部用<code>yield from</code>调用另一个coroutine实现异步操作。</p><h2 id="async-await实现异步IO"><a href="#async-await实现异步IO" class="headerlink" title="async/await实现异步IO"></a>async/await实现异步IO</h2><p>为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法<code>async</code>和<code>await</code>，可以让coroutine的代码更简洁易读。</p><p>请注意，<code>async</code>和<code>await</code>是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换：</p><ol><li>把<code>@asyncio.coroutine</code>替换为<code>async</code>；</li><li>把<code>yield from</code>替换为<code>await</code>。</li></ol><p>让我们对比一下上一节的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'Hello world! (%s)'</span> % threading.currentThread())</span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'Hello again! (%s)'</span> % threading.currentThread())</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks = [hello(), hello()]</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure><p>用新语法重新编写如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"Hello world!"</span>)</span><br><span class="line">    r = <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"Hello again!"</span>)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks = [hello(), hello()]</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure><p>剩下的代码保持不变。</p><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><p>Python从3.5版本开始为<code>asyncio</code>提供了<code>async</code>和<code>await</code>的新语法；</p><p>注意新语法只能用在Python 3.5以及后续版本，如果使用3.4版本，则仍需使用上一节的方案。</p><h2 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h2><p><img src="/Python/基础语法/Python多进程、多线程、协程/47075401c21c4d87abc198d510b280b9.awebp" alt="img" style="zoom: 77%;"></p><p>首先介绍一个什么是CPU密集型计算、IO密集型计算？</p><ul><li>CPU密集型（CPU-bound）：CPU密集型也叫做计算密集型，是指IO在很短的时间内就可以完成，CPU需要大量的计算和处理，特点是CPU占用率高。例如压缩解压缩、加密解密、正则表达式搜索</li><li>IO密集型（IO-bound）：IO密集型是指系统运作大部分时间是CPU在等待I/O(硬盘/内存)的读/写操作，CPU占用率较低。例如文件处理程序，网络爬虫程序，读写数据库程序</li></ul><p>其次，对比一下多进程、多线程、多协程。</p><p><img src="/Python/基础语法/Python多进程、多线程、协程/cbb20fc6ddff43618f9ed0a2a0616905.awebp" alt="avatar" style="zoom:77%;"></p><p>那么该如何选择呢？</p><p><img src="/Python/基础语法/Python多进程、多线程、协程/f844661095f84bfc90ae088e13dc3940.awebp" alt="avatar" style="zoom:77%;"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824" target="_blank" rel="noopener">协程</a><br><a href="https://juejin.cn/post/7088521649070276644#heading-6" target="_blank" rel="noopener">python并发编程 多进程 多线程 多协程</a><br><a href="https://blog.csdn.net/SL_World/article/details/86597738" target="_blank" rel="noopener">Python异步IO之协程(一):从yield from到async的使用</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以下内容主要参考了廖雪峰老师的Python教程，在不懂的地方添加了自己的注释。&lt;/p&gt;
&lt;h2 id=&quot;多进程&quot;&gt;&lt;a href=&quot;#多进程&quot; class=&quot;headerlink&quot; title=&quot;多进程&quot;&gt;&lt;/a&gt;多进程&lt;/h2&gt;&lt;p&gt;Unix/Linux操作系统提供了一个&lt;code&gt;fork()&lt;/code&gt;系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是&lt;code&gt;fork()&lt;/code&gt;调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。&lt;/p&gt;
&lt;p&gt;子进程永远返回&lt;code&gt;0&lt;/code&gt;，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用&lt;code&gt;getppid()&lt;/code&gt;就可以拿到父进程的ID。&lt;/p&gt;
&lt;p&gt;Python的&lt;code&gt;os&lt;/code&gt;模块封装了常见的系统调用，其中就包括&lt;code&gt;fork&lt;/code&gt;，可以在Python程序中轻松创建子进程：&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/categories/Python/"/>
    
      <category term="基础语法" scheme="https://www.zdaiot.com/categories/Python/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    
    
      <category term="多线程" scheme="https://www.zdaiot.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="Python" scheme="https://www.zdaiot.com/tags/Python/"/>
    
      <category term="多进程" scheme="https://www.zdaiot.com/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
      <category term="协程" scheme="https://www.zdaiot.com/tags/%E5%8D%8F%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>并发、并行、线程、进程、异步、同步、阻塞、非阻塞</title>
    <link href="https://www.zdaiot.com/Python/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/%E5%B9%B6%E5%8F%91%E3%80%81%E5%B9%B6%E8%A1%8C%E3%80%81%E7%BA%BF%E7%A8%8B%E3%80%81%E8%BF%9B%E7%A8%8B%E3%80%81%E5%BC%82%E6%AD%A5%E3%80%81%E5%90%8C%E6%AD%A5%E3%80%81%E9%98%BB%E5%A1%9E%E3%80%81%E9%9D%9E%E9%98%BB%E5%A1%9E/"/>
    <id>https://www.zdaiot.com/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/</id>
    <published>2023-07-03T14:02:23.000Z</published>
    <updated>2023-07-03T14:02:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="并发-Concurrency-与并行-Parallelism"><a href="#并发-Concurrency-与并行-Parallelism" class="headerlink" title="并发(Concurrency)与并行(Parallelism)"></a>并发(Concurrency)与并行(Parallelism)</h2><p><strong>并发：</strong>两个或多个事件在同一时间间隔内发生。这些事情宏观上同时发生，微观上交替进行。</p><p><strong>并行：</strong>两个或多个事件在同一时刻同时发生。</p><h3 id="并发"><a href="#并发" class="headerlink" title="并发"></a>并发</h3><p>假设你同时被赋予了唱歌和吃饭的任务。在给定的时间，你要么唱歌，要么吃东西，因为在这两种情况下，你的嘴都会受到影响。因此，为了做到这一点，你会吃一段时间，然后唱歌，重复这个过程，直到你的食物吃完或歌曲结束。所以你并发完成了任务。</p><p>在单核环境中，通过上下文切换，在同一时间段内执行的任务发生并发，即在特定的时间段，只有一个任务被执行。</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/1E3lhTuU_P3bePvL6Nfwf_A.jpeg" alt="img" style="zoom: 50%;"></p><p>在多核环境中，可以通过同时执行多个任务的并行来实现并发。但是并发性仍不可少。比如对于4核CPU，同一时刻可以用四个程序并行执行，但通常我们需要四个以上程序同时工作，此时就需要并发了。</p><h3 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h3><p>假设你有两项任务：做饭和打电话给你的朋友。你可以同时做这两件事。你既可以做饭，也可以打电话。现在你是在并行地做你的任务。</p><p>并行意味着同时执行两个或多个任务。</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/1QbyO_eNcYHw8cUpvVR5AZw.jpeg" alt="img" style="zoom:50%;"></p><h3 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h3><ul><li>Concurrency and Parallelism refer to computer architectures which focus on how our tasks or computations are performed.</li><li>在单核环境中，通过上下文切换，在同一时间段内执行的任务发生并发，即在特定的时间段，只有一个任务被执行。</li><li>在多核环境中，可以通过同时执行多个任务的并行来实现并发。</li></ul><h2 id="线程-Threads-与进程-Processes"><a href="#线程-Threads-与进程-Processes" class="headerlink" title="线程(Threads)与进程(Processes)"></a>线程(Threads)与进程(Processes)</h2><p>线程是CPU调度的最小单元，进程是资源分配的最小单元。</p><p>进程是running program的一个instance，一个program可以有多个进程。</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/1y9TmSoiKM2zJboi7EZeBJw.png" alt="img" style="zoom: 67%;"></p><h2 id="同步-Synchronous-和异步-Asynchronous"><a href="#同步-Synchronous-和异步-Asynchronous" class="headerlink" title="同步(Synchronous)和异步(Asynchronous)"></a>同步(Synchronous)和异步(Asynchronous)</h2><p><strong>同步：</strong>任务一个接一个地执行。每个任务等待任何先前的任务完成，然后执行。</p><p><strong>异步：</strong>当一个任务被执行时，您可以切换到另一个任务，而无需等待前一个任务完成。</p><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p>想象一下，你被安排写两封信，一封是给你妈妈的，另一封是给你最好的朋友的。你不能同时写两封信，除非你是个两手都能写的人。</p><p>在同步编程模型中，任务一个接一个地执行。每个任务都会等待之前的任何任务完成，然后执行。</p><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>想象一下，有人让你做三明治，然后在洗衣机里洗衣服。你可以把衣服放进洗衣机里，不用等它洗好，你就可以去做三明治了。</p><p>在这里，您异步执行了这两个任务。在异步编程模型中，当执行一项任务时，您可以切换到另一项任务，而无需等待前一项任务完成。</p><p>那么，在同步调用下，调用方不再继续执行而是暂停等待，被调函数执行完后很自然的就是调用方继续执行，那么异步调用下调用方怎知到被调函数是否执行完成呢？</p><p>这就分为了两种情况：</p><ol><li>调用方根本就不关心执行结果</li><li>调用方后续需要知道执行结果</li></ol><p>第一种情况比较简单，该情况无需讨论。第二种情况下通常有两种实现方式：</p><ul><li><p>通知机制，也就是说当任务执行完成后发送信号用来通知调用方任务完成，注意这里的信号就有很多实现方式了，Linux中的signal，或者使用信号量等机制都可以实现。</p></li><li><p>回调，也就是我们常说的callback，</p></li></ul><h2 id="关系-1"><a href="#关系-1" class="headerlink" title="关系"></a>关系</h2><h3 id="单线程和多线程中的同步和异步"><a href="#单线程和多线程中的同步和异步" class="headerlink" title="单线程和多线程中的同步和异步"></a>单线程和多线程中的同步和异步</h3><h4 id="同步-1"><a href="#同步-1" class="headerlink" title="同步"></a>同步</h4><p>单线程，每个任务都被一个接一个地执行。每个任务都等待其前一个任务执行。</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/1oXA1XakEWmM4vC8S3fM5xA.jpeg" alt="img" style="zoom: 50%;"></p><p>多线程：Tasks get executed in different threads but wait for any other executing tasks on any other thread.</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/19E5Tw3mw1S22Dg680pK0Qw.jpeg" alt="img" style="zoom: 50%;"></p><h4 id="异步-1"><a href="#异步-1" class="headerlink" title="异步"></a>异步</h4><p>单线程：任务开始执行时不需要等待其他任务完成。在给定的时间，执行单个任务。</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/1OVbZwG4ErTtsSFiTX3lh6w.jpeg" alt="img" style="zoom:50%;"></p><p>多线程：Tasks get executed in different threads without waiting for any tasks and independently finish off their executions.</p><p><img src="/Python/基础语法/并发、并行、线程、进程、异步、同步、阻塞、非阻塞/1nxqNBCOh4Zuos1BXTjkJJQ.jpeg" alt="img" style="zoom:50%;"></p><h3 id="同步、异步与并发性、并行性"><a href="#同步、异步与并发性、并行性" class="headerlink" title="同步、异步与并发性、并行性"></a>同步、异步与并发性、并行性</h3><p>异步编程模型帮助我们实现并发。</p><p>多线程环境中的异步编程模型是实现并行性的一种方式。</p><h3 id="同步、异步与阻塞、非阻塞"><a href="#同步、异步与阻塞、非阻塞" class="headerlink" title="同步、异步与阻塞、非阻塞"></a>同步、异步与阻塞、非阻塞</h3><p>详细参考<a href="https://www.zhihu.com/question/19732473/answer/241673170" target="_blank" rel="noopener">怎样理解阻塞非阻塞与同步异步的区别？ - 萧萧的回答 - 知乎</a>。</p><ol><li>阻塞/非阻塞， 同步/异步的概念要注意讨论的上下文：</li></ol><ul><li>在进程通信层面， 阻塞/非阻塞， 同步/异步基本是同义词， 但是需要注意区分讨论的对象是发送方还是接收方。发送方阻塞/非阻塞（同步/异步）和接收方的阻塞/非阻塞（同步/异步） 是互不影响的。</li><li>在 IO 系统调用层面（ IO system call ）层面， <strong>非阻塞 IO 系统调用</strong> 和 <strong>异步 IO 系统调用</strong>存在着一定的差别， 它们都不会阻塞进程， 但是返回结果的方式和内容有所差别， 但是都属于非阻塞系统调用（ non-blocing system call ）。具体来说，<ul><li>一个<strong>非阻塞I/O 系统调用 read()</strong> 操作立即返回的是任何可以立即拿到的数据， 可以是完整的结果， 也可以是不完整的结果， 还可以是一个空值。</li><li>而<strong>异步I/O系统调用</strong> read（）结果必须是完整的， 但是这个操作完成的通知可以延迟到将来的一个时间点。</li></ul></li></ul><ol><li>非阻塞系统调用（non-blocking I/O system call 与 asynchronous I/O system call） 的存在可以用来实现线程级别的 I/O 并发， 与通过多进程实现的 I/O 并发相比可以减少内存消耗以及进程切换的开销。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Concurrency and Parallelism -&gt; Way tasks are executed.</p><p>Synchronous and Asynchronous -&gt; Programming model.</p><p>Single Threaded and Multi-Threaded -&gt; The environment of task execution.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d" target="_blank" rel="noopener">Concurrency, Parallelism, Threads, Processes, Async, and Sync — Related? </a><br><a href="https://zhuanlan.zhihu.com/p/270428703" target="_blank" rel="noopener">从小白到高手，你需要理解同步与异步</a><br><a href="https://www.zhihu.com/question/19732473/answer/241673170" target="_blank" rel="noopener">怎样理解阻塞非阻塞与同步异步的区别？ - 萧萧的回答 - 知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;并发-Concurrency-与并行-Parallelism&quot;&gt;&lt;a href=&quot;#并发-Concurrency-与并行-Parallelism&quot; class=&quot;headerlink&quot; title=&quot;并发(Concurrency)与并行(Parallelism)&quot;&gt;&lt;/a&gt;并发(Concurrency)与并行(Parallelism)&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;并发：&lt;/strong&gt;两个或多个事件在同一时间间隔内发生。这些事情宏观上同时发生，微观上交替进行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;并行：&lt;/strong&gt;两个或多个事件在同一时刻同时发生。&lt;/p&gt;
&lt;h3 id=&quot;并发&quot;&gt;&lt;a href=&quot;#并发&quot; class=&quot;headerlink&quot; title=&quot;并发&quot;&gt;&lt;/a&gt;并发&lt;/h3&gt;&lt;p&gt;假设你同时被赋予了唱歌和吃饭的任务。在给定的时间，你要么唱歌，要么吃东西，因为在这两种情况下，你的嘴都会受到影响。因此，为了做到这一点，你会吃一段时间，然后唱歌，重复这个过程，直到你的食物吃完或歌曲结束。所以你并发完成了任务。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/categories/Python/"/>
    
      <category term="基础语法" scheme="https://www.zdaiot.com/categories/Python/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/tags/Python/"/>
    
      <category term="并发" scheme="https://www.zdaiot.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
      <category term="并行" scheme="https://www.zdaiot.com/tags/%E5%B9%B6%E8%A1%8C/"/>
    
      <category term="线程" scheme="https://www.zdaiot.com/tags/%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="进程" scheme="https://www.zdaiot.com/tags/%E8%BF%9B%E7%A8%8B/"/>
    
      <category term="异步" scheme="https://www.zdaiot.com/tags/%E5%BC%82%E6%AD%A5/"/>
    
      <category term="同步" scheme="https://www.zdaiot.com/tags/%E5%90%8C%E6%AD%A5/"/>
    
      <category term="阻塞" scheme="https://www.zdaiot.com/tags/%E9%98%BB%E5%A1%9E/"/>
    
      <category term="非阻塞" scheme="https://www.zdaiot.com/tags/%E9%9D%9E%E9%98%BB%E5%A1%9E/"/>
    
  </entry>
  
  <entry>
    <title>Python基础语法</title>
    <link href="https://www.zdaiot.com/Python/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/Python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    <id>https://www.zdaiot.com/Python/基础语法/Python基础语法/</id>
    <published>2023-06-29T03:20:23.000Z</published>
    <updated>2023-06-29T03:20:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="u参数"><a href="#u参数" class="headerlink" title="u参数"></a>u参数</h2><p>有的时候在运行Python的时候，会遇到<code>python  -u xx.py</code>，这是什么意思呢？</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>python中标准错误（std.err）和标准输出(std.out)的输出规则：标准输出默认需要缓存后再输出到屏幕，而标准错误则直接打印到屏幕。如在<code>test.py</code>中有如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"> </span><br><span class="line">sys.stdout.write(&quot;Stdout1&quot;)</span><br><span class="line">sys.stderr.write(&quot;Stderr1&quot;)</span><br><span class="line">sys.stdout.write(&quot;Stdout2&quot;)</span><br><span class="line">sys.stderr.write(&quot;Stderr2&quot;)</span><br></pre></td></tr></table></figure><p>其中sys.stdout.write()和sys.stderr.write()均是向屏幕打印的语句。其实python中的print语句就是调用了sys.stdout.write(),例如在打印对象调用print obj 时，事实上是调用了 sys.stdout.write(obj+’\n’)。</p><p>预想的结果是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stdout1Stderr1Stdout2Stderr2</span><br></pre></td></tr></table></figure><p>实际的结果为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stderr1Stderr2Stdout1Stdout2</span><br></pre></td></tr></table></figure><p>原因：是python缓存机制，虽然stderr和stdout默认都是指向屏幕的，但是stderr是无缓存的，程序往stderr输出一个字符，就会在屏幕上显示一个；而stdout是有缓存的，只有遇到换行或者积累到一定的大小，才会显示出来。这就是为什么上面的会最先显示两个stderr的原因。</p><blockquote><p>注意要使用<code>python test.py</code>才能验证，不要在ipython中。</p></blockquote><h3 id="u参数的作用"><a href="#u参数的作用" class="headerlink" title="-u参数的作用"></a>-u参数的作用</h3><p>python命令加上-u（unbuffered）参数后会强制其标准输出也同标准错误一样不通过缓存直接打印到屏幕。</p><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stdout1Stderr1Stdout2Stderr2</span><br></pre></td></tr></table></figure><h2 id="字符串前u-r-b"><a href="#字符串前u-r-b" class="headerlink" title="字符串前u,r,b"></a>字符串前u,r,b</h2><p>u/U：表示unicode字符串。不是仅仅是针对中文, 可以针对任何的字符串，代表是对字符串进行unicode编码。 </p><p>r/R：非转义的原始字符串。与普通字符相比，其他相对特殊的字符，其中可能包含转义字符，即那些，反斜杠加上对应字母，表示对应的特殊含义的，比如最常见的”\n”表示换行，”\t”表示Tab等。而如果是以r开头，那么说明后面的字符，都是普通的字符了，即如果是“\n”那么表示一个反斜杠字符，一个字母n，而不是表示换行了。以r开头的字符，常用于正则表达式，对应着re模块。变量前面可以添加<code>repr</code>。</p><p>b:bytes。</p><h2 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h2><h3 id="字符在内存中的编码"><a href="#字符在内存中的编码" class="headerlink" title="字符在内存中的编码"></a>字符在内存中的编码</h3><p>字符在内存里的表示是unicode，如果要存盘或者发到网络就经过编码，具体为使用<code>encode</code>函数将其转为bytes形式，然后对端收到依次解码，具体为使用<code>decode</code>函数将其转为str形式。</p><p>Python 3里面，str在内存里是unicode表示的，所以’中文’ == ‘\u4e2d\u6587’，类型都是str。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'\u4e2d\u6587'</span></span><br><span class="line"><span class="string">'中文'</span></span><br><span class="line"><span class="comment"># 1个汉字用unicode表示，是2个byte，这里\u4e2d是十六进制的写法。4e是0100 1110，2d是0010 1101，合起来16位2bytes。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># encode默认参数是'utf-8'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中文'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'\u4e2d\u6587'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># '\u4e2d'是unicode表示的字符，unicode只是表示它的一个形式，但本质上被表示的对象还是字符，是str而不是bytes</span></span><br></pre></td></tr></table></figure><p>对str编码，本质上还是对str表示的字符编码，可以用ascii（如果字符属于ascii字符集的话），也可以用utf-8，也可以用gb2312（中文），都行。但是注意并没有unicode这个encode形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="string">'\u0041'</span>).encode(<span class="string">'ascii'</span>)</span><br><span class="line"><span class="string">b'A'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'A'</span>.encode(<span class="string">'ascii'</span>)</span><br><span class="line"><span class="string">b'A'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'A'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'A'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中文'</span>.encode(<span class="string">'gb2312'</span>)</span><br><span class="line"><span class="string">b'\xd6\xd0\xce\xc4'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误的方式</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中文'</span>.encode(<span class="string">'ascii'</span>)</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">UnicodeEncodeError                        Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-19</span><span class="number">-76</span>f41cd8dafa&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 '中文'.encode('ascii')</span><br><span class="line"></span><br><span class="line">UnicodeEncodeError: <span class="string">'ascii'</span> codec can<span class="string">'t encode characters in position 0-1: ordinal not in range(128)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 错误的方式</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; '</span>中文<span class="string">'.encode('</span>unicode<span class="string">')</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span></span><br><span class="line"><span class="string">LookupError: unknown encoding: unicode</span></span><br></pre></td></tr></table></figure><p>编码后是bytes，俗称的01010101，如果这个010101不在ascii的表示范围内，就会显示成\x（010101的十六进制形式）。</p><p>这就是说，像汉字编码成bytes以后，去查看这个bytes肯定只能看到\x系列，因为这个bytes的内容肯定不在ascii范围内；但如果换英文，就可以看到对应的英文字母，不过不要误会，本质上它还是没有含义的010101而不是字符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">"abc"</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中文'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span></span><br><span class="line"><span class="comment"># 1个汉字，按utf-8编码，一般是3个bytes，\xe4是十六进制表示的1个byte。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'A'</span>.encode(<span class="string">'ascii'</span>)</span><br><span class="line"><span class="string">b'A'</span></span><br><span class="line"><span class="comment"># 注意区分b'A'和'A'，虽然编码后看到的结果是b'A'，但这个结果跟'A'没有关系。</span></span><br><span class="line"><span class="comment"># 这个结果b'A'就是一串0101，具体说就是0100 0001这样一个1个byte，是表示'A'还是其他符号，要看解编码形式。</span></span><br><span class="line"><span class="comment"># b'A'已经是内存里的形式，占1个byte；而'A'，由于我们说python 3在内存里是按unicode形式表示字符，所以占的是2个byte。</span></span><br></pre></td></tr></table></figure><p>由于对同一个英文字符，ascii编码和utf-8编码的结果是一致的，所以用一个编码然后再另一个解码，是可以成功还原的。不过一般是不会这么做的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="string">'abc'</span>.encode(<span class="string">'ascii'</span>).decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'abc'</span></span><br></pre></td></tr></table></figure><p>爬虫若拿到的是形如0101的bytes，首先会指定一个编码做decode，这时候可能会碰到部分不符合出错，可以加上ignore参数试试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xff'</span>.decode(<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line"><span class="string">'中'</span></span><br></pre></td></tr></table></figure><p><strong>ord函数获取字符的整数表示和chr数把编码转换为对应的字符</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ord(<span class="string">'A'</span>)</span><br><span class="line"><span class="number">65</span></span><br><span class="line">ord(<span class="string">'中'</span>)</span><br><span class="line"><span class="number">20013</span></span><br><span class="line">chr(<span class="number">66</span>)</span><br><span class="line"><span class="string">'B'</span></span><br><span class="line">chr(<span class="number">25991</span>)</span><br><span class="line"><span class="string">'文'</span></span><br></pre></td></tr></table></figure><p><strong>另外，对str和对bytes用len，意义是不同的。</strong></p><p>len(str)统计字符数，len(bytes)统计bytes数，即——这串010101一共是多少个bits，除以8就是bytes。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(<span class="string">'中文'</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="comment"># len(str)统计字符数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>byte1 = <span class="string">'中文'</span>.encode(<span class="string">'gb2312'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>byte2 = <span class="string">'中文'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>byte1</span><br><span class="line"><span class="string">b'\xd6\xd0\xce\xc4'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>byte2</span><br><span class="line"><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(byte1)</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(byte2)</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="comment"># len(bytes)统计bytes数。</span></span><br></pre></td></tr></table></figure><h3 id="如何判断字符串编码方式"><a href="#如何判断字符串编码方式" class="headerlink" title="如何判断字符串编码方式"></a>如何判断字符串编码方式</h3><p>我们经常遇到的编码其实主要的就只有三种：utf-8，gbk，unicode</p><ul><li>unicode一般是 <code>\u</code> 带头的，然后后面跟四位数字或字母，例如 <code>\u6d4b\u8bd5</code> ，一个 <code>\u</code> 对应一个汉字</li><li>utf-8一般是 <code>\x</code> 带头的，后面跟两位字母或数字，例如 <code>\xe6\xb5\x8b\xe8\xaf\x95\xe5\x95\x8a</code> ，三个 <code>\x</code> 代表一个汉字</li><li>gbk一般是 <code>\x</code> 带头的，后面跟两位字母或数字，例如 <code>\xb2\xe2\xca\xd4\xb0\xa1</code> ，两个 <code>\x</code> 代表一个汉字。</li></ul><p>除了粗略的根据是否乱码看编码方式外，还可以用chardet模块猜测、</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chardet</span><br><span class="line"></span><br><span class="line">raw = <span class="string">u'我是一只小小鸟'</span></span><br><span class="line">print(chardet.detect(raw.encode(<span class="string">'utf-8'</span>)))</span><br><span class="line">print(chardet.detect(raw.encode(<span class="string">'gbk'</span>)))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'encoding'</span>: <span class="string">'utf-8'</span>, <span class="string">'confidence'</span>: 0.99, <span class="string">'language'</span>: <span class="string">''</span>&#125;</span><br><span class="line">&#123;<span class="string">'encoding'</span>: <span class="string">'GB2312'</span>, <span class="string">'confidence'</span>: 0.99, <span class="string">'language'</span>: <span class="string">'Chinese'</span>&#125;</span><br></pre></td></tr></table></figure><p>chardet模块可以计算这个字符串是某个编码的概率，基本对于99%的应用场景，这个模块都够用了。</p><h3 id="x字符串转码"><a href="#x字符串转码" class="headerlink" title="\x字符串转码"></a>\x字符串转码</h3><p>在使用Python的时候，经常遇到类似于<code>\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c</code>的字符串，其实它是utf-8编码，但数据类型是字符串类型，而不是bytes类型的utf-8编码。如果我们需要将\x开头的字符串编码转换中文。</p><p><strong>方法一：</strong>先将字符串编码指定为unicode_escape，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">'\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line">s = s.encode(<span class="string">'unicode_escape'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到bytes类型数据（单斜杠变成双斜杠）</span></span><br><span class="line"><span class="string">b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\xe4\\xb8\\x96\\xe7\\x95\\x8c'</span></span><br></pre></td></tr></table></figure><p>接着再对bytes类型进行utf-8解码，得到字符串，将字符串中的 “ \x “ 替换为 “ % “，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bytes to string</span></span><br><span class="line">s.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">ss = s.decode(<span class="string">'utf-8'</span>).replace(<span class="string">'\\x'</span>, <span class="string">'%'</span>)</span><br><span class="line"><span class="comment"># 替换作用就是将字符串改为url的utf-8编码格式</span></span><br><span class="line">%e4%bd%a0%e5%a5%bd%e4%b8%<span class="number">96</span>%e7%<span class="number">95</span>%<span class="number">8</span>c</span><br></pre></td></tr></table></figure><p>最后利用urllib中的unquote方法将url编码解码，得到中文</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">un = urllib.parse.unquote(ss)</span><br><span class="line"><span class="comment"># 你好世界</span></span><br></pre></td></tr></table></figure><p><strong>方法二：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">'\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line"></span><br><span class="line">s.encode(<span class="string">'raw_unicode_escape'</span>)</span><br><span class="line"><span class="comment"># b'\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line"></span><br><span class="line">s.encode(<span class="string">'raw_unicode_escape'</span>).decode()</span><br><span class="line"><span class="comment"># '你好世界'</span></span><br></pre></td></tr></table></figure><h3 id="u字符串转码"><a href="#u字符串转码" class="headerlink" title="\u字符串转码"></a>\u字符串转码</h3><p>在使用Python的时候，经常遇到类似于<code>&#39;\u5403\u9e21\u6218\u573a&#39;</code>的字符串，其实它是Unicode编码，可以直接解码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">63</span>]: s = <span class="string">'\u5403\u9e21\u6218\u573a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: s</span><br><span class="line">Out[<span class="number">64</span>]: <span class="string">'吃鸡战场'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">74</span>]: a = <span class="string">'\u5403\u9e21\u6218\u573a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: a</span><br><span class="line">Out[<span class="number">75</span>]: <span class="string">'吃鸡战场'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">76</span>]: b = a.encode(<span class="string">'unicode-escape'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: b</span><br><span class="line">Out[<span class="number">77</span>]: <span class="string">b'\\u5403\\u9e21\\u6218\\u573a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">78</span>]: c = b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">79</span>]: c</span><br><span class="line">Out[<span class="number">79</span>]: <span class="string">'\\u5403\\u9e21\\u6218\\u573a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">80</span>]: c.encode()</span><br><span class="line">Out[<span class="number">80</span>]: <span class="string">b'\\u5403\\u9e21\\u6218\\u573a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">81</span>]: c.encode().decode(<span class="string">'unicode-escape'</span>)</span><br><span class="line">Out[<span class="number">81</span>]: <span class="string">'吃鸡战场'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">82</span>]: b.decode(<span class="string">'unicode-escape'</span>)</span><br><span class="line">Out[<span class="number">82</span>]: <span class="string">'吃鸡战场'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">95</span>]: c.encode() == b</span><br><span class="line">Out[<span class="number">95</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">103</span>]: <span class="string">b'\u5403\u9e21\u6218\u573a'</span></span><br><span class="line">Out[<span class="number">103</span>]: <span class="string">b'\\u5403\\u9e21\\u6218\\u573a'</span></span><br></pre></td></tr></table></figure><blockquote><p>关于<code>unicode-escape</code>以及<code>raw-unicode-escape</code>，可以参考<a href="https://docs.python.org/zh-cn/3/library/codecs.html?highlight=raw_unicode_escape" target="_blank" rel="noopener">codecs —- 编解码器注册和相关基类</a>。</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/wonengguwozai/article/details/81668240" target="_blank" rel="noopener">解析python 命令的-u参数</a><br><a href="https://blog.csdn.net/YungGuo/article/details/110197818" target="_blank" rel="noopener">关于\x开头的字符串编码转换中文解决方法</a><br><a href="https://zhuanlan.zhihu.com/p/37359861" target="_blank" rel="noopener">【Python】笔记：关于\u和\x</a><br><a href="https://www.jb51.net/article/248728.htm" target="_blank" rel="noopener">python中的编码和解码及\x和\u问题</a><br><a href="https://blog.csdn.net/u013230234/article/details/79352397" target="_blank" rel="noopener">python学习:字符串前面添加u,r,b的含义</a><br><a href="https://blog.csdn.net/qq_28295425/article/details/106288474" target="_blank" rel="noopener">python对变量的字符串不转义 变量如何加r</a><br><a href="Python3 encode中的unicode-escape和raw_unicode_escape">https://blog.csdn.net/mijichui2153/article/details/105516152</a><br><a href="https://www.cnblogs.com/Xjng/p/5093905.html" target="_blank" rel="noopener">不得不知道的Python字符串编码相关的知识</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;u参数&quot;&gt;&lt;a href=&quot;#u参数&quot; class=&quot;headerlink&quot; title=&quot;u参数&quot;&gt;&lt;/a&gt;u参数&lt;/h2&gt;&lt;p&gt;有的时候在运行Python的时候，会遇到&lt;code&gt;python  -u xx.py&lt;/code&gt;，这是什么意思呢？&lt;/p&gt;
&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;python中标准错误（std.err）和标准输出(std.out)的输出规则：标准输出默认需要缓存后再输出到屏幕，而标准错误则直接打印到屏幕。如在&lt;code&gt;test.py&lt;/code&gt;中有如下内容：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;import sys&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sys.stdout.write(&amp;quot;Stdout1&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sys.stderr.write(&amp;quot;Stderr1&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sys.stdout.write(&amp;quot;Stdout2&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sys.stderr.write(&amp;quot;Stderr2&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/categories/Python/"/>
    
      <category term="基础语法" scheme="https://www.zdaiot.com/categories/Python/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/tags/Python/"/>
    
      <category term="u" scheme="https://www.zdaiot.com/tags/u/"/>
    
  </entry>
  
  <entry>
    <title>Nginx反向代理OpenAI接口</title>
    <link href="https://www.zdaiot.com/Linux/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/Nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86OpenAI%E6%8E%A5%E5%8F%A3/"/>
    <id>https://www.zdaiot.com/Linux/博客搭建/Nginx反向代理OpenAI接口/</id>
    <published>2023-06-27T08:13:16.000Z</published>
    <updated>2023-06-27T08:13:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>在开始之前，我们需要进行一些准备工作：</p><ul><li>一台境外服务器，比如说腾讯云新加坡服务器</li><li>安装有Nginx环境，centos可以用<code>yum install nginx</code>安装。</li><li>一个域名，比如说<code>www.xxxx.com</code>，以及对应的ssl证书（腾讯云可以在<a href="https://console.cloud.tencent.com/ssl" target="_blank" rel="noopener">这里</a>申请免费的证书）</li></ul><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>执行<code>vim /etc/nginx/nginx.conf</code>，添加如下配置：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 省略其它配置，可以保持默认配置不变</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">     <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">     server_name &#123;your_domain_name&#125;;</span><br><span class="line">     ssl_certificate &#123;your_cert_path&#125;;</span><br><span class="line">     ssl_certificate_key &#123;your_cert_key_path&#125;;</span><br><span class="line">     <span class="attribute">ssl_session_cache</span> shared:le_nginx_SSL:<span class="number">1m</span>;</span><br><span class="line">     <span class="attribute">ssl_session_timeout</span> <span class="number">1440m</span>;</span><br><span class="line">     <span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span> TLSv1.<span class="number">3</span>;</span><br><span class="line">     <span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line">     <span class="attribute">ssl_ciphers</span> TLS13-AES-<span class="number">256</span>-GCM-SHA384:TLS13-CHACHA20-POLY1305-SHA256:TLS13-AES-<span class="number">128</span>-GCM-SHA256:TLS13-AES-<span class="number">128</span>-CCM-<span class="number">8</span>-SHA256:TLS13-AES-<span class="number">128</span>-CCM-SHA256:EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+ECDSA+AES128:EECDH+aRSA+AES128:RSA+AES128:EECDH+ECDSA+AES256:EECDH+aRSA+AES256:RSA+AES256:EECDH+ECDSA+3DES:EECDH+aRSA+3DES:RSA+3DES:!MD5;</span><br><span class="line">     <span class="attribute">location</span> / &#123;</span><br><span class="line">     <span class="attribute">proxy_pass</span>  https://api.openai.com/;</span><br><span class="line">     <span class="attribute">proxy_ssl_server_name</span> <span class="literal">on</span>;</span><br><span class="line">     <span class="attribute">proxy_set_header</span> Host api.openai.com;</span><br><span class="line">     <span class="attribute">proxy_set_header</span> Connection <span class="string">''</span>;</span><br><span class="line">     <span class="attribute">proxy_http_version</span> <span class="number">1</span>.<span class="number">1</span>;</span><br><span class="line">     <span class="attribute">chunked_transfer_encoding</span> <span class="literal">off</span>;</span><br><span class="line">     <span class="attribute">proxy_buffering</span> <span class="literal">off</span>;</span><br><span class="line">     <span class="attribute">proxy_cache</span> <span class="literal">off</span>;</span><br><span class="line">     <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$remote_addr</span>;</span><br><span class="line">     <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>{your_domain_name}</code>可以填写<code>www.xxx.com</code>。</p><p>关于配置的介绍，可部分参考<a href="https://www.wyr.me/post/743" target="_blank" rel="noopener">Nginx反向代理OpenAI API</a></p></blockquote><p>重启<code>nginx</code>服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo nginx -s stop </span><br><span class="line">sudo nginx</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>配置环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> OPENAI_API_KEY=sk-xxxxx</span><br><span class="line"><span class="built_in">export</span> OPENAI_API_ORG=org-xxxx</span><br></pre></td></tr></table></figure><p>测试模型列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl https://xxx.com/v1/models \</span><br><span class="line">  -H <span class="string">"Authorization: Bearer <span class="variable">$OPENAI_API_KEY</span>"</span> \</span><br><span class="line">  -H <span class="string">"OpenAI-Organization: <span class="variable">$OPENAI_API_ORG</span>"</span></span><br></pre></td></tr></table></figure><p>测试对话：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl https://xxx.com/v1/chat/completions   -H <span class="string">"Content-Type: application/json"</span>   -H <span class="string">"Authorization: Bearer <span class="variable">$OPENAI_API_KEY</span>"</span>   -d <span class="string">'&#123;</span></span><br><span class="line"><span class="string">    "model": "gpt-3.5-turbo",</span></span><br><span class="line"><span class="string">    "messages": [&#123;"role": "user", "content": "Hello!"&#125;]</span></span><br><span class="line"><span class="string">&#125;'</span></span><br></pre></td></tr></table></figure><blockquote><p>这里使用<code>https://xxx.com</code>与<code>https://www.xxx.com</code>均可。</p></blockquote><p>若使用openai-python包，则注意要设置openai代理服务器地址，主要有两种方法：</p><p>方法一：在<code>openai/__init__.py</code>（例如<code>/data/home/zdaiot/.local/lib/python3.10/site-packages/openai/__init__.py</code>）中添加如下行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">api_base = os.environ.get(<span class="string">"OPENAI_API_BASE"</span>, <span class="string">"https://api.openai.com/v1"</span>)</span><br><span class="line">api_base = <span class="string">"https://xxx.com/v1"</span>  <span class="comment"># 添加的行</span></span><br></pre></td></tr></table></figure><p>方法二：添加环境变量<code>export OPENAI_API_BASE= https://xxx.com/v1</code>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.wyr.me/post/743" target="_blank" rel="noopener">Nginx反向代理OpenAI API</a><br><a href="https://blog.csdn.net/u013534071/article/details/129773592" target="_blank" rel="noopener">使用Nginx反向代理OpenAI API</a><br><a href="https://www.jianshu.com/p/19d62d087d97" target="_blank" rel="noopener">如何用Nginx反向代理openAI接口</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;准备&quot;&gt;&lt;a href=&quot;#准备&quot; class=&quot;headerlink&quot; title=&quot;准备&quot;&gt;&lt;/a&gt;准备&lt;/h2&gt;&lt;p&gt;在开始之前，我们需要进行一些准备工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一台境外服务器，比如说腾讯云新加坡服务器&lt;/li&gt;
&lt;li&gt;安装有Nginx环境，centos可以用&lt;code&gt;yum install nginx&lt;/code&gt;安装。&lt;/li&gt;
&lt;li&gt;一个域名，比如说&lt;code&gt;www.xxxx.com&lt;/code&gt;，以及对应的ssl证书（腾讯云可以在&lt;a href=&quot;https://console.cloud.tencent.com/ssl&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;申请免费的证书）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h2&gt;&lt;p&gt;执行&lt;code&gt;vim /etc/nginx/nginx.conf&lt;/code&gt;，添加如下配置：&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/categories/Linux/"/>
    
      <category term="博客搭建" scheme="https://www.zdaiot.com/categories/Linux/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Nginx" scheme="https://www.zdaiot.com/tags/Nginx/"/>
    
      <category term="OpenAI" scheme="https://www.zdaiot.com/tags/OpenAI/"/>
    
  </entry>
  
  <entry>
    <title>Systemd</title>
    <link href="https://www.zdaiot.com/Linux/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/Systemd/"/>
    <id>https://www.zdaiot.com/Linux/常用指令/Systemd/</id>
    <published>2023-05-13T03:55:08.000Z</published>
    <updated>2023-05-13T03:55:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要参考了<a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">Systemd 入门教程：命令篇</a>。</p><p>Systemd 是 Linux 系统工具，用来启动<a href="http://www.ruanyifeng.com/blog/2016/02/linux-daemon.html" target="_blank" rel="noopener">守护进程</a>，已成为大多数发行版的标准配置。</p><h2 id="守护进程定义"><a href="#守护进程定义" class="headerlink" title="守护进程定义"></a>守护进程定义</h2><p><a href="http://baike.baidu.com/view/53123.htm" target="_blank" rel="noopener">“守护进程”</a>（daemon）就是一直在后台运行的进程（daemon）。比如说我们要开启一个服务<code>node server.js</code>，怎么才能让它变成系统的守护进程（daemon），成为一种服务（service），一直在那里运行，而不是退出终端就停止呢？有如下几种方法，更详细的介绍参考<a href="http://www.ruanyifeng.com/blog/2016/02/linux-daemon.html" target="_blank" rel="noopener">Linux 守护进程的启动方法</a>。</p><ol><li>使用<code>$ nohup node server.js &amp;</code>指令。</li><li>使用Screen 命令与 Tmux 命令。</li></ol><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><h3 id="init"><a href="#init" class="headerlink" title="init"></a>init</h3><p>历史上，<a href="http://www.ruanyifeng.com/blog/2013/08/linux_boot_process.html" target="_blank" rel="noopener">Linux 的启动</a>一直采用<a href="https://en.wikipedia.org/wiki/Init" target="_blank" rel="noopener"><code>init</code></a>进程。在类Unix 的计算机操作系统中，Init（初始化的简称）是在启动计算机系统期间启动的第一个进程。init 是一个守护进程，它将持续运行，直到系统关闭。它是所有其他进程的直接或间接的父进程。</p><p>因为init 的参数全在<code>/etc/init.d</code>目录下，所以使用 init 启动一个服务，应该这样做：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/nginx start</span><br></pre></td></tr></table></figure><h3 id="service"><a href="#service" class="headerlink" title="service"></a>service</h3><p>service是一个运行<code>System V init</code>（也就是<code>/etc/init.d</code> 目录下的参数）的脚本命令。</p><blockquote><p>System V，曾经也被称为AT&amp;T System V，是Unix操作系统众多版本中的一支。它最初由AT&amp;T开发，在1983年第一次发布。一共发行了4个System V的主要版本：版本1、2、3和4。System V Release 4，或者称为SVR4，是最成功的版本，成为一些UNIX共同特性的源头，例如 ”SysV 初始化脚本“ （/etc/init.d），用来控制系统启动和关闭，System V Interface Definition (SVID) 是一个System V 如何工作的标准定义。</p></blockquote><p>所以分析可知service 是去<code>/etc/init.d</code>目录下执行相关程序。</p><p>使用 service 启动一个服务：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ service nginx start</span><br></pre></td></tr></table></figure><p>可以理解成 service 就是<code>init.d</code> 的一种实现方式。所以这两者启动方式（或者是停止、重启）并没有什么区别。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /etc/init.d/nginx start</span><br><span class="line">// 等价于</span><br><span class="line">$ service nginx start</span><br></pre></td></tr></table></figure><p>这两种方法都有两个缺点。</p><p>一是启动时间长。<code>init</code>进程是串行启动，只有前一个进程启动完，才会启动下一个进程。</p><p>二是启动脚本复杂。<code>init</code>进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。</p><h2 id="Systemd-概述"><a href="#Systemd-概述" class="headerlink" title="Systemd 概述"></a>Systemd 概述</h2><p>Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。</p><p>根据 Linux 惯例，字母<code>d</code>是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。</p><p>使用了 Systemd，就不需要再用<code>init</code>了。Systemd 取代了<code>initd</code>，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl --version</span><br></pre></td></tr></table></figure><p>上面的命令查看 Systemd 的版本。</p><p>Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合。</p><p><img src="/Linux/常用指令/Systemd/bg2016030703.png" alt="img" style="zoom:67%;"></p><h2 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a>系统管理</h2><p>Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。</p><h3 id="systemctl"><a href="#systemctl" class="headerlink" title="systemctl"></a>systemctl</h3><p><code>systemctl</code>是 Systemd 的主命令，用于管理系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重启系统</span></span><br><span class="line">$ sudo systemctl reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭系统，切断电源</span></span><br><span class="line">$ sudo systemctl poweroff</span><br><span class="line"></span><br><span class="line"><span class="comment"># CPU停止工作</span></span><br><span class="line">$ sudo systemctl halt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暂停系统</span></span><br><span class="line">$ sudo systemctl <span class="built_in">suspend</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 让系统进入冬眠状态</span></span><br><span class="line">$ sudo systemctl hibernate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 让系统进入交互式休眠状态</span></span><br><span class="line">$ sudo systemctl hybrid-sleep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动进入救援状态（单用户状态）</span></span><br><span class="line">$ sudo systemctl rescue</span><br></pre></td></tr></table></figure><h3 id="systemd-analyze"><a href="#systemd-analyze" class="headerlink" title="systemd-analyze"></a>systemd-analyze</h3><p><code>systemd-analyze</code>命令用于查看启动耗时。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看启动耗时</span></span><br><span class="line">$ systemd-analyze                                                                                       </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看每个服务的启动耗时</span></span><br><span class="line">$ systemd-analyze blame</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示瀑布状的启动过程流</span></span><br><span class="line">$ systemd-analyze critical-chain</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示指定服务的启动流</span></span><br><span class="line">$ systemd-analyze critical-chain atd.service</span><br></pre></td></tr></table></figure><h3 id="hostnamectl"><a href="#hostnamectl" class="headerlink" title="hostnamectl"></a>hostnamectl</h3><p><code>hostnamectl</code>命令用于查看当前主机的信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示当前主机的信息</span></span><br><span class="line">$ hostnamectl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置主机名。</span></span><br><span class="line">$ sudo hostnamectl <span class="built_in">set</span>-hostname rhel7</span><br></pre></td></tr></table></figure><h3 id="localectl"><a href="#localectl" class="headerlink" title="localectl"></a>localectl</h3><p><code>localectl</code>命令用于查看本地化设置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看本地化设置</span></span><br><span class="line">$ localectl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置本地化参数。</span></span><br><span class="line">$ sudo localectl <span class="built_in">set</span>-locale LANG=en_GB.utf8</span><br><span class="line">$ sudo localectl <span class="built_in">set</span>-keymap en_GB</span><br></pre></td></tr></table></figure><h3 id="timedatectl"><a href="#timedatectl" class="headerlink" title="timedatectl"></a>timedatectl</h3><p><code>timedatectl</code>命令用于查看当前时区设置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前时区设置</span></span><br><span class="line">$ timedatectl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示所有可用的时区</span></span><br><span class="line">$ timedatectl list-timezones                                                                                 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置当前时区</span></span><br><span class="line">$ sudo timedatectl <span class="built_in">set</span>-timezone America/New_York</span><br><span class="line">$ sudo timedatectl <span class="built_in">set</span>-time YYYY-MM-DD</span><br><span class="line">$ sudo timedatectl <span class="built_in">set</span>-time HH:MM:SS</span><br></pre></td></tr></table></figure><h3 id="loginctl"><a href="#loginctl" class="headerlink" title="loginctl"></a>loginctl</h3><p><code>loginctl</code>命令用于查看当前登录的用户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出当前session</span></span><br><span class="line">$ loginctl list-sessions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出当前登录用户</span></span><br><span class="line">$ loginctl list-users</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出显示指定用户的信息</span></span><br><span class="line">$ loginctl show-user ruanyf</span><br></pre></td></tr></table></figure><h2 id="Unit"><a href="#Unit" class="headerlink" title="Unit"></a>Unit</h2><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><p>Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。</p><p>Unit 一共分成 12 种。</p><ul><li>Service unit：系统服务</li><li>Target unit：多个 Unit 构成的一个组</li><li>Device Unit：硬件设备</li><li>Mount Unit：文件系统的挂载点</li><li>Automount Unit：自动挂载点</li><li>Path Unit：文件或路径</li><li>Scope Unit：不是由 Systemd 启动的外部进程</li><li>Slice Unit：进程组</li><li>Snapshot Unit：Systemd 快照，可以切回某个快照</li><li>Socket Unit：进程间通信的 socket</li><li>Swap Unit：swap 文件</li><li>Timer Unit：定时器</li></ul><p><code>systemctl list-units</code>命令可以查看当前系统的所有 Unit 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出正在运行的 Unit</span></span><br><span class="line">$ systemctl list-units</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有Unit，包括没有找到配置文件的或者启动失败的</span></span><br><span class="line">$ systemctl list-units --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有没有运行的 Unit</span></span><br><span class="line">$ systemctl list-units --all --state=inactive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有加载失败的 Unit</span></span><br><span class="line">$ systemctl list-units --failed</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有正在运行的、类型为 service 的 Unit</span></span><br><span class="line">$ systemctl list-units --<span class="built_in">type</span>=service</span><br></pre></td></tr></table></figure><h3 id="Unit-的状态"><a href="#Unit-的状态" class="headerlink" title="Unit 的状态"></a>Unit 的状态</h3><p><code>systemctl status</code>命令用于查看系统状态和单个 Unit 的状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示系统状态</span></span><br><span class="line">$ systemctl status</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示单个 Unit 的状态</span></span><br><span class="line">$ sysystemctl status bluetooth.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示远程主机的某个 Unit 的状态</span></span><br><span class="line">$ systemctl -H root@rhel7.example.com status httpd.service</span><br></pre></td></tr></table></figure><p>除了<code>status</code>命令，<code>systemctl</code>还提供了三个查询状态的简单方法，主要供脚本内部的判断语句使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示某个 Unit 是否正在运行</span></span><br><span class="line">$ systemctl is-active application.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示某个 Unit 是否处于启动失败状态</span></span><br><span class="line">$ systemctl is-failed application.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示某个 Unit 服务是否建立了启动链接</span></span><br><span class="line">$ systemctl is-enabled application.service</span><br></pre></td></tr></table></figure><h3 id="Unit-管理"><a href="#Unit-管理" class="headerlink" title="Unit 管理"></a>Unit 管理</h3><p>对于用户来说，最常用的是下面这些命令，用于启动和停止 Unit（主要是 service）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 立即启动一个服务</span></span><br><span class="line">$ sudo systemctl start apache.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 立即停止一个服务</span></span><br><span class="line">$ sudo systemctl stop apache.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启一个服务</span></span><br><span class="line">$ sudo systemctl restart apache.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死一个服务的所有子进程</span></span><br><span class="line">$ sudo systemctl <span class="built_in">kill</span> apache.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载一个服务的配置文件</span></span><br><span class="line">$ sudo systemctl reload apache.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重载所有修改过的配置文件</span></span><br><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示某个 Unit 的所有底层参数</span></span><br><span class="line">$ systemctl show httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示某个 Unit 的指定属性的值</span></span><br><span class="line">$ systemctl show -p CPUShares httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置某个 Unit 的指定属性</span></span><br><span class="line">$ sudo systemctl <span class="built_in">set</span>-property httpd.service CPUShares=500</span><br></pre></td></tr></table></figure><h3 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h3><p>Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。</p><p><code>systemctl list-dependencies</code>命令列出一个 Unit 的所有依赖。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl list-dependencies nginx.service</span><br></pre></td></tr></table></figure><p>上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用<code>--all</code>参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl list-dependencies --all nginx.service</span><br></pre></td></tr></table></figure><h2 id="Unit-的配置文件"><a href="#Unit-的配置文件" class="headerlink" title="Unit 的配置文件"></a>Unit 的配置文件</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。</p><p>Systemd 默认从目录<code>/etc/systemd/system/</code>读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录<code>/usr/lib/systemd/system/</code>，真正的配置文件存放在那个目录。</p><p><code>systemctl enable</code>命令用于在上面两个目录之间，建立符号链接关系。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">enable</span> clamd@scan.service</span><br><span class="line"><span class="comment"># 等同于</span></span><br><span class="line">$ sudo ln -s <span class="string">'/usr/lib/systemd/system/clamd@scan.service'</span> <span class="string">'/etc/systemd/system/multi-user.target.wants/clamd@scan.service'</span></span><br></pre></td></tr></table></figure><p>如果配置文件里面设置了开机启动，<code>systemctl enable</code>命令相当于激活开机启动。</p><p>与之对应的，<code>systemctl disable</code>命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">disable</span> clamd@scan.service</span><br></pre></td></tr></table></figure><p>配置文件的后缀名，就是该 Unit 的种类，比如<code>sshd.socket</code>。如果省略，Systemd 默认后缀名为<code>.service</code>，所以<code>sshd</code>会被理解成<code>sshd.service</code>。</p><h3 id="配置文件的状态"><a href="#配置文件的状态" class="headerlink" title="配置文件的状态"></a>配置文件的状态</h3><p><code>systemctl list-unit-files</code>命令用于列出所有配置文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有配置文件</span></span><br><span class="line">$ systemctl list-unit-files</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出指定类型的配置文件</span></span><br><span class="line">$ systemctl list-unit-files --<span class="built_in">type</span>=service</span><br></pre></td></tr></table></figure><p>这个命令会输出一个列表。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl list-unit-files</span><br><span class="line"></span><br><span class="line">UNIT FILE              STATE</span><br><span class="line">chronyd.service        enabled</span><br><span class="line">clamd@.service         static</span><br><span class="line">clamd@scan.service     disabled</span><br></pre></td></tr></table></figure><p>这个列表显示每个配置文件的状态，一共有四种。</p><ul><li>enabled：已建立启动链接</li><li>disabled：没建立启动链接</li><li>static：该配置文件没有<code>[Install]</code>部分（无法执行），只能作为其他配置文件的依赖</li><li>masked：该配置文件被禁止建立启动链接</li></ul><p>注意，从配置文件的状态无法看出，该 Unit 是否正在运行。这必须执行前面提到的<code>systemctl status</code>命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status bluetooth.service</span><br></pre></td></tr></table></figure><p>一旦修改配置文件，就要让 SystemD 重新加载配置文件，然后重新启动，否则修改不会生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line">$ sudo systemctl restart httpd.service</span><br></pre></td></tr></table></figure><h3 id="配置文件的格式"><a href="#配置文件的格式" class="headerlink" title="配置文件的格式"></a>配置文件的格式</h3><p>配置文件就是普通的文本文件，可以用文本编辑器打开。</p><p><code>systemctl cat</code>命令可以查看配置文件的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl cat atd.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=ATD daemon</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/usr/bin/atd</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>从上面的输出可以看到，配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如<code>[Unit]</code>。注意，配置文件的区块名和字段名，都是大小写敏感的。</p><p>每个区块内部是一些等号连接的键值对。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[Section]</span><br><span class="line">Directive1=value</span><br><span class="line">Directive2=value</span><br><span class="line"></span><br><span class="line">. . .</span><br></pre></td></tr></table></figure><p>注意，键值对的等号两侧不能有空格。</p><h3 id="配置文件的区块"><a href="#配置文件的区块" class="headerlink" title="配置文件的区块"></a>配置文件的区块</h3><p><code>[Unit]</code>区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。</p><ul><li><code>Description</code>：简短描述</li><li><code>Documentation</code>：文档地址</li><li><code>Requires</code>：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败</li><li><code>Wants</code>：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败</li><li><code>BindsTo</code>：与<code>Requires</code>类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行</li><li><code>Before</code>：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动</li><li><code>After</code>：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动</li><li><code>Conflicts</code>：这里指定的 Unit 不能与当前 Unit 同时运行</li><li><code>Condition...</code>：当前 Unit 运行必须满足的条件，否则不会运行</li><li><code>Assert...</code>：当前 Unit 运行必须满足的条件，否则会报启动失败</li></ul><p><code>[Install]</code>通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。</p><ul><li><code>WantedBy</code>：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入<code>/etc/systemd/system</code>目录下面以 Target 名 + <code>.wants</code>后缀构成的子目录中</li><li><code>RequiredBy</code>：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入<code>/etc/systemd/system</code>目录下面以 Target 名 + <code>.required</code>后缀构成的子目录中</li><li><code>Alias</code>：当前 Unit 可用于启动的别名</li><li><code>Also</code>：当前 Unit 激活（enable）时，会被同时激活的其他 Unit</li></ul><p><code>[Service]</code>区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。</p><ul><li><code>Type</code>：定义启动时的进程行为。它有以下几种值。</li><li><code>Type=simple</code>：默认值，执行<code>ExecStart</code>指定的命令，启动主进程</li><li><code>Type=forking</code>：以 fork 方式从父进程创建子进程，创建后父进程会立即退出</li><li><code>Type=oneshot</code>：一次性进程，Systemd 会等当前服务退出，再继续往下执行</li><li><code>Type=dbus</code>：当前服务通过 D-Bus 启动</li><li><code>Type=notify</code>：当前服务启动完毕，会通知<code>Systemd</code>，再继续往下执行</li><li><code>Type=idle</code>：若有其他任务执行完毕，当前服务才会运行</li><li><code>ExecStart</code>：启动当前服务的命令</li><li><code>ExecStartPre</code>：启动当前服务之前执行的命令</li><li><code>ExecStartPost</code>：启动当前服务之后执行的命令</li><li><code>ExecReload</code>：重启当前服务时执行的命令</li><li><code>ExecStop</code>：停止当前服务时执行的命令</li><li><code>ExecStopPost</code>：停止当其服务之后执行的命令</li><li><code>RestartSec</code>：自动重启当前服务间隔的秒数</li><li><code>Restart</code>：定义何种情况 Systemd 会自动重启当前服务，可能的值包括<code>always</code>（总是重启）、<code>on-success</code>、<code>on-failure</code>、<code>on-abnormal</code>、<code>on-abort</code>、<code>on-watchdog</code></li><li><code>TimeoutSec</code>：定义 Systemd 停止当前服务之前等待的秒数</li><li><code>Environment</code>：指定环境变量</li></ul><p>Unit 配置文件的完整字段清单，请参考<a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html" target="_blank" rel="noopener">官方文档</a>。</p><h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><p>启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。</p><p>简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于 “状态点”，启动某个 Target 就好比启动到某种状态。</p><p>传统的<code>init</code>启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前系统的所有 Target</span></span><br><span class="line">$ systemctl list-unit-files --<span class="built_in">type</span>=target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看一个 Target 包含的所有 Unit</span></span><br><span class="line">$ systemctl list-dependencies multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看启动时的默认 Target</span></span><br><span class="line">$ systemctl get-default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置启动时的默认 Target</span></span><br><span class="line">$ sudo systemctl <span class="built_in">set</span>-default multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换 Target 时，默认不关闭前一个 Target 启动的进程，</span></span><br><span class="line"><span class="comment"># systemctl isolate 命令改变这种行为，</span></span><br><span class="line"><span class="comment"># 关闭前一个 Target 里面所有不属于后一个 Target 的进程</span></span><br><span class="line">$ sudo systemctl isolate multi-user.target</span><br></pre></td></tr></table></figure><p>Target 与 传统 RunLevel 的对应关系如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Traditional runlevel      New target name     Symbolically linked to...</span><br><span class="line"></span><br><span class="line">Runlevel 0           |    runlevel0.target -poweroff.target</span><br><span class="line">Runlevel 1           |    runlevel1.target -rescue.target</span><br><span class="line">Runlevel 2           |    runlevel2.target -multi-user.target</span><br><span class="line">Runlevel 3           |    runlevel3.target -multi-user.target</span><br><span class="line">Runlevel 4           |    runlevel4.target -multi-user.target</span><br><span class="line">Runlevel 5           |    runlevel5.target -graphical.target</span><br><span class="line">Runlevel 6           |    runlevel6.target -reboot.target</span><br></pre></td></tr></table></figure><p>它与<code>init</code>进程的主要差别如下。</p><p><strong>（1）默认的 RunLevel</strong>（在<code>/etc/inittab</code>文件设置）现在被默认的 Target 取代，位置是<code>/etc/systemd/system/default.target</code>，通常符号链接到<code>graphical.target</code>（图形界面）或者<code>multi-user.target</code>（多用户命令行）。</p><p><strong>（2）启动脚本的位置</strong>，以前是<code>/etc/init.d</code>目录，符号链接到不同的 RunLevel 目录 （比如<code>/etc/rc3.d</code>、<code>/etc/rc5.d</code>等），现在则存放在<code>/lib/systemd/system</code>和<code>/etc/systemd/system</code>目录。</p><p><strong>（3）配置文件的位置</strong>，以前<code>init</code>进程的配置文件是<code>/etc/inittab</code>，各种服务的配置文件存放在<code>/etc/sysconfig</code>目录。现在的配置文件主要存放在<code>/lib/systemd</code>目录，在<code>/etc/systemd</code>目录里面的修改可以覆盖原始设置。</p><h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><p>Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用<code>journalctl</code>一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是<code>/etc/systemd/journald.conf</code>。</p><p><code>journalctl</code>功能强大，用法非常多。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有日志（默认情况下 ，只保存本次启动的日志）</span></span><br><span class="line">$ sudo journalctl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看内核日志（不显示应用日志）</span></span><br><span class="line">$ sudo journalctl -k</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统本次启动的日志</span></span><br><span class="line">$ sudo journalctl -b</span><br><span class="line">$ sudo journalctl -b -0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看上一次启动的日志（需更改设置）</span></span><br><span class="line">$ sudo journalctl -b -1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定时间的日志</span></span><br><span class="line">$ sudo journalctl --since=<span class="string">"2012-10-30 18:17:16"</span></span><br><span class="line">$ sudo journalctl --since <span class="string">"20 min ago"</span></span><br><span class="line">$ sudo journalctl --since yesterday</span><br><span class="line">$ sudo journalctl --since <span class="string">"2015-01-10"</span> --until <span class="string">"2015-01-11 03:00"</span></span><br><span class="line">$ sudo journalctl --since 09:00 --until <span class="string">"1 hour ago"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示尾部的最新10行日志</span></span><br><span class="line">$ sudo journalctl -n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示尾部指定行数的日志</span></span><br><span class="line">$ sudo journalctl -n 20</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实时滚动显示最新日志</span></span><br><span class="line">$ sudo journalctl -f</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定服务的日志</span></span><br><span class="line">$ sudo journalctl /usr/lib/systemd/systemd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定进程的日志</span></span><br><span class="line">$ sudo journalctl _PID=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某个路径的脚本的日志</span></span><br><span class="line">$ sudo journalctl /usr/bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定用户的日志</span></span><br><span class="line">$ sudo journalctl _UID=33 --since today</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某个 Unit 的日志</span></span><br><span class="line">$ sudo journalctl -u nginx.service</span><br><span class="line">$ sudo journalctl -u nginx.service --since today</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实时滚动显示某个 Unit 的最新日志</span></span><br><span class="line">$ sudo journalctl -u nginx.service -f</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并显示多个 Unit 的日志</span></span><br><span class="line">$ journalctl -u nginx.service -u php-fpm.service --since today</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定优先级（及其以上级别）的日志，共有8级</span></span><br><span class="line"><span class="comment"># 0: emerg</span></span><br><span class="line"><span class="comment"># 1: alert</span></span><br><span class="line"><span class="comment"># 2: crit</span></span><br><span class="line"><span class="comment"># 3: err</span></span><br><span class="line"><span class="comment"># 4: warning</span></span><br><span class="line"><span class="comment"># 5: notice</span></span><br><span class="line"><span class="comment"># 6: info</span></span><br><span class="line"><span class="comment"># 7: debug</span></span><br><span class="line">$ sudo journalctl -p err -b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志默认分页输出，--no-pager 改为正常的标准输出</span></span><br><span class="line">$ sudo journalctl --no-pager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 JSON 格式（单行）输出</span></span><br><span class="line">$ sudo journalctl -b -u nginx.service -o json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 JSON 格式（多行）输出，可读性更好</span></span><br><span class="line">$ sudo journalctl -b -u nginx.serviceqq</span><br><span class="line"> -o json-pretty</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示日志占据的硬盘空间</span></span><br><span class="line">$ sudo journalctl --disk-usage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定日志文件占据的最大空间</span></span><br><span class="line">$ sudo journalctl --vacuum-size=1G</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定日志文件保存多久</span></span><br><span class="line">$ sudo journalctl --vacuum-time=1years</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://baike.baidu.com/item/System%20V/1376562" target="_blank" rel="noopener">System V</a><br><a href="https://segmentfault.com/a/1190000038458363" target="_blank" rel="noopener">Linux init、service、systemctl 三者区别</a><br><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">Systemd 入门教程：命令篇</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要参考了&lt;a href=&quot;http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Systemd 入门教程：命令篇&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Systemd 是 Linux 系统工具，用来启动&lt;a href=&quot;http://www.ruanyifeng.com/blog/2016/02/linux-daemon.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;守护进程&lt;/a&gt;，已成为大多数发行版的标准配置。&lt;/p&gt;
&lt;h2 id=&quot;守护进程定义&quot;&gt;&lt;a href=&quot;#守护进程定义&quot; class=&quot;headerlink&quot; title=&quot;守护进程定义&quot;&gt;&lt;/a&gt;守护进程定义&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://baike.baidu.com/view/53123.htm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;“守护进程”&lt;/a&gt;（daemon）就是一直在后台运行的进程（daemon）。比如说我们要开启一个服务&lt;code&gt;node server.js&lt;/code&gt;，怎么才能让它变成系统的守护进程（daemon），成为一种服务（service），一直在那里运行，而不是退出终端就停止呢？有如下几种方法，更详细的介绍参考&lt;a href=&quot;http://www.ruanyifeng.com/blog/2016/02/linux-daemon.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linux 守护进程的启动方法&lt;/a&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用&lt;code&gt;$ nohup node server.js &amp;amp;&lt;/code&gt;指令。&lt;/li&gt;
&lt;li&gt;使用Screen 命令与 Tmux 命令。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/categories/Linux/"/>
    
      <category term="常用指令" scheme="https://www.zdaiot.com/categories/Linux/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/tags/Linux/"/>
    
      <category term="service" scheme="https://www.zdaiot.com/tags/service/"/>
    
      <category term="Systemd" scheme="https://www.zdaiot.com/tags/Systemd/"/>
    
  </entry>
  
  <entry>
    <title>剖析 NPM 的包管理机制</title>
    <link href="https://www.zdaiot.com/Linux/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/%E5%89%96%E6%9E%90%20NPM%20%E7%9A%84%E5%8C%85%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/"/>
    <id>https://www.zdaiot.com/Linux/博客搭建/剖析 NPM 的包管理机制/</id>
    <published>2023-04-12T09:14:11.000Z</published>
    <updated>2023-04-12T09:14:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>一直对node.js与npm感觉很陌生，最近需要用到了，所以这里学习一下。以下文章主要参考了ConardLi的<a href="https://blog.conardli.top/2019/12/17/engineering/npm/#3-6-%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B" target="_blank" rel="noopener">剖析 NPM 的包管理机制</a>。因为本人纯小白，所以会添加一些其它的知识。</p><p>现如今，前端开发的同学已经离不开 <code>npm</code> 这个包管理工具，其优秀的<strong>包版本管理机制</strong>承载了整个繁荣发展的<code>NodeJS</code>社区，理解其内部机制非常有利于加深我们对模块开发的理解、各项前端工程化的配置以加快我们排查问题（相信不少同学收到过各种依赖问题的困扰）的速度。</p><p>本文从三个角度：<code>package.json</code>、版本管理、依赖安装结合具体实例对 <code>npm</code> 的包管理机制进行了详细分析。</p><h2 id="剖析-package-json"><a href="#剖析-package-json" class="headerlink" title="剖析 package.json"></a>剖析 package.json</h2><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef28758e8c0tplv-t2oaga2asx-image.png" alt></p><p>在 <code>Node.js</code> 中，模块是一个库或框架，也是一个 <code>Node.js</code> 项目。<code>Node.js</code> 项目遵循模块化的架构，当我们创建了一个 <code>Node.js</code> 项目，意味着创建了一个模块，这个模块必须有一个描述文件，即 <code>package.json</code>。它是我们最常见的配置文件，但是它里面的配置你真的有详细了解过吗？配置一个合理的 <code>package.json</code> 文件直接决定着我们项目的质量，所以首先带大家分析下 <code>package.json</code> 的各项详细配置。</p><h3 id="必备属性"><a href="#必备属性" class="headerlink" title="必备属性"></a>必备属性</h3><p><code>package.json</code> 中有非常多的属性，其中必须填写的只有两个：<code>name</code> 和 <code>version</code> ，这两个属性组成一个 <code>npm</code> 模块的唯一标识。</p><h4 id="npm包命名规则"><a href="#npm包命名规则" class="headerlink" title="npm包命名规则"></a>npm包命名规则</h4><p><code>name</code> 即模块名称，其命名时需要遵循官方的一些规范和建议：</p><ul><li><p>包名会成为模块<code>url</code>、命令行中的一个参数或者一个文件夹名称，任何非<code>url</code>安全的字符在包名中都不能使用，可以使用 <code>validate-npm-package-name</code> 包来检测包名是否合法。</p></li><li><p>语义化包名，可以帮助开发者更快的找到需要的包，并且避免意外获取错误的包。</p></li><li><p>若包名称中存在一些符号，将符号去除后不得与现有包名重复</p></li></ul><p>例如：由于<code>react-native</code>已经存在，<code>react.native</code>、<code>reactnative</code>都不可以再创建。</p><ul><li>如果你的包名与现有的包名太相近导致你不能发布这个包，那么推荐将这个包发布到你的作用域下。</li></ul><p>例如：用户名 <code>conard</code>，那么作用域为 <code>@conard</code>，发布的包可以是<code>@conard/react</code>。</p><h4 id="查看包是否被占用"><a href="#查看包是否被占用" class="headerlink" title="查看包是否被占用"></a>查看包是否被占用</h4><p><code>name</code> 是一个包的唯一标识，不得和其他包名重复，我们可以执行 <code>npm view packageName</code> 查看包是否被占用，并可以查看它的一些基本信息：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef28368aba9tplv-t2oaga2asx-image.png" alt></p><p>若包名称从未被使用过，则会抛出 <code>404</code> 错误：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef28544db20tplv-t2oaga2asx-image.png" alt></p><p>另外，你还可以去 <code>https://www.npmjs.com/</code> 查询更多更详细的包信息。</p><h3 id="描述信息"><a href="#描述信息" class="headerlink" title="描述信息"></a>描述信息</h3><h4 id="基本描述"><a href="#基本描述" class="headerlink" title="基本描述"></a>基本描述</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"description"</span>: <span class="string">"An enterprise-class UI design language and React components implementation"</span>,</span><br><span class="line">  <span class="attr">"keywords"</span>: [</span><br><span class="line">    <span class="string">"ant"</span>,</span><br><span class="line">    <span class="string">"component"</span>,</span><br><span class="line">    <span class="string">"components"</span>,</span><br><span class="line">    <span class="string">"design"</span>,</span><br><span class="line">    <span class="string">"framework"</span>,</span><br><span class="line">    <span class="string">"frontend"</span>,</span><br><span class="line">    <span class="string">"react"</span>,</span><br><span class="line">    <span class="string">"react-component"</span>,</span><br><span class="line">    <span class="string">"ui"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>description</code>用于添加模块的的描述信息，方便别人了解你的模块。</p><p><code>keywords</code>用于给你的模块添加关键字。</p><p>当然，他们的还有一个非常重要的作用，就是利于模块检索。当你使用 <code>npm search</code> 检索模块时，会到<code>description</code> 和 <code>keywords</code> 中进行匹配。写好 <code>description</code> 和 <code>keywords</code> 有利于你的模块获得更多更精准的曝光：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2871aa774tplv-t2oaga2asx-image.png" alt></p><h4 id="开发人员"><a href="#开发人员" class="headerlink" title="开发人员"></a>开发人员</h4><p>描述开发人员的字段有两个：<code>author</code> 和 <code>contributors</code>， <code>author</code> 指包的主要作者，一个 <code>author</code> 对应一个人。 <code>contributors</code> 指贡献者信息，一个 <code>contributors</code> 对应多个贡献者，值为数组，对人的描述可以是一个字符串，也可以是下面的结构：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    <span class="attr">"name"</span> : <span class="string">"ConardLi"</span>, </span><br><span class="line">    <span class="attr">"email"</span> : <span class="string">"lisqPersion@163.com"</span>, </span><br><span class="line">    <span class="attr">"url"</span> : <span class="string">"https://github.com/ConardLi"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"homepage"</span>: <span class="string">"http://ant.design/"</span>,</span><br><span class="line">  <span class="attr">"bugs"</span>: &#123;</span><br><span class="line">    <span class="attr">"url"</span>: <span class="string">"https://github.com/ant-design/ant-design/issues"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"repository"</span>: &#123;</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"git"</span>,</span><br><span class="line">    <span class="attr">"url"</span>: <span class="string">"https://github.com/ant-design/ant-design"</span></span><br><span class="line">  &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>homepage</code> 用于指定该模块的主页。</p><p><code>repository</code> 用于指定模块的代码仓库。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef285e805c7tplv-t2oaga2asx-image.png" alt></p><p><code>bugs</code> 指定一个地址或者一个邮箱，对你的模块存在疑问的人可以到这里提出问题。</p><h3 id="依赖配置"><a href="#依赖配置" class="headerlink" title="依赖配置"></a>依赖配置</h3><p>我们的项目可能依赖一个或多个外部依赖包，根据依赖包的不同用途，我们将他们配置在下面几个属性下：<code>dependencies、devDependencies、peerDependencies、bundledDependencies、optionalDependencies</code>。</p><h4 id="配置规则"><a href="#配置规则" class="headerlink" title="配置规则"></a>配置规则</h4><p>在介绍几种依赖配置之前，首先我们来看一下依赖的配置规则，你看到的依赖包配置可能是下面这样的：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">"dependencies": &#123;</span><br><span class="line">     "antd": "ant-design/ant-design#4.0.0-alpha.8",</span><br><span class="line">     "axios": "^1.2.0",</span><br><span class="line">     "test-js": "file:../test",</span><br><span class="line">     "test2-js": "http://cdn.com/test2-js.tar.gz",</span><br><span class="line">     "core-js": "^1.1.5",</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>依赖配置遵循下面几种配置规则：</p><ul><li><code>依赖包名称:VERSION</code><ul><li><code>VERSION</code>是一个遵循<code>SemVer</code>规范的版本号配置，<code>npm install</code> 时将到npm服务器下载符合指定版本范围的包。</li></ul></li><li><code>依赖包名称:DWONLOAD_URL</code><ul><li><code>DWONLOAD_URL</code> 是一个可下载的<code>tarball</code>压缩包地址，模块安装时会将这个<code>.tar</code>下载并安装到本地。</li></ul></li><li><code>依赖包名称:LOCAL_PATH</code><ul><li><code>LOCAL_PATH</code> 是一个本地的依赖包路径，例如 <code>file:../pacakges/pkgName</code>。适用于你在本地测试一个<code>npm</code>包，不应该将这种方法应用于线上。</li></ul></li><li><code>依赖包名称:GITHUB_URL</code><ul><li><code>GITHUB_URL</code> 即 <code>github</code> 的 <code>username/modulename</code> 的写法，例如：<code>ant-design/ant-design</code>，你还可以在后面指定 <code>tag</code> 和 <code>commit id</code>。</li></ul></li><li><code>依赖包名称:GIT_URL</code><ul><li><code>GIT_URL</code> 即我们平时clone代码库的 <code>git url</code>，其遵循以下形式：</li></ul></li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;protocol&gt;:<span class="comment">//[&lt;user&gt;[:&lt;password&gt;]@]&lt;hostname&gt;[:&lt;port&gt;][:][/]&lt;path&gt;[#&lt;commit-ish&gt; | #semver:&lt;semver&gt;]</span></span><br></pre></td></tr></table></figure><p>其中 <code>protocal</code> 可以是以下几种形式：</p><ul><li><code>git://github.com/user/project.git#commit-ish</code></li><li><code>git+ssh://user@hostname:project.git#commit-ish</code></li><li><code>git+ssh://user@hostname/project.git#commit-ish</code></li><li><code>git+http://user@hostname/project/blah.git#commit-ish</code></li><li><code>git+https://user@hostname/project/blah.git#commit-ish</code></li></ul><h4 id="dependencies"><a href="#dependencies" class="headerlink" title="dependencies"></a>dependencies</h4><p><code>dependencies</code> 指定了项目运行所依赖的模块，开发环境和生产环境的依赖模块都可以配置到这里，例如</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"dependencies": &#123;</span><br><span class="line">     "lodash": "^4.17.13",</span><br><span class="line">     "moment": "^2.24.0",</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="devDependencies"><a href="#devDependencies" class="headerlink" title="devDependencies"></a>devDependencies</h4><p>有一些包有可能你只是在开发环境中用到，例如你用于检测代码规范的 <code>eslint</code> ,用于进行测试的 <code>jest</code> ，用户使用你的包时即使不安装这些依赖也可以正常运行，反而安装他们会耗费更多的时间和资源，所以你可以把这些依赖添加到 <code>devDependencies</code> 中，这些依赖照样会在你本地进行 <code>npm install</code> 时被安装和管理，但是不会被安装到生产环境：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"devDependencies": &#123;</span><br><span class="line">     "jest": "^24.3.1",</span><br><span class="line">     "eslint": "^6.1.0",</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="peerDependencies"><a href="#peerDependencies" class="headerlink" title="peerDependencies"></a>peerDependencies</h4><p><code>peerDependencies</code> 用于指定你正在开发的模块所依赖的版本以及用户安装的依赖包版本的兼容性。</p><p>上面的说法可能有点太抽象，我们直接拿 <code>ant-design</code> 来举个例子，<code>ant-design</code> 的 <code>package.json</code> 中有如下配置：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"peerDependencies": &#123;</span><br><span class="line">  "react": "&gt;=16.0.0",</span><br><span class="line">  "react-dom": "&gt;=16.0.0"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当你正在开发一个系统，使用了 <code>ant-design</code> ，所以也肯定需要依赖 <code>React</code>。同时， <code>ant-design</code> 也是需要依赖 <code>React</code> 的，它要保持稳定运行所需要的 <code>React</code> 版本是<code>16.0.0</code>，而你开发时依赖的 <code>React</code> 版本是 <code>15.x</code>：</p><p>这时，<code>ant-design</code> 要使用 <code>React</code>，并将其引入：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> React <span class="keyword">from</span> <span class="string">'react'</span>;</span><br><span class="line"><span class="keyword">import</span> * <span class="keyword">as</span> ReactDOM <span class="keyword">from</span> <span class="string">'react-dom'</span>;</span><br></pre></td></tr></table></figure><p>这时取到的是宿主环境也就是你的环境中的 <code>React</code> 版本，这就可能造成一些问题。在 <code>npm2</code> 的时候，指定上面的 <code>peerDependencies</code> 将意味着强制宿主环境安装 <code>react@&gt;=16.0.0和react-dom@&gt;=16.0.0</code> 的版本。</p><p><code>npm3</code> 以后不会再要求 <code>peerDependencies</code> 所指定的依赖包被强制安装，相反 <code>npm3</code> 会在安装结束后检查本次安装是否正确，如果不正确会给用户打印警告提示。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"dependencies": &#123;</span><br><span class="line">  "react": "15.6.0",</span><br><span class="line">  "antd": "^3.22.0"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如，我在项目中依赖了 <code>antd</code> 的最新版本，然后依赖了 <code>react</code> 的 <code>15.6.0</code>版本，在进行依赖安装时将给出以下警告：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2b2a3e5c4tplv-t2oaga2asx-image.png" alt></p><h4 id="optionalDependencies"><a href="#optionalDependencies" class="headerlink" title="optionalDependencies"></a>optionalDependencies</h4><p>某些场景下，依赖包可能不是强依赖的，这个依赖包的功能可有可无，当这个依赖包无法被获取到时，你希望 <code>npm install</code> 继续运行，而不会导致失败，你可以将这个依赖放到 <code>optionalDependencies</code> 中，注意 <code>optionalDependencies</code> 中的配置将会覆盖掉 <code>dependencies</code> 所以只需在一个地方进行配置。</p><p>当然，引用 <code>optionalDependencies</code> 中安装的依赖时，一定要做好异常处理，否则在模块获取不到时会导致报错。</p><h4 id="bundledDependencies"><a href="#bundledDependencies" class="headerlink" title="bundledDependencies"></a>bundledDependencies</h4><p>和以上几个不同，<code>bundledDependencies</code> 的值是一个数组，数组里可以指定一些模块，这些模块将在这个包发布时被一起打包。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"bundledDependencies": ["package1" , "package2"]</span><br></pre></td></tr></table></figure><h3 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"license"</span>: <span class="string">"MIT"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>license</code> 字段用于指定软件的开源协议，开源协议里面详尽表述了其他人获得你代码后拥有的权利，可以对你的的代码进行何种操作，何种操作又是被禁止的。同一款协议有很多变种，协议太宽松会导致作者丧失对作品的很多权利，太严格又不便于使用者使用及作品的传播，所以开源作者要考虑自己对作品想保留哪些权利，放开哪些限制。</p><blockquote><p>软件协议可分为开源和商业两类，对于商业协议，或者叫法律声明、许可协议，每个软件会有自己的一套行文，由软件作者或专门律师撰写，对于大多数人来说不必自己花时间和精力去写繁长的许可协议，选择一份广为流传的开源协议就是个不错的选择。</p></blockquote><p>以下就是几种主流的开源协议：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2b7fb2674tplv-t2oaga2asx-image.png" alt></p><ul><li><code>MIT</code>：只要用户在项目副本中包含了版权声明和许可声明，他们就可以拿你的代码做任何想做的事情，你也无需承担任何责任。</li><li><code>Apache</code>：类似于 <code>MIT</code>，同时还包含了贡献者向用户提供专利授权相关的条款。</li><li><code>GPL</code>：修改项目代码的用户再次分发源码或二进制代码时，必须公布他的相关修改。</li></ul><p>如果你对开源协议有更详细的要求，可以到 <a href="https://choosealicense.com/" target="_blank" rel="noopener">https://choosealicense.com/</a> 获取更详细的开源协议说明。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2b2f744d8tplv-t2oaga2asx-image.png" alt></p><h3 id="目录、文件相关"><a href="#目录、文件相关" class="headerlink" title="目录、文件相关"></a>目录、文件相关</h3><h4 id="程序入口"><a href="#程序入口" class="headerlink" title="程序入口"></a>程序入口</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"main"</span>: <span class="string">"lib/index.js"</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>main</code> 属性可以指定程序的主入口文件，例如，上面 <code>antd</code> 指定的模块入口 <code>lib/index.js</code> ，当我们在代码用引入 <code>antd</code> 时：<code>import { notification } from &#39;antd&#39;;</code> 实际上引入的就是 <code>lib/index.js</code> 中暴露出去的模块。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2bd8639b2tplv-t2oaga2asx-image.png" alt></p><h4 id="命令行工具入口"><a href="#命令行工具入口" class="headerlink" title="命令行工具入口"></a>命令行工具入口</h4><p>当你的模块是一个命令行工具时，你需要为命令行工具指定一个入口，即指定你的命令名称和本地可指定文件的对应关系。如果是全局安装，npm 将会使用符号链接把可执行文件链接到 <code>/usr/local/bin</code>，如果是本地安装，会链接到 <code>./node_modules/.bin/</code>。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"bin"</span>: &#123;</span><br><span class="line">    <span class="attr">"conard"</span>: <span class="string">"./bin/index.js"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如上面的配置：当你的包安装到全局时：<code>npm</code> 会在 <code>/usr/local/bin</code>下创建一个以 <code>conard</code> 为名字的软链接，指向全局安装下来的 <code>conard</code> 包下面的 <code>&quot;./bin/index.js&quot;</code>。这时你在命令行执行 <code>conard</code> 则会调用链接到的这个js文件。</p><blockquote><p>这里不再过多展开，更多内容在我后续的命令行工具文章中会进行详细讲解。</p></blockquote><h4 id="发布文件配置"><a href="#发布文件配置" class="headerlink" title="发布文件配置"></a>发布文件配置</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"files"</span>: [</span><br><span class="line">      <span class="string">"dist"</span>,</span><br><span class="line">      <span class="string">"lib"</span>,</span><br><span class="line">      <span class="string">"es"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>files</code> 属性用于描述你 <code>npm publish</code> 后推送到 <code>npm</code> 服务器的文件列表，如果指定文件夹，则文件夹内的所有内容都会包含进来。我们可以看到下载后的包是下面的目录结构：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2d5915089tplv-t2oaga2asx-image.png" alt></p><blockquote><p>另外，你还可以通过配置一个 <code>.npmignore</code> 文件来排除一些文件, 防止大量的垃圾文件推送到 <code>npm</code>, 规则上和你用的 <code>.gitignore</code> 是一样的。<code>.gitignore</code> 文件也可以充当<code>.npmignore</code> 文件。</p></blockquote><h4 id="man"><a href="#man" class="headerlink" title="man"></a>man</h4><p><code>man</code> 命令是 <code>Linux</code> 下的帮助指令，通过 <code>man</code> 指令可以查看 <code>Linux</code> 中的指令帮助、配置文件帮助和编程帮助等信息。</p><p>如果你的 <code>node.js</code> 模块是一个全局的命令行工具，在 <code>package.json</code> 通过 <code>man</code>  属性可以指定 <code>man</code> 命令查找的文档地址。</p><p><code>man</code> 文件必须以数字结尾，或者如果被压缩了，以 <code>.gz</code> 结尾。数字表示文件将被安装到 <code>man</code> 的哪个部分。如果 <code>man</code> 文件名称不是以模块名称开头的，安装的时候会给加上模块名称前缀。</p><p>例如下面这段配置：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  <span class="attr">"man"</span> : [ </span><br><span class="line">    <span class="string">"/Users/isaacs/dev/npm/cli/man/man1/npm-access.1"</span>,</span><br><span class="line">    <span class="string">"/Users/isaacs/dev/npm/cli/man/man1/npm-audit.1"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在命令行输入 <code>man npm-audit</code> ：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2c221c0cdtplv-t2oaga2asx-image.png" alt></p><h4 id="规范项目目录"><a href="#规范项目目录" class="headerlink" title="规范项目目录"></a>规范项目目录</h4><p>一个 <code>node.js</code> 模块是基于 <code>CommonJS</code> 模块化规范实现的，严格按照 <code>CommonJS</code> 规范，模块目录下除了必须包含包描述文件 <code>package.json</code> 以外，还需要包含以下目录：</p><ul><li><code>bin</code>：存放可执行二进制文件的目录</li><li><code>lib</code>：存放js代码的目录</li><li><code>doc</code>：存放文档的目录</li><li><code>test</code>：存放单元测试用例代码的目录</li><li>…</li></ul><p>在模块目录中你可能没有严格按照以上结构组织或命名，你可以通过在 <code>package.json</code> 指定 <code>directories</code> 属性来指定你的目录结构和上述的规范结构的对应情况。除此之外 <code>directories</code> 属性暂时没有其他应用。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"directories"</span>: &#123;</span><br><span class="line">    <span class="string">"lib"</span>: <span class="string">"src/lib/"</span>,</span><br><span class="line">    <span class="string">"bin"</span>: <span class="string">"src/bin/"</span>,</span><br><span class="line">    <span class="string">"man"</span>: <span class="string">"src/man/"</span>,</span><br><span class="line">    <span class="string">"doc"</span>: <span class="string">"src/doc/"</span>,</span><br><span class="line">    <span class="string">"example"</span>: <span class="string">"src/example/"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>不过官方文档表示，虽然目前这个属性没有什么重要作用，未来可能会整出一些花样出来，例如：doc 中存放的 markdown 文件、example 中存放的示例文件，可能会友好的展示出来。</p></blockquote><h3 id="脚本配置"><a href="#脚本配置" class="headerlink" title="脚本配置"></a>脚本配置</h3><h4 id="script"><a href="#script" class="headerlink" title="script"></a>script</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"scripts"</span>: &#123;</span><br><span class="line">    <span class="attr">"test"</span>: <span class="string">"jest --config .jest.js --no-cache"</span>,</span><br><span class="line">    <span class="attr">"dist"</span>: <span class="string">"antd-tools run dist"</span>,</span><br><span class="line">    <span class="attr">"compile"</span>: <span class="string">"antd-tools run compile"</span>,</span><br><span class="line">    <span class="attr">"build"</span>: <span class="string">"npm run compile &amp;&amp; npm run dist"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>scripts</code> 用于配置一些脚本命令的缩写，各个脚本可以互相组合使用，这些脚本可以覆盖整个项目的生命周期，配置后可使用 <code>npm run command</code> 进行调用。如果是 <code>npm</code> 关键字，则可以直接调用。例如，上面的配置制定了以下几个命令：<code>npm run test</code>、<code>npm run dist</code>、<code>npm run compile</code>、<code>npm run build</code>。</p><h4 id="config"><a href="#config" class="headerlink" title="config"></a>config</h4><p><code>config</code> 字段用于配置脚本中使用的环境变量，例如下面的配置，可以在脚本中使用<code>process.env.npm_package_config_port</code>进行获取。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"config"</span> : &#123; <span class="attr">"port"</span> : <span class="string">"8080"</span> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="发布配置"><a href="#发布配置" class="headerlink" title="发布配置"></a>发布配置</h3><h4 id="preferGlobal"><a href="#preferGlobal" class="headerlink" title="preferGlobal"></a>preferGlobal</h4><p>如果你的 <code>node.js</code> 模块主要用于安装到全局的命令行工具，那么该值设置为 <code>true</code> ，当用户将该模块安装到本地时，将得到一个警告。这个配置并不会阻止用户安装，而是会提示用户防止错误使用而引发一些问题。</p><h4 id="private"><a href="#private" class="headerlink" title="private"></a>private</h4><p>如果将 <code>private</code> 属性设置为 <code>true</code>，npm将拒绝发布它，这是为了防止一个私有模块被无意间发布出去。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2f07ab9ectplv-t2oaga2asx-image.png" alt></p><h4 id="publishConfig"><a href="#publishConfig" class="headerlink" title="publishConfig"></a>publishConfig</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"publishConfig": &#123;</span><br><span class="line">  "registry": "https://registry.npmjs.org/"</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>发布模块时更详细的配置，例如你可以配置只发布某个 <code>tag</code>、配置发布到的私有 <code>npm</code> 源。更详细的配置可以参考 <a href="http://caibaojian.com/npm/misc/config.html" target="_blank" rel="noopener">npm-config</a> </p><h4 id="os"><a href="#os" class="headerlink" title="os"></a>os</h4><p>假如你开发了一个模块，只能跑在 <code>darwin</code> 系统下，你需要保证 <code>windows</code> 用户不会安装到你的模块，从而避免发生不必要的错误。</p><p>使用 <code>os</code> 属性可以帮助你完成以上的需求，你可以指定你的模块只能被安装在某些系统下，或者指定一个不能安装的系统黑名单：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"os" : [ "darwin", "linux" ]</span><br><span class="line">"os" : [ "!win32" ]</span><br></pre></td></tr></table></figure><p>例如，我把一个测试模块指定一个系统黑名单：<code>&quot;os&quot; : [ &quot;!darwin&quot; ]</code>，当我在此系统下安装它时会爆出如下错误：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef2f8072f86tplv-t2oaga2asx-image.png" alt></p><blockquote><p>在node环境下可以使用 process.platform 来判断操作系统。</p></blockquote><h4 id="cpu"><a href="#cpu" class="headerlink" title="cpu"></a>cpu</h4><p>和上面的 <code>os</code> 类似，我们可以用 <code>cpu</code> 属性更精准的限制用户安装环境：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"cpu" : [ "x64", "ia32" ]</span><br><span class="line">"cpu" : [ "!arm", "!mips" ]</span><br></pre></td></tr></table></figure><blockquote><p>在node环境下可以使用 process.arch 来判断 cpu 架构。</p></blockquote><h2 id="剖析包版本管理机制"><a href="#剖析包版本管理机制" class="headerlink" title="剖析包版本管理机制"></a>剖析包版本管理机制</h2><p><code>Nodejs</code>成功离不开 <code>npm</code> 优秀的依赖管理系统。在介绍整个依赖系统之前，必须要了解 <code>npm</code>如何管理依赖包的版本，本章将介绍 <code>npm包</code> 的版本发布规范、如何管理各种依赖包的版本以及一些关于包版本的最佳实践。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef301559a8etplv-t2oaga2asx-image.png" alt></p><h3 id="查看npm包版本"><a href="#查看npm包版本" class="headerlink" title="查看npm包版本"></a>查看npm包版本</h3><p>你可以执行 <code>npm view package version</code> 查看某个 <code>package</code> 的最新版本。</p><p>执行 <code>npm view conard versions</code> 查看某个 <code>package</code> 在npm服务器上所有发布过的版本。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3016562c8tplv-t2oaga2asx-image.png" alt></p><p>执行 <code>npm ls</code> 可查看当前仓库依赖树上所有包的版本信息。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef302c760fetplv-t2oaga2asx-image.png" alt></p><h3 id="SemVer规范"><a href="#SemVer规范" class="headerlink" title="SemVer规范"></a>SemVer规范</h3><p><code>npm包</code> 中的模块版本都需要遵循 <code>SemVer</code>规范——由 <code>Github</code> 起草的一个具有指导意义的，统一的版本号表示规则。实际上就是 <code>Semantic Version</code>（语义化版本）的缩写。</p><blockquote><p>SemVer规范官网： <a href="https://semver.org/" target="_blank" rel="noopener">https://semver.org/</a></p></blockquote><h4 id="标准版本"><a href="#标准版本" class="headerlink" title="标准版本"></a>标准版本</h4><p><code>SemVer</code>规范的标准版本号采用 <code>X.Y.Z</code> 的格式，其中 X、Y 和 Z 为非负的整数，且禁止在数字前方补零。X 是主版本号、Y 是次版本号、而 Z 为修订号。每个元素必须以数值来递增。</p><ul><li>主版本号(<code>major</code>)：当你做了不兼容的API 修改</li><li>次版本号(<code>minor</code>)：当你做了向下兼容的功能性新增</li><li>修订号(<code>patch</code>)：当你做了向下兼容的问题修正。 </li></ul><p>例如：<code>1.9.1 -&gt; 1.10.0 -&gt; 1.11.0</code></p><h4 id="先行版本"><a href="#先行版本" class="headerlink" title="先行版本"></a>先行版本</h4><p>当某个版本改动比较大、并非稳定而且可能无法满足预期的兼容性需求时，你可能要先发布一个先行版本。</p><p>先行版本号可以加到“主版本号.次版本号.修订号”的后面，先加上一个连接号再加上一连串以句点分隔的标识符和版本编译信息。</p><ul><li>内部版本(<code>alpha</code>): </li><li>公测版本(<code>beta</code>): </li><li>正式版本的候选版本<code>rc</code>: 即 <code>Release candiate</code></li></ul><h4 id="React的版本"><a href="#React的版本" class="headerlink" title="React的版本"></a>React的版本</h4><p>下面我们来看看 <code>React</code> 的历史版本：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3224e9335tplv-t2oaga2asx-image.gif" alt></p><p>可见是严格按照 <code>SemVer</code> 规范来发版的：</p><ul><li>版本号严格按照 <code>主版本号.次版本号.修订号</code> 格式命名</li><li>版本是严格递增的，：<code>16.8.0 -&gt; 16.8.1 -&gt; 16.8.2</code></li><li>发布重大版本或版本改动较大时，先发布<code>alpha</code>、<code>beta</code>、<code>rc</code>等先行版本</li></ul><h4 id="发布版本"><a href="#发布版本" class="headerlink" title="发布版本"></a>发布版本</h4><p>在修改 <code>npm</code> 包某些功能后通常需要发布一个新的版本，我们通常的做法是直接去修改 <code>package.json</code> 到指定版本。如果操作失误，很容易造成版本号混乱，我们可以借助符合 <code>Semver</code> 规范的命令来完成这一操作：</p><ul><li><code>npm version patch</code> : 升级修订版本号</li><li><code>npm version minor</code> : 升级次版本号</li><li><code>npm version major</code> : 升级主版本号</li></ul><h3 id="版本工具使用"><a href="#版本工具使用" class="headerlink" title="版本工具使用"></a>版本工具使用</h3><p>在开发中肯定少不了对一些版本号的操作，如果这些版本号符合 <code>SemVer</code>规范 ，我们可以借助用于操作版本的npm包<code>semver</code>来帮助我们进行比较版本大小、提取版本信息等操作。</p><blockquote><p>Npm 也使用了该工具来处理版本相关的工作。</p></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install semver</span><br></pre></td></tr></table></figure><ul><li><p>比较版本号大小</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">semver.gt(<span class="string">'1.2.3'</span>, <span class="string">'9.8.7'</span>) <span class="comment">// false</span></span><br><span class="line">semver.lt(<span class="string">'1.2.3'</span>, <span class="string">'9.8.7'</span>) <span class="comment">// true</span></span><br></pre></td></tr></table></figure></li><li><p>判断版本号是否符合规范，返回解析后符合规范的版本号。</p></li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">semver.valid(<span class="string">'1.2.3'</span>) <span class="comment">// '1.2.3'</span></span><br><span class="line">semver.valid(<span class="string">'a.b.c'</span>) <span class="comment">// null</span></span><br></pre></td></tr></table></figure><ul><li>将其他版本号强制转换成semver版本号</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">semver.valid(semver.coerce(<span class="string">'v2'</span>)) <span class="comment">// '2.0.0'</span></span><br><span class="line">semver.valid(semver.coerce(<span class="string">'42.6.7.9.3-alpha'</span>)) <span class="comment">// '42.6.7'</span></span><br></pre></td></tr></table></figure><ul><li>一些其他用法</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">semver.clean(<span class="string">'  =v1.2.3   '</span>) <span class="comment">// '1.2.3'</span></span><br><span class="line">semver.satisfies(<span class="string">'1.2.3'</span>, <span class="string">'1.x || &gt;=2.5.0 || 5.0.0 - 7.2.3'</span>) <span class="comment">// true</span></span><br><span class="line">semver.minVersion(<span class="string">'&gt;=1.0.0'</span>) <span class="comment">// '1.0.0'</span></span><br></pre></td></tr></table></figure><p>以上都是semver最常见的用法，更多详细内容可以查看 semver文档：<a href="https://github.com/npm/node-semver" target="_blank" rel="noopener">https://github.com/npm/node-semver</a></p><h3 id="依赖版本管理"><a href="#依赖版本管理" class="headerlink" title="依赖版本管理"></a>依赖版本管理</h3><p>我们经常看到，在 <code>package.json</code> 中各种依赖的不同写法：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">"dependencies": &#123;</span><br><span class="line">  "signale": "1.4.0",</span><br><span class="line">  "figlet": "*",</span><br><span class="line">  "react": "16.x",</span><br><span class="line">  "table": "~5.4.6",</span><br><span class="line">  "yargs": "^14.0.0"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前面三个很容易理解：</p><ul><li><code>&quot;signale&quot;: &quot;1.4.0&quot;</code>: 固定版本号</li><li><code>&quot;figlet&quot;: &quot;*&quot;</code>: 任意版本（<code>&gt;=0.0.0</code>）</li><li><code>&quot;react&quot;: &quot;16.x&quot;</code>: 匹配主要版本（<code>&gt;=16.0.0 &lt;17.0.0</code>）</li><li><code>&quot;react&quot;: &quot;16.3.x&quot;</code>: 匹配主要版本和次要版本（<code>&gt;=16.3.0 &lt;16.4.0</code>）</li></ul><p>再来看看后面两个，版本号中引用了 <code>~</code> 和 <code>^</code> 符号：</p><ul><li><code>~</code>: 当安装依赖时获取到有新版本时，安装到 <code>x.y.z</code> 中 <code>z</code> 的最新的版本。即保持主版本号、次版本号不变的情况下，保持修订号的最新版本。</li><li><code>^</code>: 当安装依赖时获取到有新版本时，安装到 <code>x.y.z</code> 中 <code>y</code> 和 <code>z</code> 都为最新版本。 即保持主版本号不变的情况下，保持次版本号、修订版本号为最新版本。</li></ul><p>在 <code>package.json</code> 文件中最常见的应该是 <code>&quot;yargs&quot;: &quot;^14.0.0&quot;</code> 这种格式的 依赖, 因为我们在使用 <code>npm install package</code> 安装包时，<code>npm</code> 默认安装当前最新版本，然后在所安装的版本号前加 <code>^</code> 号。</p><p>注意，当主版本号为 <code>0</code> 的情况，会被认为是一个不稳定版本，情况与上面不同：</p><ul><li>主版本号和次版本号都为 <code>0</code>: <code>^0.0.z</code>、<code>~0.0.z</code> 都被当作固定版本，安装依赖时均不会发生变化。</li><li>主版本号为 <code>0</code>: <code>^0.y.z</code> 表现和 <code>~0.y.z</code> 相同，只保持修订号为最新版本。</li></ul><blockquote><p>1.0.0 的版本号用于界定公共 API。当你的软件发布到了正式环境，或者有稳定的API时，就可以发布1.0.0版本了。所以，当你决定对外部发布一个正式版本的npm包时，把它的版本标为1.0.0。</p></blockquote><h3 id="锁定依赖版本"><a href="#锁定依赖版本" class="headerlink" title="锁定依赖版本"></a>锁定依赖版本</h3><h4 id="lock文件"><a href="#lock文件" class="headerlink" title="lock文件"></a>lock文件</h4><p>实际开发中，经常会因为各种依赖不一致而产生奇怪的问题，或者在某些场景下，我们不希望依赖被更新，建议在开发中使用 <code>package-lock.json</code>。</p><p>锁定依赖版本意味着在我们不手动执行更新的情况下，每次安装依赖都会安装固定版本。保证整个团队使用版本号一致的依赖。</p><p>每次安装固定版本，无需计算依赖版本范围，大部分场景下能大大加速依赖安装时间。</p><blockquote><p>使用 package-lock.json 要确保npm的版本在5.6以上，因为在5.0 - 5.6中间，对 package-lock.json的处理逻辑进行过几次更新，5.6版本后处理逻辑逐渐稳定。</p></blockquote><p>关于 <code>package-lock.json</code> 详细的结构，我们会在后面的章节进行解析。</p><h4 id="定期更新依赖"><a href="#定期更新依赖" class="headerlink" title="定期更新依赖"></a>定期更新依赖</h4><p>我们的目的是保证团队中使用的依赖一致或者稳定，而不是永远不去更新这些依赖。实际开发场景下，我们虽然不需要每次都去安装新的版本，仍然需要定时去升级依赖版本，来让我们享受依赖包升级带来的问题修复、性能提升、新特性更新。</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef31ae6c1ebtplv-t2oaga2asx-image.png" alt></p><p>使用 <code>npm outdated</code> 可以帮助我们列出有哪些还没有升级到最新版本的依赖：</p><ul><li>黄色表示不符合我们指定的语意化版本范围 - 不需要升级</li><li>红色表示符合指定的语意化版本范围 - 需要升级</li></ul><p>执行 <code>npm update</code> 会升级所有的红色依赖。</p><h3 id="依赖版本选择的最佳实践"><a href="#依赖版本选择的最佳实践" class="headerlink" title="依赖版本选择的最佳实践"></a>依赖版本选择的最佳实践</h3><h4 id="版本发布"><a href="#版本发布" class="headerlink" title="版本发布"></a>版本发布</h4><ul><li>对外部发布一个正式版本的npm包时，把它的版本标为<code>1.0.0</code>。</li><li>某个包版本发行后，任何修改都必须以新版本发行。</li><li>版本号严格按照 <code>主版本号.次版本号.修订号</code> 格式命名</li><li>版本号发布必须是严格递增的</li><li>发布重大版本或版本改动较大时，先发布<code>alpha、beta、rc</code>等先行版本</li></ul><h4 id="依赖范围选择"><a href="#依赖范围选择" class="headerlink" title="依赖范围选择"></a>依赖范围选择</h4><ul><li>主工程依赖了很多子模块，都是团队成员开发的<code>npm</code>包，此时建议把版本前缀改为<code>~</code>，如果锁定的话每次子依赖更新都要对主工程的依赖进行升级，非常繁琐，如果对子依赖完全信任，直接开启<code>^</code>每次升级到最新版本。</li><li>主工程跑在<code>docker</code>线上，本地还在进行子依赖开发和升级，在<code>docker</code>版本发布前要锁定所有依赖版本，确保本地子依赖发布后线上不会出问题。</li></ul><h4 id="保持依赖一致"><a href="#保持依赖一致" class="headerlink" title="保持依赖一致"></a>保持依赖一致</h4><ul><li>确保<code>npm</code>的版本在<code>5.6</code>以上，确保默认开启 <code>package-lock.json</code> 文件。</li><li>由初始化成员执行 <code>npm inatall</code> 后，将 <code>package-lock.json</code> 提交到远程仓库。不要直接提交 <code>node_modules</code>到远程仓库。</li><li>定期执行 <code>npm update</code> 升级依赖，并提交 <code>lock</code> 文件确保其他成员同步更新依赖，不要手动更改 <code>lock</code> 文件。</li></ul><h4 id="依赖变更"><a href="#依赖变更" class="headerlink" title="依赖变更"></a>依赖变更</h4><ul><li>升级依赖: 修改 <code>package.json</code>文件的依赖版本，执行 <code>npm install</code></li><li>降级依赖: 直接执行 <code>npm install package@version</code>(改动<code>package.json</code>不会对依赖进行降级)</li><li>注意改动依赖后提交<code>lock</code>文件</li></ul><h2 id="剖析-npm-install-原理"><a href="#剖析-npm-install-原理" class="headerlink" title="剖析 npm install 原理"></a>剖析 npm install 原理</h2><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef327ccaba5tplv-t2oaga2asx-image.png" alt></p><p><code>npm install</code> 大概会经过上面的几个流程，这一章就来讲一讲各个流程的实现细节、发展以及为何要这样实现。</p><h3 id="嵌套结构"><a href="#嵌套结构" class="headerlink" title="嵌套结构"></a>嵌套结构</h3><p>我们都知道，执行 <code>npm install</code> 后，依赖包被安装到了 <code>node_modules</code> ，下面我们来具体了解下，<code>npm</code> 将依赖包安装到 <code>node_modules</code> 的具体机制是什么。</p><p>在 <code>npm</code> 的早期版本， <code>npm</code> 处理依赖的方式简单粗暴，以递归的形式，严格按照 <code>package.json</code> 结构以及子依赖包的 <code>package.json</code> 结构将依赖安装到他们各自的 <code>node_modules</code> 中。直到有子依赖包不在依赖其他模块。</p><p>举个例子，我们的模块 <code>my-app</code> 现在依赖了两个模块：<code>buffer</code>、<code>ignore</code>：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"my-app"</span>,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"buffer"</span>: <span class="string">"^5.4.3"</span>,</span><br><span class="line">    <span class="attr">"ignore"</span>: <span class="string">"^5.1.4"</span>,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>ignore</code>是一个纯 <code>JS</code> 模块，不依赖任何其他模块，而 <code>buffer</code> 又依赖了下面两个模块：<code>base64-js</code> 、 <code>ieee754</code>。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"buffer"</span>,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"base64-js"</span>: <span class="string">"^1.0.2"</span>,</span><br><span class="line">    <span class="attr">"ieee754"</span>: <span class="string">"^1.1.4"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么，执行 <code>npm install</code> 后，得到的 <code>node_modules</code> 中模块目录结构就是下面这样的：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef33997d7f2tplv-t2oaga2asx-image.png" alt></p><p>这样的方式优点很明显， <code>node_modules</code> 的结构和 <code>package.json</code> 结构一一对应，层级结构明显，并且保证了每次安装目录结构都是相同的。</p><p>但是，试想一下，如果你依赖的模块非常之多，你的 <code>node_modules</code> 将非常庞大，嵌套层级非常之深：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef33d822969tplv-t2oaga2asx-image.png" alt></p><ul><li>在不同层级的依赖中，可能引用了同一个模块，导致大量冗余。</li><li>在 <code>Windows</code> 系统中，文件路径最大长度为260个字符，嵌套层级过深可能导致不可预知的问题。</li></ul><h3 id="扁平结构"><a href="#扁平结构" class="headerlink" title="扁平结构"></a>扁平结构</h3><p>为了解决以上问题，<code>NPM</code> 在 <code>3.x</code> 版本做了一次较大更新。其将早期的嵌套结构改为扁平结构：</p><ul><li>安装模块时，不管其是直接依赖还是子依赖的依赖，优先将其安装在 <code>node_modules</code> 根目录。</li></ul><p>还是上面的依赖结构，我们在执行 <code>npm install</code> 后将得到下面的目录结构：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3518941f2tplv-t2oaga2asx-image.png" alt></p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3519475d1tplv-t2oaga2asx-image.png" alt></p><p>此时我们若在模块中又依赖了 <code>base64-js@1.0.1</code> 版本：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"my-app"</span>,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"buffer"</span>: <span class="string">"^5.4.3"</span>,</span><br><span class="line">    <span class="attr">"ignore"</span>: <span class="string">"^5.1.4"</span>,</span><br><span class="line">    <span class="attr">"base64-js"</span>: <span class="string">"1.0.1"</span>,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>当安装到相同模块时，判断已安装的模块版本是否符合新模块的版本范围，如果符合则跳过，不符合则在当前模块的 <code>node_modules</code> 下安装该模块。</li></ul><p>此时，我们在执行 <code>npm install</code> 后将得到下面的目录结构：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef355ae3b37tplv-t2oaga2asx-image.png" alt></p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef35ae17872tplv-t2oaga2asx-image.png" alt></p><p>对应的，如果我们在项目代码中引用了一个模块，模块查找流程如下：</p><ul><li>在当前模块路径下搜索</li><li>在当前模块 <code>node_modules</code> 路径下搜素</li><li>在上级模块的 <code>node_modules</code> 路径下搜索</li><li>…</li><li>直到搜索到全局路径中的 <code>node_modules</code> </li></ul><p>假设我们又依赖了一个包 <code>buffer2@^5.4.3</code>，而它依赖了包 <code>base64-js@1.0.3</code>，则此时的安装结构是下面这样的：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef377260d67tplv-t2oaga2asx-image.png" alt></p><p>所以 <code>npm 3.x</code> 版本并未完全解决老版本的模块冗余问题，甚至还会带来新的问题。</p><p>试想一下，你的APP假设没有依赖 <code>base64-js@1.0.1</code> 版本，而你同时依赖了依赖不同 <code>base64-js</code> 版本的 <code>buffer</code> 和 <code>buffer2</code>。由于在执行 <code>npm install</code> 的时候，按照 <code>package.json</code> 里依赖的顺序依次解析，则 <code>buffer</code> 和 <code>buffer2</code> 在  <code>package.json</code> 的放置顺序则决定了 <code>node_modules</code> 的依赖结构：</p><p>先依赖<code>buffer2</code>：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3824eba10tplv-t2oaga2asx-image.png" alt></p><p>先依赖<code>buffer</code>：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef38a55f11etplv-t2oaga2asx-image.png" alt></p><p>另外，为了让开发者在安全的前提下使用最新的依赖包，我们在 <code>package.json</code> 通常只会锁定大版本，这意味着在某些依赖包小版本更新后，同样可能造成依赖结构的改动，依赖结构的不确定性可能会给程序带来不可预知的问题。</p><h3 id="Lock文件"><a href="#Lock文件" class="headerlink" title="Lock文件"></a>Lock文件</h3><p>为了解决 <code>npm install</code> 的不确定性问题，在 <code>npm 5.x</code> 版本新增了 <code>package-lock.json</code> 文件，而安装方式还沿用了 <code>npm 3.x</code> 的扁平化的方式。 </p><p> <code>package-lock.json</code> 的作用是锁定依赖结构，即只要你目录下有 <code>package-lock.json</code> 文件，那么你每次执行 <code>npm install</code> 后生成的 <code>node_modules</code> 目录结构一定是完全相同的。</p><p>例如，我们有如下的依赖结构：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"my-app"</span>,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"buffer"</span>: <span class="string">"^5.4.3"</span>,</span><br><span class="line">    <span class="attr">"ignore"</span>: <span class="string">"^5.1.4"</span>,</span><br><span class="line">    <span class="attr">"base64-js"</span>: <span class="string">"1.0.1"</span>,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在执行 <code>npm install</code> 后生成的 <code>package-lock.json</code> 如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"my-app"</span>,</span><br><span class="line">  <span class="attr">"version"</span>: <span class="string">"1.0.0"</span>,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"base64-js"</span>: &#123;</span><br><span class="line">      <span class="attr">"version"</span>: <span class="string">"1.0.1"</span>,</span><br><span class="line">      <span class="attr">"resolved"</span>: <span class="string">"https://registry.npmjs.org/base64-js/-/base64-js-1.0.1.tgz"</span>,</span><br><span class="line">      <span class="attr">"integrity"</span>: <span class="string">"sha1-aSbRsZT7xze47tUTdW3i/Np+pAg="</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"buffer"</span>: &#123;</span><br><span class="line">      <span class="attr">"version"</span>: <span class="string">"5.4.3"</span>,</span><br><span class="line">      <span class="attr">"resolved"</span>: <span class="string">"https://registry.npmjs.org/buffer/-/buffer-5.4.3.tgz"</span>,</span><br><span class="line">      <span class="attr">"integrity"</span>: <span class="string">"sha512-zvj65TkFeIt3i6aj5bIvJDzjjQQGs4o/sNoezg1F1kYap9Nu2jcUdpwzRSJTHMMzG0H7bZkn4rNQpImhuxWX2A=="</span>,</span><br><span class="line">      <span class="attr">"requires"</span>: &#123;</span><br><span class="line">        <span class="attr">"base64-js"</span>: <span class="string">"^1.0.2"</span>,</span><br><span class="line">        <span class="attr">"ieee754"</span>: <span class="string">"^1.1.4"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">        <span class="attr">"base64-js"</span>: &#123;</span><br><span class="line">          <span class="attr">"version"</span>: <span class="string">"1.3.1"</span>,</span><br><span class="line">          <span class="attr">"resolved"</span>: <span class="string">"https://registry.npmjs.org/base64-js/-/base64-js-1.3.1.tgz"</span>,</span><br><span class="line">          <span class="attr">"integrity"</span>: <span class="string">"sha512-mLQ4i2QO1ytvGWFWmcngKO//JXAQueZvwEKtjgQFM4jIK0kU+ytMfplL8j+n5mspOfjHwoAg+9yhb7BwAHm36g=="</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"ieee754"</span>: &#123;</span><br><span class="line">      <span class="attr">"version"</span>: <span class="string">"1.1.13"</span>,</span><br><span class="line">      <span class="attr">"resolved"</span>: <span class="string">"https://registry.npmjs.org/ieee754/-/ieee754-1.1.13.tgz"</span>,</span><br><span class="line">      <span class="attr">"integrity"</span>: <span class="string">"sha512-4vf7I2LYV/HaWerSo3XmlMkp5eZ83i+/CDluXi/IGTs/O1sejBNhTtnxzmRZfvOUqj7lZjqHkeTvpgSFDlWZTg=="</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"ignore"</span>: &#123;</span><br><span class="line">      <span class="attr">"version"</span>: <span class="string">"5.1.4"</span>,</span><br><span class="line">      <span class="attr">"resolved"</span>: <span class="string">"https://registry.npmjs.org/ignore/-/ignore-5.1.4.tgz"</span>,</span><br><span class="line">      <span class="attr">"integrity"</span>: <span class="string">"sha512-MzbUSahkTW1u7JpKKjY7LCARd1fU5W2rLdxlM4kdkayuCwZImjkpluF9CM1aLewYJguPDqewLam18Y6AU69A8A=="</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们来具体看看上面的结构：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3a81eb51ftplv-t2oaga2asx-image.png" alt></p><p>最外面的两个属性 <code>name</code> 、<code>version</code> 同 <code>package.json</code> 中的 <code>name</code> 和 <code>version</code> ，用于描述当前包名称和版本。</p><p><code>dependencies</code> 是一个对象，对象和 <code>node_modules</code> 中的包结构一一对应，对象的 <code>key</code> 为包名称，值为包的一些描述信息：</p><ul><li><code>version</code>：包版本 —— 这个包当前安装在 <code>node_modules</code> 中的版本</li><li><code>resolved</code>：包具体的安装来源</li><li><code>integrity</code>：包 <code>hash</code> 值，基于 <code>Subresource Integrity</code> 来验证已安装的软件包是否被改动过、是否已失效</li><li><code>requires</code>：对应子依赖的依赖，与子依赖的 <code>package.json</code> 中 <code>dependencies</code>的依赖项相同。</li><li><code>dependencies</code>：结构和外层的 <code>dependencies</code> 结构相同，存储安装在子依赖 <code>node_modules</code> 中的依赖包。</li></ul><p>这里注意，并不是所有的子依赖都有 <code>dependencies</code> 属性，只有子依赖的依赖和当前已安装在根目录的  <code>node_modules</code> 中的依赖冲突之后，才会有这个属性。</p><p>例如，回顾下上面的依赖关系：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef35ae17872tplv-t2oaga2asx-image.png" alt></p><p>我们在 <code>my-app</code> 中依赖的 <code>base64-js@1.0.1</code> 版本与 <code>buffer</code> 中依赖的 <code>base64-js@^1.0.2</code> 发生冲突，所以  <code>base64-js@1.0.1</code>  需要安装在 <code>buffer</code> 包的 <code>node_modules</code> 中，对应了 <code>package-lock.json</code> 中 <code>buffer</code> 的 <code>dependencies</code> 属性。这也对应了 <code>npm</code> 对依赖的扁平化处理方式。</p><p>所以，根据上面的分析， <code>package-lock.json</code> 文件 和 <code>node_modules</code> 目录结构是一一对应的，即项目目录下存在  <code>package-lock.json</code> 可以让每次安装生成的依赖目录结构保持相同。</p><p>另外，项目中使用了 <code>package-lock.json</code> 可以显著加速依赖安装时间。</p><p>我们使用 <code>npm i  --timing=true  --loglevel=verbose</code> 命令可以看到 <code>npm install</code> 的完整过程，下面我们来对比下使用 <code>lock</code> 文件和不使用 <code>lock</code> 文件的差别。在对比前先清理下<code>npm</code> 缓存。</p><p>不使用 <code>lock</code> 文件：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef39713273atplv-t2oaga2asx-image.gif" alt></p><p>使用 <code>lock</code> 文件：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3b5e532e0tplv-t2oaga2asx-image.gif" alt></p><p>可见， <code>package-lock.json</code> 中已经缓存了每个包的具体版本和下载链接，不需要再去远程仓库进行查询，然后直接进入文件完整性校验环节，减少了大量网络请求。</p><h4 id="使用建议"><a href="#使用建议" class="headerlink" title="使用建议"></a>使用建议</h4><p>开发系统应用时，建议把 <code>package-lock.json</code> 文件提交到代码版本仓库，从而保证所有团队开发者以及 <code>CI</code> 环节可以在执行 <code>npm install</code> 时安装的依赖版本都是一致的。</p><p>在开发一个 <code>npm</code>包 时，你的 <code>npm</code>包 是需要被其他仓库依赖的，由于上面我们讲到的扁平安装机制，如果你锁定了依赖包版本，你的依赖包就不能和其他依赖包共享同一 <code>semver</code> 范围内的依赖包，这样会造成不必要的冗余。所以我们不应该把<code>package-lock.json</code> 文件发布出去（ <code>npm</code> 默认也不会把 <code>package-lock.json</code> 文件发布出去）。</p><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>在执行 <code>npm install</code> 或 <code>npm update</code>命令下载依赖后，除了将依赖包安装在<code>node_modules</code> 目录下外，还会在本地的缓存目录缓存一份。</p><p>通过 <code>npm config get cache</code> 命令可以查询到：在 <code>Linux</code> 或 <code>Mac</code> 默认是用户主目录下的 <code>.npm/_cacache</code> 目录。</p><p>在这个目录下又存在两个目录：<code>content-v2</code>、<code>index-v5</code>，<code>content-v2</code> 目录用于存储 <code>tar</code>包的缓存，而<code>index-v5</code>目录用于存储<code>tar</code>包的 <code>hash</code>。</p><p>npm 在执行安装时，可以根据 <code>package-lock.json</code> 中存储的 <code>integrity、version、name</code> 生成一个唯一的 <code>key</code> 对应到 <code>index-v5</code> 目录下的缓存记录，从而找到 <code>tar</code>包的 <code>hash</code>，然后根据 <code>hash</code> 再去找缓存的 <code>tar</code>包直接使用。</p><p>我们可以找一个包在缓存目录下搜索测试一下，在 <code>index-v5</code> 搜索一下包路径：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep <span class="string">"https://registry.npmjs.org/base64-js/-/base64-js-1.0.1.tgz"</span> -r index-v5</span><br></pre></td></tr></table></figure><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3b8fb68f5tplv-t2oaga2asx-image.png" alt></p><p>然后我们将json格式化：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"key"</span>: <span class="string">"pacote:version-manifest:https://registry.npmjs.org/base64-js/-/base64-js-1.0.1.tgz:sha1-aSbRsZT7xze47tUTdW3i/Np+pAg="</span>,</span><br><span class="line">  <span class="attr">"integrity"</span>: <span class="string">"sha512-C2EkHXwXvLsbrucJTRS3xFHv7Mf/y9klmKDxPTE8yevCoH5h8Ae69Y+/lP+ahpW91crnzgO78elOk2E6APJfIQ=="</span>,</span><br><span class="line">  <span class="attr">"time"</span>: <span class="number">1575554308857</span>,</span><br><span class="line">  <span class="attr">"size"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">"metadata"</span>: &#123;</span><br><span class="line">    <span class="attr">"id"</span>: <span class="string">"base64-js@1.0.1"</span>,</span><br><span class="line">    <span class="attr">"manifest"</span>: &#123;</span><br><span class="line">      <span class="attr">"name"</span>: <span class="string">"base64-js"</span>,</span><br><span class="line">      <span class="attr">"version"</span>: <span class="string">"1.0.1"</span>,</span><br><span class="line">      <span class="attr">"engines"</span>: &#123;</span><br><span class="line">        <span class="attr">"node"</span>: <span class="string">"&gt;= 0.4"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"dependencies"</span>: &#123;&#125;,</span><br><span class="line">      <span class="attr">"optionalDependencies"</span>: &#123;&#125;,</span><br><span class="line">      <span class="attr">"devDependencies"</span>: &#123;</span><br><span class="line">        <span class="attr">"standard"</span>: <span class="string">"^5.2.2"</span>,</span><br><span class="line">        <span class="attr">"tape"</span>: <span class="string">"4.x"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"bundleDependencies"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"peerDependencies"</span>: &#123;&#125;,</span><br><span class="line">      <span class="attr">"deprecated"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="attr">"_resolved"</span>: <span class="string">"https://registry.npmjs.org/base64-js/-/base64-js-1.0.1.tgz"</span>,</span><br><span class="line">      <span class="attr">"_integrity"</span>: <span class="string">"sha1-aSbRsZT7xze47tUTdW3i/Np+pAg="</span>,</span><br><span class="line">      <span class="attr">"_shasum"</span>: <span class="string">"6926d1b194fbc737b8eed513756de2fcda7ea408"</span>,</span><br><span class="line">      <span class="attr">"_shrinkwrap"</span>: <span class="literal">null</span>,</span><br><span class="line">      <span class="attr">"bin"</span>: <span class="literal">null</span>,</span><br><span class="line">      <span class="attr">"_id"</span>: <span class="string">"base64-js@1.0.1"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"finalized-manifest"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的 <code>_shasum</code> 属性 <code>6926d1b194fbc737b8eed513756de2fcda7ea408</code> 即为 <code>tar</code> 包的 <code>hash</code>， <code>hash</code>的前几位 <code>6926</code> 即为缓存的前两层目录，我们进去这个目录果然找到的压缩后的依赖包：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3bc635b03tplv-t2oaga2asx-image.png" alt></p><blockquote><p>以上的缓存策略是从 npm v5 版本开始的，在 npm v5 版本之前，每个缓存的模块在 ~/.npm 文件夹中以模块名的形式直接存储，储存结构是{cache}/{name}/{version}。</p></blockquote><p><code>npm</code> 提供了几个命令来管理缓存数据：</p><ul><li><code>npm cache add</code>：官方解释说这个命令主要是 <code>npm</code> 内部使用，但是也可以用来手动给一个指定的 package 添加缓存。</li><li><code>npm cache clean</code>：删除缓存目录下的所有数据，为了保证缓存数据的完整性，需要加上 <code>--force</code> 参数。</li><li><code>npm cache verify</code>：验证缓存数据的有效性和完整性，清理垃圾数据。</li></ul><p>基于缓存数据，npm 提供了离线安装模式，分别有以下几种：</p><ul><li><code>--prefer-offline</code>： 优先使用缓存数据，如果没有匹配的缓存数据，则从远程仓库下载。</li><li><code>--prefer-online</code>： 优先使用网络数据，如果网络数据请求失败，再去请求缓存数据，这种模式可以及时获取最新的模块。</li><li><code>--offline</code>： 不请求网络，直接使用缓存数据，一旦缓存数据不存在，则安装失败。</li></ul><h3 id="文件完整性"><a href="#文件完整性" class="headerlink" title="文件完整性"></a>文件完整性</h3><p>上面我们多次提到了文件完整性，那么什么是文件完整性校验呢？</p><p>在下载依赖包之前，我们一般就能拿到 <code>npm</code> 对该依赖包计算的 <code>hash</code> 值，例如我们执行 <code>npm info</code> 命令，紧跟 <code>tarball</code>(下载链接) 的就是 <code>shasum</code>(<code>hash</code>) ：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3c2a2dac0tplv-t2oaga2asx-image.png" alt></p><p>用户下载依赖包到本地后，需要确定在下载过程中没有出现错误，所以在下载完成之后需要在本地在计算一次文件的 <code>hash</code> 值，如果两个 <code>hash</code> 值是相同的，则确保下载的依赖是完整的，如果不同，则进行重新下载。</p><h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p>注意，下面作者在这整体流程的时候，说直接将包解压到 <code>node_modules</code>。其实这样是不严谨的，当遇到需要安装<code>nodejs-addons</code>的时候（对应包下面有<code>binding.gyp</code>文件），并不仅是压缩，还会自动执行<code>node-gyp rebuild</code>编译C/C++代码。关于<code>nodejs-addons</code>的详细介绍可以看下面<code>刨根问底之node-gyp</code>部分的介绍。</p><blockquote><p>该部分的<a href="https://docs.npmjs.com/cli/v9/configuring-npm/package-json" target="_blank" rel="noopener">官方文档</a>说明如下：</p><p>If there is a <code>binding.gyp</code> file in the root of your package and you have not defined an <code>install</code> or <code>preinstall</code> script, npm will default the <code>install</code> command to compile using node-gyp.</p></blockquote><p>好了，我们再来整体总结下上面的流程：</p><ul><li>检查 <code>.npmrc</code> 文件：优先级为：项目级的 <code>.npmrc</code> 文件 &gt; 用户级的 <code>.npmrc</code> 文件&gt; 全局级的 <code>.npmrc</code> 文件 &gt; npm 内置的 <code>.npmrc</code> 文件</li><li><p>检查项目中有无 <code>lock</code> 文件。</p></li><li><p>无 <code>lock</code> 文件：</p><ul><li>从 <code>npm</code> 远程仓库获取包信息</li><li>根据 <code>package.json</code> 构建依赖树，构建过程：<ul><li>构建依赖树时，不管其是直接依赖还是子依赖的依赖，优先将其放置在 <code>node_modules</code> 根目录。</li><li>当遇到相同模块时，判断已放置在依赖树的模块版本是否符合新模块的版本范围，如果符合则跳过，不符合则在当前模块的 <code>node_modules</code> 下放置该模块。</li><li>注意这一步只是确定逻辑上的依赖树，并非真正的安装，后面会根据这个依赖结构去下载或拿到缓存中的依赖包</li></ul></li><li>在缓存中依次查找依赖树中的每个包<ul><li>不存在缓存：<ul><li>从 <code>npm</code> 远程仓库下载包</li><li>校验包的完整性</li><li>校验不通过：<ul><li>重新下载</li></ul></li><li>校验通过：<ul><li>将下载的包复制到 <code>npm</code> 缓存目录</li><li>将下载的包按照依赖结构解压到 <code>node_modules</code> </li></ul></li></ul></li><li>存在缓存：将缓存按照依赖结构解压到 <code>node_modules</code> </li></ul></li><li>将包解压到 <code>node_modules</code></li><li>生成 <code>lock</code> 文件</li></ul></li><li><p>有 <code>lock</code> 文件： </p><ul><li>检查 <code>package.json</code> 中的依赖版本是否和 <code>package-lock.json</code> 中的依赖有冲突。</li><li>如果没有冲突，直接跳过获取包信息、构建依赖树过程，开始在缓存中查找包信息，后续过程相同</li></ul></li></ul><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef327ccaba5tplv-t2oaga2asx-image.png" alt></p><p>上面的过程简要描述了 <code>npm install</code> 的大概过程，这个过程还包含了一些其他的操作，例如执行你定义的一些生命周期函数，你可以执行 <code>npm install package --timing=true --loglevel=verbose</code> 来查看某个包具体的安装流程和细节。</p><h3 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h3><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3dde095abtplv-t2oaga2asx-image.png" alt></p><p><code>yarn</code> 是在 <code>2016</code> 年发布的，那时 <code>npm</code> 还处于 <code>V3</code> 时期，那时候还没有 <code>package-lock.json</code> 文件，就像上面我们提到的：不稳定性、安装速度慢等缺点经常会受到广大开发者吐槽。此时，<code>yarn</code> 诞生：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3e0ee2fbdtplv-t2oaga2asx-image.png" alt></p><p>上面是官网提到的 <code>yarn</code> 的优点，在那个时候还是非常吸引人的。当然，后来 <code>npm</code> 也意识到了自己的问题，进行了很多次优化，在后面的优化（<code>lock</code>文件、缓存、默认-s…）中，我们多多少少能看到 <code>yarn</code> 的影子，可见 <code>yarn</code> 的设计还是非常优秀的。</p><p> <code>yarn</code> 也是采用的是 <code>npm v3</code> 的扁平结构来管理依赖，安装依赖后默认会生成一个 <code>yarn.lock</code> 文件，还是上面的依赖关系，我们看看 <code>yarn.lock</code> 的结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.</span><br><span class="line"># yarn lockfile v1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base64-js@1.0.1:</span><br><span class="line">  version &quot;1.0.1&quot;</span><br><span class="line">  resolved &quot;https://registry.yarnpkg.com/base64-js/-/base64-js-1.0.1.tgz#6926d1b194fbc737b8eed513756de2fcda7ea408&quot;</span><br><span class="line">  integrity sha1-aSbRsZT7xze47tUTdW3i/Np+pAg=</span><br><span class="line"></span><br><span class="line">base64-js@^1.0.2:</span><br><span class="line">  version &quot;1.3.1&quot;</span><br><span class="line">  resolved &quot;https://registry.yarnpkg.com/base64-js/-/base64-js-1.3.1.tgz#58ece8cb75dd07e71ed08c736abc5fac4dbf8df1&quot;</span><br><span class="line">  integrity sha512-mLQ4i2QO1ytvGWFWmcngKO//JXAQueZvwEKtjgQFM4jIK0kU+ytMfplL8j+n5mspOfjHwoAg+9yhb7BwAHm36g==</span><br><span class="line"></span><br><span class="line">buffer@^5.4.3:</span><br><span class="line">  version &quot;5.4.3&quot;</span><br><span class="line">  resolved &quot;https://registry.yarnpkg.com/buffer/-/buffer-5.4.3.tgz#3fbc9c69eb713d323e3fc1a895eee0710c072115&quot;</span><br><span class="line">  integrity sha512-zvj65TkFeIt3i6aj5bIvJDzjjQQGs4o/sNoezg1F1kYap9Nu2jcUdpwzRSJTHMMzG0H7bZkn4rNQpImhuxWX2A==</span><br><span class="line">  dependencies:</span><br><span class="line">    base64-js &quot;^1.0.2&quot;</span><br><span class="line">    ieee754 &quot;^1.1.4&quot;</span><br><span class="line"></span><br><span class="line">ieee754@^1.1.4:</span><br><span class="line">  version &quot;1.1.13&quot;</span><br><span class="line">  resolved &quot;https://registry.yarnpkg.com/ieee754/-/ieee754-1.1.13.tgz#ec168558e95aa181fd87d37f55c32bbcb6708b84&quot;</span><br><span class="line">  integrity sha512-4vf7I2LYV/HaWerSo3XmlMkp5eZ83i+/CDluXi/IGTs/O1sejBNhTtnxzmRZfvOUqj7lZjqHkeTvpgSFDlWZTg==</span><br><span class="line"></span><br><span class="line">ignore@^5.1.4:</span><br><span class="line">  version &quot;5.1.4&quot;</span><br><span class="line">  resolved &quot;https://registry.yarnpkg.com/ignore/-/ignore-5.1.4.tgz#84b7b3dbe64552b6ef0eca99f6743dbec6d97adf&quot;</span><br><span class="line">  integrity sha512-MzbUSahkTW1u7JpKKjY7LCARd1fU5W2rLdxlM4kdkayuCwZImjkpluF9CM1aLewYJguPDqewLam18Y6AU69A8A==</span><br></pre></td></tr></table></figure><p>可见其和 <code>package-lock.json</code> 文件还是比较类似的，还有一些区别就是：</p><ul><li><code>package-lock.json</code> 使用的是 <code>json</code> 格式，<code>yarn.lock</code> 使用的是一种自定义格式</li><li><code>yarn.lock</code> 中子依赖的版本号不是固定的，意味着单独又一个 <code>yarn.lock</code> 确定不了 <code>node_modules</code> 目录结构，还需要和 <code>package.json</code> 文件进行配合。而 <code>package-lock.json</code> 只需要一个文件即可确定。</li></ul><p><code>yarn</code> 的缓策略看起来和 <code>npm v5</code> 之前的很像，每个缓存的模块被存放在独立的文件夹，文件夹名称包含了模块名称、版本号等信息。使用命令 <code>yarn cache dir</code> 可以查看缓存数据的目录：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef3eb66af39tplv-t2oaga2asx-image.png" alt></p><blockquote><p><code>yarn</code> 默认使用 <code>prefer-online</code> 模式，即优先使用网络数据，如果网络数据请求失败，再去请求缓存数据。</p></blockquote><h2 id="npm常用命令"><a href="#npm常用命令" class="headerlink" title="npm常用命令"></a>npm常用命令</h2><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 创建项目</span><br><span class="line">npm init</span><br><span class="line">// 直接使用默认值创建项目</span><br><span class="line">npm init -y</span><br></pre></td></tr></table></figure><h3 id="镜像源相关"><a href="#镜像源相关" class="headerlink" title="镜像源相关"></a>镜像源相关</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.查看镜像源</span><br><span class="line">npm get registry</span><br><span class="line">2.切换官方源</span><br><span class="line">npm config <span class="built_in">set</span> registry http://www.npmjs.org</span><br><span class="line">3.切换淘宝源</span><br><span class="line">npm config <span class="built_in">set</span> registry http://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><h3 id="安装模块"><a href="#安装模块" class="headerlink" title="安装模块"></a>安装模块</h3><p>使用<code>npm install 模块名</code>来安装，你可以使用其简写<code>npm i</code>。</p><h4 id="一次性安装多个模块"><a href="#一次性安装多个模块" class="headerlink" title="一次性安装多个模块"></a>一次性安装多个模块</h4><p>无需为你要安装的每个模块都输入一遍 npm i 指令，像这样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm i gulp-pug</span><br><span class="line">npm i gulp-debug</span><br><span class="line">npm i gulp-sass</span><br></pre></td></tr></table></figure><p>你只需要输入一行命令即可一次性批量安装模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i gulp-pug gulp-debug gulp-sass</span><br></pre></td></tr></table></figure><p>如果安装的所有模块的前缀是相同的，则可以这样安装，无需输入完整模块名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i gulp&#123;-debug,-sass,-pug&#125;</span><br></pre></td></tr></table></figure><h4 id="使用一些安装标志的快捷方式"><a href="#使用一些安装标志的快捷方式" class="headerlink" title="使用一些安装标志的快捷方式"></a>使用一些安装标志的快捷方式</h4><p>如果你想安装一些包到<code>生产环境依赖</code>下面，你通常是这样安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm i gulp --save-prod</span><br><span class="line">// 或者</span><br><span class="line">npm i gulp -P</span><br></pre></td></tr></table></figure><p>同理，开发环境下的依赖安装，你可以用 <code>-D</code> 代替 <code>--save-dev</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm i gulp --save-dev</span><br><span class="line">// 或者</span><br><span class="line">npm i gulp -D</span><br></pre></td></tr></table></figure><p>当你<code>不带任何安装标志时</code>，npm <code>默认</code>将模块作为依赖项目添加到<code>package.json</code>文件中。如果你想避免这样，你可以使用 no-save, 这样安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i vue --no-save</span><br></pre></td></tr></table></figure><h4 id="npm-install-—save-和-npm-install-—save-dev-的区别"><a href="#npm-install-—save-和-npm-install-—save-dev-的区别" class="headerlink" title="npm install —save 和 npm install —save-dev 的区别"></a>npm install —save 和 npm install —save-dev 的区别</h4><p>以 npm 安装echarts为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install echarts</span><br></pre></td></tr></table></figure><ul><li>会把 echarts 包安装到 <code>node_modules</code> 目录中</li><li><code>不会修改 package.json</code></li><li>之后运行 <code>npm install</code> 命令时，<code>不会自动安装 echarts</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install echarts --save</span><br></pre></td></tr></table></figure><ul><li>会把 echarts 包安装到 node_modules 目录中</li><li>会在 package.json 的 dependencies 属性下添加 echarts</li><li>之后运行 npm install 命令时，会自动安装 echarts 到 node_modules 目录中</li><li>之后运行 npm install -production 或者注明 NODE_ENV 变量值为 production 时，会自动安装 echarts 到 node_modules 目录中</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install echarts --save-dev</span><br></pre></td></tr></table></figure><ul><li>会把 echarts 包安装到 <code>node_modules</code> 目录中</li><li><code>会在 package.json 的 devDependencies 属性下添加 echarts</code></li><li>之后运行 npm install 命令时，会自动安装 echarts 到 node_modules 目录中</li><li>之后运行 <code>npm install -production</code> 或者<code>注明 NODE_ENV 变量值为 production</code> 时，<code>不会自动安装</code>echarts 到 node_modules 目录中</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><code>devDependencies</code>节点下的模块是我们在<code>开发时</code>需要用的，比如项目中使用的 gulp ，压缩 css、js 的模块。这些模块在我们的项目部署后是不需要的，所以我们可以使用 <code>-save-dev</code> 的形式安装。</p><p>像 echarts 这些模块是项目运行必备的，应该安装在 dependencies 节点下，所以我们应该使用 <code>--save</code> 的形式安装。</p><h3 id="获取安装包信息"><a href="#获取安装包信息" class="headerlink" title="获取安装包信息"></a>获取安装包信息</h3><p>使用 npm view xxx 或 npm v xxx 可以查看包信息，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm v vue</span><br></pre></td></tr></table></figure><h3 id="安装指定版本安装包"><a href="#安装指定版本安装包" class="headerlink" title="安装指定版本安装包"></a>安装指定版本安装包</h3><p>如果你想安装一个不是最新版本的安装包，你可以指定某个版本来安装，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i vue@2.5.15</span><br></pre></td></tr></table></figure><p>鉴于记住标签比记住版本数字容易多了，你可以使用用 npm v 命令来查到的版本信息列表里面的 dist-tag 来安装, 比如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i vue@beta</span><br></pre></td></tr></table></figure><h3 id="升级依赖包"><a href="#升级依赖包" class="headerlink" title="升级依赖包"></a>升级依赖包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 更新全局包：</span><br><span class="line">npm update &lt;name&gt; -g</span><br><span class="line"></span><br><span class="line">// 更新生产环境依赖包：</span><br><span class="line">npm update &lt;name&gt; --save</span><br><span class="line"></span><br><span class="line">// 更新开发环境依赖包：</span><br><span class="line">npm update &lt;name&gt; --save-dev</span><br></pre></td></tr></table></figure><h3 id="卸载包"><a href="#卸载包" class="headerlink" title="卸载包"></a>卸载包</h3><p>如果你不想转到 package.json 文件并手动删除依赖包，则可以用以下方法删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall vue</span><br></pre></td></tr></table></figure><p>这个命令会删除 node_modules 文件夹及 package.json 中对应的包。当然，你也可以用 rm,un 或者 r 来达到相同的效果:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm rm vue</span><br></pre></td></tr></table></figure><p>如果由于某些原因，你只想从 node_modules 文件夹中删除安装包，但是想在 package.json 中保留其依赖项，那么你可以使用 no-save 标志，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm rm vue --no-save</span><br></pre></td></tr></table></figure><h3 id="依赖枚举"><a href="#依赖枚举" class="headerlink" title="依赖枚举"></a>依赖枚举</h3><p>如果你想看一下你的项目依赖了哪些安装包，你可以这样看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm ls</span><br></pre></td></tr></table></figure><p>这个命令会将你项目的依赖列举出来，并且各个安装包的依赖也会显示出来。如果你只想看本项目的依赖，你可以这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm ls --depth=0</span><br></pre></td></tr></table></figure><p>这样打印出来的结果就是本项目的依赖，像这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">├── jquery@3.3.1</span><br><span class="line">├── vue@2.5.17</span><br><span class="line">└── yarn@1.12.3</span><br></pre></td></tr></table></figure><p>当然，你也可以加上 g 来看看你全局安装的依赖包，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm ls -g -depth 0</span><br></pre></td></tr></table></figure><h3 id="过期依赖枚举"><a href="#过期依赖枚举" class="headerlink" title="过期依赖枚举"></a>过期依赖枚举</h3><p>大多数时候，你需要保持本地依赖的更新，你可以在项目目录下先查看一下安装包有没有版本更新，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm outdate</span><br></pre></td></tr></table></figure><p>这个命令将会列出所有你可能有更新的过时的安装包列表，如图：  </p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/watermark,type_ZmFuZ3poZW5naGVpdGk.png" alt></p><h3 id="执行测试"><a href="#执行测试" class="headerlink" title="执行测试"></a>执行测试</h3><p>你可以使用<code>npm run tests</code>来执行测试用例，但是你可以更方便地用 npm test 或者 npm t 来执行。</p><h3 id="显示可用脚本"><a href="#显示可用脚本" class="headerlink" title="显示可用脚本"></a>显示可用脚本</h3><p>我们可以通过打开 package.json 文件来查看有哪些可执行的脚本，但是我们还可以这样查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run</span><br></pre></td></tr></table></figure><p>如果在 package.json 中有如下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"scripts"</span>: &#123;</span><br><span class="line">  <span class="string">"test"</span>: <span class="string">"jest"</span>,</span><br><span class="line">  <span class="string">"build"</span>: <span class="string">"gulp build"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么执行这个命令之后，会显示以下信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Lifecycle scripts included <span class="keyword">in</span> npm:</span><br><span class="line">  <span class="built_in">test</span></span><br><span class="line">    jest</span><br><span class="line">available via `npm run-script`:</span><br><span class="line">  build</span><br><span class="line">    gulp-build</span><br></pre></td></tr></table></figure><h3 id="列出所有-NPM-环境的可用变量"><a href="#列出所有-NPM-环境的可用变量" class="headerlink" title="列出所有 NPM 环境的可用变量"></a>列出所有 NPM 环境的可用变量</h3><p>你可以使用这个命令来列出所有 NPM 环境的可用变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run env | grep npm_</span><br></pre></td></tr></table></figure><p>执行后，将会打印出这样的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm_config_fetch_retry_maxtimeout=60000</span><br><span class="line">npm_config_tag_version_prefix=v</span><br><span class="line">npm_config_strict_ssl=<span class="literal">true</span></span><br><span class="line">npm_config_sso_type=oauth</span><br></pre></td></tr></table></figure><p>这样变量的用处就是，可以在脚本中使用它们，还可以创建自己的变量。</p><h3 id="创建自己的-NPM-可用变量"><a href="#创建自己的-NPM-可用变量" class="headerlink" title="创建自己的 NPM 可用变量"></a>创建自己的 NPM 可用变量</h3><p>你可以在 package.json 中添加新的 key 来创建自己的 npm 变量，可以是任何 key ，我更喜欢将所有的 npm 变量都放在一个 config 中，这样看起来比较清晰：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"config"</span>: &#123;</span><br><span class="line">  <span class="string">"build_folder"</span>:<span class="string">"./dist"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你添加了之后，重新执行<code>npm run env | grep npm_</code>，就能看到以下信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm_package_config_build_folder=./dist</span><br><span class="line">npm_config_fetch_retry_maxtimeout=60000</span><br><span class="line">npm_config_tag_version_prefix=v</span><br><span class="line">npm_config_strict_ssl=<span class="literal">true</span></span><br><span class="line">npm_config_sso_type=oauth</span><br><span class="line">.</span><br></pre></td></tr></table></figure><p>默认情况下，npm 会重命名你的变量，给其加上<code>前缀npm_package</code>，并将其结构保留在 package.json 文件中，即变为<code>config_build_folder</code>。</p><h3 id="在-npm-脚本中使用-npm-变量"><a href="#在-npm-脚本中使用-npm-变量" class="headerlink" title="在 npm 脚本中使用 npm 变量"></a>在 npm 脚本中使用 npm 变量</h3><p>你可以看到可用变量的完整列表，如果你想使用这些变量中的任何值，就可以在 package.json 中使用了，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"scripts"</span>: &#123;</span><br><span class="line">  <span class="string">"build"</span>: <span class="string">"gulp build --dist <span class="variable">$npm_package_config_build_folder</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当你执行 npm run build 的时候，实际执行的是这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gulp build --dist ./dist</span><br></pre></td></tr></table></figure><h2 id="刨根问底之-node-gyp"><a href="#刨根问底之-node-gyp" class="headerlink" title="刨根问底之 node-gyp"></a>刨根问底之 node-gyp</h2><p>在我们写 node addon 时，需要使用 node-gyp 命令行工具，大部分同学会用<code>configue</code>生成配置文件，然后使用<code>build</code>进行构建。但是 node-gyp 到底是什么？底层有什么呢？下面我们来刨根问底。</p><p>本文的线索是自底向上的讲解 node-gyp 的各层次依赖，主要有以下几个部分：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. make</span><br><span class="line">2. make install</span><br><span class="line">3. cmake</span><br><span class="line">4. gyp</span><br><span class="line">5. node-gyp</span><br></pre></td></tr></table></figure><p>层次结构如下图所示：</p><p><img src="/Linux/博客搭建/剖析 NPM 的包管理机制/687474703a2f2f7366312d687363646e2d746f.png" alt="img"></p><h3 id="make"><a href="#make" class="headerlink" title="make"></a>make</h3><p>从源文件到可执行文件叫做编译（包括预编译、编译、链接），而 make 作为构建工具掌握着编译的过程，也就是如何去编译、文件编译的顺序等。</p><p>make 是最常用的构建工具，针对用户制定的构建规则（makefile）去执行响应的任务。make 会根据构建规则去查找依赖，决定编译顺序等。大致了解可参考 <a href="http://www.ruanyifeng.com/blog/2015/02/make.html" target="_blank" rel="noopener">Make 命令教程</a></p><p>Makefile（makefile）中定义了 make 的构建规则，当然也可以自己指定规则文件。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ make -f rules.txt</span><br><span class="line"># 或者</span><br><span class="line">$ make --file=rules.txt</span><br></pre></td></tr></table></figure><p>Makefile 由一条条的规则组成，每条规则由 target(目标)、source（前置条件 / 依赖）、command(指令) 三者组成。</p><p>形式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;target&gt; : &lt;prerequisites&gt; </span><br><span class="line">[tab]  &lt;commands&gt;</span><br></pre></td></tr></table></figure><p>当<code>make target</code>时，主要做了以下几件事：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.检查目标是否存在</span><br><span class="line">2.如果不存在目标</span><br><span class="line">· 检查目标的依赖是否存在</span><br><span class="line">· 不存在则调用`make source`；存在并且没有变化（修改时间戳小于target），不操作</span><br><span class="line">· 执行target中的command指令</span><br><span class="line">2.如果存在目标</span><br><span class="line">· 检查依赖是否发生变化</span><br><span class="line">· 没有变化则不需要执行，有变化则执行`make source`后执行command</span><br></pre></td></tr></table></figure><p>以编译一个 C++ 文件的规则为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hellomake: hellomake.c hellofunc.c</span><br><span class="line">     gcc -o hellomake hellomake.c hellofunc.c -I.</span><br></pre></td></tr></table></figure><p>当我们执行 make hellomake，会使用 gcc 编译器编译产出 hellomake。如果 make 不带有参数，则执行 makefile 中的第一条指令。</p><p>make 也允许我们定义一些纯指令（伪指令）去执行一些操作，相当于把上面的 target 写成指令名称，只不过在 command 中不生成文件，所以每次执行该规则时都会执行 command。为了和真实的目标文件做区分，make 中使用了<code>.PHONY</code>关键字，关键字. PHONY 可以解决这问题，告诉 make 该目标是 “假的”（磁盘上其实没有这个目标文件）。例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.PHONY: clean</span><br><span class="line">clean:</span><br><span class="line">        rm *.o temp</span><br></pre></td></tr></table></figure><p>由于 makefile 目标只能写一个，所以我们可以使用 all 来将多个目标组合起来。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all: executable1 executable2</span><br></pre></td></tr></table></figure><p>一般情况下可以把 all 放在 makefile 的第一行，这样不带参数执行 make 就会找到 all。</p><h3 id="make-install"><a href="#make-install" class="headerlink" title="make install"></a>make install</h3><p>make install 用来安装文件，它从 Makefile 中读取指令，安装到系统目录中。</p><h3 id="cmake"><a href="#cmake" class="headerlink" title="cmake"></a>cmake</h3><p>上面提到了 make，似乎已经够了，如果我是一个开发者，我定义了 makefile，让使用者执行 make 编译就好了。但是不同平台的编译器、动态链接库的路径都有可能不同，如果想让你的软件能够跨平台编译、运行，必须要保证能够在不同平台编译。如果使用上面的 Make 工具，就得为每一种标准写一次 Makefile，这是很繁琐并且容易出错的地方。</p><p>cmake 的出现就是为了解决上述问题，它首先允许开发者编写一种平台无关的 CMakeList.txt 文件来定制整个编译流程，cmake 会根据操作系统选择不同编译器，当然也可以在 CMakeList.txt 中去指定，执行 cmake 时会目标用户的平台和自定义的配置生成所需的 Makefile 或工程文件，如 Unix 的 Makefile、Windows 的 Visual Studio。</p><p>CMake 是一个跨平台的安装 (编译) 工具, 可以用简单的语句来描述所有平台的安装(编译过程)。他能够输出各种各样的 makefile 或者 project 文件，能测试编译器所支持的 C++ 特性，类似 UNIX 下的 automake。</p><p>在 linux 平台下使用 CMake 生成 Makefile 并编译的流程如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.编写 CMake 配置文件 CMakeLists.txt 。</span><br><span class="line">2.执行命令 cmake PATH 或者 ccmake PATH 生成 Makefile。其中，PATH是CMakeLists.txt 所在的目录。</span><br><span class="line">3.使用 make 命令进行编译。</span><br></pre></td></tr></table></figure><p>CMakeList.txt 中由面向过程的一条条指令组成，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># CMake 最低版本号要求</span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"># 项目信息</span><br><span class="line">project (Demo3)</span><br><span class="line"># 查找当前目录下的所有源文件</span><br><span class="line"># 并将名称保存到 DIR_SRCS 变量</span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"># 添加 math 子目录</span><br><span class="line">add_subdirectory(math)</span><br><span class="line"># 指定生成目标 </span><br><span class="line">add_executable(Demo main.cc)</span><br><span class="line"># 添加链接库</span><br><span class="line">target_link_libraries(Demo MathFunctions)</span><br></pre></td></tr></table></figure><p>具体可参考 <a href="https://cmake.org/cmake/help/cmake2.4docs.html" target="_blank" rel="noopener">cmake 文档</a></p><h3 id="GYP"><a href="#GYP" class="headerlink" title="GYP"></a>GYP</h3><p>Gyp 是一个类似 CMake 的项目生成工具, 用于管理你的源代码, 在 google code 主页上唯一的一句 slogan 是”GYP can Generate Your Projects.”。GYP 是由 Chromium 团队开发的跨平台自动化项目构建工具，Chromium 便是通过 GYP 进行项目构建管理。</p><p>首先看 GYP 与 cmake 类似，那为什要有 GYP 呢？GYP 和 cmake 有哪些相同点、不同点呢？</p><h4 id="GYP-vs-cmake"><a href="#GYP-vs-cmake" class="headerlink" title="GYP vs cmake"></a>GYP vs cmake</h4><h5 id="相同点："><a href="#相同点：" class="headerlink" title="相同点："></a>相同点：</h5><p>支持跨平台项目工程文件输出，Windows 平台默认是 Visual Studio，Linux 平台默认是 Makefile，Mac 平台默认是 Xcode，这个功能 CMake 也同样支持，只是缺少了 Xcode。</p><h5 id="不同点："><a href="#不同点：" class="headerlink" title="不同点："></a>不同点：</h5><p>配置文件形式不同，GYP 的配置文件更像一个 “配置文件”，而 Cmake 的上述所言更像一个面向过程的一个脚本，也就是说在项目设置的层次上进行抽象；同时 GYP 支持交叉编译。</p><p>具体比较可参考 <a href="http://www.jtianling.com/gyp-developer&#39;s-description-of-gyp.html" target="_blank" rel="noopener">GYP vs. CMake</a></p><h4 id="GYP-配置"><a href="#GYP-配置" class="headerlink" title="GYP 配置"></a>GYP 配置</h4><p>GYP 的配置文件以<code>.gyp</code>结尾，一个典型的<code>.gyp</code>文件如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &apos;variables&apos;: &#123;</span><br><span class="line">      .</span><br><span class="line">      .</span><br><span class="line">      .</span><br><span class="line">    &#125;,</span><br><span class="line">    &apos;includes&apos;: [</span><br><span class="line">      &apos;../build/common.gypi&apos;,</span><br><span class="line">    ],</span><br><span class="line">    &apos;target_defaults&apos;: &#123;</span><br><span class="line">      .</span><br><span class="line">      .</span><br><span class="line">      .</span><br><span class="line">    &#125;,</span><br><span class="line">    &apos;targets&apos;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &apos;target_name&apos;: &apos;target_1&apos;,</span><br><span class="line">          .</span><br><span class="line">          .</span><br><span class="line">          .</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &apos;target_name&apos;: &apos;target_2&apos;,</span><br><span class="line">          .</span><br><span class="line">          .</span><br><span class="line">          .</span><br><span class="line">      &#125;,</span><br><span class="line">    ],</span><br><span class="line">    &apos;conditions&apos;: [</span><br><span class="line">      [&apos;OS==&quot;linux&quot;&apos;, &#123;</span><br><span class="line">        &apos;targets&apos;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &apos;target_name&apos;: &apos;linux_target_3&apos;,</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">          &#125;,</span><br><span class="line">        ],</span><br><span class="line">      &#125;],</span><br><span class="line">      [&apos;OS==&quot;win&quot;&apos;, &#123;</span><br><span class="line">        &apos;targets&apos;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &apos;target_name&apos;: &apos;windows_target_4&apos;,</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">          &#125;,</span><br><span class="line">        ],</span><br><span class="line">      &#125;, &#123; # OS != &quot;win&quot;</span><br><span class="line">        &apos;targets&apos;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &apos;target_name&apos;: &apos;non_windows_target_5&apos;,</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">              .</span><br><span class="line">          &#125;,</span><br><span class="line">      &#125;],</span><br><span class="line">    ],</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><code>variables</code> : 定义可以在文件其他地方访问的变量；</p><p><code>includes</code> : 将要被引入到该文件中的文件列表，通常是以<code>.gypi结尾的文件</code>；</p><p><code>target_defaults</code> : 将作用域所有目标的默认配置；</p><p><code>targets</code>: 构建的目标列表，每个 target 中包含构建此目标的所有配置；</p><p><code>conditions</code>: 条件列表，会根据不同条件选择不同的配置项。在最顶级的配置中，通常是平台特定的目标配置。</p><p>具体可参考 <a href="https://gyp.gsrc.io/docs/InputFormatReference.md" target="_blank" rel="noopener">GYP 文档</a></p><h3 id="node-gyp"><a href="#node-gyp" class="headerlink" title="node-gyp"></a>node-gyp</h3><p>node-gyp 是一个跨平台的命令行工具，目的是编译 node addon 模块。</p><p>常用的命令有<code>configure</code>和<code>build</code>，<code>configure</code> 原理就是利用 gyp 生成不同的编译配置文件，<code>build</code>则根据不同平台、不同构建配置进行编译。</p><h4 id="configure"><a href="#configure" class="headerlink" title="configure"></a>configure</h4><p>我们分步骤看下 configure 的代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">findPython(python, function (err, found) &#123;</span><br><span class="line">    if (err) &#123;</span><br><span class="line">      callback(err)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      python = found</span><br><span class="line">      getNodeDir()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>由于 GYP 是 python 写的，所以这里首先找当前系统下的 python，内部利用的是<code>which</code>这个第三方库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">function getNodeDir () &#123;</span><br><span class="line"></span><br><span class="line">    // &apos;python&apos; should be set by now</span><br><span class="line">    process.env.PYTHON = python</span><br><span class="line"></span><br><span class="line">    if (gyp.opts.nodedir) &#123;</span><br><span class="line">      // --nodedir was specified. use that for the dev files</span><br><span class="line">      nodeDir = gyp.opts.nodedir.replace(/^~/, osenv.home())</span><br><span class="line"></span><br><span class="line">      log.verbose(&apos;get node dir&apos;, &apos;compiling against specified --nodedir dev files: %s&apos;, nodeDir)</span><br><span class="line">      createBuildDir()</span><br><span class="line"></span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      gyp.commands.install([ release.version ], function (err, version) &#123;</span><br><span class="line">        if (err) return callback(err)</span><br><span class="line">        log.verbose(&apos;get node dir&apos;, &apos;target node version installed:&apos;, release.versionDir)</span><br><span class="line">        nodeDir = path.resolve(gyp.devDir, release.versionDir)</span><br><span class="line">        createBuildDir()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>找到 node 所在目录，如果没有，则下载 node 压缩包并解压。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">function createBuildDir () &#123;</span><br><span class="line">    log.verbose(&apos;build dir&apos;, &apos;attempting to create &quot;build&quot; dir: %s&apos;, buildDir)</span><br><span class="line">    mkdirp(buildDir, function (err, isNew) &#123;</span><br><span class="line">      if (err) return callback(err)</span><br><span class="line">      log.verbose(&apos;build dir&apos;, &apos;&quot;build&quot; dir needed to be created?&apos;, isNew)</span><br><span class="line">      if (win &amp;&amp; (!gyp.opts.msvs_version || gyp.opts.msvs_version === &apos;2017&apos;)) &#123;</span><br><span class="line">        findVS2017(function (err, vsSetup) &#123;</span><br><span class="line">          if (err) &#123;</span><br><span class="line">            log.verbose(&apos;Not using VS2017:&apos;, err.message)</span><br><span class="line">            createConfigFile()</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            createConfigFile(null, vsSetup)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        createConfigFile()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>创建 build 目录，这里区分了是否有 vs，查找 vs 的方法是打开 powershell(windows)，试图打开 vs。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">function createConfigFile (err, vsSetup) &#123;</span><br><span class="line">    if (err) return callback(err)</span><br><span class="line"></span><br><span class="line">    var configFilename = &apos;config.gypi&apos;</span><br><span class="line">    var configPath = path.resolve(buildDir, configFilename)</span><br><span class="line"></span><br><span class="line">    if (vsSetup) &#123;</span><br><span class="line">      // GYP doesn&apos;t (yet) have support for VS2017, so we force it to VS2015</span><br><span class="line">      // to avoid pulling a floating patch that has not landed upstream.</span><br><span class="line">      // Ref: https://chromium-review.googlesource.com/#/c/433540/</span><br><span class="line">      gyp.opts.msvs_version = &apos;2015&apos;</span><br><span class="line">      process.env[&apos;GYP_MSVS_VERSION&apos;] = 2015</span><br><span class="line">      process.env[&apos;GYP_MSVS_OVERRIDE_PATH&apos;] = vsSetup.path</span><br><span class="line">      defaults[&apos;msbuild_toolset&apos;] = &apos;v141&apos;</span><br><span class="line">      defaults[&apos;msvs_windows_target_platform_version&apos;] = vsSetup.sdk</span><br><span class="line">      variables[&apos;msbuild_path&apos;] = path.join(vsSetup.path, &apos;MSBuild&apos;, &apos;15.0&apos;,</span><br><span class="line">                                            &apos;Bin&apos;, &apos;MSBuild.exe&apos;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // loop through the rest of the opts and add the unknown ones as variables.</span><br><span class="line">    // this allows for module-specific configure flags like:</span><br><span class="line">    //</span><br><span class="line">    //   $ node-gyp configure --shared-libxml2</span><br><span class="line">    Object.keys(gyp.opts).forEach(function (opt) &#123;</span><br><span class="line">      if (opt === &apos;argv&apos;) return</span><br><span class="line">      if (opt in gyp.configDefs) return</span><br><span class="line">      variables[opt.replace(/-/g, &apos;_&apos;)] = gyp.opts[opt]</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    configs.push(configPath)</span><br><span class="line">    fs.writeFile(configPath, [prefix, json, &apos;&apos;].join(&apos;\n&apos;), findConfigs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里创建<code>config.gypi文件</code>，主要包含<code>target_defaults</code>和<code>variables</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">// config = [&apos;config.gypi&apos;]</span><br><span class="line">  function runGyp (err) &#123;</span><br><span class="line">    if (err) return callback(err)</span><br><span class="line"></span><br><span class="line">    if (!~argv.indexOf(&apos;-f&apos;) &amp;&amp; !~argv.indexOf(&apos;--format&apos;)) &#123;</span><br><span class="line">      if (win) &#123;</span><br><span class="line">        log.verbose(&apos;gyp&apos;, &apos;gyp format was not specified; forcing &quot;msvs&quot;&apos;)</span><br><span class="line">        // force the &apos;make&apos; target for non-Windows</span><br><span class="line">        argv.push(&apos;-f&apos;, &apos;msvs&apos;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        log.verbose(&apos;gyp&apos;, &apos;gyp format was not specified; forcing &quot;make&quot;&apos;)</span><br><span class="line">        // force the &apos;make&apos; target for non-Windows</span><br><span class="line">        argv.push(&apos;-f&apos;, &apos;make&apos;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (win &amp;&amp; !hasMsvsVersion()) &#123;</span><br><span class="line">      if (&apos;msvs_version&apos; in gyp.opts) &#123;</span><br><span class="line">        argv.push(&apos;-G&apos;, &apos;msvs_version=&apos; + gyp.opts.msvs_version)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        argv.push(&apos;-G&apos;, &apos;msvs_version=auto&apos;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // include all the &quot;.gypi&quot; files that were found</span><br><span class="line">    configs.forEach(function (config) &#123;</span><br><span class="line">      argv.push(&apos;-I&apos;, config)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    // For AIX and z/OS we need to set up the path to the exports file</span><br><span class="line">    // which contains the symbols needed for linking. </span><br><span class="line">    var node_exp_file = undefined</span><br><span class="line">    if (process.platform === &apos;aix&apos; || process.platform === &apos;os390&apos;) &#123;</span><br><span class="line">      var ext = process.platform === &apos;aix&apos; ? &apos;exp&apos; : &apos;x&apos;</span><br><span class="line">      var node_root_dir = findNodeDirectory()</span><br><span class="line">      var candidates = undefined </span><br><span class="line">      if (process.platform === &apos;aix&apos;) &#123;</span><br><span class="line">        candidates = [&apos;include/node/node&apos;,</span><br><span class="line">                      &apos;out/Release/node&apos;,</span><br><span class="line">                      &apos;out/Debug/node&apos;,</span><br><span class="line">                      &apos;node&apos;</span><br><span class="line">                     ].map(function(file) &#123;</span><br><span class="line">                       return file + &apos;.&apos; + ext</span><br><span class="line">                     &#125;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        candidates = [&apos;out/Release/obj.target/libnode&apos;,</span><br><span class="line">                      &apos;out/Debug/obj.target/libnode&apos;,</span><br><span class="line">                      &apos;lib/libnode&apos;</span><br><span class="line">                     ].map(function(file) &#123;</span><br><span class="line">                       return file + &apos;.&apos; + ext</span><br><span class="line">                     &#125;)</span><br><span class="line">      &#125;</span><br><span class="line">      var logprefix = &apos;find exports file&apos;</span><br><span class="line">      node_exp_file = findAccessibleSync(logprefix, node_root_dir, candidates)</span><br><span class="line">      if (node_exp_file !== undefined) &#123;</span><br><span class="line">        log.verbose(logprefix, &apos;Found exports file: %s&apos;, node_exp_file)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        var msg = msgFormat(&apos;Could not find node.%s file in %s&apos;, ext, node_root_dir)</span><br><span class="line">        log.error(logprefix, &apos;Could not find exports file&apos;)</span><br><span class="line">        return callback(new Error(msg))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // this logic ported from the old `gyp_addon` python file</span><br><span class="line">    var gyp_script = path.resolve(__dirname, &apos;..&apos;, &apos;gyp&apos;, &apos;gyp_main.py&apos;)</span><br><span class="line">    var addon_gypi = path.resolve(__dirname, &apos;..&apos;, &apos;addon.gypi&apos;)</span><br><span class="line">    var common_gypi = path.resolve(nodeDir, &apos;include/node/common.gypi&apos;)</span><br><span class="line">    fs.stat(common_gypi, function (err, stat) &#123;</span><br><span class="line">      if (err)</span><br><span class="line">        common_gypi = path.resolve(nodeDir, &apos;common.gypi&apos;)</span><br><span class="line"></span><br><span class="line">      var output_dir = &apos;build&apos;</span><br><span class="line">      if (win) &#123;</span><br><span class="line">        // Windows expects an absolute path</span><br><span class="line">        output_dir = buildDir</span><br><span class="line">      &#125;</span><br><span class="line">      var nodeGypDir = path.resolve(__dirname, &apos;..&apos;)</span><br><span class="line">      var nodeLibFile = path.join(nodeDir,</span><br><span class="line">        !gyp.opts.nodedir ? &apos;&lt;(target_arch)&apos; : &apos;$(Configuration)&apos;,</span><br><span class="line">        release.name + &apos;.lib&apos;)</span><br><span class="line"></span><br><span class="line">      argv.push(&apos;-I&apos;, addon_gypi)</span><br><span class="line">      argv.push(&apos;-I&apos;, common_gypi)</span><br><span class="line">      argv.push(&apos;-Dlibrary=shared_library&apos;)</span><br><span class="line">      argv.push(&apos;-Dvisibility=default&apos;)</span><br><span class="line">      argv.push(&apos;-Dnode_root_dir=&apos; + nodeDir)</span><br><span class="line">      if (process.platform === &apos;aix&apos; || process.platform === &apos;os390&apos;) &#123;</span><br><span class="line">        argv.push(&apos;-Dnode_exp_file=&apos; + node_exp_file)</span><br><span class="line">      &#125;</span><br><span class="line">      argv.push(&apos;-Dnode_gyp_dir=&apos; + nodeGypDir)</span><br><span class="line">      argv.push(&apos;-Dnode_lib_file=&apos; + nodeLibFile)</span><br><span class="line">      argv.push(&apos;-Dmodule_root_dir=&apos; + process.cwd())</span><br><span class="line">      argv.push(&apos;-Dnode_engine=&apos; +</span><br><span class="line">        (gyp.opts.node_engine || process.jsEngine || &apos;v8&apos;))</span><br><span class="line">      argv.push(&apos;--depth=.&apos;)</span><br><span class="line">      argv.push(&apos;--no-parallel&apos;)</span><br><span class="line"></span><br><span class="line">      // tell gyp to write the Makefile/Solution files into output_dir</span><br><span class="line">      argv.push(&apos;--generator-output&apos;, output_dir)</span><br><span class="line"></span><br><span class="line">      // tell make to write its output into the same dir</span><br><span class="line">      argv.push(&apos;-Goutput_dir=.&apos;)</span><br><span class="line"></span><br><span class="line">      // enforce use of the &quot;binding.gyp&quot; file</span><br><span class="line">      argv.unshift(&apos;binding.gyp&apos;)</span><br><span class="line"></span><br><span class="line">      // execute `gyp` from the current target nodedir</span><br><span class="line">      argv.unshift(gyp_script)</span><br><span class="line"></span><br><span class="line">      // make sure python uses files that came with this particular node package</span><br><span class="line">      var pypath = [path.join(__dirname, &apos;..&apos;, &apos;gyp&apos;, &apos;pylib&apos;)]</span><br><span class="line">      if (process.env.PYTHONPATH) &#123;</span><br><span class="line">        pypath.push(process.env.PYTHONPATH)</span><br><span class="line">      &#125;</span><br><span class="line">      process.env.PYTHONPATH = pypath.join(win ? &apos;;&apos; : &apos;:&apos;)</span><br><span class="line"></span><br><span class="line">      var cp = gyp.spawn(python, argv)</span><br><span class="line">      cp.on(&apos;exit&apos;, onCpExit)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里主要是区分了不同平台，给 GYP 命令加入各种参数，其中<code>-I</code>代表 include，最后执行 gyp 脚本生成构建配置文件，比如 unix 下生成 makefile。</p><h4 id="build"><a href="#build" class="headerlink" title="build"></a>build</h4><p><code>build</code>比较简单，言简意赅就是就是区分不同平台，收集不同参数，利用不同编译工具进行编译。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command = win ? &apos;msbuild&apos; : makeCommand</span><br></pre></td></tr></table></figure><p>区分编译工具。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">function loadConfigGypi () &#123;</span><br><span class="line">    fs.readFile(configPath, &apos;utf8&apos;, function (err, data) &#123;</span><br><span class="line">      if (err) &#123;</span><br><span class="line">        if (err.code == &apos;ENOENT&apos;) &#123;</span><br><span class="line">          callback(new Error(&apos;You must run `node-gyp configure` first!&apos;))</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          callback(err)</span><br><span class="line">        &#125;</span><br><span class="line">        return</span><br><span class="line">      &#125;</span><br><span class="line">      config = JSON.parse(data.replace(/\#.+\n/, &apos;&apos;))</span><br><span class="line"></span><br><span class="line">      // get the &apos;arch&apos;, &apos;buildType&apos;, and &apos;nodeDir&apos; vars from the config</span><br><span class="line">      buildType = config.target_defaults.default_configuration</span><br><span class="line">      arch = config.variables.target_arch</span><br><span class="line">      nodeDir = config.variables.nodedir</span><br><span class="line"></span><br><span class="line">      if (&apos;debug&apos; in gyp.opts) &#123;</span><br><span class="line">        buildType = gyp.opts.debug ? &apos;Debug&apos; : &apos;Release&apos;</span><br><span class="line">      &#125;</span><br><span class="line">      if (!buildType) &#123;</span><br><span class="line">        buildType = &apos;Release&apos;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      log.verbose(&apos;build type&apos;, buildType)</span><br><span class="line">      log.verbose(&apos;architecture&apos;, arch)</span><br><span class="line">      log.verbose(&apos;node dev dir&apos;, nodeDir)</span><br><span class="line"></span><br><span class="line">      if (win) &#123;</span><br><span class="line">        findSolutionFile()</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        doWhich()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>加载<code>config.gypi</code>, 为构建收集一波参数。如果在 windows 下，收集<code>build/*.sln</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">function doBuild () &#123;</span><br><span class="line"></span><br><span class="line">    // Enable Verbose build</span><br><span class="line">    var verbose = log.levels[log.level] &lt;= log.levels.verbose</span><br><span class="line">    if (!win &amp;&amp; verbose) &#123;</span><br><span class="line">      argv.push(&apos;V=1&apos;)</span><br><span class="line">    &#125;</span><br><span class="line">    if (win &amp;&amp; !verbose) &#123;</span><br><span class="line">      argv.push(&apos;/clp:Verbosity=minimal&apos;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (win) &#123;</span><br><span class="line">      // Turn off the Microsoft logo on Windows</span><br><span class="line">      argv.push(&apos;/nologo&apos;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Specify the build type, Release by default</span><br><span class="line">    if (win) &#123;</span><br><span class="line">      var archLower = arch.toLowerCase()</span><br><span class="line">      var p = archLower === &apos;x64&apos; ? &apos;x64&apos; :</span><br><span class="line">              (archLower === &apos;arm&apos; ? &apos;ARM&apos; : &apos;Win32&apos;)</span><br><span class="line">      argv.push(&apos;/p:Configuration=&apos; + buildType + &apos;;Platform=&apos; + p)</span><br><span class="line">      if (jobs) &#123;</span><br><span class="line">        var j = parseInt(jobs, 10)</span><br><span class="line">        if (!isNaN(j) &amp;&amp; j &gt; 0) &#123;</span><br><span class="line">          argv.push(&apos;/m:&apos; + j)</span><br><span class="line">        &#125; else if (jobs.toUpperCase() === &apos;MAX&apos;) &#123;</span><br><span class="line">          argv.push(&apos;/m:&apos; + require(&apos;os&apos;).cpus().length)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      argv.push(&apos;BUILDTYPE=&apos; + buildType)</span><br><span class="line">      // Invoke the Makefile in the &apos;build&apos; dir.</span><br><span class="line">      argv.push(&apos;-C&apos;)</span><br><span class="line">      argv.push(&apos;build&apos;)</span><br><span class="line">      if (jobs) &#123;</span><br><span class="line">        var j = parseInt(jobs, 10)</span><br><span class="line">        if (!isNaN(j) &amp;&amp; j &gt; 0) &#123;</span><br><span class="line">          argv.push(&apos;--jobs&apos;)</span><br><span class="line">          argv.push(j)</span><br><span class="line">        &#125; else if (jobs.toUpperCase() === &apos;MAX&apos;) &#123;</span><br><span class="line">          argv.push(&apos;--jobs&apos;)</span><br><span class="line">          argv.push(require(&apos;os&apos;).cpus().length)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (win) &#123;</span><br><span class="line">      // did the user specify their own .sln file?</span><br><span class="line">      var hasSln = argv.some(function (arg) &#123;</span><br><span class="line">        return path.extname(arg) == &apos;.sln&apos;</span><br><span class="line">      &#125;)</span><br><span class="line">      if (!hasSln) &#123;</span><br><span class="line">        argv.unshift(gyp.opts.solution || guessedSolution)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var proc = gyp.spawn(command, argv)</span><br><span class="line">    proc.on(&apos;exit&apos;, onExit)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行编译命令。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>希望阅读完本篇文章能对你有如下帮助：</p><ul><li>了解 <code>pacakge.json</code> 中的各项详细配置从而对项目工程化配置有更进一步的见解</li><li>掌握 <code>npm</code> 的版本管理机制，能合理配置依赖版本</li><li>理解 <code>npm install</code> 安装原理，能合理运用 <code>npm</code>缓存、<code>package-lock.json</code></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://juejin.im/post/6844903552012255245" target="_blank" rel="noopener">https://juejin.im/post/6844903552012255245</a></li><li><a href="https://www.zhihu.com/question/305539244/answer/551386426" target="_blank" rel="noopener">https://www.zhihu.com/question/305539244/answer/551386426</a></li><li><a href="https://zhuanlan.zhihu.com/p/37285173" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37285173</a></li><li><a href="https://semver.org/lang/zh-CN/" target="_blank" rel="noopener">https://semver.org/lang/zh-CN/</a></li><li><a href="http://deadhorse.me/nodejs/2014/04/27/semver-in-nodejs.html" target="_blank" rel="noopener">http://deadhorse.me/nodejs/2014/04/27/semver-in-nodejs.html</a></li><li><a href="http://caibaojian.com/npm/files/package.json.html" target="_blank" rel="noopener">http://caibaojian.com/npm/files/package.json.html</a></li><li><a href="https://blog.csdn.net/weixin_42752574/article/details/102717415" target="_blank" rel="noopener">npm常用命令(npm install —save 和npm install —save-dev的区别)</a></li><li><a href="https://github.com/tsy77/blog/issues/5" target="_blank" rel="noopener">刨根问底之node-gyp</a></li><li><a href="https://cloud.tencent.com/developer/article/1678828" target="_blank" rel="noopener">发布你的第一个nodejs c++插件</a></li><li><a href="https://blog.conardli.top/2019/12/17/engineering/npm/#3-6-%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B" target="_blank" rel="noopener">剖析 NPM 的包管理机制</a></li><li><a href="https://juejin.cn/post/6844903971220357134#heading-0" target="_blank" rel="noopener">node-gyp 实现 nodejs 调用 C++</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直对node.js与npm感觉很陌生，最近需要用到了，所以这里学习一下。以下文章主要参考了ConardLi的&lt;a href=&quot;https://blog.conardli.top/2019/12/17/engineering/npm/#3-6-%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;剖析 NPM 的包管理机制&lt;/a&gt;。因为本人纯小白，所以会添加一些其它的知识。&lt;/p&gt;
&lt;p&gt;现如今，前端开发的同学已经离不开 &lt;code&gt;npm&lt;/code&gt; 这个包管理工具，其优秀的&lt;strong&gt;包版本管理机制&lt;/strong&gt;承载了整个繁荣发展的&lt;code&gt;NodeJS&lt;/code&gt;社区，理解其内部机制非常有利于加深我们对模块开发的理解、各项前端工程化的配置以加快我们排查问题（相信不少同学收到过各种依赖问题的困扰）的速度。&lt;/p&gt;
&lt;p&gt;本文从三个角度：&lt;code&gt;package.json&lt;/code&gt;、版本管理、依赖安装结合具体实例对 &lt;code&gt;npm&lt;/code&gt; 的包管理机制进行了详细分析。&lt;/p&gt;
&lt;h2 id=&quot;剖析-package-json&quot;&gt;&lt;a href=&quot;#剖析-package-json&quot; class=&quot;headerlink&quot; title=&quot;剖析 package.json&quot;&gt;&lt;/a&gt;剖析 package.json&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/Linux/博客搭建/剖析 NPM 的包管理机制/16f0eef28758e8c0tplv-t2oaga2asx-image.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/categories/Linux/"/>
    
      <category term="博客搭建" scheme="https://www.zdaiot.com/categories/Linux/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="npm" scheme="https://www.zdaiot.com/tags/npm/"/>
    
      <category term="yarn" scheme="https://www.zdaiot.com/tags/yarn/"/>
    
      <category term="node.js" scheme="https://www.zdaiot.com/tags/node-js/"/>
    
  </entry>
  
  <entry>
    <title>git原理简介</title>
    <link href="https://www.zdaiot.com/Tools/Git/git%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/"/>
    <id>https://www.zdaiot.com/Tools/Git/git原理简介/</id>
    <published>2023-03-28T09:52:27.000Z</published>
    <updated>2023-03-28T09:52:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>在学习git的时候，感觉<a href="https://jingsam.github.io/" target="_blank" rel="noopener">jingsam</a>写的很不错，边学习边理解一下。强烈建立看原文，作者很牛掰。</p><h2 id="Git对象"><a href="#Git对象" class="headerlink" title="Git对象"></a>Git对象</h2><p><strong>从根本上来讲，Git是一个内容寻址的文件系统，其次才是一个版本控制系统。</strong>记住这点，对于理解Git的内部原理及其重要。所谓“内容寻址的文件系统”，意思是<strong>根据文件内容的hash码来定位文件</strong>。这就意味着同样内容的文件，在这个文件系统中会指向同一个位置，不会重复存储。</p><p>Git对象包含三种：数据对象、树对象、提交对象。Git文件系统的设计思路与linux文件系统相似，即将<strong>文件的内容与文件的属性分开存储</strong>，文件内容以“装满字节的袋子”存储在文件系统中，文件名、所有者、权限等文件属性信息则另外开辟区域进行存储。在Git中，<strong>数据对象相当于文件内容，树对象相当于文件目录树，提交对象则是对文件系统的快照。</strong></p><p>下面的章节，会分别对每种对象进行说明。开始说明之前，先初始化一个Git文件系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir git-test</span><br><span class="line">$ <span class="built_in">cd</span> git-test</span><br><span class="line">$ git init</span><br></pre></td></tr></table></figure><p>接下来的操作都会在<code>git-test</code>这个目录中进行。</p><h3 id="数据对象"><a href="#数据对象" class="headerlink" title="数据对象"></a>数据对象</h3><p>数据对象是文件的内容，不包括文件名、权限等信息。Git会根据文件内容计算出一个hash值，以hash值作为文件索引存储在Git文件系统中。由于相同的文件内容的hash值是一样的，因此Git将同样内容的文件只会存储一次。<code>git hash-object</code>可以用来计算文件内容的hash值，并将生成的数据对象存储到Git文件系统中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'version 1'</span> | git <span class="built_in">hash</span>-object -w --stdin</span><br><span class="line">83baae61804e65cc73a7201a7252750c76066a30</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'version 2'</span> | git <span class="built_in">hash</span>-object -w --stdin</span><br><span class="line">1f7a7a472abf3dd9643fd615f6da379c4acb3e3a</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'new file'</span> | git <span class="built_in">hash</span>-object -w --stdin</span><br><span class="line">fa49b077972391ad58037050f2a75f74e3671e92</span><br></pre></td></tr></table></figure><p>上面示例中，<code>-w</code>表示将数据对象写入到Git文件系统中，如果不加这个选项，那么只计算文件的hash值而不写入；<code>--stdin</code>表示从标准输入中获取文件内容，当然也可以指定一个文件路径代替此选项。</p><p>上面讲数据对象写入到Git文件系统中，那如何读取数据对象呢？<code>git cat-file</code>可以用来实现所有Git对象的读取，包括数据对象、树对象、提交对象的查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 83baae61804e65cc73a7201a7252750c76066a30</span><br><span class="line">version 1</span><br><span class="line">$ git cat-file -t 83baae61804e65cc73a7201a7252750c76066a30</span><br><span class="line">blob</span><br></pre></td></tr></table></figure><p>上面示例中，<code>-p</code>表示查看Git对象的内容，<code>-t</code>表示查看Git对象的类型。</p><p>通过这一节，我们能够对Git文件系统中的数据对象进行读写。但是，我们需要记住每一个数据对象的hash值，才能访问到Git文件系统中的任意数据对象，这显然是不现实的。数据对象只是解决了文件内容存储的问题，而文件名的存储则需要通过下一节的树对象来解决。</p><h3 id="树对象"><a href="#树对象" class="headerlink" title="树对象"></a>树对象</h3><p>树对象是文件目录树，记录了文件获取目录的名称、类型、模式信息。使用<code>git update-index</code>可以为数据对象指定名称和模式，然后使用<code>git write-tree</code>将树对象写入到Git文件系统中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git update-index --add --cacheinfo 100644 83baae61804e65cc73a7201a7252750c76066a30 test.txt</span><br><span class="line">$ git write-tree</span><br><span class="line">d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br></pre></td></tr></table></figure><p><code>--add</code>表示新增文件名，如果第一次添加某一文件名，必须使用此选项；<code>--cacheinfo &lt;mode&gt; &lt;object&gt; &lt;path&gt;</code>是要添加的数据对象的模式、hash值和路径，<code>&lt;path&gt;</code>意味着为数据对象不仅可以指定单纯的文件名，也可以使用路径。另外要注意的是，使用<code>git update-index</code>添加完文件后，一定要使用<code>git write-tree</code>写入到Git文件系统中，否则只会存在于index区域。</p><p>树对象仍然可以使用<code>git cat-file</code>查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">100644 blob 83baae61804e65cc73a7201a7252750c76066a30  test.txt</span><br><span class="line">$ git cat-file -t d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">tree</span><br></pre></td></tr></table></figure><p>上面表示这个树对象只有<code>test.txt</code>这个文件，接下来我们将<code>version 2</code>的数据对象指定为<code>test.txt</code>，并添加一个新文件<code>new.txt</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git update-index --cacheinfo 100644 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a test.txt</span><br><span class="line">$ git update-index --add --cacheinfo 100644 fa49b077972391ad58037050f2a75f74e3671e92 new.txt</span><br><span class="line">$ git write-tree</span><br><span class="line">0155eb4229851634a0f03eb265b69f5a2d56f341</span><br></pre></td></tr></table></figure><p>查看树对象<code>0155eb</code>，可以发现这个树对象有两个文件了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 0155eb4229851634a0f03eb265b69f5a2d56f341</span><br><span class="line">100644 blob fa49b077972391ad58037050f2a75f74e3671e92  new.txt</span><br><span class="line">100644 blob 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a  test.txt</span><br></pre></td></tr></table></figure><p>我们甚至可以使用<code>git read-tree</code>，将已添加的树对象读取出来，作为当前树的子树：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">read</span>-tree --prefix=bak d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">$ git write-tree</span><br><span class="line">3c4e9cd789d88d8d89c1073707c3585e41b0e614</span><br></pre></td></tr></table></figure><p><code>--prefix</code>表示把子树对象放到哪个目录下。查看树对象，可以发现当前树对象有一个文件夹和两个文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 3c4e9cd789d88d8d89c1073707c3585e41b0e614</span><br><span class="line">040000 tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579  bak</span><br><span class="line">100644 blob fa49b077972391ad58037050f2a75f74e3671e92  new.txt</span><br><span class="line">100644 blob 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a  test.txt</span><br></pre></td></tr></table></figure><p>最终，整个树对象的结构如下图：</p><p><img src="/Tools/Git/git原理简介/2018-06-03-1.png" alt="img" style="zoom:50%;"></p><p>树对象解决了文件名的问题，而且，由于我们是分阶段提交树对象的，树对象可以看做是开发阶段源代码目录树的一次次快照，因此我们可以是用树对象作为源代码版本管理。但是，这里仍然有问题需要解决，即我们需要记住每个树对象的hash值，才能找到个阶段的源代码文件目录树。在源代码版本控制中，我们还需要知道谁提交了代码、什么时候提交的、提交的说明信息等，接下来的提交对象就是为了解决这个问题的。</p><h3 id="提交对象"><a href="#提交对象" class="headerlink" title="提交对象"></a>提交对象</h3><p>提交对象是用来保存提交的作者、时间、说明这些信息的，可以使用<code>git commit-tree</code>来将提交对象写入到Git文件系统中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'first commit'</span> | git commit-tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a</span><br></pre></td></tr></table></figure><p>上面<code>commit-tree</code>除了要指定提交的树对象，也要提供提交说明，至于提交的作者和时间，则是根据环境变量自动生成，并不需要指定。这里需要提醒一点的是，读者在测试时，得到的提交对象hash值一般和这里不一样，这是因为提交的作者和时间是因人而异的。</p><p>提交对象的查看，也是使用<code>git cat-file</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure><p>上面是属于首次提交，那么接下来的提交还需要指定使用<code>-p</code>指定父提交对象，这样代码版本才能成为一条时间线：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'second commit'</span> | git commit-tree 0155eb4229851634a0f03eb265b69f5a2d56f341 -p db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br></pre></td></tr></table></figure><p>使用<code>git cat-file</code>查看一下新的提交对象，可以看到相比于第一次提交，多了<code>parent</code>部分：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">tree 0155eb4229851634a0f03eb265b69f5a2d56f341</span><br><span class="line">parent db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022722 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022722 +0800</span><br><span class="line"></span><br><span class="line">second commit</span><br></pre></td></tr></table></figure><p>最后，我们再将树对象<code>3c4e9c</code>提交：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'third commit'</span> | git commit-tree 3c4e9cd789d88d8d89c1073707c3585e41b0e614 -p d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>使用<code>git log</code>可以查看整个提交历史：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">log</span> --<span class="built_in">stat</span> 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">commit 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">Author: jingsam &lt;jing-sam@qq.com&gt;</span><br><span class="line">Date:   Sun Jun 3 18:47:29 2018 +0800</span><br><span class="line"></span><br><span class="line">    third commit</span><br><span class="line"></span><br><span class="line"> bak/test.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"></span><br><span class="line">commit d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">Author: jingsam &lt;jing-sam@qq.com&gt;</span><br><span class="line">Date:   Sun Jun 3 18:45:22 2018 +0800</span><br><span class="line"></span><br><span class="line">    second commit</span><br><span class="line"></span><br><span class="line"> new.txt  | 1 +</span><br><span class="line"> test.txt | 2 +-</span><br><span class="line"> 2 files changed, 2 insertions(+), 1 deletion(-)</span><br><span class="line"></span><br><span class="line">commit db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">Author: jingsam &lt;jing-sam@qq.com&gt;</span><br><span class="line">Date:   Sun Jun 3 18:41:43 2018 +0800</span><br><span class="line"></span><br><span class="line">    first commit</span><br><span class="line"></span><br><span class="line"> test.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure><p>最终的提交对象的结构如下图：</p><p><img src="/Tools/Git/git原理简介/2018-06-03-2.png" alt="img" style="zoom:50%;"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>Git中的数据对象解决了数据存储的问题，树对象解决了文件名存储问题，提交对象解决了提交信息的存储问题。</strong>另外，对于git中的某个文件，一般来说，每次修改之后commit，都会新建一个数据对象进行存储（可以看后面关于<code>.git/objects/pack</code>文件夹的介绍）。</p><p>从Git设计中可以看出，Linus对一个源代码版本控制系统做了很好的抽象和解耦，每种对象解决的问题都很明确，相比于使用一种数据结构，无疑更灵活和更易维护。每种Git对象都有一个hash值，这个值是怎么计算出来的？Git的各种对象是如何存储的？我们继续看下一节。</p><h2 id="Git对象的hash方法"><a href="#Git对象的hash方法" class="headerlink" title="Git对象的hash方法"></a>Git对象的hash方法</h2><p>Git中的数据对象、树对象和提交对象的hash方法原理是一样的，可以描述为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">header = <span class="string">"&lt;type&gt; "</span> + content.length + <span class="string">"\0"</span></span><br><span class="line"><span class="built_in">hash</span> = sha1(header + content)</span><br></pre></td></tr></table></figure><p>上面公式表示，Git在计算对象hash时，首先会在对象头部添加一个<code>header</code>。这个<code>header</code>由3部分组成：第一部分表示对象的类型，可以<strong>取值<code>blob</code>、<code>tree</code>、<code>commit</code>以分别表示数据对象、树对象、提交对象</strong>；第二部分是数据的字节长度；第三部分是一个空字节，用来将<code>header</code>和<code>content</code>分隔开。将<code>header</code>添加到<code>content</code>头部之后，<strong>使用<code>sha1</code>算法计算出一个40位的hash值</strong>。</p><p>在手动计算Git对象的hash时，有两点需要注意：</p><ol><li><p><strong><code>header</code>中第二部分关于数据长度的计算，一定是字节的长度而不是字符串的长度</strong>；</p></li><li><p><strong><code>header + content</code>的操作并不是字符串级别的拼接，而是二进制级别的拼接</strong>。</p></li></ol><p>各种Git对象的hash方法相同，不同的在于：</p><ol><li><p>头部类型不同，数据对象是<code>blob</code>，树对象是<code>tree</code>，提交对象是<code>commit</code>；</p></li><li><p>数据内容不同，数据对象的内容可以是任意内容，而树对象和提交对象的内容有固定的格式。</p></li></ol><p>接下来分别讲数据对象、树对象和提交对象的具体的hash方法。</p><h3 id="数据对象-1"><a href="#数据对象-1" class="headerlink" title="数据对象"></a>数据对象</h3><p>数据对象的格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blob &lt;content length&gt;&lt;NULL&gt;&lt;content&gt;</span><br></pre></td></tr></table></figure><p>从上一节中我们知道，使用<code>git hash-object</code>可以计算出一个40位的hash值，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"what is up, doc?"</span> | git <span class="built_in">hash</span>-object --stdin</span><br><span class="line">bd9dbf5aae1a3862dd1526723246b20206e5fc37</span><br></pre></td></tr></table></figure><p>注意，上面在<code>echo</code>后面使用了<code>-n</code>选项，用来阻止自动在字符串末尾添加换行符，否则会导致实际传给<code>git hash-object</code>是<code>what is up, doc?\n</code>，而不是我们直观认为的<code>what is up, doc?</code>。</p><p>为验证前面提到的Git对象hash方法，我们使用<code>openssl sha1</code>来手动计算<code>what is up, doc?</code>的hash值：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"blob 16\0what is up, doc?"</span> | openssl sha1</span><br><span class="line">bd9dbf5aae1a3862dd1526723246b20206e5fc37</span><br></pre></td></tr></table></figure><p>可以发现，手动计算出的hash值与<code>git hash-object</code>计算出来的一模一样。</p><p>在Git对象hash方法的注意事项中，提到<strong><code>header</code>中第二部分关于数据长度的计算，一定是字节的长度而不是字符串的长度</strong>。由于<code>what is up, doc?</code>只有英文字符，在UTF8中恰好字符的长度和字节的长度都等于16，很容易将这个长度误解为字符的长度。假设我们以<code>中文</code>来试验：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"中文"</span> | git <span class="built_in">hash</span>-object --stdin</span><br><span class="line">efbb13322ba66f682e179ebff5eeb1bd6ef83972</span><br><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"blob 2\0中文"</span> | openssl sha1</span><br><span class="line">d1dc2c3eed26b05289bddb857713b60b8c23ed29</span><br></pre></td></tr></table></figure><p>我们可以看到，<code>git hash-object</code>和<code>openssl sha1</code>计算出来的hash值根本不一样。这是因为<code>中文</code>两个字符作为UTF格式存储后的字符长度不是2，具体是多少呢？可以使用<code>wc</code>来计算：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"中文"</span> | wc -c</span><br><span class="line">       6</span><br></pre></td></tr></table></figure><p><code>中文</code>字符串的字节长度是6，重新手动计算发现得出的hash值就能对应上了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"blob 6\0中文"</span> | openssl sha1</span><br><span class="line">efbb13322ba66f682e179ebff5eeb1bd6ef83972</span><br></pre></td></tr></table></figure><h3 id="树对象-1"><a href="#树对象-1" class="headerlink" title="树对象"></a>树对象</h3><p>树对象的内容格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree &lt;content length&gt;&lt;NUL&gt;&lt;file mode&gt; &lt;filename&gt;&lt;NUL&gt;&lt;item sha&gt;...</span><br></pre></td></tr></table></figure><p>需要注意的是，<code>&lt;item sha&gt;</code>部分是二进制形式的sha1码，而不是十六进制形式的sha1码。</p><p>我们从上一节摘出一个树对象做实验，其内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">100644 blob 83baae61804e65cc73a7201a7252750c76066a30  test.txt</span><br></pre></td></tr></table></figure><p>我们首先使用<code>xxd</code>把<code>83baae61804e65cc73a7201a7252750c76066a30</code>转换成为二进制形式，并将结果保存为<code>sha1.txt</code>以方便后面做追加操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"83baae61804e65cc73a7201a7252750c76066a30"</span> | xxd -r -p &gt; sha1.txt</span><br><span class="line">$ cat tree-items.txt</span><br><span class="line">���a�Ne�s� rRu</span><br><span class="line">              vj0%</span><br></pre></td></tr></table></figure><p>接下来构造content部分，并保存至文件<code>content.txt</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"100644 test.txt\0"</span> | cat - sha1.txt &gt; content.txt</span><br><span class="line">$ cat content.txt</span><br><span class="line">100644 test.txt���a�Ne�s� rRu</span><br><span class="line">                             vj0%</span><br></pre></td></tr></table></figure><p>计算content的长度：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat content.txt | wc -c</span><br><span class="line">      36</span><br></pre></td></tr></table></figure><p>那么最终该树对象的内容为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"tree 36\0"</span> | cat - content.txt</span><br><span class="line">tree 36100644 test.txt���a�Ne�s� rRu</span><br><span class="line">                                    vj0%</span><br></pre></td></tr></table></figure><p>最后使用<code>openssl sha1</code>计算hash值，可以发现和实验的hash值是一样的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"tree 36\0"</span> | cat - content.txt | openssl sha1</span><br><span class="line">d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br></pre></td></tr></table></figure><h3 id="提交对象-1"><a href="#提交对象-1" class="headerlink" title="提交对象"></a>提交对象</h3><p>提交对象的格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">commit &lt;content length&gt;&lt;NUL&gt;tree &lt;tree sha&gt;</span><br><span class="line">parent &lt;parent sha&gt;</span><br><span class="line">[parent &lt;parent sha&gt; <span class="keyword">if</span> several parents from merges]</span><br><span class="line">author &lt;author name&gt; &lt;author e-mail&gt; &lt;timestamp&gt; &lt;timezone&gt;</span><br><span class="line">committer &lt;author name&gt; &lt;author e-mail&gt; &lt;timestamp&gt; &lt;timezone&gt;</span><br><span class="line"></span><br><span class="line">&lt;commit message&gt;</span><br></pre></td></tr></table></figure><p>我们从上一节摘出一个提交对象做实验，其内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'first commit'</span> | git commit-tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">$ git cat-file -p db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure><p>这里需要注意的是，由于<code>echo &#39;first commit&#39;</code>没有添加<code>-n</code>选项，因此实际的提交信息是<code>first commit\n</code>。使用<code>wc</code>计算出提交内容的字节数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span></span><br><span class="line"><span class="string">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span></span><br><span class="line"><span class="string">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">first commit\n"</span> | wc -c</span><br><span class="line">     163</span><br></pre></td></tr></table></figure><p>那么，这个提交对象的<code>header</code>就是<code>commit 163\0</code>，手动把头部添加到提交内容中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">commit 163\0tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit\n</span><br></pre></td></tr></table></figure><p>使用<code>openssl sha1</code>计算这个上面内容的hash值：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n <span class="string">"commit 163\0tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span></span><br><span class="line"><span class="string">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span></span><br><span class="line"><span class="string">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">first commit\n"</span> | openssl sha1</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a</span><br></pre></td></tr></table></figure><p>可以看见，与实验的hash值是一样的。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>这篇文章详细地分析了Git中的数据对象、树对象和提交对象的hash方法，可以发现原理是非常简单的。数据对象和提交对象打印出来的内容与存储内容组织是一模一样的，可以很直观的理解。对于树对象，其打印出来的内容和实际存储是有区别的，增加了一些实现上的难度。例如，使用二进制形式的hash值而不是直观的十六进制形式。</p><h2 id="Git对象如何存储"><a href="#Git对象如何存储" class="headerlink" title="Git对象如何存储"></a>Git对象如何存储</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>数据对象、树对象和提交对象都是存储在<code>.git/objects</code>目录下，目录的结构如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.git</span><br><span class="line">|-- objects</span><br><span class="line">    |-- 01</span><br><span class="line">    |   |-- 55eb4229851634a0f03eb265b69f5a2d56f341</span><br><span class="line">    |-- 1f</span><br><span class="line">    |   |-- 7a7a472abf3dd9643fd615f6da379c4acb3e3a</span><br><span class="line">    |-- 83</span><br><span class="line">        |-- baae61804e65cc73a7201a7252750c76066a30</span><br></pre></td></tr></table></figure><p>从上面的目录结构可以看出，Git对象的40位hash分为两部分：头两位作为文件夹，后38位作为对象文件名。所以一个Git对象的存储路径规则为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.git/objects/<span class="built_in">hash</span>[0, 2]/<span class="built_in">hash</span>[2, 40]</span><br></pre></td></tr></table></figure><p>这里就产生了一个疑问：为什么Git要这么设计目录结构，而不直接用Git对象的40位hash作为文件名？原因是有两点：</p><ol><li><p>有些文件系统对目录下的文件数量有限制。例如，FAT32限制单目录下的最大文件数量是65535个，如果使用U盘拷贝Git文件就可能出现问题。</p></li><li><p>有些文件系统访问文件是一个线性查找的过程，目录下的文件越多，访问越慢。</p></li></ol><p>在<code>Git对象哈希</code>小节中，我们知道Git对象会在原内容前加个一个头部：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store = header + content</span><br></pre></td></tr></table></figure><p>Git对象在存储前，会使用zlib的deflate算法进行压缩，即简要描述为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zlib_store = zlib.deflate(store)</span><br></pre></td></tr></table></figure><p>压缩后的<code>zlib_store</code>按照Git对象的路径规则存储到<code>.git/objects</code>目录下。</p><p>总结下Git对象存储的算法步骤：</p><ol><li><p>计算<code>content</code>长度，构造<code>header</code>;</p></li><li><p>将<code>header</code>添加到<code>content</code>前面，构造Git对象；</p></li><li><p>使用sha1算法计算Git对象的40位hash码；</p></li><li><p>使用zlib的deflate算法压缩Git对象；</p></li><li><p>将压缩后的Git对象存储到<code>.git/objects/hash[0, 2]/hash[2, 40]</code>路径下;</p></li></ol><h3 id="Nodejs实现"><a href="#Nodejs实现" class="headerlink" title="Nodejs实现"></a>Nodejs实现</h3><p>接下来，我们使用Nodejs来实现<code>git hash-object -w</code>的功能，即计算Git对象的hash值并存储到Git文件系统中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">const fs = require(<span class="string">'fs'</span>)</span><br><span class="line">const crypto = require(<span class="string">'crypto'</span>)</span><br><span class="line">const zlib = require(<span class="string">'zlib'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> gitHashObject(content, <span class="built_in">type</span>) &#123;</span><br><span class="line">  // 构造header</span><br><span class="line">  const header = `<span class="variable">$&#123;type&#125;</span> <span class="variable">$&#123;Buffer.from(content).length&#125;</span>\0`</span><br><span class="line"></span><br><span class="line">  // 构造Git对象</span><br><span class="line">  const store = Buffer.concat([Buffer.from(header), Buffer.from(content)])</span><br><span class="line"></span><br><span class="line">  // 计算<span class="built_in">hash</span></span><br><span class="line">  const sha1 = crypto.createHash(<span class="string">'sha1'</span>)</span><br><span class="line">  sha1.update(store)</span><br><span class="line">  const <span class="built_in">hash</span> = sha1.digest(<span class="string">'hex'</span>)</span><br><span class="line"></span><br><span class="line">  // 压缩Git对象</span><br><span class="line">  const zlib_store = zlib.deflateSync(store)</span><br><span class="line"></span><br><span class="line">  // 存储Git对象</span><br><span class="line">  fs.mkdirSync(`.git/objects/<span class="variable">$&#123;hash.substring(0, 2)&#125;</span>`)</span><br><span class="line">  fs.writeFileSync(`.git/objects/<span class="variable">$&#123;hash.substring(0, 2)&#125;</span>/<span class="variable">$&#123;hash.substring(2, 40)&#125;</span>`, zlib_store)</span><br><span class="line"></span><br><span class="line">  console.log(<span class="built_in">hash</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 调用入口</span><br><span class="line">gitHashObject(process.argv[2], process.argv[3])</span><br></pre></td></tr></table></figure><p>最后，测试下能否正确存储Git对象：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ node index.js <span class="string">'hello, world'</span> blob</span><br><span class="line">8c01d89ae06311834ee4b1fab2f0414d35f01102</span><br><span class="line">$ git cat-file -p 8c01d89ae06311834ee4b1fab2f0414d35f01102</span><br><span class="line">hello, world</span><br></pre></td></tr></table></figure><p>由此可见，我们生成了一个合法的Git数据对象，证明算法是正确的。</p><h2 id="Git引用"><a href="#Git引用" class="headerlink" title="Git引用"></a>Git引用</h2><p>首先来搞清楚什么是Git引用，前文讲了Git提交对象的哈希、存储原理，理论上我们只要知道该对象的hash值，就能往前推出整个提交历史，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">log</span> --pretty=oneline 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31 third commit</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c second commit</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a first commit</span><br></pre></td></tr></table></figure><p>现在问题来了，提交对象的这40位hash值不好记忆，Git引用相当于给40位hash值取一个别名，便于识别和读取。<strong>Git引用对象都存储在<code>.git/refs</code>目录下，该目录下有3个子文件夹<code>heads</code>、<code>tags</code>和<code>remotes</code>，分别对应于HEAD引用、标签引用和远程引用</strong>，下面分别讲一讲每种引用的原理。</p><h3 id="HEAD引用"><a href="#HEAD引用" class="headerlink" title="HEAD引用"></a>HEAD引用</h3><p><strong>HEAD引用是用来指向每个分支的最后一次提交对象</strong>，这样切换到一个分支之后，才能知道分支的“尾巴”在哪里。<strong>HEAD引用存储在<code>.git/refs/heads</code>目录下</strong>，有多少个分支，就有相应的同名HEAD引用对象。例如代码库里面有<code>master</code>和<code>test</code>两个分支，那么<code>.git/refs/heads</code>目录下就存在<code>master</code>和<code>test</code>两个文件，分别记录了分支的最后一次提交。</p><p>HEAD引用的内容就是提交对象的hash值，理论上我们可以手动地构造一个HEAD引用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"3ac728ac62f0a7b5ac201fd3ed1f69165df8be31"</span> &gt; .git/refs/heads/master</span><br></pre></td></tr></table></figure><p>Git提供了一个专有命令<code>update-ref</code>，用来查看和修改Git引用对象，当然也包括HEAD引用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git update-ref refs/heads/master 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">$ git update-ref refs/heads/master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>上面的命令我们将<code>master</code>分支的HEAD指向了<code>3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</code>，现在用<code>git log</code>查看下<code>master</code>的提交历史，可以发现最后一次提交就是所更新的hash值：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">log</span> --pretty=oneline master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31 (HEAD -&gt; master) third commit</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c second commit</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a first commit</span><br></pre></td></tr></table></figure><blockquote><p>这里的<code>HEAD -&gt; master</code>表示当前整个代码库级别的HEAD引用为master分支。具体含义请往下看。</p></blockquote><p>同理，可以使用同样的方法更新<code>test</code>分支的HEAD：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git update-ref refs/heads/<span class="built_in">test</span> d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">$ git <span class="built_in">log</span> --pretty=oneline <span class="built_in">test</span></span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c (<span class="built_in">test</span>) second commit</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a first commit</span><br></pre></td></tr></table></figure><blockquote><p>若当前整个代码库级别的HEAD引用为test分支，则会显示<code>(HEAD -&gt; test)</code>。</p></blockquote><p><code>.git/refs/heads</code>目录下存储了每个分支的HEAD，那怎么知道代码库当前处于哪个分支呢？这就需要一个代码库级别的HEAD引用。<strong><code>.git/HEAD</code>这个文件就是整个代码库级别的HEAD引用</strong>。我们先查看一下<code>.git/HEAD</code>文件的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/HEAD</span><br><span class="line">ref: refs/heads/master</span><br></pre></td></tr></table></figure><p>我们发现<code>.git/HEAD</code>文件的内容不是40位hash值，而像是指向<code>.git/refs/heads/master</code>。尝试切换到<code>test</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout <span class="built_in">test</span></span><br><span class="line">$ cat .git/HEAD</span><br><span class="line">ref: refs/heads/<span class="built_in">test</span></span><br></pre></td></tr></table></figure><p>切换分支后，<code>.git/HEAD</code>文件的内容也跟着指向<code>.git/refs/heads/test</code>。<code>.git/HEAD</code>也是HEAD引用对象，与一般引用不同的是，它是“符号引用”。符号引用类似于文件的快捷方式，链接到要引用的对象上。</p><p>Git提供专门的命令<code>git symbolic-ref</code>，用来查看和更新符号引用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git symbolic-ref HEAD refs/heads/master</span><br><span class="line">$ git symbolic-ref HEAD refs/heads/<span class="built_in">test</span></span><br></pre></td></tr></table></figure><p>至此，我们分析了两种HEAD引用，一种是分支级别的HEAD引用，用来记录各分支的最后一次提交，存储在<code>.git/refs/heads</code>目录下，使用<code>git update-ref</code>来维护；一种是代码库级别的HEAD引用，用来记录代码库所处的分支，存储在<code>.git/HEAD</code>文件，使用<code>git symbolic-ref</code>来维护。</p><h3 id="标签引用"><a href="#标签引用" class="headerlink" title="标签引用"></a>标签引用</h3><p>标签引用，顾名思义就是给Git对象打标签，便于记忆。例如，我们可以将某个提交对象打v1.0标签，表示是1.0版本。标签引用都存储在<code>.git/refs/tags</code>里面。</p><p>标签引用和HEAD引用本质是Git引用对象，同样使用<code>git update-ref</code>来查看和修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git update-ref refs/tags/v1.0 d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">$ cat .git/refs/tags/v1.0</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br></pre></td></tr></table></figure><p>还有一种标签引用称为“附注引用”，可以为标签添加说明信息。上面的标签引用打了一个<code>v1.0</code>的标签表示发布1.0版本，有时候发布软件的时候除了版本号信息，还要写更新说明。附注引用就是用来实现打标签的同时，也可以附带说明信息。</p><p>附注引用是怎么实现的呢？与常规标签引用不同的是，它不直接指向提交对象，而是新建一个Git对象存储到<code>.git/objects</code>中，用来记录附注信息，然后附注标签指向这个Git对象。</p><p>使用<code>git tag</code>建立一个附注标签：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git tag -a v1.1 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31 -m <span class="string">"test tag"</span></span><br><span class="line">$ cat .git/refs/tags/v1.1</span><br><span class="line">8be4d8e4e8e80711dd7bae304ccfa63b35a6eb8c</span><br></pre></td></tr></table></figure><p>使用<code>git cat-file</code>来查看附注标签所指向的Git对象：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 8be4d8e4e8e80711dd7bae304ccfa63b35a6eb8c</span><br><span class="line">object 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line"><span class="built_in">type</span> commit</span><br><span class="line">tag v1.1</span><br><span class="line">tagger jingsam &lt;jing-sam@qq.com&gt; 1529481368 +0800</span><br><span class="line"></span><br><span class="line"><span class="built_in">test</span> tag</span><br></pre></td></tr></table></figure><p>可以看到，上面的Git对象存储了我们填写的附注信息。</p><p>总之，普通的标签引用和附注引用同样都是存储的是40位hash值，指向一个Git对象，所不同的是普通的标签引用是直接指向提交对象，而附注标签是指向一个附注对象，附注对象再指向具体的提交对象。</p><p>另外，本质上标签引用并不是只可以指向提交对象，实际上可以指向任何Git对象，即可以给任何Git对象打标签。</p><h3 id="远程引用"><a href="#远程引用" class="headerlink" title="远程引用"></a>远程引用</h3><p>远程引用，类似于<code>.git/refs/heads</code>中存储的本地仓库各分支的最后一次提交，在<code>.git/refs/remotes</code>是用来记录多个远程仓库各分支的最后一次提交。</p><p>我们可以使用<code>git remote</code>来管理远程分支：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin git@github.com:jingsam/git-test.git</span><br></pre></td></tr></table></figure><p>上面添加了一个<code>origin</code>远程链接，接下来我们把本地仓库的<code>master</code>推送到远程仓库上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br><span class="line">Counting objects: 9, <span class="keyword">done</span>.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (5/5), <span class="keyword">done</span>.</span><br><span class="line">Writing objects: 100% (9/9), 720 bytes | 360.00 KiB/s, <span class="keyword">done</span>.</span><br><span class="line">Total 9 (delta 0), reused 0 (delta 0)</span><br><span class="line">To github.com:jingsam/git-test.git</span><br><span class="line"> * [new branch]      master -&gt; master</span><br></pre></td></tr></table></figure><p>这时候在<code>.git/refs/remotes</code>中的远程引用就会更新：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/refs/remotes/origin/master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>和本地仓库的<code>master</code>比较一下，发现是一模一样的，表示远程分支和本地分支是同步的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/refs/heads/master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>由于远程引用也是Git引用对象，所以理论上也可以使用<code>git update-ref</code>来手动维护。但是，我们需要先把代码与远程仓库进行同步，在远程仓库中找到对应分支的HEAD，然后使用<code>git update-ref</code>进行更新，过程比较麻烦。而我们在执行<code>git pull</code>或<code>git push</code>这样的高层命令的时候，远程引用会自动更新。</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>到这里，三种Git引用都已分析完毕。总的来说，三种Git引用都统一存储到<code>.git/refs</code>目录下，Git引用中的内容都是40位的hash值，指向某个Git对象，这个对象可以是任意的Git对象，可以是数据对象、树对象、提交对象。三种Git引用都可以使用<code>git update-ref</code>来手动维护。</p><p>三种Git引用对象所不同的是，分别存储于<code>.git/refs/heads</code>、<code>.git/refs/tags</code>、<code>.git/refs/remotes</code>，存储的文件夹不同，赋予了引用对象不同的功能。<strong>HEAD引用用来记录本地各个分支的最后一次提交，标签引用用来给任意Git对象打标签，远程引用正式用来记录远程各个分支的最后一次提交。</strong></p><h2 id="git文件夹"><a href="#git文件夹" class="headerlink" title=".git文件夹"></a>.git文件夹</h2><p>新建一个git仓库，新建一个<code>a.py</code>，第一次提交<code>add a.py</code>后，<code>.git</code>文件夹如下所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── COMMIT_EDITMSG</span><br><span class="line">├── HEAD</span><br><span class="line">├── config</span><br><span class="line">├── description</span><br><span class="line">├── hooks</span><br><span class="line">│   ├── applypatch-msg.sample</span><br><span class="line">│   ├── commit-msg.sample</span><br><span class="line">│   ├── fsmonitor-watchman.sample</span><br><span class="line">│   ├── post-update.sample</span><br><span class="line">│   ├── pre-applypatch.sample</span><br><span class="line">│   ├── pre-commit.sample</span><br><span class="line">│   ├── pre-merge-commit.sample</span><br><span class="line">│   ├── pre-push.sample</span><br><span class="line">│   ├── pre-rebase.sample</span><br><span class="line">│   ├── pre-receive.sample</span><br><span class="line">│   ├── prepare-commit-msg.sample</span><br><span class="line">│   ├── push-to-checkout.sample</span><br><span class="line">│   └── update.sample</span><br><span class="line">├── index</span><br><span class="line">├── info</span><br><span class="line">│   └── exclude</span><br><span class="line">├── logs</span><br><span class="line">│   ├── HEAD</span><br><span class="line">│   └── refs</span><br><span class="line">│       └── heads</span><br><span class="line">│           └── master</span><br><span class="line">├── objects</span><br><span class="line">│   ├── b4</span><br><span class="line">│   │   └── 615222fc0906f8bbd5a2f6c86824ed7cdff6be</span><br><span class="line">│   ├── c2</span><br><span class="line">│   │   └── e5936fe3786bc72d6bf8f7278f733571c09e48</span><br><span class="line">│   ├── ed</span><br><span class="line">│   │   └── 0a1ffd4b9ad6c8d2dd687ebd30762087cec86e</span><br><span class="line">│   ├── info</span><br><span class="line">│   └── pack</span><br><span class="line">└── refs</span><br><span class="line">    ├── heads</span><br><span class="line">    │   └── master</span><br><span class="line">    └── tags</span><br></pre></td></tr></table></figure><h3 id="COMMIT-EDITMSG"><a href="#COMMIT-EDITMSG" class="headerlink" title="COMMIT_EDITMSG"></a>COMMIT_EDITMSG</h3><p>COMMIT-EDITMSG是一个临时文件，存储最后一次提交的message，当敲入<code>git commit</code>命令，不加<code>-m</code>的话， 会打开编辑器，其实就是在编辑此文件，而你退出编辑器后，git 会把此文件内容写入 commit 记录。 而执行<code>git commit -m &#39;add a.py&#39;</code>时，<code>add a.py</code>就是COMMIT_EDITMSG的文件内容。</p><p>例如打开文件如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add a.py</span><br></pre></td></tr></table></figure><p>该文件的一个应用场景：当你git pull 远程仓库后，新增了很多提交，淹没了本地提交记录，直接 <code>cat .git/COMMIT_EDITMSG</code> 就可以弄清楚自己最后工作的位置了。</p><h3 id="HEAD"><a href="#HEAD" class="headerlink" title="HEAD"></a>HEAD</h3><p>整个代码库级别的HEAD引用，也就是当前位于哪个分支。打开内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ref: refs/heads/master</span><br></pre></td></tr></table></figure><h3 id="config"><a href="#config" class="headerlink" title="config"></a>config</h3><p><code>config</code> 文件包含项目特有的配置选项。</p><p>Git配置分为三个级别：</p><ol><li>系统级别：system。</li><li>全局级别（用户级别）：global。</li><li>本地级别：local。</li></ol><p>本地级别的配置信息，就记录在<code>config</code> 文件中，使用<code>git config --local</code>或<code>git config 不加任何参数</code>命令进行配置。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[core]</span><br><span class="line">repositoryformatversion = 0</span><br><span class="line">filemode = false</span><br><span class="line">bare = false</span><br><span class="line">logallrefupdates = true</span><br><span class="line">symlinks = false</span><br><span class="line">ignorecase = true</span><br></pre></td></tr></table></figure><h3 id="description"><a href="#description" class="headerlink" title="description"></a>description</h3><p>仓库的描述信息。打开文件内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unnamed repository; edit this file <span class="string">'description'</span> to name the repository.</span><br></pre></td></tr></table></figure><p>说明：该文件仅供 GitWeb (Github 的一种前身) 程序使用，我们无需关心。</p><h3 id="hooks"><a href="#hooks" class="headerlink" title="hooks"></a>hooks</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>和其它版本控制系统一样，Git 能在特定的重要动作发生时，触发自定义脚本（钩子脚本）。这些被称为钩子的脚本可以在提交 (commit)、变基 (rebase)、拉取 ( pull ) 操作的前后运行。脚本名预示着它的执行时机。如我们可以编写 pre-push 的作为钩子，进行推送代码前的检查。</p><p>钩子分为：客户端的钩子和服务器端的钩子。客户端钩子由提交和合并这样的操作所调用，而服务器端钩子作用于接收被推送的提交这样的联网操作。</p><p>当你用 <code>git init</code> 命令初始化一个新版本库时，Git 默认会在这个目录中放置一些示例脚本。这些脚本都是 shell 脚本，其中一些还混杂了 Perl 代码。你可以使用任何你熟悉的语言编写Git钩子脚本，如Ruby 或 Python等编写的可执行脚本，都可以正常使用。比如打开<code>hooks/pre-push.sample</code>，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">remote=<span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">url=<span class="string">"<span class="variable">$2</span>"</span></span><br><span class="line"></span><br><span class="line">zero=$(git <span class="built_in">hash</span>-object --stdin &lt;/dev/null | tr <span class="string">'[0-9a-f]'</span> <span class="string">'0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> local_ref local_oid remote_ref remote_oid</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="string">"<span class="variable">$local_oid</span>"</span> = <span class="string">"<span class="variable">$zero</span>"</span></span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="comment"># Handle delete</span></span><br><span class="line">:</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="string">"<span class="variable">$remote_oid</span>"</span> = <span class="string">"<span class="variable">$zero</span>"</span></span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="comment"># New branch, examine all commits</span></span><br><span class="line">range=<span class="string">"<span class="variable">$local_oid</span>"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="comment"># Update to existing branch, examine new commits</span></span><br><span class="line">range=<span class="string">"<span class="variable">$remote_oid</span>..<span class="variable">$local_oid</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check for WIP commit</span></span><br><span class="line">commit=$(git rev-list -n 1 --grep <span class="string">'^WIP'</span> <span class="string">"<span class="variable">$range</span>"</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> -n <span class="string">"<span class="variable">$commit</span>"</span></span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> &gt;&amp;2 <span class="string">"Found WIP commit in <span class="variable">$local_ref</span>, not pushing"</span></span><br><span class="line"><span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p>将编写好的可执行脚本（不带扩展名），放入 <code>.git</code> 目录下的 <code>hooks</code> 子目录中，即可激活该钩子脚本。</p><ol><li><p><code>pre-commit</code> 钩子：在创建提交信息前运行，它用于检查即将提交的快照。例如，检查是否有所遗漏，确保测试运行，以及核查代码。</p><p>如果该钩子以非零值退出，Git 将放弃此次提交，不过你可以用 <code>git commit --no-verify</code> 来绕过这个环节。</p><p>你可以利用该钩子，来检查代码风格是否一致、尾随空白字符是否存在，或新方法的文档是否适当等操作。</p></li><li><p><code>commit-msg</code> 钩子：接收一个参数，此参数存有当前提交信息的临时文件的路径。 如果该钩子脚本以非零值退出，Git 将放弃提交，因此，可以用来在提交通过前验证项目状态或提交信息。</p></li><li><p><code>post-commit</code> 钩子：在整个提交过程完成后运行。 它不接收任何参数，但你可以很容易地通过运行 <code>git log -1 HEAD</code> 来获得最后一次的提交信息。 该钩子一般用于通知之类的事情。</p></li></ol><p>这里只简单介绍三个<code>hooks</code>目录中的钩子脚本，如果想查看更多钩子示例脚本说明，可以查看<a href="https://git-scm.com/book/zh/v2/%E8%87%AA%E5%AE%9A%E4%B9%89-Git-Git-%E9%92%A9%E5%AD%90#_git_hooks" target="_blank" rel="noopener">Git 钩子</a>，简略信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -F1：在列出的文件名称后加一符号；例如可执行档则加 "*", 目录则加 "/"，1代表一个文件占据一行。</span></span><br><span class="line">$ ls -F1 hooks</span><br><span class="line">prepare-commit-msg.sample*  <span class="comment"># git commit 之前，编辑器启动之前触发，传入 COMMIT_FILE，COMMIT_SOURCE，SHA1</span></span><br><span class="line">commit-msg.sample*          <span class="comment"># git commit 之前，编辑器退出后触发，传入 COMMIT_EDITMSG 文件名</span></span><br><span class="line">pre-commit.sample*          <span class="comment"># git commit 之前，commit-msg 通过后触发，譬如校验文件名是否含中文</span></span><br><span class="line">pre-push.sample*            <span class="comment"># git push 之前触发</span></span><br><span class="line"></span><br><span class="line">pre-receive.sample*         <span class="comment"># git push 之后，服务端更新 ref 前触发</span></span><br><span class="line">update.sample*              <span class="comment"># git push 之后，服务端更新每一个 ref 时触发，用于针对每个 ref 作校验等</span></span><br><span class="line">post-update.sample*         <span class="comment"># git push 之后，服务端更新 ref 后触发</span></span><br><span class="line"></span><br><span class="line">pre-rebase.sample*          <span class="comment"># git rebase 之前触发，传入 rebase 分支作参数</span></span><br><span class="line">applypatch-msg.sample*      <span class="comment"># 用于 git am 命令提交信息校验</span></span><br><span class="line">pre-applypatch.sample*      <span class="comment"># 用于 git am 命令执行前动作</span></span><br><span class="line">fsmonitor-watchman.sample*  <span class="comment"># 配合 core.fsmonitor 设置来更好监测文件变化</span></span><br></pre></td></tr></table></figure><h3 id="index"><a href="#index" class="headerlink" title="index"></a>index</h3><p><code>index</code>文件：该文件就是我们平时说的 <strong>暂存区</strong> （stage），是一个二进制文件，保存了下次将提交的文件列表信息，我们执行<code>git add</code>命令后，这个文件就会更新刚刚添加的文件信息。</p><p>我们可以使用<code>git ls-files --stage</code>命令看到当前仓库中每一个文件及其所对应的文件对象。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$  git ls-files --stage     </span><br><span class="line">100644 4d91c814eb85fd49d32441ef1d4e7aa921e59374 0       .gitignore</span><br></pre></td></tr></table></figure><p>提示：刚刚初始化的Git本地版本库中是没有<code>index</code>文件的，只有执行一次暂存操作后，才在<code>.git</code>目录自动生成<code>index</code>文件。</p><h3 id="info"><a href="#info" class="headerlink" title="info"></a>info</h3><p><code>info</code> 目录包含一个全局性排除（global exclude）文件， 用以放置那些不希望被记录在 <code>.gitignore</code> 文件中的忽略模式（ignored patterns），它不会影响到其他人，也不会提交到版本库中去。<code>.gitignore</code> 文件会被提交到版本库。</p><h3 id="logs"><a href="#logs" class="headerlink" title="logs"></a>logs</h3><p>此文件夹主要记录每个分支的每次修改的日志。比如说<code>logs/refs/heads/master</code>内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0000000000000000000000000000000000000000 ed0a1ffd4b9ad6c8d2dd687ebd30762087cec86e zdaiot &lt;zdaiot@163.com&gt; 1685004210 +0800commit (initial): add a.py</span><br></pre></td></tr></table></figure><p>打开logs文件夹可以看到其中有两个文件，<code>refs</code>文件夹和<code>HEAD</code>文件。</p><h4 id="HEAD文件"><a href="#HEAD文件" class="headerlink" title="HEAD文件"></a>HEAD文件</h4><p>HEAD文件保存的是，所有的引起HEAD指针移动的操作记录，使用<code>git reflog</code>命令，查询的结果就是来自这个文件。</p><h4 id="refs目录"><a href="#refs目录" class="headerlink" title="refs目录"></a>refs目录</h4><p><code>refs</code>文件夹中有两个文件夹：<code>heads</code>目录和<code>remotes</code>目录。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ git remote -v                                                            </span><br><span class="line">group   xxxx.git (fetch)</span><br><span class="line">group   xxxx.git (push)</span><br><span class="line">origin  yyyy.git (fetch)</span><br><span class="line">origin  yyyy.git (push)</span><br><span class="line"></span><br><span class="line">$ tree .git/logs</span><br><span class="line">.</span><br><span class="line">├── HEAD</span><br><span class="line">└── refs</span><br><span class="line">    ├── heads</span><br><span class="line">    │   ├── develop</span><br><span class="line">    │   └── master</span><br><span class="line">    └── remotes</span><br><span class="line">        ├── group   <span class="comment"># 一个远程链接</span></span><br><span class="line">        │   ├── develop</span><br><span class="line">        │   └── master</span><br><span class="line">        └── origin  <span class="comment"># 另外一个远程链接</span></span><br><span class="line">            ├── develop</span><br><span class="line">            └── master</span><br></pre></td></tr></table></figure><ul><li><code>heads</code>目录中都是以分支命名的文件，即：每个文件名对应着本地版本库的一个分支。每个文件中，记录的都是该分支历史操作记录。</li><li><code>remotes</code>目录和<code>heads</code>目录的作用同理，只不过<code>remotes</code>目录中存储的是远程分支的历史操作记录。</li></ul><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>heads目录中所有分支历史操作记录的总和，是HEAD文件文件的内容。</p><p>例如版本库中有两个分支，分别查看他们历史操作记录，我们可以看到，<code>master</code>分支和<code>dev</code>分支的历史操作记录总和，就是HEAD文件中的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.查看master分支的历史操作记录</span></span><br><span class="line">L@DESKTOP-T2AI2SU MINGW64 /j/git-repository/git_learning (dev)</span><br><span class="line">$ cat .git/logs/refs/heads/master</span><br><span class="line">0000000000000000000000000000000000000000 f58d840994fe43cf8cdf8fe036dbadb393b7df62 sun_wk &lt;sun_wk@126.com&gt; 1619073338 +0800      commit (initial): 第1次提交</span><br><span class="line">f58d840994fe43cf8cdf8fe036dbadb393b7df62 d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 sun_wk &lt;sun_wk@126.com&gt; 1619073495 +0800      commit: 第2次提交</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.查看dev分支的历史操作记录</span></span><br><span class="line">L@DESKTOP-T2AI2SU MINGW64 /j/git-repository/git_learning (dev)</span><br><span class="line">$ cat .git/logs/refs/heads/dev</span><br><span class="line">0000000000000000000000000000000000000000 d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 sun_wk &lt;sun_wk@126.com&gt; 1619073563 +0800      branch: Created from HEAD</span><br><span class="line">d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 d00abd3a31bfa95cc88e1fb2df21421b2d3743a9 sun_wk &lt;sun_wk@126.com&gt; 1619073598 +0800      commit: 第3次提交</span><br><span class="line">d00abd3a31bfa95cc88e1fb2df21421b2d3743a9 9ec0e8f8a861d8da31427f0508aa4c00d3242988 sun_wk &lt;sun_wk@126.com&gt; 1619073751 +0800      commit: 第4次提交</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.查看HEAD文件的历史操作记录。</span></span><br><span class="line">L@DESKTOP-T2AI2SU MINGW64 /j/git-repository/git_learning (dev)</span><br><span class="line">$ cat .git/logs/HEAD</span><br><span class="line">0000000000000000000000000000000000000000 f58d840994fe43cf8cdf8fe036dbadb393b7df62 sun_wk &lt;sun_wk@126.com&gt; 1619073338 +0800      commit (initial): 第1次提交</span><br><span class="line">f58d840994fe43cf8cdf8fe036dbadb393b7df62 d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 sun_wk &lt;sun_wk@126.com&gt; 1619073495 +0800      commit: 第2次提交</span><br><span class="line">d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 sun_wk &lt;sun_wk@126.com&gt; 1619073563 +0800      checkout: moving from master to dev</span><br><span class="line">d6f67eeefe7e0d49cdaf6cadb50d219e9b0d5674 d00abd3a31bfa95cc88e1fb2df21421b2d3743a9 sun_wk &lt;sun_wk@126.com&gt; 1619073598 +0800      commit: 第3次提交</span><br><span class="line">d00abd3a31bfa95cc88e1fb2df21421b2d3743a9 9ec0e8f8a861d8da31427f0508aa4c00d3242988 sun_wk &lt;sun_wk@126.com&gt; 1619073751 +0800      commit: 第4次提交</span><br></pre></td></tr></table></figure><h3 id="objects"><a href="#objects" class="headerlink" title="objects"></a>objects</h3><p>详细可以看前文<code>Git对象如何存储</code>小节。我们这里只介绍pack和info文件夹。</p><p>Git 往磁盘保存对象时默认使用的格式叫松散对象 (loose object) 格式，当你对同一个文件修改哪怕一行，git 都会使用全新的文件存储这个修改了的文件，放在了objects中。Git 时不时地将这些对象打包至一个叫 packfile 的二进制文件以节省空间并提高效率，当版本库中有太多的松散对象，或者你手动执行 <code>git gc</code> 命令，或者你向远程服务器执行推送时，Git 都会这样做。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ find .git/objects -<span class="built_in">type</span> f</span><br><span class="line">.git/objects/6d/c700c37fb6af03239b8ea6f1d58db1a8819464</span><br><span class="line"></span><br><span class="line">$ git gc</span><br><span class="line">Enumerating objects: 1, <span class="keyword">done</span>.</span><br><span class="line">Counting objects: 100% (1/1), <span class="keyword">done</span>.</span><br><span class="line">Writing objects: 100% (1/1), <span class="keyword">done</span>.</span><br><span class="line">Total 1 (delta 0), reused 0 (delta 0)</span><br><span class="line"></span><br><span class="line">$ find .git/objects -<span class="built_in">type</span> f</span><br><span class="line">.git/objects/info/packs</span><br><span class="line">.git/objects/pack/pack-d9059144205ae43ab3472bebfd7976a8f52de3c2.idx</span><br><span class="line">.git/objects/pack/pack-d9059144205ae43ab3472bebfd7976a8f52de3c2.pack</span><br><span class="line"></span><br><span class="line">$ cat .git/objects/info/packs</span><br><span class="line">P pack-d9059144205ae43ab3472bebfd7976a8f52de3c2.pack</span><br></pre></td></tr></table></figure><p>这里只有6d一个文件夹，已经成功打包到pack里了，即使有很多很多文件对象，执行 git gc 后都会全部打包到 pack 里。.pack 存储对象文件，.idx 是索引文件，用于允许它们被随机访问；info 文件夹记录对象存储的附加信息，这里存储着打包后的文件名。</p><h3 id="refs"><a href="#refs" class="headerlink" title="refs"></a>refs</h3><p>详细可以看前文<code>Git引用小节</code></p><h3 id="packed-refs"><a href="#packed-refs" class="headerlink" title="packed-refs"></a>packed-refs</h3><p>前面有提过<code>git gc</code>会打包objects，其实它会做的另一件事是打包你的引用到一个单独的文件。 假设你的仓库包含以下分支与标签：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ find .git/refs -<span class="built_in">type</span> f</span><br><span class="line">.git/refs/heads/experiment</span><br><span class="line">.git/refs/heads/master</span><br><span class="line">.git/refs/tags/v1.0</span><br><span class="line">.git/refs/tags/v1.1</span><br></pre></td></tr></table></figure><p>如果你执行了 <code>git gc</code> 命令，<code>refs</code> 目录中将不会再有这些文件。 为了保证效率 Git 会将它们移动到名为 <code>.git/packed-refs</code> 的文件中，就像这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/packed-refs</span><br><span class="line"><span class="comment"># pack-refs with: peeled fully-peeled</span></span><br><span class="line">cac0cab538b970a37ea1e769cbbde608743bc96d refs/heads/experiment</span><br><span class="line">ab1afef80fac8e34258ff41fc1b867c702daa24b refs/heads/master</span><br><span class="line">cac0cab538b970a37ea1e769cbbde608743bc96d refs/tags/v1.0</span><br><span class="line">9585191f37f7b0fb9444f35a9bf50de191beadc2 refs/tags/v1.1</span><br><span class="line">^1a410efbd13591db07496601ebc7a059dd55cfe9</span><br></pre></td></tr></table></figure><p><strong>如果你更新了引用，Git 并不会修改这个文件，而是向 <code>refs/heads</code> 创建一个新的文件</strong>。 为了获得指定引用的正确 SHA-1 值，<strong>Git 会首先在 <code>refs</code> 目录中查找指定的引用，然后再到 <code>packed-refs</code> 文件中查找</strong>。 所以，如果你在 <code>refs</code> 目录中找不到一个引用，那么它或许在 <code>packed-refs</code> 文件中。</p><p>注意这个文件的最后一行，它会以 <code>^</code> 开头。 这个符号表示它上一行的标签是tag标签，<code>^</code> 所在的那一行是tag标签指向的那个提交。</p><p>最后，值得注意的是，<code>.git/packed-refs</code>中关于refs的内容并不是固定的。直接<code>git clone</code>的可能含有<code>refs/remotes/origin/xxx</code>与<code>refs/tags/xxx</code>，而<code>git clone --mirror</code>得到的bare仓库，可能含有<code>refs/heads/xxx</code>与<code>refs/tags/xxx</code>。具体情况可以直接查看该文件的内容。</p><h2 id="与-的区别"><a href="#与-的区别" class="headerlink" title="~与^的区别"></a>~与^的区别</h2><p>波浪号<code>~</code>，英文名叫 <strong>tilde</strong>。脱字符<code>^</code>，英文名叫<strong>caret</strong>。那么关于这两个该怎么区分呢？</p><p>在<a href="https://stackoverflow.com/a/12527561/15304315" target="_blank" rel="noopener">What’s the difference between HEAD^ and HEAD~ in Git?</a>中介绍如下：</p><p><strong>Rules of thumb</strong></p><ul><li>Use <code>~</code> most of the time — to go back a number of generations, usually what you want</li><li>Use <code>^</code> on merge commits — because they have two or more (immediate) parents</li></ul><p><strong>Mnemonics:</strong></p><ul><li>Tilde <code>~</code> is almost linear in appearance and wants to go backward in a straight line</li><li>Caret <code>^</code> suggests an interesting segment of a tree or a fork in the road</li></ul><p>该如何理解呢？看如下例子。在该例子中，A、B、D、G位于同一个branch，D由G与H merge合并而来，所以<code>G=D^1</code>，G是D第一个parent；<code>H=D^2</code>，H是D的第二个parent。</p><p>B是A的当前分支所在时间线中的父节点，所以<code>B=A~1</code>；D是A在当前分支所在时间线中的父节点的父节点，所以<code>D=A~2</code>。</p><p><img src="/Tools/Git/git原理简介/pDAzG.png" alt="enter image description here" style="zoom:67%;"></p><p>关于更详细的例子可以参考<a href="https://git-scm.com/docs/git-rev-parse#Documentation/git-rev-parse.txt-emltrevgtltngtemegemHEADv1510em：" target="_blank" rel="noopener">https://git-scm.com/docs/git-rev-parse#Documentation/git-rev-parse.txt-emltrevgtltngtemegemHEADv1510em：</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">G   H   I   J</span><br><span class="line"> \ /     \ /</span><br><span class="line">  D   E   F</span><br><span class="line">   \  |  / \</span><br><span class="line">    \ | /   |</span><br><span class="line">     \|/    |</span><br><span class="line">      B     C</span><br><span class="line">       \   /</span><br><span class="line">        \ /</span><br><span class="line">         A</span><br><span class="line"></span><br><span class="line">A =      = A^0</span><br><span class="line">B = A^   = A^1     = A~1</span><br><span class="line">C =      = A^2</span><br><span class="line">D = A^^  = A^1^1   = A~2</span><br><span class="line">E = B^2  = A^^2</span><br><span class="line">F = B^3  = A^^3</span><br><span class="line">G = A^^^ = A^1^1^1 = A~3</span><br><span class="line">H = D^2  = B^^2    = A^^^2  = A~2^2</span><br><span class="line">I = F^   = B^3^    = A^^3^</span><br><span class="line">J = F^2  = B^3^2   = A^^3^2</span><br></pre></td></tr></table></figure><h2 id="SSH与HTTPS区别"><a href="#SSH与HTTPS区别" class="headerlink" title="SSH与HTTPS区别"></a>SSH与HTTPS区别</h2><p>git可以使用四种主要的协议来传输资料: 本地协议（Local），HTTP 协议，SSH（Secure Shell）协议及 git 协议。其中，本地协议由于目前大都是进行远程开发和共享代码所以一般不常用，而git协议由于缺乏授权机制且较难架设所以也不常用。</p><p>最常用的便是SSH和HTTP(S)协议。git关联远程仓库可以使用http协议或者ssh协议。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p><strong>ssh：</strong></p><ul><li>一般使用22端口；</li><li>通过先在本地生成SSH密钥对再把公钥上传到服务器；</li><li>速度相较慢点</li></ul><p><strong>https：</strong></p><ul><li>一般使用443端口；</li><li>通过用户名/密码授权，可用性比较高；</li><li>速度相较快点</li></ul><p>一般企业防火墙会打开80和443这两个http/https协议的端口，因此在架设了企业防火墙的时候使用http就可以很好的绕开安全限制使用git了，很方便；而对于ssh来说，企业防火墙很可能没打开22端口。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p><strong>clone项目：</strong></p><ul><li>使用ssh方式时，需要配置ssh key，即要将生成的SSH密钥对的公钥上传至服务器；</li><li>使用https方式时，没有要求，可以直接克隆下来。</li></ul><blockquote><p>当clone public仓库时，若没有上传本地ssh key，则无法通过ssh clone仓库。</p></blockquote><p><strong>push项目</strong>（前提是你有这个仓库的push权限）：</p><ul><li><p>使用ssh方式时，不需要验证用户名和密码，之前配置过ssh key(如果你没设置密码)，直接push即可；</p></li><li><p>使用https方式时，需要验证用户名和密码。</p></li></ul><p><strong>总结：</strong></p><ul><li><p>HTTPS利于匿名访问，适合开源项目，可以方便被别人克隆和读取(但没有push权限)；</p></li><li><p>SSH不利于匿名访问，比较适合内部项目，只要配置了SSH公钥极可自由实现clone和push操作。</p></li></ul><h2 id="LFS"><a href="#LFS" class="headerlink" title="LFS"></a>LFS</h2><p>Git 是<em>分布式</em> 版本控制系统，这意味着在克隆过程中会将仓库的整个历史记录传输到客户端。对于包涵大文件（尤其是经常被修改的大文件）的项目，初始克隆需要大量时间，因为客户端会下载每个文件的每个版本。<strong>Git LFS</strong>（Large File Storage）是由 Atlassian, GitHub 以及其他开源贡献者开发的 Git 扩展，它通过延迟地（lazily）下载大文件的相关版本来减少大文件在仓库中的影响，具体来说，<strong>大文件是在 checkout 的过程中下载的，而不是 clone 或 fetch 过程中下载的</strong>（这意味着你在后台定时 fetch 远端仓库内容到本地时，并不会下载大文件内容，而是在你 checkout 到工作区的时候才会真正去下载大文件的内容）。</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>Git LFS 通过将仓库中的大文件替换为微小的<em>指针（pointer）</em> 文件来做到这一点。在正常使用期间，你将永远不会看到这些指针文件，因为它们是由 Git LFS 自动处理的：</p><ol><li><p>当你添加（<strong>执行 git add 命令</strong>）一个文件到你的仓库时，Git LFS 用一个指针替换其内容，并将文件内容存储在本地 Git LFS 缓存中（<strong>本地 Git LFS 缓存位于仓库的.git/lfs/objects 目录中</strong>）。</p><p><img src="/Tools/Git/git原理简介/v2-ba2b7ea48f0a48396fe656657ee19682_b.jpg" alt="img" style="zoom: 67%;"></p></li><li><p>当你推送新的提交到服务器时，新推送的提交引用的所有 Git LFS 文件都会从本地 Git LFS 缓存传输到绑定到 Git 仓库的远程 Git LFS 存储（<strong>即 LFS 文件内容会直接从本地 Git LFS 缓存传输到远程 Git LFS 存储服务器</strong>）。</p><p><img src="/Tools/Git/git原理简介/v2-546c2213c530bb6b1e61c377d5225a16_b.jpg" alt="img" style="zoom:67%;"></p></li><li><p>当你 checkout 一个包含 Git LFS 指针的提交时，指针文件将替换为本地 Git LFS 缓存中的文件，或者从远端 Git LFS 存储区下载。</p></li></ol><p><img src="/Tools/Git/git原理简介/v2-805341628b82fdd7a68876d9e953aa46_b.jpg" alt="img" style="zoom:67%;"></p><h3 id="LFS-的指针文件"><a href="#LFS-的指针文件" class="headerlink" title="LFS 的指针文件"></a>LFS 的指针文件</h3><p><strong>LFS 的指针文件是一个文本文件，存储在 Git 仓库中，对应大文件的内容存储在 LFS 服务器里，而不是 Git 仓库中</strong>。</p><p>下面为一个图片 LFS 文件的指针文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">version https://git-lfs.github.com/spec/v1</span><br><span class="line">oid sha256:5b62e134d2478ae0bbded57f6be8f048d8d916cb876f0656a8a6d1363716d999</span><br><span class="line">size 285</span><br></pre></td></tr></table></figure><p><strong>指针文件很小，小于 1KB。其格式为 key-value 格式，第一行为指针文件规范 URL，第二行为文件的对象 id，也即 LFS 文件的存储对象文件名，可以在.git/lfs/objects 目录中找到该文件的存储对象，第三行为文件的实际大小（单位为字节）。所有 LFS 指针文件都是这种格式。</strong></p><p>Git LFS 是无缝的：在你的工作副本中，你只会看到实际的文件内容。这意味着你不需要更改现有的 Git 工作流程就可以使用 Git LFS。你只需按常规进行 git checkout、编辑文件、git add 和 git commit。git clone 和 git pull 将明显更快，因为你只下载实际检出的提交所引用的大文件版本，而不是曾经存在过的文件的每一个版本。</p><p>为了使用 Git LFS，你将需要一个支持 Git LFS 的托管服务器，例如github。用户将需要<strong><a href="https://git-lfs.github.com/" target="_blank" rel="noopener">安装 Git LFS 命令行客户端</a></strong>，或支持 Git LFS 的 GUI 客户端，例如<strong><a href="https://www.sourcetreeapp.com/" target="_blank" rel="noopener">Sourcetree</a></strong>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://stackoverflow.com/a/12527561/15304315" target="_blank" rel="noopener">What’s the difference between HEAD^ and HEAD~ in Git?</a><br><a href="https://stackoverflow.com/a/29120883/15304315" target="_blank" rel="noopener">What’s the difference between HEAD^ and HEAD~ in Git?</a><br><a href="https://www.cnblogs.com/wannananana/p/12059806.html" target="_blank" rel="noopener">【git】git中使用https和ssh协议的区别以及它们的用法</a><br><a href="https://jingsam.github.io/2018/06/03/git-objects.html" target="_blank" rel="noopener">Git内部原理之Git对象</a><br><a href="https://jingsam.github.io/2018/06/15/git-storage.html" target="_blank" rel="noopener">Git内部原理之Git对象存储</a><br><a href="https://jingsam.github.io/2018/06/09/git-hash.html" target="_blank" rel="noopener">Git内部原理之Git对象哈希</a><br><a href="https://jingsam.github.io/2018/10/12/git-reference.html" target="_blank" rel="noopener">Git内部原理之Git引用</a><br><a href="https://www.jianshu.com/p/4cc3959dd6c2" target="_blank" rel="noopener">【学了就忘】Git原理 — 58.详解.git目录（二）</a><br><a href="https://www.jianshu.com/p/23012e3f9519" target="_blank" rel="noopener">【学了就忘】Git原理 — 57.详解.git目录（一）</a><br><a href="https://zhuanlan.zhihu.com/p/106243588" target="_blank" rel="noopener">通过 .git 目录深入理解 Git！</a><br><a href="https://juejin.cn/post/7082011668984627213" target="_blank" rel="noopener">Git｜探寻Git如何管理文件版本的小秘密</a><br><a href="https://blog.csdn.net/start_mao/article/details/94609238" target="_blank" rel="noopener">Git——.git目录详解</a><br><a href="https://juejin.cn/post/6844903986839945229" target="_blank" rel="noopener">解析.git文件夹，深入了解git内部原理</a><br><a href="https://git-scm.com/book/en/v2/Git-Internals-Maintenance-and-Data-Recovery" target="_blank" rel="noopener">10.7 Git Internals - Maintenance and Data Recovery</a><br><a href="https://zhuanlan.zhihu.com/p/146683392" target="_blank" rel="noopener">详解 Git 大文件存储（Git LFS）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在学习git的时候，感觉&lt;a href=&quot;https://jingsam.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;jingsam&lt;/a&gt;写的很不错，边学习边理解一下。强烈建立看原文，作者很牛掰。&lt;/p&gt;
&lt;h2 id=&quot;Git对象&quot;&gt;&lt;a href=&quot;#Git对象&quot; class=&quot;headerlink&quot; title=&quot;Git对象&quot;&gt;&lt;/a&gt;Git对象&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;从根本上来讲，Git是一个内容寻址的文件系统，其次才是一个版本控制系统。&lt;/strong&gt;记住这点，对于理解Git的内部原理及其重要。所谓“内容寻址的文件系统”，意思是&lt;strong&gt;根据文件内容的hash码来定位文件&lt;/strong&gt;。这就意味着同样内容的文件，在这个文件系统中会指向同一个位置，不会重复存储。&lt;/p&gt;
&lt;p&gt;Git对象包含三种：数据对象、树对象、提交对象。Git文件系统的设计思路与linux文件系统相似，即将&lt;strong&gt;文件的内容与文件的属性分开存储&lt;/strong&gt;，文件内容以“装满字节的袋子”存储在文件系统中，文件名、所有者、权限等文件属性信息则另外开辟区域进行存储。在Git中，&lt;strong&gt;数据对象相当于文件内容，树对象相当于文件目录树，提交对象则是对文件系统的快照。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面的章节，会分别对每种对象进行说明。开始说明之前，先初始化一个Git文件系统：&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://www.zdaiot.com/categories/Tools/"/>
    
      <category term="Git" scheme="https://www.zdaiot.com/categories/Tools/Git/"/>
    
    
      <category term="github" scheme="https://www.zdaiot.com/tags/github/"/>
    
      <category term="git" scheme="https://www.zdaiot.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>bash属性、startup文件与history</title>
    <link href="https://www.zdaiot.com/Linux/%E7%BB%B4%E6%8A%A4/bash%E5%B1%9E%E6%80%A7%E3%80%81startup%E6%96%87%E4%BB%B6%E4%B8%8Ehistory/"/>
    <id>https://www.zdaiot.com/Linux/维护/bash属性、startup文件与history/</id>
    <published>2023-03-14T07:14:18.000Z</published>
    <updated>2023-03-14T07:14:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bash与shell区别"><a href="#bash与shell区别" class="headerlink" title="bash与shell区别"></a>bash与shell区别</h2><p>shell有多个含义：</p><ul><li>Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。</li><li>Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。</li></ul><p>bash（GNU Bourne-Again Shell）是最常用的一种shell，是当前大多数Linux发行版的默认Shell。除了bash外，其它的shell还有zsh、sh等。</p><blockquote><p>sh的全名是Bourne Shell。名字中的玻恩就是这个Shell的作者。而bash的全名是Bourne Again Shell。最开始在Unix系统中流行的是sh，而bash作为sh的改进版本，提供了更加丰富的功能。一般来说，都推荐使用bash作为默认的Shell。</p></blockquote><p><strong>如何查看当前系统中默认shell？</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$SHELL</span></span><br></pre></td></tr></table></figure><p>当前正在使用的 Shell 不一定是默认 Shell，一般来说，<code>ps</code>命令结果的倒数第二行是当前 Shell。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ps</span><br><span class="line">  PID TTY          TIME CMD</span><br><span class="line"> 4467 pts/0    00:00:00 bash</span><br><span class="line"> 5379 pts/0    00:00:00 ps</span><br></pre></td></tr></table></figure><p>Shell相当于是一个翻译，把我们在计算机上的操作或我们的命令，翻译为计算机可识别的二进制命令，传递给内核，以便调用计算机硬件执行相关的操作；同时，计算机执行完命令后，再通过Shell翻译成自然语言，呈现在我们面前。</p><p><img src="/Linux/维护/bash属性、startup文件与history/v2-6af56fdefc44ea333fa6f1409b4a72dc_b.jpg" alt="img" style="zoom:50%;"></p><h2 id="bash运行方式"><a href="#bash运行方式" class="headerlink" title="bash运行方式"></a>bash运行方式</h2><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><p>Linux shell是用户与Linux系统进行交互的媒介，而bash作为目前Linux系统中最常用的shell，它在运行时具有两种属性，即“<strong>交互</strong>”与“<strong>登陆</strong>”。</p><ul><li>交互式，是shell的一种运行模式，交互式shell等待你输入命令，并且立即执行，然后将结果反馈给你。这是每个CLI用户都非常熟悉的流程：登录、执行一些命令、登出。当你登出后，这个shell就终止了。</li><li>而非交互式，是shell的另一种运行模式，它专门被用来执行预先设定的命令。在这种模式下，shell不与用户进行交互，而是读取存放在脚本文件中的命令并执行它们。当它读到文件的结尾，这个shell就终止了。</li></ul><p>那么我们启动的时候，如何才能进行进行交互或登陆呢？</p><p>根据bash手册上的描述：</p><blockquote><p>An interactive shell is one started without non-option arguments and without the -c option whose standard input and error are both connected to terminals (as determined by isatty(3)), or one started with the -i option.</p></blockquote><p>从上面的描述看，只要执行bash命令的时候，不带有“选项以外的参数”或者-c选项，就会启动一个交互式shell。</p><ul><li>“选项以外的参数”指的就是shell的脚本文件；</li><li>-c选项将指定字符串作为命令读入bash，也就相当于执行指定的命令，它和前者有些类似，只是不从脚本文件中读取罢了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[chen@localhost Temp]$ <span class="built_in">echo</span> <span class="string">"uname -r; date"</span> &gt; script.sh</span><br><span class="line">[chen@localhost Temp]$ bash ./script.sh <span class="comment"># 非交互式</span></span><br><span class="line">3.10.0-514.el7.x86_64</span><br><span class="line">Tue Apr 18 14:43:50 CST 2017</span><br><span class="line">[chen@localhost Temp]$ </span><br><span class="line">[chen@localhost Temp]$ bash -c <span class="string">"uname -r; date"</span>  <span class="comment"># 非交互式</span></span><br><span class="line">3.10.0-514.el7.x86_64</span><br><span class="line">Tue Apr 18 14:44:49 CST 2017</span><br><span class="line">[chen@localhost Temp]$</span><br></pre></td></tr></table></figure><p>另外，从上面描述来看，通常来说，用于执行脚本的shell都是“非交互式”的，但我们也有办法把它启动为“交互式”shell，方法就是在执行bash命令时，添加<code>-i</code>选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[chen@localhost Temp]$ bash -c <span class="string">"echo \$-"</span></span><br><span class="line">hBc</span><br><span class="line"><span class="comment"># 我们看到，添加了-i选项的bash -c命令为我们启动了一个“交互式”shell。</span></span><br><span class="line">[chen@localhost Temp]$ bash -i -c <span class="string">"echo \$-"</span>  <span class="comment"># 交互式</span></span><br><span class="line">himBHc</span><br></pre></td></tr></table></figure><blockquote><p>这里解释一下<code>echo \$-</code>：It shows your Builtin Set Flags. <code>man bash</code> then look for SHELL BUILTIN COMMANDS and then look for the <code>set subsection</code>. You will find the meanings of all those flags:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; h: Remember the location of commands as they are looked up for execution.  This is enabled by default.</span><br><span class="line">&gt; i: interactive</span><br><span class="line">&gt; m: Monitor mode.  Job control is enabled</span><br><span class="line">&gt; B: The shell performs brace expansion (see Brace Expansion above).  This is on by default</span><br><span class="line">&gt; H: Enable !  style history substitution.  This option is on by default when the shell is interactive.</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h3 id="判断"><a href="#判断" class="headerlink" title="判断"></a>判断</h3><p>那么我们如何在shell脚本或者startup文件中判断当前shell的运行方式呢？</p><p>我们首先来看，bash手册的描述：</p><blockquote><p><code>PS1</code> is set and <code>$-</code> includes <code>i</code> if bash is interactive, allowing a shell script or a startup file to test this state.</p></blockquote><p>也就是说，可以判断变量<code>PS1</code>是否有值，或者判断变量<code>$-</code>是否包含<code>i</code>，实现在shell脚本或者startup文件中判断当前shell的运行方式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在shell脚本中写入如下语句，通过输出判断当前shell运行方式</span></span><br><span class="line">[chen@localhost Temp]$ cat ./test1.sh </span><br><span class="line"><span class="built_in">echo</span> <span class="string">"\$0   : <span class="variable">$0</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"\$-   : $-"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"\$PS1 : <span class="variable">$PS1</span>"</span></span><br><span class="line">[chen@localhost Temp]$ bash ./test1.sh     <span class="comment"># 非交互式shell</span></span><br><span class="line"><span class="variable">$0</span>   : ./test1.sh</span><br><span class="line">$-   : hB</span><br><span class="line"><span class="variable">$PS1</span> : </span><br><span class="line">[chen@localhost Temp]$ bash -i ./test1.sh  <span class="comment"># 交互式shell</span></span><br><span class="line"><span class="variable">$0</span>   : ./test1.sh</span><br><span class="line">$-   : himB</span><br><span class="line"><span class="variable">$PS1</span> : [\u@\h \W]\$</span><br></pre></td></tr></table></figure><h2 id="登陆shell与非登陆shell"><a href="#登陆shell与非登陆shell" class="headerlink" title="登陆shell与非登陆shell"></a>登陆shell与非登陆shell</h2><h3 id="含义-1"><a href="#含义-1" class="headerlink" title="含义"></a>含义</h3><p>“登陆shell”通常指的是：</p><ol><li>用户通过输入用户名/密码（或证书认证）后启动的shell；</li><li>通过带有<code>-l|--login</code>参数的<code>bash</code>命令启动的shell。例如，系统启动、远程登录、使用<code>su -</code>切换用户、通过<code>bash --login</code>命令启动bash等。</li></ol><p>而其他情况启动的shell基本上就都是“非登陆shell”了。例如，从图形界面启动终端、使用<code>su</code>切换用户、通过<code>bash</code>命令启动bash等。</p><h3 id="判断-1"><a href="#判断-1" class="headerlink" title="判断"></a>判断</h3><p>根据bash手册上的描述：</p><blockquote><p>A login shell is one whose first character of argument zero is a <code>-</code>, or one started with the <code>--login</code> option.</p></blockquote><p>我们可以通过在shell中<code>echo $0</code>查看，显示<code>-bash</code>的一定是“登陆shell”，反之显示<code>bash</code>的则不好说。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[chen@localhost ~]$ bash --login</span><br><span class="line">[chen@localhost ~]$ <span class="built_in">echo</span> <span class="variable">$0</span></span><br><span class="line">bash</span><br></pre></td></tr></table></figure><p>可以看出，使用<code>bash --login</code>启动的“登陆shell”，其<code>$0</code>也并非以<code>-</code>开头，这也就是为什么手册上的描述里使用“or”的原因。</p><p>另外，当我们执行<code>exit</code>命令退出shell时，也可以观察到它们的不同之处：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[chen@localhost ~]$ bash --login</span><br><span class="line">[chen@localhost ~]$ <span class="built_in">exit</span>   <span class="comment"># 退出登陆shell</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">[chen@localhost ~]$ bash</span><br><span class="line">[chen@localhost ~]$ <span class="built_in">exit</span>   <span class="comment"># 退出非登陆shell</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure><p>原则上讲，我们使用<code>logout</code>退出“登陆shell”，使用<code>exit</code>退出“非登录shell”。但其实<code>exit</code>命令会判断当前shell的“登陆”属性，并分别调用<code>logout</code>或<code>exit</code>指令，因此使用起来相对方便。</p><h3 id="主要区别"><a href="#主要区别" class="headerlink" title="主要区别"></a>主要区别</h3><p>对于用户而言，“登录shell”和“非登陆shell”的主要区别在于<strong>启动shell时所执行的startup文件不同</strong>。</p><p>简单来说，“登录shell”执行的startup文件为<code>~/.bash_profile</code>，而“非登陆shell”执行的startup文件为<code>~/.bashrc</code>。</p><p>下面我们进行详细说明。</p><h2 id="bash的startup文件"><a href="#bash的startup文件" class="headerlink" title="bash的startup文件"></a>bash的startup文件</h2><p>它支持的startup文件也并不单一，甚至容易让人感到费解。接下来以<strong>CentOS7</strong>系统为例，对bash的startup文件进行一些必要的梳理和总结。</p><p>根据bash手册的描述：</p><blockquote><ul><li><p>/etc/profile<br>The systemwide initialization file, executed for login shells</p></li><li><p>/etc/bash.bash_logout<br>The systemwide login shell cleanup file, executed when a login shell exits</p></li><li><p>~/.bash_profile<br>The personal initialization file, executed for login shells</p></li><li><p>~/.bashrc<br>The individual per-interactive-shell startup file</p></li><li><p>~/.bash_logout<br>The individual login shell cleanup file, executed when a login shell exits</p></li></ul></blockquote><p>此外，bash还支持<code>~/.bash_login</code>和<code>~/.profile</code>文件，作为对其他shell的兼容，它们与<code>~/.bash_profile</code>文件的作用是相同的。</p><p>备注：Debian系统会使用<code>~/.profile</code>文件取代<code>~/.bash_profile</code>文件，因此在相关细节上，会与CentOS略有不同。</p><h3 id="profile与rc系列"><a href="#profile与rc系列" class="headerlink" title="profile与rc系列"></a>profile与rc系列</h3><p>通过名字的不同，我们可以直观地将startup文件分为“profile”与“rc”两个系列，其实他们的功能都很类似，但是使用的场景不同，这也是大家最容易忽略的地方。</p><p>所谓的不同场景，其实就是shell的运行模式。我们知道运行中的bash有“交互”和“登陆”两种属性，而执行“profile”系列还是“rc”系列，就与shell的这两个属性有关。</p><p><strong>原理上讲，“登陆shell”启动时会加载“profile”系列的startup文件，而“交互式非登陆shell”启动时会加载“rc”系列的startup文件。</strong></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于“登录shell”而言，“交互式”执行“登陆”和“登出”相关的“profile”系列startup文件，“非交互式”只执行“登陆”相关的“profile”系列startup文件；对于“非登陆shell”而言，“交互式”执行“rc”系列的startup文件，而“非交互式”执行的配置文件由环境变量<code>BASH_ENV</code>指定。</p><p>Linux中startup文件区分全局和个人：全局startup文件放在<code>/etc</code>目录下，用于设置所有用户共同的配置，除非你清楚地知道你在做的事情，否则不要轻易改动它们；个人startup文件放在<code>~</code>目录下，用于设置某个用户的个性化配置。</p><p><code>~/.bash_profile</code>会显式调用<code>~/.bashrc</code>文件，而<code>~/.bashrc</code>又会显式调用<code>/etc/bashrc</code>文件，这是为了让所有交互式界面看起来一样。无论你是从远程登录（登陆shell），还是从图形界面打开终端（非登陆shell），你都拥有相同的提示符，因为环境变量<code>PS1</code>在<code>/etc/bashrc</code>文件中被统一设置过。</p><p>下面我来对startup文件进行一个完整的总结：</p><div class="table-container"><table><thead><tr><th>startup文件</th><th>交互登陆</th><th>非交互登陆</th><th>交互非登陆</th><th>非交互非登陆</th></tr></thead><tbody><tr><td>/etc/profile</td><td>直接执行1</td><td>直接执行1</td><td>-</td><td>-</td></tr><tr><td>~/.bash_profile</td><td>直接执行2</td><td>直接执行2</td><td>-</td><td>-</td></tr><tr><td>~/.bash_login</td><td>条件执行2</td><td>条件执行2</td><td>-</td><td>-</td></tr><tr><td>~/.profile</td><td>条件执行2</td><td>条件执行2</td><td>-</td><td>-</td></tr><tr><td>~/.bash_logout</td><td>直接执行3</td><td>不执行</td><td>-</td><td>-</td></tr><tr><td>/etc/bash.bash_logout</td><td>直接执行4</td><td>不执行</td><td>-</td><td>-</td></tr><tr><td>~/.bashrc</td><td>引用执行2.1</td><td>引用执行2.1</td><td>直接执行1</td><td>-</td></tr><tr><td>/etc/bashrc</td><td>引用执行2.2</td><td>引用执行2.2</td><td>引用执行1.1</td><td>-</td></tr></tbody></table></div><p>备注：</p><ol><li>“直接执行”表示此文件被系统直接调用，它的执行是无条件的；</li><li>“条件执行”表示此文件被系统调用是有先决条件的（没有优先级更高的文件可用）；</li><li>“引用执行”表示此文件不是被系统直接调用的，而是被其他文件显式调用的；</li><li>后面的数字表示文件被调用的顺序，数字越大调用越靠后；</li><li>“非交互非登陆”shell的配置文件可以由<code>BASH_ENV</code>环境变量指定；</li></ol><p>如果你想对bash的功能进行设置或者是定义一些别名，推荐你修改<code>~/.bashrc</code>文件，这样无论你以何种方式打开shell，你的配置都会生效。而如果你要更改一些环境变量，推荐你修改<code>~/.bash_profile</code>文件，因为考虑到shell的继承特性，这些更改确实只应该被执行一次（而不是多次）。针对所有用户进行全局设置，推荐你在<code>/etc/profile.d</code>目录下添加以<code>.sh</code>结尾的文件，而不是去修改全局startup文件。</p><blockquote><p>具体更加详细的解释可以参考文章<a href="https://blog.csdn.net/sch0120/article/details/70256318" target="_blank" rel="noopener">关于“.bash_profile”和“.bashrc”区别的总结</a>。</p></blockquote><h2 id="bash-history"><a href="#bash-history" class="headerlink" title="bash history"></a>bash history</h2><p>默认情况下, bash 只在退出的时候更新命令历史, 而且这个”更新”是用新版直接覆盖旧版。这会使你无法保持一份完整的命令历史记录, 原因有两个：</p><ul><li>如果一个用户登录多次，这种覆盖的机制会使得只有最后一个退出的 bash 能保存它的历史记录。(一个登录的用户打开多个终端模拟器, 或者使用 screen/tmux 等工具启动多个 bash 等也在此列 )</li><li>如果你的 bash 异常退出了 – 比如网络故障，防火墙更改，或者它的进程被杀掉了 – 会话中所有的历史记录都会丢失。</li></ul><p>你设置在 .bashrc 文件中添加下面这句就够了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">shopt</span> -s histappend</span><br></pre></td></tr></table></figure><p>它让 shell 退出时是添加新记录，而不是覆盖原来的文件。这样你关闭多个终端时就不会挨个覆盖了。</p><p>顺便，你也许想把历史记录保存条数设置大一点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置历史记录条数</span></span><br><span class="line"><span class="built_in">export</span> HISTFILESIZE=100000</span><br><span class="line"><span class="comment"># 设置显示历史记录条数</span></span><br><span class="line"><span class="built_in">export</span> HISTSIZE=10000</span><br></pre></td></tr></table></figure><p>另外下面非常推荐设置，可以让你能够用方向键翻阅历史</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">bind</span> <span class="string">'"\e[A": history-search-backward'</span></span><br><span class="line"><span class="built_in">bind</span> <span class="string">'"\e[B": history-search-forward'</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.runoob.com/linux/linux-shell.html" target="_blank" rel="noopener">Shell 教程</a><br><a href="https://wangdoc.com/bash/intro" target="_blank" rel="noopener">Bash 简介</a><br><a href="https://zhuanlan.zhihu.com/p/56532223" target="_blank" rel="noopener">Bash编程入门-1：Shell与Bash</a><br><a href="https://blog.csdn.net/sch0120/article/details/70226903" target="_blank" rel="noopener">关于“交互式-非交互式”与“登录-非登陆”shell的总结</a><br><a href="https://stackoverflow.com/questions/35432562/why-does-running-echo-output-himbh-on-the-bash-shell" target="_blank" rel="noopener">Why does running “echo $-“ output “himBH” on the bash shell?</a><br><a href="https://blog.csdn.net/sch0120/article/details/70256318" target="_blank" rel="noopener">关于“.bash_profile”和“.bashrc”区别的总结</a><br><a href="https://www.zhihu.com/question/19863362/answer/37672858" target="_blank" rel="noopener">bash 下 history 会因多个终端而覆盖丢失，有好的解决方案吗？ - Zhou Zhao的回答 - 知乎</a><br><a href="https://felixc.at/2013/09/how-to-avoid-losing-any-history-lines/" target="_blank" rel="noopener">[译] 如何防止丢失任何 bash 历史命令?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;bash与shell区别&quot;&gt;&lt;a href=&quot;#bash与shell区别&quot; class=&quot;headerlink&quot; title=&quot;bash与shell区别&quot;&gt;&lt;/a&gt;bash与shell区别&lt;/h2&gt;&lt;p&gt;shell有多个含义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。&lt;/li&gt;
&lt;li&gt;Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;bash（GNU Bourne-Again Shell）是最常用的一种shell，是当前大多数Linux发行版的默认Shell。除了bash外，其它的shell还有zsh、sh等。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sh的全名是Bourne Shell。名字中的玻恩就是这个Shell的作者。而bash的全名是Bourne Again Shell。最开始在Unix系统中流行的是sh，而bash作为sh的改进版本，提供了更加丰富的功能。一般来说，都推荐使用bash作为默认的Shell。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/categories/Linux/"/>
    
      <category term="维护" scheme="https://www.zdaiot.com/categories/Linux/%E7%BB%B4%E6%8A%A4/"/>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/tags/Linux/"/>
    
      <category term="bashrc" scheme="https://www.zdaiot.com/tags/bashrc/"/>
    
      <category term="bash_profile" scheme="https://www.zdaiot.com/tags/bash-profile/"/>
    
  </entry>
  
  <entry>
    <title>决策树（下）—— XGBoost、LightGBM</title>
    <link href="https://www.zdaiot.com/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89%E2%80%94%E2%80%94%20XGBoost%E3%80%81LightGBM/"/>
    <id>https://www.zdaiot.com/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/</id>
    <published>2022-10-31T15:48:11.000Z</published>
    <updated>2022-10-31T15:48:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要参考了<a href="https://www.zhihu.com/people/is-aze" target="_blank" rel="noopener">阿泽</a>作者的笔记，对于不理解的地方，我会添加个人注释。</p><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是目标函数的定义。故本文将从数学原理和工程实现上进行介绍，并在最后介绍下 Xgboost 的优点。</p><h3 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>我们知道 XGBoost 是由 k 个基模型组成的一个加法运算式：</p><script type="math/tex; mode=display">\hat{y}_i=\sum_{t=1}^{k}\ f_t(x_i) \\</script><p>其中 $f_k$ 为第 $k$ 个基模型， $\hat{y}_i$ 为第 $i$ 个样本的预测值。</p><p>损失函数可由预测值 $\hat{y}_i$ 与真实值 $y_i$ 进行表示：</p><script type="math/tex; mode=display">L=\sum_{i=1}^n l( y_i, \hat{y}_i) \\</script><p>其中 $n$ 为样本数量。</p><p>我们知道模型的预测精度由模型的偏差和方差共同决定，<strong>损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的损失函数 $L$ 与抑制模型复杂度的正则项 $\Omega$ 组成</strong>，所以我们有：</p><script type="math/tex; mode=display">Obj =\sum_{i=1}^n l(\hat{y}_i, y_i) + \sum_{t=1}^k \Omega(f_t) \\</script><p>$\Omega$ 为模型的正则项，由于 XGBoost 支持决策树也支持线性模型，所以这里再不展开描述。</p><p>我们知道 boosting 模型是前向加法，以第 $t$ 步的模型为例，模型对第 $i$ 个样本 $x_{i}$ 的预测为：</p><script type="math/tex; mode=display">\hat{y}_i^t= \hat{y}_i^{t-1} + f_t(x_i) \\</script><p>其中 $\hat{y}_i^{t-1}$ 由第 $t-1$ 步的模型给出的预测值，是已知常数，$f_t(x_i)$ 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成：</p><script type="math/tex; mode=display">\begin{align} Obj^{(t)} &= \sum_{i=1}^nl(y_i, \hat{y}_i^t) + \sum_{i=1}^t\Omega(f_i) \\ &= \sum_{i=1}^n l\left(y_i, \hat{y}_i^{t-1} + f_t(x_i) \right) + \sum_{i=1}^t \Omega(f_i) \end{align} \\</script><p>求此时最优化目标函数，就相当于求解 $f_t(x_i)$ 。</p><blockquote><p>泰勒公式是将一个在 $x=x_0$ 处具有 $n$ 阶导数的函数 $f(x)$ 利用关于 $x-x_0$ 的 $n$ 次多项式来逼近函数的方法，若函数 $f(x)$ 在包含 $x_0$ 的某个闭区间 [a,b] 上具有 $n$ 阶导数，且在开区间 (a,b) 上具有 $n+1$ 阶导数，则对闭区间 [a,b] 上任意一点 $x$ 有 $\displaystyle f(x)=\sum_{i=0}^{n}\frac{f^{(i)}(x_0)}{i!}(x-x_0)^ i+R_n(x)$ ，其中的多项式称为函数在 $x_0$ 处的泰勒展开式， $R_n(x)$ 是泰勒公式的余项且是 $(x−x_0)^n$ 的高阶无穷小。</p></blockquote><p>根据泰勒公式我们把函数 $f(x+\Delta x)$ 在点 $x$ 处进行泰勒的二阶展开，可得到如下等式：</p><script type="math/tex; mode=display">f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac12 f''(x)\Delta x^2 \\</script><p>我们把 $\hat{y}_i^{t-1}$ 视为 $x$ ， $f_t(x_i)$ 视为 $\Delta x$ ，故可以将目标函数写为：</p><script type="math/tex; mode=display">Obj^{(t)} = \sum_{i=1}^n \left[ l(y_i, \hat{y}_i^{t-1}) + g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \sum_{i=1}^t \Omega(f_i) \\</script><p>其中 $g_{i}$ 为损失函数的一阶导， $h_{i}$ 为损失函数的二阶导，<strong>注意这里的导是对 $\hat{y}_i^{t-1}$ 求导</strong>。</p><p>我们以平方损失函数为例：</p><script type="math/tex; mode=display">\sum_{i=1}^n \left(y_i - (\hat{y}_i^{t-1} + f_t(x_i)) \right)^2 \\</script><p>则：</p><script type="math/tex; mode=display">\begin{align} g_i &= \frac{\partial (\hat{y}^{t-1} - y_i)^2}{\partial {\hat{y}^{t-1} } } = 2(\hat{y}^{t-1} - y_i) \\ h_i &=\frac{\partial^2(\hat{y}^{t-1} - y_i)^2}{ {\hat{y}^{t-1} } } = 2 \end{align} \\</script><p>由于在第 $t$ 步时 $\hat{y}_i^{t-1}$ 其实是一个已知的值，所以 $l(y_i, \hat{y}_i^{t-1})$ 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成：</p><script type="math/tex; mode=display">Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \sum_{i=1}^t \Omega(f_i) \\</script><p>所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 $\hat{y}^{t-1}$ 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 $f(x)$ ，最后根据加法模型得到一个整体模型。</p><h4 id="基于决策树的目标函数"><a href="#基于决策树的目标函数" class="headerlink" title="基于决策树的目标函数"></a>基于决策树的目标函数</h4><p>我们知道 Xgboost 的基模型<strong>不仅支持决策树，还支持线性模型</strong>，这里我们主要介绍基于决策树的目标函数。</p><p>我们可以将决策树定义为 $f_t(x)=w_{q(x)}$ ，其中$w \in \mathbf{R}^{T}, q: \mathbf{R}^{d} \rightarrow\{1,2, \cdots, T\}$，$t$表示boosting在进行前向加法时的第$t$个模型。 $x$ 为某一样本，这里的 $q(x)$ 代表了该样本在哪个叶子结点上，而 $w_q$ 则代表了叶子结点取值 $w$ ，所以 $w_{q(x)}$ 就代表了每个样本的取值 $w$ （即预测值）。如下图所示：</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/image-20221101221923407.png" alt="image-20221101221923407" style="zoom: 33%;"></p><p>决策树的复杂度可由叶子数 $T$ 组成，叶子节点越少模型越简单，此外叶子节点也不应该含有过高的权重 $w$ （类比 LR 的每个变量的权重），所以目标函数的正则项可以定义为：</p><script type="math/tex; mode=display">\Omega(f_t)=\gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 \\</script><p>即决策树模型的复杂度由生成的所有决策树的叶子节点数量，和所有节点权重所组成的向量的 $L_2$ 范式共同决定。下图给出了基于决策树的 XGBoost 的正则项的求解方式。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/image-20221101222030743.png" alt="image-20221101222030743" style="zoom:40%;"></p><p>我们设 $I_j= \{ i \vert q(x_i)=j \}$ 为第 $j$ 个叶子节点的样本集合，故我们的目标函数可以写成：</p><script type="math/tex; mode=display">\begin{align} Obj^{(t)} &\approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) \\ &= \sum_{i=1}^n \left[ g_iw_{q(x_i)} + \frac12h_iw_{q(x_i)}^2 \right] + \gamma T + \frac12 \lambda \sum_{j=1}^Tw_j^2 \\ &= \sum_{j=1}^T \left[(\sum_{i \in I_j}g_i)w_j + \frac12(\sum_{i \in I_j}h_i + \lambda)w_j^2 \right] + \gamma T \end{align} \\</script><p>第二步到第三步可能看的不是特别明白，这边做些解释：第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后在求损失函数。即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 $\sum_{i \in I_j}g_i$ 和 $\sum_{i \in I_j}h_i$ 这两项， $w_j$ 为第 $j$ 个叶子节点取值。</p><p>为简化表达式，我们定义 $G_j=\sum_{i \in I_j}g_i ， H_j=\sum_{i \in I_j}h_i$ ，则目标函数为：</p><script type="math/tex; mode=display">Obj^{(t)} = \sum_{j=1}^T \left[G_jw_j + \frac12(H_j + \lambda)w_j^2 \right] + \gamma T \\</script><p>这里我们要注意 $G_j $和 $H_j$ 是前 t-1 步得到的结果，其值已知可视为常数，只有最后一棵树的叶子节点 $w_j$ 不确定，那么将目标函数对 $w_j$ 求一阶导，并令其等于 0 ，则可以求得叶子结点 $j$ 对应的权值：</p><script type="math/tex; mode=display">w_j^*=-\frac{G_j}{H_j+\lambda} \\</script><p>所以目标函数可以化简为：</p><script type="math/tex; mode=display">Obj = -\frac12 \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \\</script><p>下图给出目标函数计算的例子，求每个节点每个样本的一阶导数 $g_i$ 和二阶导数 $h_i$ ，然后针对每个节点对所含样本求和得到的 $G_j$ 和 $H_j$ ，最后遍历决策树的节点即可得到目标函数。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/image-20221101222937890.png" alt="image-20221101222937890" style="zoom:50%;"></p><h4 id="最优切分点划分算法"><a href="#最优切分点划分算法" class="headerlink" title="最优切分点划分算法"></a>最优切分点划分算法</h4><p>在决策树的生长过程中，一个非常关键的问题是如何找到叶子的节点的最优切分点，Xgboost 支持两种分裂节点的方法——贪心算法和近似算法。</p><p><strong>1）贪心算法</strong></p><ol><li>从深度为 0 的树开始，对每个叶节点枚举所有的可用特征；</li><li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；</li><li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集</li><li>回到第 1 步，递归执行到满足特定条件为止</li></ol><p>那么如何计算每个特征的分裂收益呢？</p><p>假设我们在某一节点完成特征分裂，则分裂前的目标函数可以写为：</p><script type="math/tex; mode=display">Obj_{1} =-\frac12 [\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}] + \gamma \\</script><p>分裂后的目标函数为：</p><script type="math/tex; mode=display">Obj_2 = -\frac12 [ \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda}] +2\gamma \\</script><p>则对于目标函数来说，分裂后的收益为：</p><script type="math/tex; mode=display">Gain=\frac12 \left[ \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \\</script><p>注意该特征收益也可作为特征重要性输出的重要依据。</p><p>对于每次分裂，我们都需要枚举所有特征可能的分割方案，如何高效地枚举所有的分割呢？</p><p>我假设我们要枚举所有 $x &lt; a$ 这样的条件，对于某个特定的分割点 $a$ 我们要计算 $a$ 左边和右边的导数和。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/image-20221101224051238.png" alt="image-20221101224051238" style="zoom:50%;"></p><p>我们可以发现对于所有的分裂点 $a$ ，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 $G_L$ 和 $G_R$ 。然后用上面的公式计算每个分割方案的分数就可以了。</p><p>观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入新叶子的惩罚项，也就是说引入的分割带来的增益如果小于一个阀值的时候，我们可以剪掉这个分割。</p><p><strong>2）近似算法</strong></p><p>贪婪算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪婪算法这一缺点给出了近似最优解。</p><p>对于每个特征，只考察分位点可以减少计算复杂度。</p><p>该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。</p><p>在提出候选切分点时有两种策略：</p><ul><li>Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</li><li>Local：每次分裂前将重新提出候选切分点。</li></ul><p>直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分所以需要更多的候选点。</p><p>下图给出不同种分裂策略的 AUC 变换曲线，横坐标为迭代次数，纵坐标为测试集 AUC，eps 为近似算法的精度，其倒数为桶的数量。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-1da040923ad9beaf222a2dd60a8f3752_b.jpg" alt="img" style="zoom: 50%;"></p><p>我们可以看到 Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在 eps 取值合理的情况下，分位数策略可以获得与贪婪算法相同的精度。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-161382c979557b8bae1563a459cd1ed4_b.jpg" alt="img" style="zoom:67%;"></p><ul><li><strong>第一个 for 循环：</strong>对特征 $k$ 根据该特征分布的分位数找到切割点的候选集合 $S_k=\{s_{k1},s_{k2},…,s_{kl} \}$ 。XGBoost 支持 Global 策略和 Local 策略。</li><li><strong>第二个 for 循环：</strong>针对每个特征的候选集合，将样本映射到由该特征对应的候选点集构成的分桶区间中，即 ${s_{k,v}≥x_{jk}&gt;s_{k,v−1} }$ ，对每个桶统计 $G,H$ 值，最后在这些统计量上寻找最佳分裂点。</li></ul><p>下图给出近似算法的具体例子，以三分位为例：</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-5d1dd1673419599094bf44dd4b533ba9_b.jpg" alt="img" style="zoom:67%;"></p><p>根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 $G,H$ 值，最终求解节点划分的增益。</p><h4 id="加权分位数缩略图"><a href="#加权分位数缩略图" class="headerlink" title="加权分位数缩略图"></a>加权分位数缩略图</h4><p>事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 $h_i$ 作为样本的权重进行划分，如下：</p><p>那么问题来了：为什么要用 $h_i$ 进行样本加权？</p><p>我们知道模型的目标函数为：</p><script type="math/tex; mode=display">Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \sum_{i=1}^t \Omega(f_i) \\</script><p>我们稍作整理，便可以看出 $h_i$ 有对 loss 加权的作用。</p><script type="math/tex; mode=display">\begin{align} Obj^{(t)} & \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \sum_{i=1}^t \Omega(f_i) \\ \\ &= \sum_{i=1}^{n} [ g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \color{red}{+ \frac{1}{2}\frac{g_i^2}{h_i} }]+\Omega(f_t) \color{red}{+ C} \\ &= \sum_{i=1}^{n} \color{red}{\frac{1}{2}h_i} \left[ f_t(x_i) - \left( -\frac{g_i}{h_i} \right) \right]^2 + \Omega(f_t) + C \end{align} \\</script><p>其中 $\frac{1}{2}\frac{g_i^2}{h_i}$ 与 $C$ 皆为常数。我们可以看到 $h_i$ 就是平方损失函数中样本的权重。</p><p>对于样本权值相同的数据集来说，找到候选分位点已经有了解决方案（GK 算法），但是当样本权值不一样时，该如何找到候选分位点呢？（作者给出了一个 Weighted Quantile Sketch 算法，这里将不做介绍。）</p><h4 id="稀疏感知算法"><a href="#稀疏感知算法" class="headerlink" title="稀疏感知算法"></a>稀疏感知算法</h4><p>在决策树的第一篇文章中我们介绍 CART 树在应对数据缺失时的分裂策略，XGBoost 也给出了其解决方案。</p><p>XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</p><p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少，下图可以看到稀疏感知算法比 basic 算法速度块了超过 50 倍。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-e065bea4b424ea2d13b25ed2e7004aa8_b.jpg" alt="img" style="zoom:50%;"></p><h3 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h3><h4 id="块结构设计"><a href="#块结构设计" class="headerlink" title="块结构设计"></a>块结构设计</h4><p>我们知道，决策树的学习最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序。而 XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</p><ul><li>每一个块结构包括一个或多个已经排序好的特征；</li><li>缺失特征值将不进行排序；</li><li>每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；</li></ul><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/image-20221101230024796.png" alt="image-20221101230024796" style="zoom:50%;"></p><p><strong>这种块结构存储的特征之间相互独立，方便计算机进行并行计算</strong>。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p><h4 id="缓存访问优化算法"><a href="#缓存访问优化算法" class="headerlink" title="缓存访问优化算法"></a>缓存访问优化算法</h4><p>块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续，这样会造成缓存命中率低，从而影响到算法的效率。</p><p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。</p><p>此外适当调整块大小，也可以有助于缓存优化。</p><h4 id="“核外”块计算"><a href="#“核外”块计算" class="headerlink" title="“核外”块计算"></a>“核外”块计算</h4><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。</p><p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p><ul><li><strong>块压缩：</strong>对 Block 进行按列压缩，并在读取时进行解压；</li><li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li></ul><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li><strong>精度更高：</strong>GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li><li><strong>灵活性更强：</strong>GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li><li><strong>正则化：</strong>XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li><li><strong>Shrinkage（缩减）：</strong>相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li><li><strong>列抽样：</strong>XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li><li><strong>缺失值处理：</strong>XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li><li><strong>可以并行化操作：</strong>块结构可以很好的支持并行计算。</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li><li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</li></ol><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><p>LightGBM 由微软提出，主要用于解决 GDBT 在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。</p><p>从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有训练速度快、内存占用低的特点。下图分别显示了 XGBoost、XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 三者之间针对不同数据集情况下的内存和训练时间的对比：</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-e015e3c4018f44787d74a47c9e0cd040_b.jpg" alt="img"></p><p>那么 LightGBM 到底如何做到更快的训练速度和更低的内存使用的呢？</p><p>我们刚刚分析了 XGBoost 的缺点，LightGBM 为了解决这些问题提出了以下几点解决方案：</p><ol><li>单边梯度抽样算法；</li><li>直方图算法；</li><li>互斥特征捆绑算法；</li><li>基于最大深度的 Leaf-wise 的垂直生长算法；</li><li>类别特征最优分割；</li><li>特征并行和数据并行；</li><li>缓存优化。</li></ol><p>本节将继续从数学原理和工程实现两个角度介绍 LightGBM。</p><h3 id="数学原理-1"><a href="#数学原理-1" class="headerlink" title="数学原理"></a>数学原理</h3><h4 id="单边梯度抽样算法"><a href="#单边梯度抽样算法" class="headerlink" title="单边梯度抽样算法"></a>单边梯度抽样算法</h4><p>GBDT 算法的梯度大小可以反映样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算过程中只需关注梯度高的样本，极大的减少了计算量。</p><p>GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。具体算法如下所示：</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-31e5d8d2d0862eda0c40303b3cba6089_b.jpg" alt="img" style="zoom:50%;"></p><p>我们可以看到 GOSS 事先基于梯度的绝对值对样本进行排序（<strong>无需保存排序后结果</strong>），然后拿到前 a% 的梯度大的样本，和总体样本的 b%，在计算增益时，通过乘上 $\frac{1-a}{b}$ 来放大梯度小的样本的权重。<strong>一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。</strong></p><blockquote><p> 采样之前梯度小的样本数量是$整体数量<em>1-a$，采样之后变为了$整体</em>b$，会影响数据分布，乘以这个系数之后，数量就回到了$1-a$的量级。</p></blockquote><h4 id="直方图算法"><a href="#直方图算法" class="headerlink" title="直方图算法"></a>直方图算法</h4><p><strong>1) 直方图算法</strong></p><p>直方图算法的基本思想是将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。</p><p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：</p><ul><li><strong>内存占用更小：</strong>XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8；</li><li><strong>计算代价更小：</strong>计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从 $O(#data <em> #feature)$ 降低到 $O(k </em> #feature)$ ，而我们知道 $#data &gt;&gt; k$ 。</li></ul><p>虽然将特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但较粗的分割也起到了正则化的效果，一定程度上降低了模型的方差。</p><p><strong>2) 直方图加速</strong></p><p>在构建叶节点的直方图时，我们还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。在实际操作过程中，我们还可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-66982f5386b2e9be3e50a651e01b9c21_b.jpg" alt="img"></p><p><strong>3) 稀疏特征优化</strong></p><p>XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。</p><h4 id="互斥特征捆绑算法"><a href="#互斥特征捆绑算法" class="headerlink" title="互斥特征捆绑算法"></a>互斥特征捆绑算法</h4><p>高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。</p><p>针对这种想法，我们会遇到两个问题：</p><ol><li>哪些特征可以一起绑定？</li><li>特征绑定后，特征值如何确定？</li></ol><p><strong>对于问题一：</strong>EFB 算法利用特征和特征间的关系构造一个加权无向图，并将其转换为图着色算法。我们知道图着色是个 NP-Hard 问题，故采用贪婪算法得到近似解，具体步骤如下：</p><ol><li>构造一个加权无向图，顶点是特征，边是两个特征间互斥程度；</li><li>根据节点的度进行降序排序，度越大，与其他特征的冲突越大；</li><li>遍历每个特征，将它分配给现有特征包，或者新建一个特征包，是的总体冲突最小。</li></ol><p>算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大互斥率 $\gamma$ 来平衡算法的精度和效率。EFB 算法的伪代码如下所示：</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-3eb0ef1f565e344013e8f700fba617da_b.jpg" alt="img" style="zoom:50%;"></p><p>我们看到时间复杂度为 $O(#feature^2)$ ，在特征不多的情况下可以应付，但如果特征维度达到百万级别，计算量则会非常大，为了改善效率，我们提出了一个更快的解决方案：将 EFB 算法中通过构建图，根据节点度来排序的策略改成了根据非零值的技术排序，因为非零值越多，互斥的概率会越大。</p><p><strong>对于问题二：</strong>论文给出特征合并算法，其关键在于原始特征能从合并的特征中分离出来。假设 Bundle 中有两个特征值，A 取值为 [0, 10]、B 取值为 [0, 20]，为了保证特征 A、B 的互斥性，我们可以给特征 B 添加一个偏移量转换为 [10, 30]，Bundle 后的特征其取值为 [0, 30]，这样便实现了特征合并。具体算法如下所示：</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-ea09eac195f8187917685b8139dc45cf_b.jpg" alt="img" style="zoom:50%;"></p><h4 id="带深度限制的-Leaf-wise-算法"><a href="#带深度限制的-Leaf-wise-算法" class="headerlink" title="带深度限制的 Leaf-wise 算法"></a>带深度限制的 Leaf-wise 算法</h4><p>在建树的过程中有两种策略：</p><ul><li>Level-wise：基于层进行生长，直到达到停止条件；</li><li>Leaf-wise：每次分裂增益最大的叶子节点，直到达到停止条件。</li></ul><p>XGBoost 采用 Level-wise 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，降低了计算量；LightGBM 采用 Leaf-wise 的增长策略减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-76f2f27dd24fc452a9a65003e5cdd305_r.jpg" alt="img" style="zoom:80%;"></p><h4 id="类别特征最优分割"><a href="#类别特征最优分割" class="headerlink" title="类别特征最优分割"></a>类别特征最优分割</h4><p>大部分的机器学习算法都不能直接支持类别特征，一般都会对类别特征进行编码，然后再输入到模型中。常见的处理类别特征的方法为 one-hot 编码，但我们知道<strong>对于决策树来说并不推荐使用 one-hot 编码：</strong></p><ol><li>会产生样本切分不平衡问题，切分增益会非常小。使用one-hot编码的话，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。当类别值很多时，每个类别上的数据可能会比较少，这时候切分会产生不平衡，这意味着切分增益也会很小（比较直观的理解是，不平衡的切分和不切分没有区别）。如，国籍切分后，会产生是否中国，是否美国等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0。这种划分的增益非常小：较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零；</li><li>影响决策树学习：决策树依赖的是数据的统计信息，而就算one-hot编码可以在这个类别特征进行切分，也会把数据切分到零散的小空间上。在这些零散的小空间上统计信息不准确的，学习效果变差。本质是因为one-hot码编码之后的特征的表达能力较差的，特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败，最终该特征得到的重要性会比实际值低。</li></ol><p><strong>LightGBM 原生支持类别特征</strong>，采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设有某维特征有 $k$ 个类别，则有 $2^{(k-1)} - 1$ 种可能，时间复杂度为 $O(2^k)$ ，LightGBM 基于 Fisher 大佬的 《<a href="https://link.zhihu.com/?target=http%3A//www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf" target="_blank" rel="noopener">On Grouping For Maximum Homogeneity</a>》实现了 $O(klogk)$ 的时间复杂度。</p><p>下图为左边为基于 one-hot 编码进行分裂，右边为 LightGBM 基于 many-vs-many 进行分裂（叶子节点的含义是X=A或者X=C放到左孩子，其余放到右孩子），在给定深度情况下，后者能学出更好的模型。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-34558cd9eab486eed731ba7aadca5992_b.jpg" alt="img" style="zoom:50%;"></p><p>其基本思想在于每次分组时都会<strong>根据训练目标</strong>对类别特征进行分类，根据其累积值 $\frac{\sum gradient }{\sum hessian}$ 对直方图进行排序，然后在排序的直方图上找到最佳分割。此外，LightGBM 还加了约束条件正则化，防止过拟合。</p><blockquote><p>下图是根据类标sum(y)/count(y)进行排序并划分的示意图。</p></blockquote><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-9c70d63970befd6cdac3b0e530b93559_720w.jpg" alt="img" style="zoom: 40%;"></p><p>我们可以看到这种处理类别特征的方式使得 AUC 提高了 1.5 个点，且时间仅仅多了 20%。</p><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-ea588783be9403a0f7115c408389031d_b.jpg" alt="img" style="zoom: 67%;"></p><blockquote><p>对于类别特征，补充另外的处理方式。</p><ol><li><p>转成数值特征。在使用 sklearn 或 XGBoost 等不支持类别特征的最优切分工具时，可以用这个方法。常见的转换方法有: a) 把类别特征转成one-hot coding扔到NN里训练个embedding；b) 类似于CTR特征，统计每个类别对应的label(训练目标)的均值。统计的时候有一些小技巧，比如不把自身的label算进去(leave-me-out, leave-one-out)统计， 防止信息泄露。</p><p>关于”leave-me-out”的统计方法。一个简单的例子，比如样本1，3，5属于同个类别（在类别特征上的属性一样），对于样本1，可以用3和5的label均值，样本3用1和5的均值……这样可以防止每一个样本直接把自身的label信息放到特征里面，减少统计特征的信息泄露，防止过拟合。</p></li><li><p>其他的编码方法，比如binary coding等等，同样可以用于不支持类别特征的算法。这里有一个比较好的开源项目，封装了常见的各种编码方法: <a href="https://github.com/scikit-learn-contrib/categorical-encoding" target="_blank" rel="noopener">https://github.com/scikit-learn</a></p></li></ol></blockquote><h3 id="工程实现-1"><a href="#工程实现-1" class="headerlink" title="工程实现"></a>工程实现</h3><h4 id="特征并行"><a href="#特征并行" class="headerlink" title="特征并行"></a>特征并行</h4><p>传统的特征并行算法在于对数据进行垂直划分，然后使用不同机器找到不同特征的最优分裂点，基于通信整合得到最佳划分点，然后基于通信告知其他机器划分结果。</p><p>传统的特征并行方法有个很大的缺点：需要告知每台机器最终划分结果，增加了额外的复杂度（因为对数据进行垂直划分，每台机器所含数据不同，划分结果需要通过通信告知）。</p><p>LightGBM 则不进行数据垂直划分，每台机器都有训练集完整数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。</p><h4 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h4><p>传统的数据并行策略主要为水平划分数据，然后本地构建直方图并整合成全局直方图，最后在全局直方图中找出最佳划分点。</p><p>这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 $O(#machine <em> #feature </em>#bin )$ ；如果使用集成的通信，则通讯开销为 $O(2 <em> #feature </em>#bin )$ 。</p><p>LightGBM 采用分散规约（Reduce scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信。</p><h4 id="投票并行"><a href="#投票并行" class="headerlink" title="投票并行"></a>投票并行</h4><p>针对数据量特别大特征也特别多的情况下，可以采用投票并行。投票并行主要针对数据并行时数据合并的通信代价比较大的瓶颈进行优化，其通过投票的方式只合并部分特征的直方图从而达到降低通信量的目的。</p><p>大致步骤为两步：</p><ol><li>本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征；</li><li>合并时只合并每个机器选出来的特征。</li></ol><h4 id="缓存优化"><a href="#缓存优化" class="headerlink" title="缓存优化"></a>缓存优化</h4><p>上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。</p><p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p><ol><li>首先，所有的特征都采用相同的方法获得梯度（区别于不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中；</li><li>其次，因为不需要存储特征到样本的索引，降低了存储消耗，而且也不存在 Cache Miss的问题。</li></ol><p><img src="/MachineLearning/机器学习/决策树（下）—— XGBoost、LightGBM/v2-19436e5546c47fed4a85000b1fff9abb_b.jpg" alt="img"></p><h3 id="与-XGBoost-的对比"><a href="#与-XGBoost-的对比" class="headerlink" title="与 XGBoost 的对比"></a>与 XGBoost 的对比</h3><p>本节主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。</p><h4 id="内存更小"><a href="#内存更小" class="headerlink" title="内存更小"></a>内存更小</h4><ol><li>XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 O(2∗#data) 降低为 O(#bin) ，极大的减少了内存消耗；</li><li>LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；</li><li>LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。</li></ol><h4 id="速度更快"><a href="#速度更快" class="headerlink" title="速度更快"></a>速度更快</h4><ol><li>LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；</li><li>LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；</li><li>LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；</li><li>LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；</li><li>LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/87885678" target="_blank" rel="noopener">【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">陈天奇论文演讲 PPT</a><br><a href="https://www.zhihu.com/question/266195966/answer/306104444" target="_blank" rel="noopener">关于sklearn中的决策树是否应该用one-hot编码？ - 柯国霖的回答 - 知乎</a><br><a href="https://blog.csdn.net/anshuai_aw1/article/details/83275299" target="_blank" rel="noopener">Lightgbm如何处理类别特征？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要参考了&lt;a href=&quot;https://www.zhihu.com/people/is-aze&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿泽&lt;/a&gt;作者的笔记，对于不理解的地方，我会添加个人注释。&lt;/p&gt;
&lt;h2 id=&quot;XGBoost&quot;&gt;&lt;a href=&quot;#XGBoost&quot; class=&quot;headerlink&quot; title=&quot;XGBoost&quot;&gt;&lt;/a&gt;XGBoost&lt;/h2&gt;&lt;p&gt;XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是目标函数的定义。故本文将从数学原理和工程实现上进行介绍，并在最后介绍下 Xgboost 的优点。&lt;/p&gt;
&lt;h3 id=&quot;数学原理&quot;&gt;&lt;a href=&quot;#数学原理&quot; class=&quot;headerlink&quot; title=&quot;数学原理&quot;&gt;&lt;/a&gt;数学原理&lt;/h3&gt;&lt;h4 id=&quot;目标函数&quot;&gt;&lt;a href=&quot;#目标函数&quot; class=&quot;headerlink&quot; title=&quot;目标函数&quot;&gt;&lt;/a&gt;目标函数&lt;/h4&gt;
    
    </summary>
    
    
      <category term="MachineLearning" scheme="https://www.zdaiot.com/categories/MachineLearning/"/>
    
      <category term="机器学习" scheme="https://www.zdaiot.com/categories/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="决策树" scheme="https://www.zdaiot.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>决策树（中）—— Random Forest、Adaboost、GBDT</title>
    <link href="https://www.zdaiot.com/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%AD%EF%BC%89%E2%80%94%E2%80%94%20Random%20Forest%E3%80%81Adaboost%E3%80%81GBDT/"/>
    <id>https://www.zdaiot.com/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/</id>
    <published>2022-10-31T14:14:11.000Z</published>
    <updated>2022-10-31T14:14:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要参考了<a href="https://www.zhihu.com/people/is-aze" target="_blank" rel="noopener">阿泽</a>作者的笔记，对于不理解的地方，我会添加个人注释。</p><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。</p><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>Bagging 全称叫 <strong>B</strong>ootstrap <strong>agg</strong>regating（ Bootstrap 抽样方法） ，每个基学习器都会对训练集进行有放回抽样得到子训练集，比较著名的采样法为 0.632 自助法。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-a0a3cb02f629f3db360fc68b4c2153c0_b.jpg" alt="img" style="zoom:80%;"></p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting 训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法。</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-3aab53d50ab65e11ad3c9e3decf895c2_b.jpg" alt="img" style="zoom:80%;"></p><h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，其预测值将作为训练样本的特征值，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型。对测试集也做相同的操作，得到最终预测结果。</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-f6787a16c23950d129a7927269d5352a_b.jpg" alt="img" style="zoom:80%;"></p><blockquote><p>上图绿色是训练过程，红色是预测过程。</p></blockquote><p>那么，为什么集成学习会好于单个学习器呢？原因可能有三：</p><ol><li>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</li><li>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</li><li>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</li></ol><h2 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h2><p>我们从偏差和方差的角度来理解集成学习。</p><h3 id="集成学习的偏差与方差"><a href="#集成学习的偏差与方差" class="headerlink" title="集成学习的偏差与方差"></a>集成学习的偏差与方差</h3><p>偏差（Bias）描述的是预测值和真实值之差；方差（Variance）描述的是预测值作为随机变量的离散程度。放一张很经典的图：</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-60c942f91d33d9dedf9dd2c7d482af5d_b.jpg" alt="img" style="zoom:67%;"></p><ul><li><strong>偏差：</strong>描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；</li><li><strong>方差：</strong>描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li></ul><p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，<strong>但并不是所有集成学习框架中的基模型都是弱模型</strong>。<strong>Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）</strong>。</p><blockquote><p>弱模型，模型简单，偏差高，方差小，防止过拟合能力强；</p><p>强模型，模型复杂，偏差小，方差大，容易过拟合。</p></blockquote><p>在 Bagging 和 Boosting 框架中，通过计算基模型的期望和方差我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的期望为 $\mu$ ，方差 $\sigma ^ 2$ ，模型的权重为 $r$ ，两两模型间的相关系数 $\rho$ 相等。由于 Bagging 和 Boosting 的基模型都是线性组成的，那么有：</p><p>模型总体期望：</p><script type="math/tex; mode=display">\begin{align} E(F) &= E(\sum_{i}^{m}{r_i f_i}) \\ &= \sum_{i}^{m}r_i E(f_i) \end{align} \\</script><p>模型总体方差（公式推导参考协方差的性质，协方差与方差的关系）：</p><script type="math/tex; mode=display">\begin{align} Var(F) &= Var(\sum_{i}^{m}{r_i f_i}) \\ &= \sum_{i}^{m}Var(r_if_i) + \sum_{i \neq j}^{m}Cov(r_i f_i , r_j f_j) \\ &= \sum_{i}^{m} {r_i}^2 Var(f_i) + \sum_{i \neq j}^{m}\rho r_i r_j \sqrt{Var(f_i)} \sqrt{Var(f_j)} \\ &= mr^2\sigma^2 + m(m-1)\rho r^2 \sigma^2\\ &= m r^2 \sigma^2 (1-\rho) + m^2 r^2 \sigma^2 \rho \end{align} \\</script><p>模型的准确度可由偏差和方差共同决定：</p><script type="math/tex; mode=display">Error = bias^2 + var + \xi \\</script><h3 id="Bagging-的偏差与方差"><a href="#Bagging-的偏差与方差" class="headerlink" title="Bagging 的偏差与方差"></a>Bagging 的偏差与方差</h3><p>对于 Bagging 来说，每个基模型的权重等于 1/m 且期望近似相等，故我们可以得到：</p><script type="math/tex; mode=display">\begin{align} E(F) & = \sum_{i}^{m}r_i E(f_i) \\ &= m \frac{1}{m} \mu \\ &= \mu \\ Var(F) &= m r^2 \sigma^2 (1-\rho) + m^2 r^2 \sigma^2 \rho \\ &= m \frac{1}{m^2} \sigma^2 (1-\rho) + m^2 \frac{1}{m^2} \sigma^2 \rho \\ &= \frac{\sigma^2(1 - \rho)}{m} + \sigma^2 \rho \end{align} \\</script><p>通过上式我们可以看到：</p><ul><li><strong>整体模型的期望等于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。</strong></li><li><strong>整体模型的方差小于等于基模型的方差，当且仅当相关性为 1 时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。</strong>但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。</li></ul><p>在此我们知道了为什么 Bagging 中的基模型一定要为强模型，如果 Bagging 使用弱模型则会导致整体模型的偏差提高，而准确度降低。</p><p>Random Forest 是经典的基于 Bagging 框架的模型，并在此基础上通过引入特征采样和样本采样来降低基模型间的相关性，在公式中显著降低方差公式中的第二项，略微升高第一项，从而使得整体降低模型整体方差。</p><h3 id="Boosting-的偏差与方差"><a href="#Boosting-的偏差与方差" class="headerlink" title="Boosting 的偏差与方差"></a>Boosting 的偏差与方差</h3><p>对于 Boosting 来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1，针对 Boosting 化简公式为：</p><script type="math/tex; mode=display">\begin{align} E(F) & = \sum_{i}^{m}r_i E(f_i) \\ Var(F) &= m r^2 \sigma^2 (1-\rho) + m^2 r^2 \sigma^2 \rho \\ &= m \frac{1}{m^2} \sigma^2 (1-1) + m^2 \frac{1}{m^2} \sigma^2 1 \\&= \sigma^2 \end{align} \\</script><p>通过观察整体方差的表达式我们容易发现：</p><ul><li>整体模型的方差等于基模型的方差，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。</li><li>此外 Boosting 框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以随着基模型数的增多，整体模型的期望值增加，整体模型的准确度提高。</li></ul><p>基于 Boosting 框架的 Gradient Boosting Decision Tree 模型中基模型也为树模型，同 Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>我们可以使用模型的偏差和方差来近似描述模型的准确度；</li><li>对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li><li>对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li></ul><h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><p>Random Forest（随机森林），用随机的方式建立一个森林。RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。</p><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：</p><ol><li>随机选择样本（放回抽样）；</li><li>随机选择特征；</li><li>构建决策树；</li><li>随机森林投票（平均）。</li></ol><p>随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；随机选择特征具体来说，传统决策树在选择划分属性时是在当前节点的属性集合（假定有$d$个属性）中选择一个最优属性；而在RF中，对基决策树的每个节点，先<strong>从该节点的属性集合</strong>中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数$k$控制了随机性的引入程度：若$k=d$，则基决策树的构建与传统决策树相同；若令$k=1$，则是随机选择一个属性用于划分；一般情况下，推荐值$k=log_2 d$。</p><p>这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p><p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>在数据集上表现良好，相对于其他算法有较大的优势</li><li>易于并行化，在大数据集上有很大的优势；</li><li>能够处理高维度数据，不用做特征选择，不需要降维。</li><li>它可以判断特征的重要程度，可以判断出不同特征之间的相互影响</li><li>对于不平衡的数据集来说，它可以平衡误差</li><li>如果有很大一部分的特征遗失，仍可以维持准确度</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li>随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。</li><li>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的</li></ol><h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><p>AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：<strong>前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。</strong></p><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>Adaboost 迭代算法有三步：</p><ol><li>初始化训练样本的权值分布，每个样本具有相同权重；</li><li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li><li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。</li></ol><h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><p>Adaboost 模型是加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。</p><p><strong>加法模型</strong>：最终的强分类器是由若干个弱分类器加权平均得到的。</p><script type="math/tex; mode=display">F_{k}(x)=\sum_{i=1}^{k}\alpha_i f_i(x)</script><p><strong>前向分布学习算法</strong>：算法是通过一轮轮的弱学习器学习，利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重。第 k 轮的强学习器为：</p><script type="math/tex; mode=display">F_{k}(x)=\sum_{i=1}^{k}\alpha_i f_i(x)=F_{k-1}(x)+\alpha_{k}f_k(x) \\</script><p>定义损失函数为 n 个样本的指数损失函数，在分类正确的时候，指数部分为负数，单调递减；在分类错误的时候，指数部分为正数，单调递增，满足<strong>损失函数的定义</strong>：</p><script type="math/tex; mode=display">L(y,F) = \sum_\limits{i=1}^{n}exp(-y_iF_{k}(x_i)) \\</script><p>利用前向分布学习算法的关系可以得到：</p><script type="math/tex; mode=display">\begin{align} L(y, F) &= \sum_\limits{i=1}^{m}exp[(-y_i) (F_{k-1}(x_i) + \alpha_k f_k(x_i))] \\ &= \sum_\limits{i=1}^{m}exp[-y_i F_{k-1}(x_i) -y_i \alpha_k f_k(x_i)] \\ &= \sum_\limits{i=1}^{m}exp[-y_i F_{k-1}(x_i) ] exp[-y_i \alpha_k f_k(x_i)] \end{align} \\</script><p>因为 $F_{k-1}(x)$ 已知，所以令 $w_{k,i} = exp(-y_iF_{k-1}(x_i))$ ，随着每一轮迭代而将这个式子带入损失函数，损失函数转化为：</p><script type="math/tex; mode=display">L(y, F(x)) =\sum_\limits{i=1}^{m}w_{k,i}exp[-y_i\alpha_k f_k(x_i)] \\</script><p>于是分类器$f_k(x)$ 和这个分类器的权重$\alpha_k$可以表示成：</p><script type="math/tex; mode=display">\left(\alpha_{k}^{*}, f_{k}^{*}(x)\right)=\underbrace{\arg \min }_{\alpha, f} \sum_{i=1}^{N} {w}_{k i} \exp \left(-y_{i} \alpha_{k} f_{k}(x)\right)</script><p>我们<strong>先求</strong>$f_k(x)$ ，分类器的权重$\alpha_k$可以认为是一个确定的数，$f_k(x)$是使得分错的（带权重的）样本里损失函数最小的那个，可以得到：</p><script type="math/tex; mode=display">f_k(x) =\underbrace{\arg \min }_{f}\; \sum_\limits{i=1}^{m}w_{k,i}I(y_i \neq f_k(x_i)) \\</script><blockquote><p>注意：<strong>重点理解上面两个式子的等价性，这一步相当于针对不同权重的样本训练分类器</strong>。</p></blockquote><p>然后<strong>再求</strong>$\alpha_k$，将 $f_k(x)$ 带入损失函数，并对 $\alpha$ 求导，使其等于 0，则就得到了：</p><script type="math/tex; mode=display">\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k} \\</script><p>其中$e_k$ 即为我们前面的分类误差率。</p><script type="math/tex; mode=display">e_k = \frac{\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq f_k(x_i))}{\sum\limits_{i=1}^{m}w_{ki}^{’}} = \sum\limits_{i=1}^{m}w_{ki}I(y_i \neq f_k(x_i)) \\</script><p><strong>最后</strong>看样本权重的更新。利用 $F_{k}(x) = F_{k-1}(x) + \alpha_kf_k(x)$ 和 $w_{k+1,i}=w_{k,i}exp[-y_i\alpha_kf_k(x,i)]$ ，即可得：</p><script type="math/tex; mode=display">w_{k+1,i} = w_{ki}exp[-y_i\alpha_kf_k(x_i)] \\</script><p>这样就得到了样本权重更新公式。</p><blockquote><p>$\alpha_k$的详细推导过程可以参考<a href="https://www.jianshu.com/p/0d850d85dcbd" target="_blank" rel="noopener">机器学习笔记：AdaBoost 公式推导</a>。</p></blockquote><p><strong>正则化</strong></p><p>为了防止 Adaboost 过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长（learning rate）。对于前面的弱学习器的迭代</p><script type="math/tex; mode=display">F_{k}(x) = F_{k-1}(x) + \alpha_kf_k(x) \\</script><p>加上正则化项 $\mu$ 我们有：</p><script type="math/tex; mode=display">F_{k}(x) = F_{k-1}(x) + \mu\alpha_kf_k(x) \\</script><p>$\mu$ 的取值范围为 $0&lt;\mu\leq1$ 。对于同样的训练集学习效果，较小的 $\mu$ 意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ol><li>分类精度高；</li><li>可以用各种回归分类模型来构建弱学习器，非常灵活；</li><li>不容易发生过拟合。</li></ol><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol><li>对异常点敏感，异常点会获得较高权重。</li></ol><h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</p><p>Gradient Boosting 和其它 Boosting 算法一样，通过将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说 Gradient Boosting = Gradient Descent + Boosting。</p><p>和 AdaBoost 一样，Gradient Boosting 也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost 是通过提升错分数据点的权重来定位模型的不足而 Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。因此相比 AdaBoost, Gradient Boosting 可以使用更多种类的目标函数。</p><h3 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h3><p>GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）</p><h4 id="回归树（Regression-Decision-Tree）"><a href="#回归树（Regression-Decision-Tree）" class="headerlink" title="回归树（Regression Decision Tree）"></a>回归树（Regression Decision Tree）</h4><p>如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是回归树，不是分类树，这一点相当重要。</p><p>回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</p><h4 id="梯度迭代（Gradient-Boosting）"><a href="#梯度迭代（Gradient-Boosting）" class="headerlink" title="梯度迭代（Gradient Boosting）"></a>梯度迭代（Gradient Boosting）</h4><p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，GBDT 的每一棵树都是以之前树得到的残差来更新目标值，这样每一棵树的值加起来即为 GBDT 的预测值。</p><p>模型的预测值可以表示为：</p><script type="math/tex; mode=display">F_k(x) = \sum_{i=1}^{k}f_{i}(x) \\</script><p>$f_{i}(x)$ 为基模型与其权重的乘积，模型的训练目标是使预测值 $F_k(x)$ 逼近真实值 $y$，也就是说要让每个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p><script type="math/tex; mode=display">F_k(x) = F_{k-1}(x)+f_{k}(x)\\</script><p>这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使 $F_k(x)$ 逼近真实值 $y$ 。</p><p>举个例子：比如说 A 用户年龄 20 岁，第一棵树预测 12 岁，那么残差就是 8，第二棵树用 8 来学习，假设其预测为 5，那么其残差即为 3，如此继续学习即可。</p><p>那么 Gradient 从何体现？其实很简单，其<strong>残差其实是最小均方损失函数关于预测值的反向梯度(划重点)</strong>：</p><script type="math/tex; mode=display">-\frac{\partial (\frac{1}{2}(y-F_{k}(x))^2)}{\partial F_k(x)} = y-F_{k}(x) \\</script><p>也就是说，<strong>预测值和实际值的残差与损失函数的负梯度相同</strong>。</p><p>但要注意，基于残差 GBDT 容易对异常值敏感，举例：</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-52f8fe9d1990c31335bf26c0c85d7ad5_b.jpg" alt="img" style="zoom:50%;"></p><p>很明显后续的模型会对第 4 个值关注过多，这不是一种好的现象，所以一般回归类的损失函数会用<strong>绝对损失或者 Huber 损失函数</strong>来代替平方损失函数。</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-c657e78a5a9e3646dc493a3f69556c8a_b.jpg" alt="img" style="zoom:50%;"></p><p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，<strong>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0</strong>，这样后面的树就能专注于那些被分错的样本。</p><blockquote><p>可以理解：adaboost中是显示的设置了每个样本的权重，而gbdt则是由于错分样本导致残差变大，变相的加大了下一次基模型学习时样本的权重，所以两种方式不同，但有类似的效果。</p></blockquote><h4 id="缩减（Shrinkage）"><a href="#缩减（Shrinkage）" class="headerlink" title="缩减（Shrinkage）"></a>缩减（Shrinkage）</h4><p>Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p><script type="math/tex; mode=display">F_i(x)=F_{i-1}(x)+\mu f_i(x) \quad (0<\mu \leq 1) \\</script><p>Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。</p><h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ol><li>可以自动进行特征组合，拟合非线性数据；</li><li>可以灵活处理各种类型的数据。</li></ol><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ol><li>损失函数是MSE时，对异常点敏感。</li></ol><h3 id="与-Adaboost-的对比"><a href="#与-Adaboost-的对比" class="headerlink" title="与 Adaboost 的对比"></a>与 Adaboost 的对比</h3><h4 id="相同"><a href="#相同" class="headerlink" title="相同"></a>相同</h4><ol><li>都是 Boosting 家族成员，使用弱分类器；</li><li>都使用前向分布算法；</li></ol><h4 id="不同"><a href="#不同" class="headerlink" title="不同"></a>不同</h4><ol><li><strong>boosting策略不同</strong>：Adaboost强调Adaptive（自适应，每个新的模型都会基于前一个模型的表现结果进行调整），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting；GBDT 则是在确定损失函数后，本轮 cart 树的拟合目标就是沿着损失函数相对于前一轮组合树模型的负梯度方向进行拟合，也就是希望最快速度地最小化预测值与真实值之间的差异；当损失函数选择为 square loss 时候，其沿着负梯度方向拟合表现为拟合残差（选择其他损失函数不一定表现出拟合残差的性质）。</li><li><strong>损失函数不同</strong>：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数。</li></ol><h3 id="与LR-Linear-Regression-Logistic-Regression"><a href="#与LR-Linear-Regression-Logistic-Regression" class="headerlink" title="与LR(Linear Regression? Logistic Regression?)"></a>与LR(Linear Regression? Logistic Regression?)</h3><p>从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界根据是否使用核函数可以是一条直线或者曲线（如下图所示），而GBDT的决策边界可能是很多条线。</p><p><img src="/MachineLearning/机器学习/决策树（中）—— Random Forest、Adaboost、GBDT/v2-044baf1462d172dc1abac7f953571138_b.jpg" alt="img" style="zoom:50%;"></p><p>GBDT并不一定总是好于线性回归或逻辑回归。根据没有免费的午餐原则，没有一个算法是在所有问题上都能好于另一个算法的。根据奥卡姆剃刀原则，如果GBDT和线性回归或逻辑回归在某个问题上表现接近，那么我们应该选择相对比较简单的线性回归或逻辑回归。具体选择哪一个算法还是要根据实际问题来决定。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://easyai.tech/ai-definition/random-forest/" target="_blank" rel="noopener">随机森林 – Random forest</a><br><a href="https://www.jianshu.com/p/0d850d85dcbd" target="_blank" rel="noopener">机器学习笔记：AdaBoost 公式推导</a><br><a href="https://zhuanlan.zhihu.com/p/86263786" target="_blank" rel="noopener">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）</a><br><a href="https://zhuanlan.zhihu.com/p/42740654" target="_blank" rel="noopener">Adaboost, GBDT 与 XGBoost 的区别</a><br><a href="https://mp.weixin.qq.com/s/SRP1yTzqCJsniPfMpQHJ1w" target="_blank" rel="noopener">Boosting和AdaBoost的可视化的清晰的解释</a><br><a href="https://ixyzero.com/blog/archives/4242.html" target="_blank" rel="noopener">机器学习算法中GBDT与AdaBoost的区别与联系</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要参考了&lt;a href=&quot;https://www.zhihu.com/people/is-aze&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿泽&lt;/a&gt;作者的笔记，对于不理解的地方，我会添加个人注释。&lt;/p&gt;
&lt;h2 id=&quot;集成学习&quot;&gt;&lt;a href=&quot;#集成学习&quot; class=&quot;headerlink&quot; title=&quot;集成学习&quot;&gt;&lt;/a&gt;集成学习&lt;/h2&gt;&lt;p&gt;常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。&lt;/p&gt;
&lt;h3 id=&quot;Bagging&quot;&gt;&lt;a href=&quot;#Bagging&quot; class=&quot;headerlink&quot; title=&quot;Bagging&quot;&gt;&lt;/a&gt;Bagging&lt;/h3&gt;&lt;p&gt;Bagging 全称叫 &lt;strong&gt;B&lt;/strong&gt;ootstrap &lt;strong&gt;agg&lt;/strong&gt;regating（ Bootstrap 抽样方法） ，每个基学习器都会对训练集进行有放回抽样得到子训练集，比较著名的采样法为 0.632 自助法。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="MachineLearning" scheme="https://www.zdaiot.com/categories/MachineLearning/"/>
    
      <category term="机器学习" scheme="https://www.zdaiot.com/categories/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="决策树" scheme="https://www.zdaiot.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>决策树（上）—— ID3、C4.5、CART</title>
    <link href="https://www.zdaiot.com/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89%E2%80%94%E2%80%94%20ID3%E3%80%81C4.5%E3%80%81CART/"/>
    <id>https://www.zdaiot.com/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/</id>
    <published>2022-10-26T13:25:11.000Z</published>
    <updated>2022-10-26T13:25:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近想学习一下LightGBM，一直对决策树不是很熟悉，所以学习一下~</p><p>本文主要参考了<a href="https://www.zhihu.com/people/is-aze" target="_blank" rel="noopener">阿泽</a>作者的笔记，对于不理解的地方，我会添加个人注释。</p><p>决策树是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>这部分内容主要参考了<a href="https://www.cnblogs.com/codeshell/p/13948083.html" target="_blank" rel="noopener">决策树算法-理论篇-如何计算信息纯度 </a>。</p><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>信息熵</strong>（一般用<strong>H</strong> 表示），度量信息熵的单位是<strong>比特</strong>。就是说，信息量的多少是可以量化的。一条信息量的多少与信息的不确定性有关，可以认为，信息量就等于不确定性的多少（信息的不确定度）。</p><p>设$X$是一个取有限个值的离散随机变量，其信息熵的计算公式如下：</p><script type="math/tex; mode=display">\mathrm{H}(\mathrm{X})=-\sum_{i=1}^{n} p\left(x_{i}\right) \log p\left(x_{i}\right)</script><p>其中，该公式的含义是：</p><ul><li>待分类的事物可以分在多个分类中，这里的$n$就是分类的数目</li><li>$H(X)$ 表示熵，数学含义是，所有类别包含的信息期望值</li><li>$\log p\left(x_{i}\right)$表示符号的信息值，$p\left(x_{i}\right) $是选择该类的概率，$p(x_i)=P(X=x_i), i=1,2,\dots, n$</li><li>公式中的$log$一般以2为底</li><li>由定义可知，熵只依赖于$X$的分布，而与$X$的取值无关，所以也可将$X$的熵记作$H(p)$。</li></ul><p>总之，就是要知道，信息量的多少是可以用数学公式计算出来的，用信息论中的专业术语就叫做信息熵。信息熵越大，信息量也就越大。</p><p>下面这张图片解释了信息熵的由来。</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/webp.webp" alt="img" style="zoom:80%;"></p><h4 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h4><p>假设我们有如下数据集：</p><div class="table-container"><table><thead><tr><th>序号</th><th>条件:天气晴朗?</th><th>条件:是否刮风?</th><th>结果:去踢球吗?</th></tr></thead><tbody><tr><td>1</td><td>是</td><td>否</td><td>去</td></tr><tr><td>2</td><td>是</td><td>是</td><td>不去</td></tr><tr><td>3</td><td>否</td><td>是</td><td>不去</td></tr><tr><td>4</td><td>否</td><td>否</td><td>不去</td></tr></tbody></table></div><p>可以看到这个表格中有4 行（第一行表头不算），4 列数据。一般在机器学习中，最后一列称为<strong>目标(target)</strong>，前边的列都称为<strong>特征(features)</strong>。</p><p>根据表格，我们可以知道，所有的分类共有2 种，也就是“去” 和“不去”，“去”出现了1 次，“不去”出现了3 次。</p><p>分别计算“去” 和“不去” 出现的概率：</p><ul><li><code>P(去) = 1 / 4 = 0.25</code></li><li><code>P(不去) = 3 / 4 = 0.75</code></li></ul><p>然后，根据熵的计算公式来计算“去”和“不去” 的信息熵，其中log 以2 为底：</p><ul><li><code>H(去) = 0.25 * log 0.25 = -0.5</code></li><li><code>H(不去) = 0.74 * log 0.75 = -0.31127812445913283</code></li></ul><p>所以，整个表格含有的信息量就是：</p><ul><li><code>H(表格) = -(H(去) + H(不去)) = 0.81127812445913283</code></li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>将计算信息熵的过程用<code>Python</code> 代码实现，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数用于计算一组数据的信息熵</span></span><br><span class="line"><span class="comment"># data_set 是一个列表，代表一组数据</span></span><br><span class="line"><span class="comment"># data_set 的元素data 也是一个列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span><span class="params">(data_set)</span>:</span></span><br><span class="line">    labels = &#123;&#125; <span class="comment"># 用于统计每个label 的数量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> data_set:</span><br><span class="line">        label = data[<span class="number">-1</span>]<span class="comment"># 只用最后一个元素做计算</span></span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> labels:</span><br><span class="line">            labels[label] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        labels[label] += <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">    ent = <span class="number">0</span> <span class="comment"># 熵</span></span><br><span class="line">    n = len(data_set)   <span class="comment"># 数据条数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算信息熵</span></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        prob = float(labels[label]) / n <span class="comment"># label 的概率</span></span><br><span class="line">        ent -= prob * math.log(prob, <span class="number">2</span>) <span class="comment"># 根据信息熵公式计算</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br></pre></td></tr></table></figure><p>下面用该函数来计算表格的信息熵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将表格转化为 python 列表</span></span><br><span class="line"><span class="comment"># "yes" 表示"去"</span></span><br><span class="line"><span class="comment"># "no" 表示"不去"</span></span><br><span class="line">data_set = [[<span class="string">'yes'</span>], [<span class="string">'no'</span>], [<span class="string">'no'</span>], [<span class="string">'no'</span>]] </span><br><span class="line">ent = calc_ent(data_set)</span><br><span class="line">print(ent)<span class="comment"># 0.811278124459</span></span><br></pre></td></tr></table></figure><p>可见，用代码计算出来的结果是 <strong>0.811278124459</strong>，跟我们手算的结果 <strong>0.81127812445913283</strong> 是一样的（保留的小数位数不同）。</p><h3 id="信息纯度"><a href="#信息纯度" class="headerlink" title="信息纯度"></a>信息纯度</h3><p>信息的纯度与信息熵成反比，可以将信息熵理解为 <strong>“不纯度”</strong> 。</p><ul><li>信息熵越大，信息量越大，数据不确定性越高，信息越杂乱，纯度越低。意味着分类的各个类型占比近似很难区分</li><li>信息熵越小，信息量越小，数据不确定性越低，信息越规整，纯度越高。意味着在数据集里我们要分类的某一种类型占比很高</li></ul><p>举一个例子，比如我们想分类A和B，用公式量化来体现就是：</p><p>1）如果分类结果中A和B各占50%，那么意味着分类结果很失败，这无异于随机地乱猜，完全没起到分类效果，公式计算结果如下：</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1666945135086-9.png" alt="图片" style="zoom: 80%;"></p><p>2）如果分类结果中A占比100%，B占比0%或者B占比100%，A占比0%时，那么意味着分类很成功，因为我们成功地区分了A和B，我们就说此时的纯度很高，公式计算结果如下：</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1666945150778-12.png" alt="图片" style="zoom:80%;"></p><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望。</p><script type="math/tex; mode=display">H(Y \mid X)=\sum_{i=1}^{n} p(x_i) H\left(Y \mid X=x_{i}\right)</script><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p><strong>信息增益就是，在根据某个属性划分数据集的前后，信息量发生的变化。</strong></p><blockquote><p>信息熵代表不纯度，只要将分类前后的不纯度相减，那就可以得到一种 <strong>“纯度提升值”</strong> 的指标，我们把它叫做 <strong>“信息增益”</strong>。</p></blockquote><p>特征$A$对训练数据集$D$的信息增益$Gain(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的条件熵$H(D|A)$之差。信息增益的计算公式如下：</p><script type="math/tex; mode=display">Gain(D,A)=H(D)-H(D|A)</script><p>所有子节点的信息熵会按照子节点在父节点中的出现的概率来计算，这叫做<strong>归一化信息熵</strong>。</p><p><strong>信息增益的目的在于，将数据集划分之后带来的纯度提升，也就是信息熵的下降</strong>。如果数据集在根据某个属性划分之后，能够获得最大的信息增益，那么这个属性就是最好的选择。</p><p>所以，我们想要找到根节点，就需要计算每个属性作为根节点时的信息增益，那么获得信息增益最大的那个属性，就是根节点。</p><h4 id="计算-1"><a href="#计算-1" class="headerlink" title="计算"></a>计算</h4><p><strong>信息增益等于按照某个属性划分前后的信息熵之差。</strong>它的计算方式简单来说，先基于特征A进行划分，再基于目标变量进行划分，这是一个嵌套的过程。</p><p>这个表格划分之前的信息熵我们已经知道了，就是我们在上面计算的结果：</p><ul><li><code>H(表格) = 0.81127812445913283</code>。</li></ul><p>接下来，我们计算按照“天气晴朗”划分的信息增益。按照“天气晴朗”划分后有两个表格。</p><p>表格1，“天气晴朗”的值为“是”：</p><div class="table-container"><table><thead><tr><th>序号</th><th>条件:天气晴朗?</th><th>条件:是否刮风?</th><th>结果:去踢球吗?</th></tr></thead><tbody><tr><td>1</td><td><strong>是</strong></td><td>否</td><td>去</td></tr><tr><td>2</td><td><strong>是</strong></td><td>是</td><td>不去</td></tr></tbody></table></div><p>分类共有2 种，也就是“去” 和“不去”，“去”出现了1 次，“不去”出现了1 次。</p><p>所以，“去” 和“不去” 出现的概率均为0.5：</p><ul><li><code>P(去) = P(不去) = 1 / 2 = 0.5</code></li></ul><p>然后，“去”和“不去” 的信息熵，其中log 以2 为底：</p><ul><li><code>H(去) = H(不去) = 0.5 * log 0.5 = -0.5</code></li></ul><p>所以，表格1 含有的信息量就是：</p><ul><li><code>H(表格1) = -(H(去) + H(不去)) = 1</code></li></ul><p>表格2，“天气晴朗”的值为“否”：</p><div class="table-container"><table><thead><tr><th>序号</th><th>条件:天气晴朗?</th><th>条件:是否刮风?</th><th>结果:去踢球吗?</th></tr></thead><tbody><tr><td>3</td><td><strong>否</strong></td><td>是</td><td>不去</td></tr><tr><td>4</td><td><strong>否</strong></td><td>否</td><td>不去</td></tr></tbody></table></div><p>所有的分类只有1 种，是“不去”。所以：</p><ul><li><code>P(不去) = 1</code></li></ul><p>然后，“不去” 的信息熵，其中log 以2 为底：</p><ul><li><code>H(不去) = 1 * log 1 = 0</code></li></ul><p>所以，表格2 含有的信息量就是：</p><ul><li><code>H(表格2) = 0</code></li></ul><p>总数据共有4 份：</p><ul><li>表格1 中有2 份，概率为 2/4 = 0.5</li><li>表格2 中有2 份，概率为 2/4 = 0.5</li></ul><p>所以，最终按照“天气晴朗”划分的信息增益为：</p><ul><li><code>G(天气晴朗) = H(表格) - (0.5*H(表格1) + 0.5*H(表格2)) = H(表格) - 0.5 = 0.31127812445913283。</code></li></ul><h2 id="决策树概述"><a href="#决策树概述" class="headerlink" title="决策树概述"></a>决策树概述</h2><p>不同于逻辑回归，决策树属于<strong>非线性模型</strong>，可以用于分类，也可用于回归。它是一种树形结构，可以认为是if-then规则的集合，是以实例为基础的归纳学习。基本思想是自顶向下，以信息增益（或信息增益比，基尼系数等）为度量构建一颗度量标准下降最快的树，每个内部节点代表一个属性的测试，直到叶子节点处只剩下同一类别的样本。它的决策流程如下所示：</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640.png" alt="图片" style="zoom:80%;"></p><h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><p>ID3 算法是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p><blockquote><p>奥卡姆剃刀：“如无必要，勿增实体”，即“简单有效原理”。</p></blockquote><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>从信息论的知识中我们知道：信息熵越大，从而样本纯度越低。ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。 其大致步骤为：</p><ol><li>初始化特征集合和数据集合；</li><li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li><li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li><li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li></ol><blockquote><p>从这个过程，我们可以发现：最开始选择的特征肯定是提供信息量最大的，因为它是遍历所有特征后选择的结果。因此，<strong>按照决策过程中特征从上到下的顺序，我们也可以将特征的重要程度进行排序。</strong>这也就解释了为什么树模型有feature_importance这个参数了。</p></blockquote><h3 id="划分标准"><a href="#划分标准" class="headerlink" title="划分标准"></a>划分标准</h3><p>ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。</p><p>数据集的信息熵：</p><script type="math/tex; mode=display">H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}</script><p>其中$C_k$表示集合$D$中属于第$k$类样本的样本子集。</p><p>针对某个特征 $A$，对于数据集 $D$ 的条件熵 $H(D|A)$为：</p><script type="math/tex; mode=display">\begin{aligned} H(D \mid A) &=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right) \\ &=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|}\left(\sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right) \end{aligned}</script><p>其中 $D_i$ 表示 $D$ 中特征 $A$ 取第$ i$ 个值的样本子集，$ D_{ik}$ 表示$ D_i $中属于第$ k $类的样本子集。</p><p>信息增益 = 信息熵 - 条件熵：</p><script type="math/tex; mode=display">Gain(D,A)=H(D)-H(D|A)</script><p>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>ID3 没有剪枝策略，容易过拟合；</li><li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1。这是因为当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，条件熵越小，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较偏向取值较多的特征；</li><li>只能用于处理离散分布的特征；</li><li>没有考虑缺失值。</li></ul><h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。</p><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>C4.5 相对于 ID3 的缺点对应有以下改进方式：</p><ul><li>引入悲观剪枝策略进行后剪枝；</li><li>引入信息增益率作为划分标准；</li><li>将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点。</li></ul><p>对于缺失值的处理可以分为两个子问题：</p><ul><li><p>问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）</p><p>C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；</p></li><li><p>问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）</p><p>C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</p></li></ul><h3 id="划分标准-1"><a href="#划分标准-1" class="headerlink" title="划分标准"></a>划分标准</h3><p>利用信息增益率可以克服信息增益的缺点，其公式为</p><script type="math/tex; mode=display">\begin{aligned} Gain_{ratio}(D,A)&=\frac{Gain(D,A)}{H_A(D)} \\ H_A(D) &=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|} \end{aligned} \\</script><p>$H_A(D)$ 称为特征 $A$ 的固有值，$n$是特征$A$取值的个数。它的计算公式与熵类似，只不过数据集的熵是依据类别进行划分的，而这里是将特征A取值相同的样本划分到同一个子集中，来计算熵。</p><blockquote><p>信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数——分裂信息(Split information)。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。</p></blockquote><p>这里需要注意，信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<strong>启发式方法</strong>：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p><h3 id="剪枝策略"><a href="#剪枝策略" class="headerlink" title="剪枝策略"></a>剪枝策略</h3><p>为什么要剪枝：过拟合的树在泛化能力的表现非常差。</p><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>在节点划分前来确定是否继续增长，及早停止增长的主要方法有：</p><ul><li>节点内数据样本低于某一阈值；</li><li>所有节点特征都已分裂；</li><li>节点划分前准确率比划分后准确率高。</li></ul><p>预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。</p><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。</p><p>C4.5 采用的<strong>悲观剪枝方法</strong>，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。</p><p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>剪枝策略可以再优化；</li><li>C4.5 用的是多叉树，用二叉树效率更高；</li><li>C4.5 只能用于分类；</li><li>C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；</li><li>C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li></ul><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。</p><h3 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h3><p>CART 包含的基本过程有分裂，剪枝和树选择。</p><ul><li><strong>分裂：</strong>分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；</li><li><strong>剪枝：</strong>采用<strong>代价复杂度剪枝</strong>，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；</li><li><strong>树选择：</strong>用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li></ul><p>CART 在 C4.5 的基础上进行了很多提升。</p><ul><li>C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；</li><li>C4.5 只能分类，CART 既可以分类也可以回归；</li><li>CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；</li><li>CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；</li><li>CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。</li></ul><h3 id="划分标准-2"><a href="#划分标准-2" class="headerlink" title="划分标准"></a>划分标准</h3><p>熵模型拥有大量耗时的对数运算，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。所以决策树分裂选取Feature的时候，要选择使基尼指数最小的Feature，但注意信息增益（率）则是选择最大值，这个值的选取是相反的。</p><p>对于给定的样本集合$D$，其基尼指数定义为：</p><script type="math/tex; mode=display">\begin{aligned} Gini(D)&=\sum_{k=1}^{K}\frac{|C_k|}{|D|}(1-\frac{|C_k|}{|D|}) \\ &=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2 \end{aligned} \\</script><p>在特征$A$的条件下，集合$D$的基尼指数定义为：</p><script type="math/tex; mode=display">\\ Gini(D|A) &= \sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)</script><p>其中 $k$ 代表类别。</p><p>基尼指数反映了从<strong>数据集中随机抽取两个样本，其类别标记不一致的概率</strong>。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，</p><p>此外，当 CART 为二分类，其表达式为：</p><script type="math/tex; mode=display">Gini(D|A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2) \\</script><p>我们可以看到在平方运算和二分类的情况下，其运算更加简单。当然其性能也与熵模型非常接近。</p><p>那么问题来了：基尼指数与熵模型性能接近，但到底与熵模型的差距有多大呢？</p><p>我们知道 $ln(x) = -1+x +o(x)$ ，所以</p><script type="math/tex; mode=display">\begin{aligned} H(X)&=-\sum_{k=1}^{K} p_{k} \ln p_{k}\\&\approx \sum_{k=1}^{K} p_{k}\left(1-p_{k}\right) \end{aligned} \\</script><p>我们可以看到，基尼指数可以理解为熵模型的一阶泰勒展开。这边在放上一张很经典的图：</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1666947861684-15.jpeg" alt="图片" style="zoom:80%;"></p><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><h4 id="连续特征"><a href="#连续特征" class="headerlink" title="连续特征"></a>连续特征</h4><p><strong>如果特征值是连续值：</strong>特征a有连续值m个，从小到大排列。m个数值就有m-1个切分点，分别使用每个切分点把连续数值离散划分成两类，将节点数据集按照划分点分为D1和D2子集，然后计算每个划分点下对应的基尼指数，对比所有信息增益比（CART基尼指数），选择值最小（最大）的一个作为最终的特征划分。</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1667184073450-18.jpeg" alt="图片" style="zoom:67%;"></p><p>以上就实现了将连续特征值离散化，但是<strong>CART与ID3，C4.5处理离散属性不同的是：如果当前节点为连续属性，则该属性（剩余的属性值）后面还可以参与子节点的产生选择过程。</strong></p><h4 id="离散特征"><a href="#离散特征" class="headerlink" title="离散特征"></a>离散特征</h4><p><strong>如果特征值是离散值：</strong>CART的处理思想与C4.5稍微所有不同。如果离散特征值多于两个，那么C4.5会在节点上根据特征值划分出多叉树。但是CART则不同，无论离散特征值有几个，在节点上都划分成二叉树。CART树是如何进行分类的呢？</p><p>还是假设特征a有m个离散值。分类标准是：每一次将其中一个特征分为一类，其它非该特征分为另外一类。依照这个标准遍历所有的分类情况，计算每种分类下的基尼指数，最后选择值最小的一个作为最终的特征划分。</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1667184294370-21.png" alt="图片" style="zoom:67%;"></p><p><strong>特征值连续和离散有各自的处理方法，不应该混淆使用。比如分类0,1,2只代表标签含义，如果进行加减的运算或者求平均则没有任何意义。因此，CART分类树会根据特征类型选择不同的划分方法，并且与C4.5不同是，它永远只有两个分支。</strong></p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1667184356009-24.jpeg" alt="图片" style="zoom:67%;"></p><h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>上文说到，模型对于缺失值的处理会分为两个子问题：</p><ol><li>如何在特征值缺失的情况下进行划分特征的选择？</li><li>选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？</li></ol><p>对于问题 1，CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。</p><p>对于问题 2，CART 算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。</p><h3 id="剪枝策略-1"><a href="#剪枝策略-1" class="headerlink" title="剪枝策略"></a>剪枝策略</h3><p>采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。</p><p>我们来看具体看一下代价复杂度剪枝算法：</p><p>首先我们将最大树称为 $T_0$，我们希望减少树的大小来防止过拟合，但又担心去掉节点后预测误差会增大，所以我们定义了一个损失函数来达到这两个变量之间的平衡。损失函数定义如下：</p><script type="math/tex; mode=display">C_\alpha(T)=C(T)+\alpha|T| \\</script><p>$T$ 为任意子树，$ C(T) $为预测误差， $|T|$ 为子树 $T$ 的叶子节点个数， $\alpha$ 是参数，$ C(T) $衡量训练数据的拟合程度， $|T| $衡量树的复杂度， $\alpha$ 权衡拟合程度与树的复杂度。</p><p>那么如何找到合适的 $\alpha$ 来使得复杂度和拟合度达到最好的平衡点呢，最好的办法就是令$ \alpha$ 从 0 取到正无穷，对于每一个固定的 $\alpha $，我们都可以找到使得 $C_\alpha(T) $最小的最优子树 $T(\alpha) $。当$ \alpha$ 很小的时候，$ T_0$ 是最优子树；当 $\alpha $最大时，单独的根节点是这样的最优子树。随着 $\alpha$ 增大，我们可以得到一个这样的子树序列：$ T_0, T_1, T_2, T_3, … ,T_n$ ，这里的子树$ T_{i+1} $生成是根据前一个子树 $T_i $剪掉某一个内部节点生成的。</p><p>Breiman 证明：将 $\alpha$ 从小增大， $0=\alpha_0&lt;\alpha_0&lt;…&lt;\alpha_n&lt;\infty$ ，在每个区间$ [\alpha_i,\alpha_{i+1}) $中，子树 $T_i $是这个区间里最优的。</p><p>这是代价复杂度剪枝的核心思想。</p><p>我们每次剪枝都是针对某个非叶节点，其他节点不变，所以我们只需要计算该节点剪枝前和剪枝后的损失函数即可。</p><p>对于任意内部节点 $t$，剪枝前的状态，有 $|T_t|$ 个叶子节点，预测误差是$ C(T_t) $；剪枝后的状态：只有本身一个叶子节点，预测误差是$ C(t) $。</p><p>因此剪枝前以 $t $节点为根节点的子树的损失函数是：</p><script type="math/tex; mode=display">C_\alpha(T)=C(T_t)+\alpha|T| \\</script><p>剪枝后的损失函数是</p><script type="math/tex; mode=display">C_\alpha(t) = C(t)+\alpha \\</script><p>通过 Breiman 证明我们知道一定存在一个 $\alpha $使得 $C_\alpha(T)=C_\alpha(t)$ ，使得这个值为：</p><script type="math/tex; mode=display">\alpha = \frac{C(t)-C(T_t)}{|T_t|-1} \\</script><p>$\alpha$ 的意义在于，$ [\alpha_i,\alpha_{i+1}) $中，子树 $T_i $是这个区间里最优的。当 $\alpha $大于这个值是，一定有 $C_\alpha(T)&gt;C_\alpha(t)$ ，也就是剪掉这个节点后都比不剪掉要更优。所以每个最优子树对应的是一个区间，在这个区间内都是最优的。</p><p>然后我们对 $T_i$ 中的每个内部节点$ t $都计算：</p><script type="math/tex; mode=display">g(t) = \frac{C(t)-C(T_t)}{|T_t|-1} \\</script><p>$g(t)$ 表示阈值，故我们每次都会减去最小的$ T_t $。</p><h3 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h3><p>CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。</p><p>CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算里，在 CART 默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重加权，对类别进行均衡。</p><p>对于一个二分类问题，节点 node 被分成类别 1 当且仅当：</p><script type="math/tex; mode=display">\frac{N_1(node)}{N_1(root)} > \frac{N_0(node)}{N_0(root)} \\</script><p>比如二分类，根节点属于 1 类和 0 类的分别有 20 和 80 个。在子节点上有 30 个样本，其中属于 1 类和 0 类的分别是 10 和 20 个。如果 10/20&gt;20/80，该节点就属于 1 类。</p><p>通过这种计算方式就无需管理数据真实的类别分布。假设有 $K $个目标类别，就可以确保根节点中每个类别的概率都是$ 1/K$。这种默认的模式被称为“先验相等”。</p><p>先验设置和加权不同之处在于先验不影响每个节点中的各类别样本的数量或者份额。先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。</p><h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><p>CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归。与分类树不同，回归树的预测变量是连续值，比如预测一个人的年龄，又或者预测季度的销售额等等。另外，回归树在<strong>选择特征的度量标准</strong>和<strong>决策树建立后预测的方式</strong>上也存在不同。</p><h4 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h4><p>对于连续值，CART 分类树采用基尼系数的大小来度量特征的各个划分点。在回归模型中，我们使用常见的RSS<strong>残差平方和</strong>。线性回归的损失函数是以最小化离差平方和的形式给出的，回归树使用的度量标准也是一样的，通过最小化残差平方和作为判断标准，公式如下：</p><script type="math/tex; mode=display">\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{2} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]</script><p>其中，$R_1、R_2$是划分的两个子集，回归树是二叉树，固只有两个子集； $c_1、c_2$是$R_1、R_2$子集的样本均值，$y_i$是样本目标变量的真实值，$j$是当前的样本特征，$s$是划分点。</p><p>上面公式的含义是：计算所有的特征以及相应所有切分点下的残差平方和，找到一组(特征$j$，切分点$s$)，以满足：分别最小化左子树和右子树的残差平方和，并在此基础上再次最小化二者之和。</p><h4 id="预测方式"><a href="#预测方式" class="headerlink" title="预测方式"></a>预测方式</h4><p>对于决策树建立后做预测的方式，上面讲到了 CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。以均值为例，进行详细描述。</p><p>一个回归树对应着输入特征空间的一个划分，以及在划分单元上的输出值。先假设数据集已被划分，R1,R2,…,Rm共m的子集，回归树要求每个划分Rm中都对应一个固定的输出值$c_m$。这个$c_m$值其实就是每个子集中所有样本的目标变量$y$的平均值，并以此$c_m$作为该子集的预测值。</p><script type="math/tex; mode=display">f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)</script><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ol><li>无论是ID3、C4.5还是CART，在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。</li><li>如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。</li></ol><h2 id="ID3、C4-5-和-CART-总结"><a href="#ID3、C4-5-和-CART-总结" class="headerlink" title="ID3、C4.5 和 CART 总结"></a>ID3、C4.5 和 CART 总结</h2><p>最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。</p><p><img src="/MachineLearning/机器学习/决策树（上）—— ID3、C4.5、CART/640-1667185689025-27.png" alt="图片" style="zoom:67%;"></p><p>除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：</p><ul><li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li><li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li><li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li><li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；</li><li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。</li></ul><h2 id="决策树算法优缺点总结"><a href="#决策树算法优缺点总结" class="headerlink" title="决策树算法优缺点总结"></a>决策树算法优缺点总结</h2><p>我们前面介绍了决策树的特征选择，生成，和剪枝，然后对ID3, C4.5和CART算法也分别进行了详细的分析。下面我们来看看决策树算法作为一个大类别的分类回归算法的优缺点。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>简单直观，生成的决策树很直观。</li><li>基本不需要预处理，不需要提前归一化，处理缺失值。</li><li>使用决策树预测的代价是O(log2m)， m为样本数。</li><li>既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</li><li>可以处理多维度输出的分类问题。</li><li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li><li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li><li>对于异常点的容错能力好，健壮性高。</li></ol><p><strong>决策树算法的缺点</strong></p><ol><li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li><li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li><li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li><li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li><li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/codeshell/p/13948083.html" target="_blank" rel="noopener">决策树算法-理论篇-如何计算信息纯度</a><br><a href="https://zhuanlan.zhihu.com/p/85731206" target="_blank" rel="noopener">【机器学习】决策树（上）——ID3、C4.5、CART（非常详细）</a><br><a href="https://www.cnblogs.com/muzixi/p/6566803.html" target="_blank" rel="noopener">决策树—信息增益，信息增益比，Geni指数的理解</a><br><a href="http://mp.weixin.qq.com/s?__biz=MzUzODYwMDAzNA==&amp;mid=2247485668&amp;idx=1&amp;sn=b7cbd37c7ed9a81ab756048942436ecb&amp;chksm=fad47fe9cda3f6ffd45001aae7889d2d565d9fc15f0c99a6db3bfcd7602fa586879e3b91d537&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">决策树学习笔记（一）：特征选择</a><br><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1666946224&amp;ver=4131&amp;signature=QE9hbabvljJmSPd4Kj1nLFCW1eURBR6N2t8CaNpGQsTdOrkwOwjroPX8eKF3GFLaQneyZrvZF5zt2aE7cQcHfwypGXurIhU2FVfZYWx2gebOiJiLPKmrCwID9UlLQU1h&amp;new=1" target="_blank" rel="noopener">决策树学习笔记（三）：CART算法，决策树总结</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近想学习一下LightGBM，一直对决策树不是很熟悉，所以学习一下~&lt;/p&gt;
&lt;p&gt;本文主要参考了&lt;a href=&quot;https://www.zhihu.com/people/is-aze&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿泽&lt;/a&gt;作者的笔记，对于不理解的地方，我会添加个人注释。&lt;/p&gt;
&lt;p&gt;决策树是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。&lt;/p&gt;
&lt;h2 id=&quot;预备知识&quot;&gt;&lt;a href=&quot;#预备知识&quot; class=&quot;headerlink&quot; title=&quot;预备知识&quot;&gt;&lt;/a&gt;预备知识&lt;/h2&gt;&lt;p&gt;这部分内容主要参考了&lt;a href=&quot;https://www.cnblogs.com/codeshell/p/13948083.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;决策树算法-理论篇-如何计算信息纯度 &lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="MachineLearning" scheme="https://www.zdaiot.com/categories/MachineLearning/"/>
    
      <category term="机器学习" scheme="https://www.zdaiot.com/categories/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="决策树" scheme="https://www.zdaiot.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="https://www.zdaiot.com/Python/%E5%B8%B8%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>https://www.zdaiot.com/Python/常用第三方包/正则表达式/</id>
    <published>2022-09-20T06:54:33.000Z</published>
    <updated>2022-09-20T06:54:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>正则表达式很常用，但是规则也很复杂，每次用的时候查资料即可。我这里只是总结一下常用的正则表达式。</p><h2 id="字符串过滤"><a href="#字符串过滤" class="headerlink" title="字符串过滤"></a>字符串过滤</h2><p>我们常常需要根据正则表达式，来对字符串进行过滤。例如仅保留汉字、数字、字母等。用法如下例所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">num = <span class="string">'a￥1aB23Cqqq$我.04'</span></span><br><span class="line">print(<span class="string">"原字符串： "</span>, num)</span><br><span class="line"><span class="comment"># 字符串只保留中文</span></span><br><span class="line">num1 = re.sub(<span class="string">u"([^\u4e00-\u9fa5])"</span>, <span class="string">""</span>, num)</span><br><span class="line">print(<span class="string">"字符串只保留中文： "</span>, num1)</span><br><span class="line"><span class="comment"># 字符串只保留英文</span></span><br><span class="line">num2 = re.sub(<span class="string">u"([^\u0041-\u005a\u0061-\u007a])"</span>, <span class="string">""</span>, num)</span><br><span class="line">print(<span class="string">"字符串只保留英文： "</span>, num2)</span><br><span class="line"><span class="comment"># 字符串只保留数字</span></span><br><span class="line">num3 = re.sub(<span class="string">u"([^\u0030-\u0039])"</span>, <span class="string">""</span>, num)</span><br><span class="line">print(<span class="string">"字符串只保留数字： "</span>, num3)</span><br><span class="line">num4 = re.sub(<span class="string">"\D"</span>, <span class="string">""</span>, num)</span><br><span class="line">print(<span class="string">"字符串只保留数字： "</span>, num4)</span><br><span class="line"><span class="comment"># 字符串保留数字.和￥</span></span><br><span class="line">num5 = re.sub(<span class="string">u"([^\u0030-\u0039\u002e\uffe5])"</span>, <span class="string">""</span>, num)</span><br><span class="line">print(<span class="string">"字符串保留数字.和￥： "</span>, num5)</span><br><span class="line"><span class="comment"># 字符串只保留英文和数字</span></span><br><span class="line">num6 = re.sub(<span class="string">u"([^\u0041-\u005a\u0061-\u007a\u0030-\u0039])"</span>, <span class="string">""</span>, num)</span><br><span class="line">print(<span class="string">"字符串只保留英文和数字： "</span>, num6)</span><br><span class="line"></span><br><span class="line">s = <span class="string">"ABC今天下雨了，abs不开心123！。?？"</span></span><br><span class="line"><span class="comment"># 字符串只保留中文、字母、数字。</span></span><br><span class="line">num7 = re.sub(<span class="string">u"([^\u4e00-\u9fa5\u0041-\u005a\u0061-\u007a\u0030-\u0039])"</span>, <span class="string">""</span>, s)</span><br><span class="line">print(<span class="string">'字符串只保留汉字、英文和数字： '</span>, num7)</span><br></pre></td></tr></table></figure><p>输出结果如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">原字符串：  a￥1aB23Cqqq$我.04</span><br><span class="line">字符串只保留中文：  我</span><br><span class="line">字符串只保留英文：  aaBCqqq</span><br><span class="line">字符串只保留数字：  12304</span><br><span class="line">字符串只保留数字：  12304</span><br><span class="line">字符串保留数字.和￥：  ￥123.04</span><br><span class="line">字符串只保留英文和数字：  a1aB23Cqqq04</span><br><span class="line">字符串只保留汉字、英文和数字：  ABC今天下雨了abs不开心123</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/BurningSilence/article/details/118488543" target="_blank" rel="noopener">python正则过滤字符串，只保留数字、字母等</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正则表达式很常用，但是规则也很复杂，每次用的时候查资料即可。我这里只是总结一下常用的正则表达式。&lt;/p&gt;
&lt;h2 id=&quot;字符串过滤&quot;&gt;&lt;a href=&quot;#字符串过滤&quot; class=&quot;headerlink&quot; title=&quot;字符串过滤&quot;&gt;&lt;/a&gt;字符串过滤&lt;/h2&gt;&lt;p&gt;我们常常需要根据正则表达式，来对字符串进行过滤。例如仅保留汉字、数字、字母等。用法如下例所示：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# coding = utf-8&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; re&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num = &lt;span class=&quot;string&quot;&gt;&#39;a￥1aB23Cqqq$我.04&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;原字符串： &quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 字符串只保留中文&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num1 = re.sub(&lt;span class=&quot;string&quot;&gt;u&quot;([^\u4e00-\u9fa5])&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;字符串只保留中文： &quot;&lt;/span&gt;, num1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 字符串只保留英文&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num2 = re.sub(&lt;span class=&quot;string&quot;&gt;u&quot;([^\u0041-\u005a\u0061-\u007a])&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;字符串只保留英文： &quot;&lt;/span&gt;, num2)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 字符串只保留数字&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num3 = re.sub(&lt;span class=&quot;string&quot;&gt;u&quot;([^\u0030-\u0039])&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;字符串只保留数字： &quot;&lt;/span&gt;, num3)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num4 = re.sub(&lt;span class=&quot;string&quot;&gt;&quot;\D&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;字符串只保留数字： &quot;&lt;/span&gt;, num4)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 字符串保留数字.和￥&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num5 = re.sub(&lt;span class=&quot;string&quot;&gt;u&quot;([^\u0030-\u0039\u002e\uffe5])&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;字符串保留数字.和￥： &quot;&lt;/span&gt;, num5)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 字符串只保留英文和数字&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num6 = re.sub(&lt;span class=&quot;string&quot;&gt;u&quot;([^\u0041-\u005a\u0061-\u007a\u0030-\u0039])&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, num)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;字符串只保留英文和数字： &quot;&lt;/span&gt;, num6)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;s = &lt;span class=&quot;string&quot;&gt;&quot;ABC今天下雨了，abs不开心123！。?？&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 字符串只保留中文、字母、数字。&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;num7 = re.sub(&lt;span class=&quot;string&quot;&gt;u&quot;([^\u4e00-\u9fa5\u0041-\u005a\u0061-\u007a\u0030-\u0039])&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;, s)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&#39;字符串只保留汉字、英文和数字： &#39;&lt;/span&gt;, num7)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;输出结果如下所示：&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/categories/Python/"/>
    
      <category term="常用第三方包" scheme="https://www.zdaiot.com/categories/Python/%E5%B8%B8%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85/"/>
    
    
      <category term="Python" scheme="https://www.zdaiot.com/tags/Python/"/>
    
      <category term="re" scheme="https://www.zdaiot.com/tags/re/"/>
    
  </entry>
  
  <entry>
    <title>nohup、&amp;、重定向使用</title>
    <link href="https://www.zdaiot.com/Linux/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/nohup%E3%80%81&amp;%E3%80%81%E9%87%8D%E5%AE%9A%E5%90%91%E4%BD%BF%E7%94%A8/"/>
    <id>https://www.zdaiot.com/Linux/常用指令/nohup、&amp;、重定向使用/</id>
    <published>2022-08-26T07:08:00.000Z</published>
    <updated>2022-08-26T07:08:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>事先声明，此文章大部分内容来源于<a href="https://blog.csdn.net/xiaojin21cen/article/details/88991768" target="_blank" rel="noopener">Linux shell 命令中nohup 、&amp;、重定向的使用</a>。不理解的地方我会添加个人见解。</p><h2 id="nohup-和-amp-使用方法"><a href="#nohup-和-amp-使用方法" class="headerlink" title="nohup 和 &amp; 使用方法"></a>nohup 和 &amp; 使用方法</h2><h3 id="nohup-（不挂断）"><a href="#nohup-（不挂断）" class="headerlink" title="nohup （不挂断）"></a>nohup （不挂断）</h3><blockquote><p><code>nohup</code> 是 no hung up 的缩写，意思是不挂断 。</p><p>使用 Xshell 等 Linux 客户端工具，远程执行 Linux 脚本时，有时候会由于网络问题，导致客户端失去连接，终端断开，脚本运行一半就意外结束了。这种时候，就可以用<code>nohup</code> 指令来运行指令，即使客户端与服务端断开，服务端的脚本仍可继续运行。</p></blockquote><p><code>nohup</code> 语法格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup  <span class="built_in">command</span>  [arg...]</span><br></pre></td></tr></table></figure><p><strong>说明：</strong></p><ul><li>除了无法进行输入操作（比如输入命令、换行、打空格等） 外 ，</li><li>标准输出保存到 <code>nohup.out</code>文件中。</li><li>关闭客户端后，命令仍然会运行，<strong>不会挂断</strong>。</li></ul><p><strong>例如：</strong></p><p>执行 <code>nohup sh test.sh</code> 脚本命令后，终端不能接收任何输入，标准输出 会输出到当前目录的<code>nohup.out</code> 文件。即使关闭 xshell 退出后，当前 session 依然继续运行。</p><h3 id="amp-（可交互）"><a href="#amp-（可交互）" class="headerlink" title="&amp; （可交互）"></a>&amp; （可交互）</h3><p><code>&amp;</code> 语法格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">command</span>   [arg...]   &amp;</span><br></pre></td></tr></table></figure><p><strong>说明：</strong></p><ul><li>能进行输入操作（比如输入命令、换行、打空格等），即 <strong>可进行交互</strong> 输入和输出的操作。</li><li>标准输出 保存到 <code>nohup.out</code>文件中。</li><li>但是 关闭客户端后，<strong>程序会就马上停止</strong>。</li></ul><p><strong>例如：</strong></p><p>执行 <code>sh test.sh &amp;</code> 脚本命令后 ，关闭 xshell，脚本程序也立刻停止。</p><h3 id="nohup-和-amp-一块使用（不挂断，可交互）"><a href="#nohup-和-amp-一块使用（不挂断，可交互）" class="headerlink" title="nohup 和 &amp; 一块使用（不挂断，可交互）"></a>nohup 和 &amp; 一块使用（不挂断，可交互）</h3><p>语法格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup   <span class="built_in">command</span>  [arg...]  &amp;</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li>能进行输入操作（比如输入命令、换行、打空格等），即 <strong>可进行交互</strong> 输入和输出的操作，</li><li>标准输出 保存到 <code>nohup.out</code> 中，</li><li>关闭客户端后命令仍然会运行。</li></ul><p><strong>例子：</strong>  </p><p>执行 <code>nohup sh test.sh &amp;</code> 命令后，能进行输入操作，标准输出 的日志写入到 <code>nohup.out</code> 文件，即使关闭 xshell，退出当前 session 后，脚本命令依然继续运行。</p><blockquote><p>输入输出问题已经解决了， 是不是就完美了？ 其实还有一个问题没有解决， 请往下看！</p></blockquote><h2 id="日志的重定向-gt"><a href="#日志的重定向-gt" class="headerlink" title="日志的重定向 &gt;"></a>日志的重定向 &gt;</h2><p>上面提到的日志文件默认名称是 <code>nohup.out</code> ，如果修改日志文件的名称，则用到 <strong><code>重定向</code></strong> ，符号是 <strong><code>&gt;</code></strong> ，语法格式是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; logFile </span><br><span class="line">&gt;&gt; logFile  # 以追加的方式</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>&gt;</code> 是重定向的符号。<code>&gt;&gt;</code>表示输出以追加的方式重定向。</li><li>logFile 是日志文件名称，最好是英文、数字。</li></ul><p>此时， <code>nohup</code>、 <code>&amp;</code> 、 <code>&gt;</code> 三者一块使用的 <strong>语法格式</strong> ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup  <span class="built_in">command</span> &gt;logFile  &amp;</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup  start.sh &gt;aa.log  &amp;</span><br></pre></td></tr></table></figure><p>说明：执行上面的命令后，可以进行输入，也能在后台运行，运行的日志输出到 <code>aa.log</code> 日志中。</p><h2 id="错误信息的处理"><a href="#错误信息的处理" class="headerlink" title="错误信息的处理"></a>错误信息的处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup  <span class="built_in">command</span> &gt;logFile  &amp;</span><br></pre></td></tr></table></figure><p>虽然解决输入输出，后台也能运行问题，但是还有一项是 <strong>错误信息</strong> 无法输出到 日志文件中，要解决这个问题，需要增加命令 <code>2 &gt; file</code> 。</p><p>标准输出 和 错误信息 同时使用，语法格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;logFile1   2 &gt;logFile2</span><br></pre></td></tr></table></figure><p>有人会疑问，<code>2</code> 是什么意思？ 请往下看。</p><h3 id="Linux-标准输入、输出、错误信息的符号"><a href="#Linux-标准输入、输出、错误信息的符号" class="headerlink" title="Linux 标准输入、输出、错误信息的符号"></a>Linux 标准输入、输出、错误信息的符号</h3><p>Linux 标准输入、输出、错误信息的符号：</p><ul><li><code>0</code> 表示 stdin (standard input) <code>标准信息输入</code> ；</li><li><code>1</code> 表示 stdout (standard output) <code>标准信息输出</code> ；</li><li><code>2</code> 表示 stderr (standard error) <code>错误信息</code> ；</li><li><code>/dev/null</code> 表示空设备文件。 如果不想输出任何的日志时，使用此参数 。</li></ul><p>再来回顾上面的示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;logFile1   2 &gt;logFile2</span><br></pre></td></tr></table></figure><ul><li><p><code>&gt; logFile1</code> ：即 <code>1 &gt;logFile1</code>，1 是<code>标准信息输出</code>，是默认的，可以省略，logFile1 是 日志文件名字。</p></li><li><p><code>2 &gt;logFile2</code> ：2 是<code>错误信息</code>，即将 <code>错误信息</code> 输出 到 logFile2 文件中 。</p></li></ul><p>到这时，明白 <code>2</code> 含义了吧！</p><h3 id="3-2、错误信息-和-标准输出-输出在同一个文件中"><a href="#3-2、错误信息-和-标准输出-输出在同一个文件中" class="headerlink" title="3.2、错误信息 和 标准输出 输出在同一个文件中"></a>3.2、错误信息 和 标准输出 输出在同一个文件中</h3><p>如果想把 错误信息 和 标准输出 在同一个文件中 ，使用 <code>2&gt;&amp;1</code> 。 语法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;logFile   2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>&gt;logFile</code> 表示 标准信息 输出到 logFile 文件中；</li><li><code>2&gt;&amp;1</code> 表示 把 2（错误信息） 重定向， 输出到 1（标准输出） 中 。</li></ul><p>两者的共同使用，表示 把 2（错误信息） 、1（标准输出） 都输出到同一个文件（logFile）中。</p><h3 id="思考：不想输出日志信息怎么办-？"><a href="#思考：不想输出日志信息怎么办-？" class="headerlink" title="思考：不想输出日志信息怎么办 ？"></a>思考：不想输出日志信息怎么办 ？</h3><p>提示：<code>/dev/null</code> 表示空设备文件。 如果不想输出任何的日志时，使用此参数 。</p><h2 id="综合使用（推荐）"><a href="#综合使用（推荐）" class="headerlink" title="综合使用（推荐）"></a>综合使用（推荐）</h2><p>综上所述， 功能最全、推荐语法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup  <span class="built_in">command</span>  &gt;logFile   2&gt;&amp;1  &amp;</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup  start.sh &gt; mySysLog.log  2&gt;&amp;1   &amp;</span><br></pre></td></tr></table></figure><p>说明： 执行命令后，并且将 <code>标准输出(1)</code>、<code>错误信息(2)</code> 写入到 mySysLog.log 文件中。</p><h2 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h2><h3 id="不停止服务，直接清空-nohup-out"><a href="#不停止服务，直接清空-nohup-out" class="headerlink" title="不停止服务，直接清空 nohup.out"></a>不停止服务，直接清空 nohup.out</h3><p>如果脚本一直运行下去，nohup.out 日志会一直增长，日志但是硬盘容量有限，怎么把日志文件的大小减少 ？  </p><p>注意，千万别直接删除日志文件，会造成服务无法输出日志，服务异常直接停止运行，这是最严重生产事故。</p><p>不停止服务，直接清空 nohup.out 文件有两种方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第1种：</span></span><br><span class="line">cat /dev/null &gt; nohup.out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第2种：</span></span><br><span class="line">cp /dev/null nohup.out</span><br></pre></td></tr></table></figure><h3 id="只记录警告级别比较高的日志"><a href="#只记录警告级别比较高的日志" class="headerlink" title="只记录警告级别比较高的日志"></a>只记录警告级别比较高的日志</h3><p>输出的日志太多，nohup.out 增长特别快，对于不重要的日记，可以不记录，选择只记录警告级别比较高的日志。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只输出错误信息到日志文件，其它日志不输出</span></span><br><span class="line">nohup ./program &gt; /dev/null   2&gt;error.log  &amp;</span><br></pre></td></tr></table></figure><h3 id="不想输出日志"><a href="#不想输出日志" class="headerlink" title="不想输出日志"></a>不想输出日志</h3><p>不想输出日志，什么日志都不要，只要服务能正常运行就行了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 什么日志也不输出</span></span><br><span class="line">nohup ./program &gt; /dev/null   2&gt;&amp;1   &amp;</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/xiaojin21cen/article/details/88991768" target="_blank" rel="noopener">Linux shell 命令中nohup 、&amp;、重定向的使用</a><br><a href="https://www.runoob.com/linux/linux-shell-io-redirections.html" target="_blank" rel="noopener">Shell 输入/输出重定向</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;事先声明，此文章大部分内容来源于&lt;a href=&quot;https://blog.csdn.net/xiaojin21cen/article/details/88991768&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linux shell 命令中nohup 、&amp;amp;、重定向的使用&lt;/a&gt;。不理解的地方我会添加个人见解。&lt;/p&gt;
&lt;h2 id=&quot;nohup-和-amp-使用方法&quot;&gt;&lt;a href=&quot;#nohup-和-amp-使用方法&quot; class=&quot;headerlink&quot; title=&quot;nohup 和 &amp;amp; 使用方法&quot;&gt;&lt;/a&gt;nohup 和 &amp;amp; 使用方法&lt;/h2&gt;&lt;h3 id=&quot;nohup-（不挂断）&quot;&gt;&lt;a href=&quot;#nohup-（不挂断）&quot; class=&quot;headerlink&quot; title=&quot;nohup （不挂断）&quot;&gt;&lt;/a&gt;nohup （不挂断）&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;nohup&lt;/code&gt; 是 no hung up 的缩写，意思是不挂断 。&lt;/p&gt;
&lt;p&gt;使用 Xshell 等 Linux 客户端工具，远程执行 Linux 脚本时，有时候会由于网络问题，导致客户端失去连接，终端断开，脚本运行一半就意外结束了。这种时候，就可以用&lt;code&gt;nohup&lt;/code&gt; 指令来运行指令，即使客户端与服务端断开，服务端的脚本仍可继续运行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;nohup&lt;/code&gt; 语法格式：&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/categories/Linux/"/>
    
      <category term="常用指令" scheme="https://www.zdaiot.com/categories/Linux/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/tags/Linux/"/>
    
      <category term="nohup" scheme="https://www.zdaiot.com/tags/nohup/"/>
    
  </entry>
  
  <entry>
    <title>top指令详解</title>
    <link href="https://www.zdaiot.com/Linux/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/top%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3/"/>
    <id>https://www.zdaiot.com/Linux/常用指令/top指令详解/</id>
    <published>2022-07-20T02:51:29.000Z</published>
    <updated>2022-07-20T02:51:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>注意以下内容，主要参考了<a href="https://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316399.html" target="_blank" rel="noopener">linux的top命令参数详解</a>。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。</p><p>top显示系统当前的进程和其他状况，是一个动态显示过程，即可以通过用户按键来不断刷新当前状态。如果在前台执行该命令，它将独占前台，直到用户终止该程序为止。 比较准确的说，top命令提供了实时的对系统处理器的状态监视。它将显示系统中CPU最“敏感”的任务列表。该命令可以按CPU使用。内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。 </p><h2 id="参数含义"><a href="#参数含义" class="headerlink" title="参数含义"></a>参数含义</h2><p>下面详细介绍它的使用方法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">top - 01:06:48 up  1:22,  1 user,  load average: 0.06, 0.60, 0.48</span><br><span class="line">Tasks:  29 total,   1 running,  28 sleeping,   0 stopped,   0 zombie</span><br><span class="line">Cpu(s):  0.3% us,  1.0% sy,  0.0% ni, 98.7% id,  0.0% wa,  0.0% hi,  0.0% si</span><br><span class="line">Mem:    191272k total,   173656k used,    17616k free,    22052k buffers</span><br><span class="line">Swap:   192772k total,        0k used,   192772k free,   123988k cached</span><br><span class="line"></span><br><span class="line">PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND</span><br><span class="line">1379 root      16   0  7976 2456 1980 S  0.7  1.3   0:11.03 sshd</span><br><span class="line">14704 root      16   0  2128  980  796 R  0.7  0.5   0:02.72 top</span><br><span class="line">1 root      16   0  1992  632  544 S  0.0  0.3   0:00.90 init</span><br><span class="line">2 root      34  19     0    0    0 S  0.0  0.0   0:00.00 ksoftirqd/0</span><br><span class="line">3 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 watchdog/0</span><br></pre></td></tr></table></figure><p>统计信息区前五行是系统整体的统计信息。第一行是任务队列信息，同 uptime 命令的执行结果。其内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">01:06:48    当前时间</span><br><span class="line">up 1:22    系统运行时间，格式为时:分</span><br><span class="line">1 user    当前登录用户数</span><br><span class="line">load average: 0.06, 0.60, 0.48    系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。</span><br></pre></td></tr></table></figure><p>第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">total 进程总数</span><br><span class="line">running 正在运行的进程数</span><br><span class="line">sleeping 睡眠的进程数</span><br><span class="line">stopped 停止的进程数</span><br><span class="line">zombie 僵尸进程数</span><br><span class="line">Cpu(s): </span><br><span class="line">0.3% us 用户空间占用CPU百分比</span><br><span class="line">1.0% sy 内核空间占用CPU百分比</span><br><span class="line">0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比</span><br><span class="line">98.7% id 空闲CPU百分比</span><br><span class="line">0.0% wa 等待输入输出的CPU时间百分比</span><br><span class="line">0.0%hi：硬件CPU中断占用百分比</span><br><span class="line">0.0%si：软中断占用百分比</span><br><span class="line">0.0%st：虚拟机占用百分比</span><br></pre></td></tr></table></figure><p>最后两行为内存信息。内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Mem:</span><br><span class="line">191272k total    物理内存总量</span><br><span class="line">173656k used    使用的物理内存总量</span><br><span class="line">17616k free    空闲内存总量</span><br><span class="line">22052k buffers    用作内核缓存的内存量</span><br><span class="line">Swap: </span><br><span class="line">192772k total    交换区总量</span><br><span class="line">0k used    使用的交换区总量</span><br><span class="line">192772k free    空闲交换区总量</span><br><span class="line">123988k cached    缓冲的交换区总量，内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小，相应的内存再次被换出时可不必再对交换区写入。</span><br></pre></td></tr></table></figure><p>进程信息区统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">序号  列名    含义</span><br><span class="line">a    PID     进程id</span><br><span class="line">b    PPID    父进程id</span><br><span class="line">c    RUSER   Real user name</span><br><span class="line">d    UID     进程所有者的用户id</span><br><span class="line">e    USER    进程所有者的用户名</span><br><span class="line">f    GROUP   进程所有者的组名</span><br><span class="line">g    TTY     启动进程的终端名。不是从终端启动的进程则显示为 ?</span><br><span class="line">h    PR      优先级</span><br><span class="line">i    NI      nice值。负值表示高优先级，正值表示低优先级</span><br><span class="line">j    P       最后使用的CPU，仅在多CPU环境下有意义</span><br><span class="line">k    %CPU    上次更新到现在的CPU时间占用百分比</span><br><span class="line">l    TIME    进程使用的CPU时间总计，单位秒</span><br><span class="line">m    TIME+   进程使用的CPU时间总计，单位1/100秒</span><br><span class="line">n    %MEM    进程使用的物理内存百分比</span><br><span class="line">o    VIRT    进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES</span><br><span class="line">p    SWAP    进程使用的虚拟内存中，被换出的大小，单位kb。</span><br><span class="line">q    RES     进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA</span><br><span class="line">r    CODE    可执行代码占用的物理内存大小，单位kb</span><br><span class="line">s    DATA    可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb</span><br><span class="line">t    SHR     共享内存大小，单位kb</span><br><span class="line">u    nFLT    页面错误次数</span><br><span class="line">v    nDRT    最后一次写入到现在，被修改过的页面数。</span><br><span class="line">w    S       进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程)</span><br><span class="line">x    COMMAND 命令名/命令行</span><br><span class="line">y    WCHAN   若该进程在睡眠，则显示睡眠中的系统函数名</span><br><span class="line">z    Flags   任务标志，参考 sched.h</span><br></pre></td></tr></table></figure><p>默认情况下仅显示比较重要的 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND 列。可以通过下面的快捷键来更改显示内容。 </p><ul><li>更改显示内容通过 <strong>f</strong> 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。</li><li>按 <strong>o</strong> 键可以改变列的显示顺序。按小写的 a-z 可以将相应的列向右移动，而大写的 A-Z 可以将相应的列向左移动。最后按回车键确定。</li><li>按大写的 <strong>F</strong> 或 <strong>O</strong> 键，然后按 a-z 可以将进程按照相应的列进行排序。而大写的 <strong>R</strong> 键可以将当前的排序倒转。</li></ul><h2 id="命令使用"><a href="#命令使用" class="headerlink" title="命令使用"></a>命令使用</h2><p>top使用格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top [-] [d] [p] [q] [c] [C] [S] [s] [n]</span><br></pre></td></tr></table></figure><p>参数说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d 指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。 </span><br><span class="line">p 通过指定监控进程ID来仅仅监控某个进程的状态。 </span><br><span class="line">q 该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。 </span><br><span class="line">S 指定累计模式 </span><br><span class="line">s 使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。 </span><br><span class="line">i 使top不显示任何闲置或者僵死进程。 </span><br><span class="line">c 显示整个命令行而不只是显示命令名</span><br></pre></td></tr></table></figure><p>附常用操作:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">top   //每隔5秒显式所有进程的资源占用情况</span><br><span class="line">top -d 2  //每隔2秒显式所有进程的资源占用情况</span><br><span class="line">top -c  //每隔5秒显式进程的资源占用情况，并显示进程的命令行参数(默认只有进程名)</span><br><span class="line">top -p 12345 -p 6789//每隔5秒显示pid是12345和pid是6789的两个进程的资源占用情况</span><br><span class="line">top -d 2 -c -p 123456 //每隔2秒显示pid是12345的进程的资源使用情况，并显式该进程启动的命令行参数</span><br></pre></td></tr></table></figure><h2 id="交互命令"><a href="#交互命令" class="headerlink" title="交互命令"></a>交互命令</h2><p>在top命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了-s选项， 其中一些命令可能会被屏蔽。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">h：显示帮助画面，给出一些简短的命令总结说明；</span><br><span class="line">k：终止一个进程；</span><br><span class="line">i：忽略闲置和僵死进程，这是一个开关式命令；</span><br><span class="line">q：退出程序；</span><br><span class="line">r：重新安排一个进程的优先级别；</span><br><span class="line">S：切换到累计模式；</span><br><span class="line">s：改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。</span><br><span class="line">输入0值则系统将不断刷新，默认值是5s；</span><br><span class="line"></span><br><span class="line">f或者F：从当前显示中添加或者删除项目；</span><br><span class="line">o或者O：改变显示项目的顺序；</span><br><span class="line">l：切换显示平均负载和启动时间信息；</span><br><span class="line">m：切换显示内存信息；</span><br><span class="line">t：切换显示进程和CPU状态信息；</span><br><span class="line">c：切换显示命令名称和完整命令行；</span><br><span class="line">M：根据驻留内存大小进行排序；</span><br><span class="line">P：根据CPU使用百分比大小进行排序；</span><br><span class="line">T：根据时间/累计时间进行排序；</span><br><span class="line">w：将当前设置写入~/.toprc文件中。</span><br></pre></td></tr></table></figure><h2 id="单位切换"><a href="#单位切换" class="headerlink" title="单位切换"></a>单位切换</h2><p>顶部的内存信息可以在top运行时按E切换，每次切换转换率为1000，只是没有单位，切换的单位为 k,m,g,t,p。</p><p>例如：</p><p><img src="/Linux/常用指令/top指令详解/top1-165828697137611.jpg" alt="top1"></p><p><img src="/Linux/常用指令/top指令详解/top2-165828701166415.jpg" alt="top2"></p><p>底下的进程信息按e切换，每次切换转换率为1000，切换的单位也是 k,m,g,t,p。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316399.html" target="_blank" rel="noopener">linux的top命令参数详解</a><br><a href="https://blog.kelu.org/tech/2017/10/07/linux-top-switch-ram.html" target="_blank" rel="noopener">Linux 在 TOP 命令中切换内存的显示单位</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;注意以下内容，主要参考了&lt;a href=&quot;https://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316399.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;linux的top命令参数详解&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。&lt;/p&gt;
&lt;p&gt;top显示系统当前的进程和其他状况，是一个动态显示过程，即可以通过用户按键来不断刷新当前状态。如果在前台执行该命令，它将独占前台，直到用户终止该程序为止。 比较准确的说，top命令提供了实时的对系统处理器的状态监视。它将显示系统中CPU最“敏感”的任务列表。该命令可以按CPU使用。内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。 &lt;/p&gt;
&lt;h2 id=&quot;参数含义&quot;&gt;&lt;a href=&quot;#参数含义&quot; class=&quot;headerlink&quot; title=&quot;参数含义&quot;&gt;&lt;/a&gt;参数含义&lt;/h2&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.zdaiot.com/categories/Linux/"/>
    
      <category term="常用指令" scheme="https://www.zdaiot.com/categories/Linux/%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    
    
      <category term="top" scheme="https://www.zdaiot.com/tags/top/"/>
    
  </entry>
  
  <entry>
    <title>Expressive TTS</title>
    <link href="https://www.zdaiot.com/DeepLearningApplications/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/Expressive%20TTS/"/>
    <id>https://www.zdaiot.com/DeepLearningApplications/语音合成/Expressive TTS/</id>
    <published>2022-06-04T17:24:03.000Z</published>
    <updated>2022-06-04T17:24:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要来源于<a href="https://zhuanlan.zhihu.com/p/507708489" target="_blank" rel="noopener">Expressive TTS（01）</a>。</p><p>Expressive TTS是目前语音合成领域中比较活跃的方向，它和单纯TTS的区别是，它更关注合成声音的风格（例如新闻播报，讲故事，解说）、情感（例如生气，兴奋，悲伤）、韵律（例如重读，强调、语调）等等。自从深度学习技术大放异彩后，语音合成模型在合成声音的自然度方面有了极大的提高（例如Tacotron，Tacotron2，WaveNet），跳词复读的问题也在最近得到了解决（例如DurIAN，FastSpeech），而深度学习不仅可以让语音的自然度得到大幅度的提升，对一些难以显式建模的特征上也有很强大的学习能力，因此，让语音合成能更加具有expressive成为了一个研究热点。</p><h2 id="第一篇：Uncovering-Latent-Style-Factors-for-Expressive-Speech-Synthesis（2017）"><a href="#第一篇：Uncovering-Latent-Style-Factors-for-Expressive-Speech-Synthesis（2017）" class="headerlink" title="第一篇：Uncovering Latent Style Factors for Expressive Speech Synthesis（2017）"></a>第一篇：Uncovering Latent Style Factors for Expressive Speech Synthesis（2017）</h2><p><a href="https://arxiv.org/abs/1711.00520" target="_blank" rel="noopener">https://arxiv.org/abs/1711.00520</a></p><p>这一篇是 google 的王宇轩大佬早在 2017 年上传到 arxiv 上的一篇文章，也是表现力语音合成领域可追溯到的比较早的一篇文章。</p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>理想情况下，生成的语音应该<strong>传达正确的信息（可理解性 intelligibility）</strong>，同时<strong>听起来像人类的语言（自然性 naturalness）</strong>，具有<strong>正确的语调（表现力 expressiveness）</strong>。 然而，大多数现有的合成模型如 Tacotron 仅关注前两个问题，并没有明确地对语调进行建模。尽管有重要的应用，如对话助手和长篇阅读，但富有表现力的 TTS 仍然被认为是一个重要的开放问题。</p><blockquote><p>韵律变化本质上是多尺度的。音调和说话持续时间的局部变化可以传达语义，而整体音调轨迹等全局属性可以传达情绪和情感。</p></blockquote><p>在这项工作中，作者引入了 “风格标记（style tokens）” 的概念，它可以被视为捕捉韵律变化的潜在变量，而单靠文本输入是无法捕捉的。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-4e78816e425a73de0cc8d4d757b0e83c_r.jpg" alt></p><p>理想情况下，一个富有表现力的 TTS 模型应该允许在合成过程中明确地控制对韵律（prosody）的选择。由于 Tacotron 只将文本作为输入，为了准确地重建训练信号，它必须学会将任何韵律信息隐式地储存在它的权重中，而我们并不能明确地控制它。为了允许显式的对韵律进行控制，作者在 Tacotron 中引入了一个专门的网络组件，用一个新的风格注意力模块（style attention）来增强现有的文本编码器的注意力模块（text attention）。新的注意力模块关注一个风格编码器（style encoder），它将 K 个 “风格标记” 作为输入，并输出它们的嵌入向量来作为风格注意力模块的输入（可以简单的理解为 K 个风格标记就是 1 到 K 的数字，然后经过一个 embedding，得到 K 个一维的 embedding vector 作为 style attention 的输入）。在解码器中，通过一种加权求和的操作将来自文本注意力和风格注意力的两个上下文向量结合起来。计算 weighted 的操作作者称为一个 controller layer（在图中并未明显体现，文章提到 The weights are predicted by a single layer MLP with sigmoid outputs）。Tacotron 模型的其余部分保持不变。  </p><p>作者提到 style tokens 的嵌入值是随机初始化的，并通过反向传播自动学习，它们的学习仅由解码器的重建损失指导。因此，风格标记本身的学习是完全无监督的。</p><p>为什么使用基于注意力的风格标记呢？</p><blockquote><p>首先，注意力有助于学习整体韵律风格的解耦（decomposition），鼓励产生具有独立韵律风格的可解释 tokens。这类似于学习一个风格原子的字典，可以结合起来重现整体风格（举个例子：每个原子都有自己的一个风格如 A 原子语速快、B 原子音调高，将 AB 原子组合起来可能能够产生一种听起来生气的情绪（即合成的语音语速又快音调又高））。<br>此外，注意力机制在解码器的时间分辨率上学习风格标记的组合，这使得时间变化的韵律操作成为可能。（简单的理解为就是比如说在生成一句长时间语音的时候，某个时刻可能音调高，某个时刻可能语速快些，从而生成的这一段语音在时间维度上可以组合不同的风格标记）。</p></blockquote><p>为什么这种方式能够在合成语音的时候实现可控性（controllability）呢？</p><blockquote><p>源自于风格编码器和文本编码器之间的一个重要区别。文本编码器是以输入的文本序列为条件的，而风格编码器则不需要输入，所有的训练序列都共享 tokens。换句话说，风格编码器计算的是训练集的先验 prior，而文本编码器计算的是后验 posteriors（以单个输入序列为条件）。这种设计允许风格标记捕捉与文本无关的韵律变化，这使得推理中的可控性得以实现。</p></blockquote><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><a href="https://link.zhihu.com/?target=https%3A//google.github.io/tacotron/publications/uncovering_latent_style_factors_for_expressive_speech_synthesis/" target="_blank" rel="noopener">Sound demos</a></p><p>为了合成特定风格的语音，作者将所选风格标记的嵌入向量广播式地添加到完整的风格嵌入矩阵 style embedding matrix 中，从而使合成的语音偏向于指定风格。同样，可以通过连续广播添加或线性内插风格嵌入向量来混合不同的风格。</p><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-36c6d4fdad3edd49131f72fcef87534d_r.jpg" alt></p><p>从图 2 中可以看出，”token 1” 大致对应于具有正常音高范围的马虎、草率（sloppy）风格，”token 8” 大致对应于机器人声音的风格，而 “token 9” 大致对应于高音调声音。这些风格在一定程度上反映在平滑的 F0 轨迹上。例如，”token 9” 倾向于比其他两个有更高的音调，而 “token 8” 的音调轨迹则保持平缓和低沉。同样的趋势可以从图 2(b) 中看出，该图是由不同的语句产生的，表明风格标记的运作独立于文本输入。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>本文提出的风格标记（style tokens）可以在无监督的情况下学习，不需要注释的标签。在 Tacotron 模型中实现了风格标记，并证明它们确实对应于不同的声音风格因素，通过在推理中指定所需的风格来实现某种程度的声音控制。</p><h2 id="第二篇：-Style-Tokens-Unsupervised-Style-Modeling-Control-and-Transfer-in-End-to-End-Speech-Synthesis（2018）"><a href="#第二篇：-Style-Tokens-Unsupervised-Style-Modeling-Control-and-Transfer-in-End-to-End-Speech-Synthesis（2018）" class="headerlink" title="第二篇： Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis（2018）"></a>第二篇： Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis（2018）</h2><p><a href="https://arxiv.org/abs/1803.09017" target="_blank" rel="noopener">https://arxiv.org/abs/1803.09017</a></p><p>这一篇是王大佬接第一篇的续作，在这一篇文章中王大佬正式提出了 GSTs（global style tokens）的概念。</p><h3 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h3><p>韵律 prosody 是语音中一些现象的汇合，如副语言信息、语调 intonation、重音 stress 和风格 style。这篇文章主要关注 style modeling，它的目标是为模型提供一种能力，这种能力能够为给定内容选择一种说话风格。风格包含丰富的信息，如意图和情感，并影响说话人对语调和语速的选择。适当的风格呈现会影响整体感知。</p><p>风格建模 style modeling 有如下几个挑战：</p><ol><li>对 “正确的” 声音风格没有客观的衡量标准，这使得建模和评估都很困难。获取大型数据集的注释可能成本很高，而且同样存在问题，因为人类评分者经常会有分歧。</li><li>富有表现力的声音的高动态范围（如音调的高低起伏大）很难建模。许多 TTS 模型，包括最近的端到端系统，只在其输入数据上学习平均的声调分布（因为输入只有文本信息，不包含声学信息），产生的语音尤其是长句子的表现力较差。并且，它们往往缺乏控制语音合成的表达方式的能力。</li></ol><h3 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-56b8b6297cc6b6ef0a4cb38b727ef602_r.jpg" alt></p><ul><li><strong>训练期间：</strong></li></ul><p>1）参考编码器 reference encoder，将可变长度的音频序列的 prosody 压缩成一个固定长度的向量，称之为 reference embedding。在训练期间，参考音频是真实音频。</p><blockquote><p>reference encoder 架构的细节：输入 log-mel spectrogram ——&gt; 6*(2DConv+Batch Norm+ReLU) ——&gt;reshape 3 dimensions (保留时间维度，将 channel 和 freq reshape 成一维，即 [batch_size, channel, freq, time]——&gt;[batch_size, channel*freq, time]) ——&gt;single layer unidirectional GRU (128 unit) ——&gt; 输出：reference embedding (the last GRU state)。</p></blockquote><p>2）reference embedding 被传递到一个风格标记层 style token layer，在那里它被用作注意力模块的查询向量。注意，这里的注意力不是用来学习对齐的。相反，它学习 referensce embedding 和随机初始化的嵌入库中 (a bank of randomly initialized embeddings) 的每个 token 之间的相似性测量。这组嵌入被称之为全局风格标记 global style tokens (GSTs) 或标记嵌入 token embeddings，在所有训练序列中共享。</p><p>3) 注意力模块输出一组组合权重，代表每个 style token 对 referensce embedding 的贡献。GSTs 的加权总和被称之为风格嵌入 style embedding，在每个时间段被传递给文本编码器进行调节。</p><blockquote><p>style token layer 由 10 (实验发现 10 个足以代表训练数据中小而丰富的韵律维度) 个 style token embeddings (为了和 text encoder 匹配，所以维度是 256D) 和一个 multi-head attention 组成。输入 reference embedding(128D) 和 style token embeddings(256D) ——&gt; multi-head attention ——&gt; 输出 style embedding (256D)。最后将 style embedding 加到对应的 text encoder states 上。</p></blockquote><p>4）style token layer 与模型的其他部分共同训练，只由 Tacotron 解码器的重建损失驱动。因此，GSTs 不需要任何明确的风格或韵律标签。</p><ul><li><strong>推理期间：</strong></li></ul><p>1）可以直接将文本编码器限定在某些标记上，如图 3 推理模式图的右侧所描述的（”以 tokenB 为条件”）。这允许在没有参考音频的情况下进行风格控制和操作。</p><p>2）可以输入一个不同的音频（其对应的文本内容不需要与要合成的文本相同）来实现风格转移。这在图 3 的推理模式图的左侧被描绘出来（”以音频为条件”）。</p><p>总的来说，GSTs 模型可以被认为是一种将 reference embedding 分解为一组基础向量或 style token embedding 的端到端方法。GSTs 层在概念上与 VQ-VAE 编码器有些类似，因为它学习了其输入的量化表示。感觉用 VQ-VAE 来做 reference embedding 的解耦也可以，但是作者说实验效果很差。</p><p><strong>GSTs embeddings 也可以被看作是一个外部存储器，存储从训练数据中提取的风格信息。参考信号在训练时指导记忆的写入，在推理时指导记忆的读取。</strong></p><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p><a href="https://link.zhihu.com/?target=https%3A//google.github.io/tacotron/publications/global_style_tokens/" target="_blank" rel="noopener">Sound demos</a></p><p>感兴趣的可以点击上边这个链接听一下文章提供的 Audio samples。</p><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>这项工作介绍了 GSTs。GSTs 是直观的，不需要明确的标签就能学习。当对富有表现力的语音数据进行训练时，GSTs 模型会产生可解释的嵌入 embeddings，可用于控制 control 和转移风格 transfer style。</p><p>然而，在实验中也发现了一些问题，比如并非所有的 single-token 都能捕捉到单一的属性：虽然一个 token 可能学会了代表说话速度，但其他 tokens 可能学会了反映训练数据中风格共现的混合属性（例如，一个低音调的 token 也能编码较慢的说话速度）。探索更多独立的风格属性学习（可以理解为解耦的更彻底）仍是目前工作的一个重点。</p><h2 id="第三篇：-Learning-latent-representations-for-style-control-and-transfer-in-end-to-end-speech-synthesis（2019）"><a href="#第三篇：-Learning-latent-representations-for-style-control-and-transfer-in-end-to-end-speech-synthesis（2019）" class="headerlink" title="第三篇： Learning latent representations for style control and transfer in end-to-end speech synthesis（2019）"></a>第三篇： Learning latent representations for style control and transfer in end-to-end speech synthesis（2019）</h2><p>Learning latent representations for style control and transfer in end-to-end speech synthesis</p><h3 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h3><p>VAE 有很多优点，如学习分解因素、平滑插值或在潜在表征 latent representation 之间连续取样，可以获得可解释的同（重）构体。</p><blockquote><p>最主要的就是，VAE 可以很容易地得到解耦 disentangle 之后的 latent code，每个 latent code 的维度都可以代表一个特定的概念，通过调整某个概念的值，我们就能控制特定的概念。比如在 image synthesis 中，调整特定维度的 latent code 就可以控制合成出来的物体的角度、大小等特定概念。</p></blockquote><p>直观地说，在语音合成中，说话人的潜在状态 latent state，如 affect 和意图 intent，有助于形成韵律 prosody、情感 emotion 或说话风格 speaker style。latent state 所起的作用与 VAE 中的潜在表征 latent representation 相当相似。因此，本文将 VAE 引入 Tacotron2，以学习说话人状态在连续空间中的潜态表示，并进一步控制语音合成中的说话风格。</p><h3 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-6b6d453d2502259d73e6c47cfcb0bec3_r.jpg" alt></p><p>如图 4 所示，整个网络结构由两部分组成，(1) 识别模型或推理网络，它将参考音频编码为固定长度的潜在表示（潜在表征 z 代表风格表示）；（2）一个基于 Tacotron2 的端到端 TTS 模型，它将综合的编码器状态（包括潜在表征和文本编码器状态）转换为具有特定风格的目标句。</p><blockquote><p>识别模型架构的细节（其中 reference encoder 和第二篇一致）：输入 mel spectrogram ——&gt; 6*(2DConv+Batch Norm+ReLU) ——&gt;reshape 3 dimensions (保留时间维度，将 channel 和 freq reshape 成一维，即 [batch_size, channel, freq, time]——&gt;[batch_size, channel*freq, time]) ——&gt;a GRU ——&gt; 输出：reference embedding (the last GRU state) ——&gt; 两个单独的全连接层 + linear activation function——&gt; 输出：均值和方差 ——&gt; 重采样 reparameterization ——&gt;输出：z。  </p><p>输出的 text encoder state 加上 z（先经过一个 linear layer 调整一下维度）被送到 Tacotron2 的 decoder 中。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-28890d23b720e5209f78594f7596061f_r.jpg" alt></p><p>损失函数 loss 为 VAE 的损失 (其中重构损失选择 L2-loss)+ $l_{stop}$ (stop token loss)。</p><h3 id="KL-collapse-problem"><a href="#KL-collapse-problem" class="headerlink" title="KL collapse problem"></a>KL collapse problem</h3><p>所谓 KL collapse problem 是指在训练过程中 KL loss 下降得比其它 loss 都快从而造成 KL loss 很快收敛到 0 并不再上升，这会造成 encoder 无法继续训练。因此作者使用了 KL annealing 来解决这个问题，具体来说，首先用一个动态调整的 weight 来控制 KL loss 的训练强度，其次是减少 KL loss 的训练次数，即每隔 K 个 step 才训练一次 KL loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kl_anneal_function</span><span class="params">(self, anneal_function, lag, step, k, x0, upper)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> anneal_function == <span class="string">'logistic'</span>:</span><br><span class="line">            <span class="keyword">return</span> float(upper/(upper+np.exp(-k*(step-x0))))</span><br><span class="line">        <span class="keyword">elif</span> anneal_function == <span class="string">'linear'</span>:</span><br><span class="line">            <span class="keyword">if</span> step &gt; lag:</span><br><span class="line">                <span class="keyword">return</span> min(upper, step/x0)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> anneal_function == <span class="string">'constant'</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># KL Divergence Loss</span></span><br><span class="line">kl_loss = <span class="number">-0.5</span> * torch.sum(<span class="number">1</span> + logvar - mu.pow(<span class="number">2</span>) - logvar.exp())</span><br><span class="line">kl_weight = self.kl_anneal_function(self.anneal_function, self.lag, step, self.k, self.x0, self.upper)</span><br><span class="line"></span><br><span class="line">total_loss = (mel_loss + gate_loss + kl_weight * kl_loss)</span><br></pre></td></tr></table></figure><h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><p>在推理阶段，在 style control 评估中，直接操作 z，不需要经过整个识别模型。在 style transfer 的评估中，需要把音频片段作为参考，并通过识别模型。 Parallel transfer 意味着目标文本信息与参考音频的相同，反之 non-parallel style transfer 意味着目标文本信息与参考音频的不同。</p><p><a href="http://home.ustc.edu.cn/~zyj008/ICASSP2019/" target="_blank" rel="noopener">Speech Demo</a></p><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-fe5c19623b709f8167cfea4775591632_r.jpg" alt></p><p>如图 5 所示，这两个 z 是通过向识别模型提供两个参考音频而得到的， $z_a$表示说话语速快和高音调， $z_d$表示说话语速慢和低音调。通过对这两个 z 的插值 interpolation 操作，可以看到生成的语音的音高和语速都在逐渐下降。这一结果表明，学习到的潜在空间在控制声谱图的趋势方面是连续的，这将进一步反映在风格的变化上。</p><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-9c2ad5e41f8b669875d4da61f50830eb_r.jpg" alt></p><p>一个解耦过的表征（也就是 z）意味着一个潜在变量（也就是其中的一个维度或者值）能够完全单独控制一个概念，并且对其他因素的变化没有影响。从图 6 可以看出，通过操纵单一维度而固定其他维度对声谱图的改变，调整其中一个维度，生成的语音只有一个属性发生变化，如 z 的几个维度可以分别控制着合成语音的音高、局部音调的变化、语速等风格属性。这表明，在这个模型中，VAE 具有学习解耦 latent state 的能力。</p><p><img src="/DeepLearningApplications/语音合成/Expressive TTS/v2-8b91f7edc330186214b7d222c01b4e73_r.jpg" alt></p><p>从图 7 可以看出，生成的声谱图和参考的声谱图具有相似的属性。</p><h3 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h3><p>本文提出了 VAE+Tacotron2 来对生成的语音进行控制，最终得到了与 GSTs 模型相似的效果。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/507708489" target="_blank" rel="noopener">Expressive TTS（01）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要来源于&lt;a href=&quot;https://zhuanlan.zhihu.com/p/507708489&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Expressive TTS（01）&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Expressive TTS是目前语音合成领域中比较活跃的方向，它和单纯TTS的区别是，它更关注合成声音的风格（例如新闻播报，讲故事，解说）、情感（例如生气，兴奋，悲伤）、韵律（例如重读，强调、语调）等等。自从深度学习技术大放异彩后，语音合成模型在合成声音的自然度方面有了极大的提高（例如Tacotron，Tacotron2，WaveNet），跳词复读的问题也在最近得到了解决（例如DurIAN，FastSpeech），而深度学习不仅可以让语音的自然度得到大幅度的提升，对一些难以显式建模的特征上也有很强大的学习能力，因此，让语音合成能更加具有expressive成为了一个研究热点。&lt;/p&gt;
&lt;h2 id=&quot;第一篇：Uncovering-Latent-Style-Factors-for-Expressive-Speech-Synthesis（2017）&quot;&gt;&lt;a href=&quot;#第一篇：Uncovering-Latent-Style-Factors-for-Expressive-Speech-Synthesis（2017）&quot; class=&quot;headerlink&quot; title=&quot;第一篇：Uncovering Latent Style Factors for Expressive Speech Synthesis（2017）&quot;&gt;&lt;/a&gt;第一篇：Uncovering Latent Style Factors for Expressive Speech Synthesis（2017）&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.00520&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1711.00520&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这一篇是 google 的王宇轩大佬早在 2017 年上传到 arxiv 上的一篇文章，也是表现力语音合成领域可追溯到的比较早的一篇文章。&lt;/p&gt;
    
    </summary>
    
    
      <category term="DeepLearningApplications" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/"/>
    
      <category term="语音合成" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
    
      <category term="语音合成" scheme="https://www.zdaiot.com/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
      <category term="Expressive TTS" scheme="https://www.zdaiot.com/tags/Expressive-TTS/"/>
    
  </entry>
  
  <entry>
    <title>语音合成模型</title>
    <link href="https://www.zdaiot.com/DeepLearningApplications/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <id>https://www.zdaiot.com/DeepLearningApplications/语音合成/语音合成模型/</id>
    <published>2022-06-04T12:28:03.000Z</published>
    <updated>2022-06-04T12:28:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇介绍几个经典的语音合成模型。</p><h2 id="什么是语音合成"><a href="#什么是语音合成" class="headerlink" title="什么是语音合成"></a>什么是语音合成</h2><p>语音合成是通过文字人工生成人类声音， 也可以说语音生成是给定一段文字去生成对应的人类读音。 这里声音是一个连续的模拟的信号。而合成过程是通过计算机， 数字信号去模拟。 这里就需要数字信号处理模拟信号信息，详细内容可参考 [1]。</p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/webp" alt="img"></p><p>Fig. 1 an example of voice signal. </p><p>图片1， 就是一个例子用来表示人类声音的信号图。 这里横轴是时间， 纵轴是声音幅度大小。声音有三个重要的指标，<strong>振幅（amplitude）</strong>, <strong>周期（period）</strong>和<strong>频率（frequency）</strong>。 振幅指的是波的高低幅度，表示声音的强弱，周期和频率互为倒数的关系， 用来表示两个波之间的时间长度，或者每秒震动的次数。 而声音合成是根据声波的特点， 用数字的方式去生成类似人声的频率和振幅， 即音频的数字化。了解了音频的数字化，也就知道了我们要生成的目标函数。</p><p>音频的数字化主要有三个步骤。</p><p><strong>取样（sampling）</strong>：在音频数字化的过程，采样是指一个固定的频率对音频信号进行采样， 采样的频率越高， 对应的音频数据的保真度就越好。 当然， 数据量越大，需要的内存也就越大。 如果想完全无损采样， 需要使用Nyquist sampling frequency， 就是原音频的频率2倍。</p><p><strong>量化 （quantization）</strong>： 采样的信号都要进行量化， 把信号的幅度变成有限的离散数值。比如从0 到 1， 只有 四个量化值可以用0， 0.25， 0.5， 0.75的话， 量化就是选择最近的量化值来表示。</p><p><strong>编码 （coding</strong>）：编码就是把每个数值用二进制的方式表示， 比如上面的例子， 就可以用2bit 二进制表示, 00, 01, 10, 11。 这样的数值用来保存在计算机上。</p><p>采样频率和采样量化级数是数字化声音的两个主要指标，直接影响声音的效果。 对于语音合成也是同样， 生成更高的采样频率和更多多的量化级数（比如16 bit）, 会产生更真实的声音。 通常有三个采样频率标准：</p><p><strong>1.</strong> 44.1kHz 采样， 用于高品质CD 音乐</p><p><strong>2.</strong> 22.05kHz 采样， 用于语音通话， 中品质音乐</p><p><strong>3</strong>. 11.025kHz 采样， 用于低品质声音。</p><p>而量化标准一般有8位字长（256阶）低品质量化 和16位字长（65536阶）高品质量化。</p><p>还有一个重要参数就是通道（channel）, 一次只采样一个声音波形为单通道， 一次采样多个声音波形就是多通道。</p><p>所以在语音合成的时候，产生的数据量是 <em>数据量=采样频率\</em> 量化位数*声道数<em>， 单位是bit/s。 一般声道数都假设为1.。 <em>*采样率和量化位数都是语音合成里的重要指标，也就是设计好的神经网络1秒钟必须生成的数据量</em></em>。</p><h2 id="语音合成流程"><a href="#语音合成流程" class="headerlink" title="语音合成流程"></a>语音合成流程</h2><p><img src="/DeepLearningApplications/语音合成/语音合成模型/webp-20220604233848795" alt="img" style="zoom:67%;"></p><p>Fig. 2 Two stage text-to-speech synthsis (source [2]) </p><h3 id="文本分析（text-analysis）"><a href="#文本分析（text-analysis）" class="headerlink" title="文本分析（text analysis）"></a>文本分析（text analysis）</h3><p>文本分析就是把文字转成类似音标的东西。 比如下图就是一个文本分析，用来分析 “PG&amp;E will file schedules on April 20. ” 文本分析主要有三个步骤：文字规范化， 语音分析， 还有韵律分析。 下面一一道来。 </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/webp-20220604233951462" alt="img"></p><p>Fig. 3 文本分析</p><h4 id="文本规范化-Text-normalization"><a href="#文本规范化-Text-normalization" class="headerlink" title="文本规范化 (Text normalization)"></a>文本规范化 (Text normalization)</h4><p>文本分析首先是要确认单词和句子的结束。 空格会被用来当做隔词符. 句子的结束一般用标点符号来确定， 比如问号和感叹号 （？！）, 但是句号有的时候要特别处理。 因为有些单词的缩写也包含句号， 比如 str. “My place on Main Str. is around the corner”. 这些特别情况一般都会采取规则（rule）的方式过滤掉。</p><p>接下来 是把非文字信息变成对应的文字， 比如句子中里有日期， 电话号码， 或者其他阿拉伯数字和符号。 这里就举个例子， 比如， I was born April 14. 就要变成， I was born April fourteen.  这个过程其实非常繁琐，现实文字中充满了 缩写，比如CS, 拼写错误， 网络用语， tmr —&gt; tomorrow. 解决方式还是主要依靠rule based method， 建立各种各样的判断关系来转变。</p><h4 id="语音分析-Phonetic-analysis"><a href="#语音分析-Phonetic-analysis" class="headerlink" title="语音分析 (Phonetic analysis)"></a>语音分析 (Phonetic analysis)</h4><p>语音分析就是把每个单词中的发音单词标出来， 比如Fig. 3 中的P, 就对应p和iy, 作为发音。 这个时候也很容易发现，发音的音标和对应的字母 不是一一对应的关系，反而需要音标去对齐 （allignment）。 这个对齐问题很经典， 可以用很多机器学习的方法去解决， 比如Expectation–maximization algorithm.</p><h4 id="韵律分析-Prosody-analysis"><a href="#韵律分析-Prosody-analysis" class="headerlink" title="韵律分析 (Prosody analysis)"></a>韵律分析 (Prosody analysis)</h4><p>韵律分析就是英语里的语音语调， 汉语中的抑扬顿挫。 我们还是以英语为例， 韵律分析主要包含了： 重音 (Accent)，边界 (boundaries), 音长 (duration)，主频率 (F0)。</p><p><strong>重音（Accent）</strong>就是指哪个音节发生重一点。 对于一个句子或者一个单词都有重音。 单词的重音一般都会标出来，英语语法里面有学过， 比如banana 这个单词， 第二个音节就是重音。 而对于句子而言，一样有的单词会重音，有的单词会发轻音。 一般有新内容的名词， 动词， 或者形容词会做重音处理。 比如下面的英语句子， surprise 就会被重音了， 而句子的重音点也会落到单词的重音上， 第二个音节rised, 就被重音啦。 英语的重音规则是一套英语语法，读者可以自行百度搜索。</p><p>I’m a little    sur<strong>prised</strong>  to hear it <strong>cha</strong>racterized    as up<strong>beat</strong>.</p><p><strong>边界 （Boundaries）</strong> 就是用来判断声调的边界的。 一般都是一个短语结束后，有个语调的边界。 比如下面的句子， For language, 就有一个边界， 而I 后面也是一个边界.</p><p>For language, I , the author of the blog, like Chinese.</p><p><strong>音长（Duration）</strong>就是每个音节的发声长度。 这个通俗易懂。 NLP 里可以假定每个音节单词长度相同都是 100ms, 或者根据英语语法， 动词， 形容词之类的去确定。 也可以通过大量的数据集去寻找规律。</p><p><strong>主频率 （F0</strong>）就是声音的主频率。 应该说做傅里叶转换后， 值 (magnitude) 最大的那个。 也是人耳听到声音认定的频率。一个成年人的声音主频率在 100-300Hz 之间。 这个值可以用 线性回归来预测， 机器学习的方法预测也可以。一般会认为，人的声音频率是连续变化的，而且一个短语说完频率是下降趋势。</p><p>文本分析就介绍完了，这个方向比较偏语言学， 传统上是语言学家的研究方向，但是随着人工智能的兴起，这些feature 已经不用人为设计了，可以用端到端学习的方法来解决。 比如谷歌的文章 <a href="https://arxiv.org/pdf/1703.10135.pdf" target="_blank" rel="noopener">TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS</a> 就解救了我们。</p><h3 id="声波生成（waveform-synthesis）"><a href="#声波生成（waveform-synthesis）" class="headerlink" title="声波生成（waveform synthesis）"></a>声波生成（waveform synthesis）</h3><p>这个部分就比较像我们算法工程师的工作内容了。 在下面， 会详细介绍如何用Wavenet 和WaveRNN 来实现这一步骤的。 </p><p>这里说所谓的waveform synthesis 就是用这些 语言特征值（text features）去生成对应的声波，也就是生成前文所说的采样频率 和 振幅大小（对应的数字信号）。 这里面主要有两个算法。</p><p><strong>串接合成（concatenative speech synthesis）</strong>： 这个方法呢， 就是把记录下来的音节拼在一起来组成一句话，在通过调整语音语调让它听起来自然些。 比较有名的有双音节拼接（Diphone Synthesis） 和单音节拼接（Unit Selection Synthesis）。这个方法比较繁琐， 需要对音节进行对齐（alignment）， 调整音节的长短之类的。</p><p><strong>参数合成 （Parametric Synthesis）</strong>： 这个方法呢， 需要的内存比较小，是通过统计的方法来生成对应的声音。 模型一般有隐马尔科夫模型 （HMM），还有最近提出的神经网络算法Wavenet, WaveRNN. </p><p>对于隐马尔科夫模型的算法， 一般都会生成梅尔频率倒谱系数 （MFCC），这个是声音的特征值。 感兴趣的可以参考<a href="https://www.cnblogs.com/BaroC/p/4283380.html" target="_blank" rel="noopener">这篇博客</a>去了解 MFCC。</p><p>对于神经网络的算法来说， 一般都是生成256 个 quantized values 基于softmax 的分类器， 对应 声音的 256 个量化值。 WaveRNN 和wavenet 就是用这种方法生成的。</p><h2 id="Tacotron和Tacotron2"><a href="#Tacotron和Tacotron2" class="headerlink" title="Tacotron和Tacotron2"></a>Tacotron和Tacotron2</h2><p>以下内容主要来源于<a href="https://www.dengbocong.cn/Paper-Reading/cea2357273fe/" target="_blank" rel="noopener">论文阅读笔记：Tacotron和Tacotron2</a>。</p><p>我们首先对 Tacotron 和 Tacotron2 论文中的关键部分进行阐述和总结，之所以两篇论文放在一起，是因为方便比较模型结构上的不同点，更清晰的了解 Tacotron2 因为改进了哪些部分，在性能上表现的比 Tacotron 更好。</p><p>语音合成系统通常包含多个阶段，例如 TTS Frontend（文本前端），Acoustic model（声学模型） 和 Vocoder（声码器），如下图更直观清晰一点：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604204302965.png" alt="在这里插入图片描述" style="zoom: 67%;"></p><p>构建这些组件通常需要广泛的领域专业知识，并且可能包含脆弱的设计选择。在很多人困扰于繁杂的特征处理的时候，Google 推出了 Tacotron，一种从文字直接合成语音的端到端的语音合成模型，虽然在效果上相较于传统方法要好，但是相比 Wavenet 并没有明显的提升（甚至不如 Wavenet），不过它更重要的意义在于 end-to-end（Wavenet 是啥将在后面对比 vocoder 的时候讲解，顺便提一下 Tacotron 使用的是 Griffin-Lim 算法，而 Tacotron2 使用的是修改版 Wavenet)。此外，相较于其他<strong>样本级自回归方法合成语音</strong>，Tacotron 和 Tacotron2 是<strong>在帧级生成语音</strong>，因此要快得多。</p><p>在传统的 Pipeline 的统计参数 TTS，通常有一个文本前端提取各种语言特征，持续时间模型，声学特征预测模型和基于复杂信号处理的声码器。而端到端的语音合成模型，只需要对文本语音进行简单的处理，就能喂给模型进行学习，极大的减少的人工干预，对文本的处理只需要进行文本规范化以及分词 token 转换（论文中使用 character，不过就语音合成而言，使用 Phoneme 字典更佳），关于文本规范化（数字、货币、时间、日期转完整单词序列）以及 text-to-phoneme 可以参见<a href="https://zhuanlan.zhihu.com/p/336872753" target="_blank" rel="noopener">利器：TTS Frontend 中英 Text-to-Phoneme Converter，附代码</a>。端到端语音合成系统的优点如下：</p><ul><li>减少对特征工程的需求</li><li>更容易适应新数据（不同语言、说话者等）</li><li>单个模型可能比组合模型更健壮，在组合模型中，每个组件的错误都可能叠加而变得更加复杂</li></ul><p><strong>端到端语音合成模型的困难所在：</strong>  </p><p>不同 Speaker styles 以及不同 pronunciations 导致的对于给定的输入，模型必须对不同的信号有着更大的健壮性，除此之外 Tacotron 原本下描述：</p><blockquote><p>TTS is a large-scale inverse problem: a highly compressed source (text) is “decompressed” into audio</p></blockquote><p>上面这句是 Tacotron 原文中说的，简单来说就是 TTS 输出是连续的，并且输出序列（音频）通常比输入序列（文本）长得多，导致预测误差迅速累积。想要了解更多关于语音合成的背景知识，可以参考文章 <a href="https://www.jianshu.com/p/46888767dcef" target="_blank" rel="noopener">Text-to-speech</a>。</p><h3 id="Tacotron模型结构"><a href="#Tacotron模型结构" class="headerlink" title="Tacotron模型结构"></a>Tacotron模型结构</h3><p>Tacotron 的基础架构是带有注意力机制（Attention Mechanism）的 Seq2Seq 模型，下图是模型的总体架构。网络部分大体可分为 4 部分，分别是左：Encoder、中：Attention、右下：Decoder、右上：Post-processing。从高层次上讲，模型将字符作为输入，并生成频谱图，然后将其转换为波形。  </p><p><img src="https://unisound.github.io/end-to-end_tts/images/tacotron.jpg" alt="Tacotron" style="zoom:50%;"></p><p>要特别说明的是架构中，raw text 经过 pre-net 后，将会把输出喂给一个叫 CBHG 的模块以映射为 hidden representation，再之后 decoder 会生成 Linear-Spectrum，再经过 Griffin-Lim 转换为波形。</p><p>raw text的选择可以可以有多种选择，以中文和英文合成系统为例：</p><ol><li><p>英文文本，训练英文模型，最直观的想法是直接将英文文本当做输入，<a href="https://arxiv.org/abs/1703.10135" target="_blank" rel="noopener">Tacotron1</a> 也是这么做的。但这样可能会引入一些问题，比如未登录词发音问题。</p></li><li><p>英文注音符，用英文注音符(比如 <a href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict" target="_blank" rel="noopener">CMUDict</a> )作为输入可以提高发音稳定性，除了注音词典，还可以引入注音前端，增强对模型的控制。 中文拼音，由于中文汉字数量多，且存在大量多音字，直接通过文本训练</p></li><li><p>中文拼音，由于中文汉字数量多，且存在大量多音字，直接通过文本训练是不现实的。所以我们退而求其次，通过拼音训练模型，拼音有注音前端生成，既去掉了汉字的冗余发音又提高了模型的可控性。</p></li><li><p>中文|英文 <a href="http://www.internationalphoneticalphabet.org/" target="_blank" rel="noopener">IPA (International Phonetic Alphabet)</a> 音标，IPA 音标是一种更强的注音体系，一套注音体系可以标注多种语言。对于中文，IPA 音标的标注粒度比拼音更细，实验中，我们观察到用 IPA 作为输入，可以略微提升对齐稳定性。另外，在中文发音人+英文发音人混合训练试验中，我们观察到了一个有意思的现象：由于中英文 IPA 标注中共享了部分发音单元，导致跨语种发音人可以学会对方的语言，也就是中文发音人可以合成英文，英文发音人可以合成中文。在这个联合学习过程中存在着迁移学习的味道。</p></li></ol><blockquote><p>根据不同的用途，Tacotron 可以输出 Linear-Spectrum 或 Mel-Spectrum，如果使用 Griffin-Lim 需要 Tacotron 输出 Linear-Spectrum；如果使用 WaveNet 做 Vocoder（即Tacotron2，下文会介绍） ，则 Tacotron 输出 Linear-Spectrum 或 Mel-Spectrum 均可，但 Mel-Spectrum 的计算代价显然更小，Tacotron2 中，作者使用 80 维 Mel-Spectrum 作为 WaveNet Vocoder 的输入。</p></blockquote><h4 id="Character-Embedding"><a href="#Character-Embedding" class="headerlink" title="Character Embedding"></a>Character Embedding</h4><p>我们知道在训练模型的时候，我们拿到的数据是一条长短不一的(text, audio)的数据，深度学习的核心其实就是大量的矩阵乘法，对于模型而言，文本类型的数据是不被接受的，所以这里我们需要先把文本转化为对应的向量。这里涉及到如下几个操作</p><p><strong>构造字典</strong></p><p>因为纯文本数据是没法作为深度学习输入的，所以我们首先得把文本转化为一个个对应的向量，这里我使用字典下标作为字典中每一个字对应的id，然后每一条文本就可以通过遍历字典转化成其对应的向量了。所以字典主要是应用在将文本转化成其在字典中对应的id，根据语料库构造，这里我使用的方法是根据语料库中的字频构造字典(我使用的是基于语料库中的字构造字典，有的人可能会先分词，基于词构造。不使用基于词是现在就算是最好的分词都会有一些误分词问题，而且基于字还可以在一定程度上缓解OOV的问题)。</p><p>然后我们就可以将文本数据转化成对应的向量作为模型的输入。</p><p><strong>embed layer</strong></p><p>光有对应的id，没法很好的表征文本信息，这里就涉及到构造词向量，关于词向量不在说明，网上有很多资料，模型中使用词嵌入层，通过训练不断的学习到语料库中的每个字的词向量。</p><p>值得注意的是，这里是随机初始化词嵌入层，另一种方法是引入预先在语料库训练的词向量(word2vec)，可以在一定程度上提升模型的效果。</p><h4 id="音频特征提取"><a href="#音频特征提取" class="headerlink" title="音频特征提取"></a>音频特征提取</h4><p>对于音频，我们主要是提取出它的mel-spectrogram，然后变换得到比较常用的音频特征MFCC。对于声音来说，它其实是一个一维的时域信号，直观上很难看出频域的变化规律，我们知道，可以使用傅里叶变化，得到它的频域信息，但是又丢失了时域信息，无法看到频域随时域的变化，这样就没法很好的描述声音， 为了解决这个问题，很多时频分析手段应运而生。短时傅里叶，小波，Wigner分布等都是常用的时频域分析方法。这里我们使用短时傅里叶。</p><p>所谓短时傅里叶变换，顾名思义，是对短时的信号做傅里叶变化。那么短时的信号怎么得到的? 是长时的信号分帧得来的。这么一想，STFT的原理非常简单，把一段长信号分帧(傅里叶变换适用于分析平稳的信号。我们假设在较短的时间跨度范围内，语音信号的变换是平坦的，这就是为什么要分帧的原因)、加窗，再对每一帧做傅里叶变换（FFT），最后把每一帧的结果沿另一个维度堆叠起来，得到类似于一幅图的二维信号形式。如果我们原始信号是声音信号，那么通过STFT展开得到的二维信号就是所谓的声谱图。</p><p>声谱图往往是很大的一张图，为了得到合适大小的声音特征，往往把它通过梅尔标度滤波器组（mel-scale filter banks），变换为梅尔频谱（mel-spectrogram）。在梅尔频谱上做倒谱分析（取对数，做DCT变换）就得到了梅尔倒谱系数（MFCC，Mel Frequency Cepstral Coefficents）。我们主要使用第三方库librosa提取MFCC特征。</p><h4 id="CBHG-内部结构说明"><a href="#CBHG-内部结构说明" class="headerlink" title="CBHG 内部结构说明"></a>CBHG 内部结构说明</h4><p>所谓 CBHG 就是作者使用的一种用来从序列中提取高层次特征的模块，如下图所示：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914117.png" alt></p><p>CBHG 使用了 1D 卷积、highway、残差链接和双向 GRU 的组合，输入序列，输出同样也是序列，因此，它从序列中提取表示非常强大。CBHG 架构流程如下：</p><ul><li>首先使用 $K$ 组 1D 卷积对输入序列进行卷积，其中第 $k$ 组表示为$C_k$ ，其卷积核的宽度为 $k$（即$k=1,2,…,K$）。 这些卷积层显式地对本地和上下文信息进行建模（类似于对 unigram，bigrams 以及 K-gram 的建模）</li><li>然后将卷积输出堆叠在一起（注意：在做卷积时，运用了padding，因此这k个卷积核输出的大小均是相同的），并进行最大化池，以增加局部不变性。注意了，最大化池使用 stride 为 1 来保留原始时间分辨率</li><li>接着将处理后的序列传递给一些固定宽度的 1D 卷积，其输出通过残差连接与原始输入序列相加，同时批量归一化用于所有卷积层</li><li>然后将输出喂到多层 highway 网络中以提取高级特征。highway nets的每一层结构为：把输入同时放入到两个一层的全连接网络中，这两个网络的激活函数分别采用了ReLu和sigmoid函数，假定输入为input，ReLu的输出为output1，sigmoid的输出为output2，那么highway layer的输出为output=output1∗output2+input∗（1−output2)。为什么要使用highway network的结构呢，其实说白了也是一种减少缓解网络加深带来过拟合问题，以及减少较深网络的训练难度的一个trick。它主要受到LSTM门限机制的启发。</li><li>最后，在顶部堆叠双向 GRU RNN，以从前向和后向上下文中提取顺序特征。</li></ul><p>在 Encoder 中，输入被 CBHG 处理之前还需要经过 pre-net 进行预处理，作者设计 pre-net（pre-net 是由全连接层 + dropout 组成的模块）的意图是让它成为一个 bottleneck layer 来提升模型的泛化能力，以及加快收敛速度。</p><h4 id="Decoder-结构说明"><a href="#Decoder-结构说明" class="headerlink" title="Decoder 结构说明"></a>Decoder 结构说明</h4><p>随后就是 Decoder 了，论文中使用两个 decoder</p><ul><li>attention decoder：attention decoder 用来生成 query vector 作为 attention 的输入，交由注意力模块生成 context vector。它用于学习如何对齐文本序列和语音帧，序列中的每个字符编码通常对应多个语音帧并且相邻的语音帧一般也具有相关性。</li><li>output decoder：output decoder 则将 query vector 和 context vector 组合在一起作为输入。</li></ul><p>作者并没有选择直接用 output decoder 来生成 spectrogram，而是生成了 80-band mel-scale spectrogram，也就是我们之前提到的 mel-spectrogram，熟悉信号处理的同学应该知道，spectrogram 的 size 通常是很大的，因此直接生成会非常耗时，而 mel-spectrogram 虽然损失了信息，但是相比 spectrogram 就小了很多，且由于它是针对人耳来设计的，因此对最终生成的波形的质量不会有很多影响。</p><p>随后使用 post-processing network（下面会讲）将 seq2seq 目标转换为波形，然后使用一个全连接层来预测 decoder 输出。Decoder 中有一个 <strong>trick 就是在每个 decoder step 预测多个 (r 个)非重叠frame，这样做可以缩减计算量，且作者发现这样做还可以加速模型的收敛</strong>。</p><blockquote><p>预测多个非重叠帧的直观解释：因为就像我们前面说到的提取音频特征的时候，我们会先分帧，相邻的帧其实是有一定的关联性的，所以每个字符在发音的时候，可能对应了多个帧，因此每个GRU单元输出为多个帧的音频文件。</p><p>论文提到 scheduled sampling 在这里使用会损失音频质量</p></blockquote><h4 id="post-processing-net-和-waveform-synthesis"><a href="#post-processing-net-和-waveform-synthesis" class="headerlink" title="post-processing net 和 waveform synthesis"></a>post-processing net 和 waveform synthesis</h4><p>和seq2seq网络不同的是，tacotron在decoder-RNN输出之后并没有直接将其作为输出通过Griffin-Lim算法合成音频，而是添加了一层post-processing模块。为什么要添加这一层呢？</p><p>首先是因为我们使用了Griffin-Lim重建算法，根据频谱生成音频，Griffin-Lim原理是：我们知道相位是描述波形变化的，我们从频谱生成音频的时候，需要考虑连续帧之间相位变化的规律，如果找不到这个规律，生成的信号和原来的信号肯定是不一样的，Griffin Lim算法解决的就是如何不弄坏左右相邻的幅度谱和自身幅度谱的情况下，求一个近似的相位，因为相位最差和最好情况下天壤之别，所有应该会有一个相位变化的迭代方案会比上一次更好一点，而Griffin Lim算法找到了这个方案。这里说了这么多，其实就是Griffin-Lim算法需要看到所有的帧。post-processing可以在一个线性频率范围内预测幅度谱（spectral magnitude)。</p><p>其次，post-processing能看到整个解码的序列，而不像seq2seq那样，只能从左至右的运行。它能够通过正向传播和反向传播的结果来修正每一帧的预测错误。</p><p>论文中使用了CBHG的结构来作为post-processing net，前面已经详细介绍过。实际上这里 post-processing net 中的 CBHG 是可以被替换成其它模块用来生成其它东西的，比如直接生成 waveform，在 Tacotron2 中，CBHG 就被替换为 Wavenet 来直接生成波形。</p><h4 id="模型详细的配置"><a href="#模型详细的配置" class="headerlink" title="模型详细的配置"></a>模型详细的配置</h4><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70.png" alt></p><p>对 Decoder 和 post-processing net 使用 L1 损失，并取平均。作者使用 32batch，并将序列 padding 到最大长度。关于 padding 的说明，Tacotron 原文如下：</p><blockquote><p>It’s a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames. However, we found that models trained this way don’t know when to stop emitting outputs, causing repeated sounds towards the end. One simple trick to get around this problem is to also reconstruct the zero-padded frames.</p></blockquote><h3 id="Tacotron2模型结构"><a href="#Tacotron2模型结构" class="headerlink" title="Tacotron2模型结构"></a>Tacotron2模型结构</h3><p>Tacotron有啥缺点呢？</p><ul><li><strong>CBHG模块的去与留？</strong></li></ul><p>Tacotron中使用了CBHG模块（包括编码器部分和解码器部分），虽然在实验中发现该模块可以一定程度上减轻过拟合问题，和减少合成语音中的发音错误，但是该模块本身比较复杂，能否用其余更简单的模块替换该模块？</p><ul><li><strong>Attention出现错误对齐的现象</strong></li></ul><p>Tacotron中使用的Attention机制能够隐式的进行语音声学参数序列与文本语言特征序列的隐式对齐，但是由于Tacotron中使用的Attention机制没有添加任何的约束，导致模型在训练的时候可能会出现错误对齐的现象，使得合成出的语音出现部分发音段发音不清晰、漏读、重复、无法结束还有误读等问题。</p><ul><li><strong>r值如何设定？</strong></li></ul><p>Tacotron中一次可生成r帧梅尔谱，r可以看成一个超参数，r可以设置的大一点，这样可以加快训练速度和合成语音的速度，但是r值如果设置的过大会破坏Attention RNN隐状态的连续性，也会导致错误对齐的现象。</p><ul><li><strong>声码器的选择</strong></li></ul><p>Tacotron使用Griffin-Lim作为vocoder来生成语音波形，这一过程会存在一定的信息丢失，导致合成出的语音音质有所下降（不够自然）。Tacotron 中作者也提到了，这个算法只是一个简单、临时的 neural vocoder 的替代，因此要改进 Tacotron 就需要有一个更好更强大的 vocoder。</p><p>接下来我们来看看 Tacotron2，它的模型大体上分为两个部分：</p><ul><li>具有注意力的循环序列到序列特征预测网络，该网络根据输入字符序列预测梅尔谱帧的序列</li><li>WaveNet 的修改版，可生成以预测的梅尔谱帧为条件的 time-domain waveform 样本</li></ul><p>结构图如下：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914399.png" style="zoom: 67%;"></p><ul><li><strong>CBHG模块的去与留？</strong></li></ul><p>在Tacotron2中，对于编码器部分的CBHG模块，作者采用了一个3<em>Conv1D+BiLSTM模块进行替代，如图2下方蓝色部分所示；对于解码器部分的CBHG模块，作者使用了Post-Net（5</em>Conv1D）和残差连接进行替代。</p><ul><li><strong>Attention出现错误对齐的现象</strong></li></ul><p>在Tacotron2中，作者使用了Location-sensitive Attention代替了原有的基于内容的注意力机制，前者在考虑内容信息的同时，也考虑了位置信息，这样就使得训练模型对齐的过程更加的容易。一定程度上缓解了部分合成的语音发音不清晰、漏读、重复等问题。对于Tacotron中无法在适当的时间结束而导致合成的语音末尾有静音段的问题，作者在Tacotron2中设计了一个stop token进行预测模型应该在什么时候进行停止解码操作。</p><ul><li><strong>r值如何设定？</strong></li></ul><p>在Tacotron2中，r值被设定为1，发现模型在一定时间内也是可以被有效训练的。猜测这归功于模型整体的复杂度下降，使得训练变得相对容易。</p><ul><li><strong>声码器的选择</strong></li></ul><p>Tacotron2 选择预测 a low-level acoustic 表示，即 mel-frequency spectrograms（Tacotron 使用 linear-frequency scale spectrograms)，Tacotron2 原文描述如下：</p><blockquote><p>This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame.</p></blockquote><p>mel-frequency spectrogram 与 linear-frequency spectrograms 有关，即短时傅立叶变换（STFT）幅度。mel-frequency 是通过对 STFT 的频率轴进行非线性变换而获得的，同时受到人类听觉系统的启发，用较少的维度表示频率内容，原因很好理解，低频中的细节对于音频质量至关重要，而高频中往往包含摩擦音等噪音，因此通常不需要对高频细节建模。</p><p>虽然 linear spectrograms 会丢弃相位信息（因此是有损的），但是诸如 Griffin-Lim 之类的算法能够估算此丢弃的信息，从而可以通过短时傅立叶逆变换进行时域转换。而 <strong>mel spectrogram 会丢弃更多信息，因此它的逆问题更具有挑战性</strong>，这个时候作者想到了 WaveNet替换了原先的Griffin-Lim，进一步加快了模型训练和推理的速度，因为wavenet可以直接将梅尔谱转换成原始的语音波形。（Tacotron2合成语音音质的提升貌似<em>主要</em>归功于Wavenet替换了原有的Griffin-Lim）。</p><p>除了 Wavenet，Tacotron2 和 Tacotron 的主要不同在于：</p><ul><li>不使用 CBHG，而是使用普通的 LSTM 和 Convolution layer</li><li>decoder 每一步只生成一个 frame</li><li>增加 post-net，即一个 5 层 CNN 来精调 mel-spectrogram</li></ul><h3 id="Tacotron实验结果"><a href="#Tacotron实验结果" class="headerlink" title="Tacotron实验结果"></a>Tacotron实验结果</h3><p>下图展示 Decoder step 中，使用不同组件学习到 attention alignment 的效果：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604205845238.png" style="zoom:90%;"></p><p>  下图展示了 post-processing net 的实验效果，可以看到有 post-processing net 的网络效果更好：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914098.png" style="zoom: 67%;"></p><p>  MOS 分数对比如下表：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/20201215105516463.png" alt></p><h3 id="Tacotron2实验结果"><a href="#Tacotron2实验结果" class="headerlink" title="Tacotron2实验结果"></a>Tacotron2实验结果</h3><p>下表展示了 Tacotron2 与各种现有系统的 MOS 分数比较。Tacotron2 的分数已经和人类不相上下了，这在很大程度上要归功于 Wavenet。  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914133.png" alt></p><p> 下表是对合成的音频的评价：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914151.png" alt></p><p>文中提到，Wavenet 在这个模型中是和剩下的模型分开训练的，Wavenet 的输入是 mel-spectrogram，输出是 waveform，这个时候就需要考虑输入的 mel-spectrogram 是选择 ground truth，还是选用 prediction，作者做了相关实验，结果如下图所示：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914172.png" alt></p><p>  可以看到使用模型生成的 mel-spectrogram 来训练的 Wavenet 取得了最好的结果，作者认为这是因为这种做法保证了数据的一致性。下表是生成 mel-spectrogram 和 linear spectrogram 的区别（结果证明 mel-spectrogram 是最好的，同时还能够减少计算，加快 inference 的时间）：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914204-4346354.png" alt></p><p>  下表是对 WaveNet 简化之后的 MOS 分数情况：  </p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/t_70-20220604203914223.png" alt></p><h3 id="关于-vocoder"><a href="#关于-vocoder" class="headerlink" title="关于 vocoder"></a>关于 vocoder</h3><p>声码器（Vocoder）在语音合成中往往被用于将生成的语音特征转换为我们所需要的语音波形。在Tacotron中，由于前端的神经网络所预测出的梅尔谱图仅包含了幅值信息而缺乏相应的相位信息，我们难以直接通过短时傅里叶变换（STFT）的逆变换将梅尔谱图还原为声音波形文件；因此，我们需要使用声码器进行相位估计，并将输入的梅尔谱图转换为语音波形。</p><p>Tacotron 使用的是 Griffin-Lim 算法，Griffin-Lim 是一种声码器，常用于语音合成，用于将语音合成系统生成的声学参数转换成语音波形，这种声码器不需要训练，不需要预知相位谱，而是通过帧与帧之间的关系估计相位信息，从而重建语音波形。更正式一点的解释是 Griffin-Lim 算法是一种已知幅度谱，未知相位谱，通过迭代生成相位谱，并用已知的幅度谱和计算得出的相位谱，重建语音波形的方法，具体可参考这篇 <a href="https://zhuanlan.zhihu.com/p/66809424" target="_blank" rel="noopener">Griffin-Lim 声码器介绍</a>。</p><p>Griffin-Lim 的优点是算法简单，可以快速建立调研环境，缺点是速度慢，很难在 CPU 上做到实时，无法实时解码也就意味着系统无法在生产环境使用。而且通过Griffin-Lim生成波形过于平滑，空洞较多，听感不佳。</p><p>种种迹象表明，Griffin-Lim 算法是音质瓶颈，经过一些列工作尤其是 <a href="https://arxiv.org/abs/1712.05884" target="_blank" rel="noopener">Tacotron2</a> ，人们逐渐意识到，Mel-Spectrogram 可以作为采样点自回归模型的 condition，利用强大的采样点自回归模型提高合成质量。</p><p>目前公认的效果有保障的采样点自回归模型主要如下几种，1) SampleRNN、2)WaveNet、3)WaveRNN。我们重点介绍前两种。</p><h4 id="SampleRNN"><a href="#SampleRNN" class="headerlink" title="SampleRNN"></a>SampleRNN</h4><p>其模型结构如下：</p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/samplernn.jpg" alt="samplernn" style="zoom: 67%;"></p><p>图3: SampleRNN 模型结构</p><p>SampleRNN 是一个精心设计的 RNN 自回归模型。标准的 RNN 模型包括 LSTM、GRU，可以用来处理一些长距离依赖的场景，比如语言模型。但对于音频采样点这样的超长距离依赖场景（比如：24k采样率，意味着 1s 中包含 24000 个采样点），RNN 处理起来已经非常困难了 。SampleRNN 的作者，将问题分解，分辨率由低到高逐层建模，例如图中，Tier3 层每时刻输入16个采样点，输出状态 S1；Tier2 层每时刻输入 4 个采样点，同时输入 Tier3 输出的 S1，输出状态 S2 ； Tier1 层每时刻输入 4 个采样点，同时输入 Tier2 输出的 S2，输出一个采样点，由于 Tier1 没有循环结构，同一时刻可以输出 4 个采样点。</p><p>如果有兴趣，可以点击 <a href="https://pan.baidu.com/s/1o8M8bGI" target="_blank" rel="noopener">SampleRNN Samples</a>，在里面你能找到总长度为 1小时 的 Samples。</p><p>总体来看模型的波形生成能力相当了得，发音、音色以及韵律风格的还原度都非常高。但 SampleRNN 也存在一些问题，最主要的是训练收敛速度太慢了，导致调参优化效率低下，我们将介绍另一个采样点自回归模型 WaveNet，相比 SampleRNN ，WaveNet 不但保留了高水平的波形生成能力，而且还提升了训练速度，单卡训练一天就能获得较好的效果。</p><h4 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h4><p>其模型结构如下</p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/wavenet.gif" alt="wavenet"></p><p>图4: 采样点自回归 WaveNet</p><p>图4, 描述了 WaveNet 这类采样点自回归模型的工作方式，模型输入若干历史采样点，输出下一采样点的预测值，也就是根据历史预测未来。如果你对 NLP 较为熟悉，一定会觉得这种工作方式很像语言模型，没错，只不过音频采样点自回归更难一些罢了，需要考虑更长的历史信息才能保证足够的预测准确率。</p><p>WaveNet 最初由 DeepMind 推出，是基于 CNN 的采样点自回归模型，由于 CNN 结构的限制，为了解决长距离依赖问题，必须想办法扩大感受野，但扩大感受野又会增加参数量。为了在扩大感受野和控制参数量间寻找平衡，作者引入所谓“扩展卷积”的结构。如上图所示，“扩张卷积”，也可以称为“空洞卷积”，顾名思义就是计算卷积时跨越若干个点，WaveNet 层叠了多层这种 1D 扩张卷积，卷积核宽度为 2 （Parallel WaveNet 为 3），Dilated 宽度随层数升高而逐渐加大。可以想象，通过这种结构，CNN 感受野随着层数的增多而指数级增加。</p><p>训练好了 WaveNet ，我们就可以来合成音频波形了。但是，你会发现这时合成的音频完全没有语义信息，听起来更像是鹦鹉学舌，效果就如上一节 SampleRNN 的样例一样。 要使 WaveNet 合成真正的语音，那么就需要为其添加 condition ，condition 包含了文本的语义信息，这些语义信息可以帮助 WaveNet 合成我们需要的波形，condition 的形式并不唯一，但本文中我们只介绍 Mel-Spectrum condition 。</p><p><strong>Mel-Spectrum condition</strong></p><p>为什么要引入 Mel-Spectrum condition 呢？有两个原因：其一是为了和 Tacotron 打通，Tacotron 的输出可以直接作为 WaveNet 的输入，构成一套完整的端到端语音合成流水线；其二是因为 Mel-Spectrum 本身包含了丰富的语音语义信息，这些语音语义信息可以支持后期的多人混合训练、以及韵律风格迁移等工作。</p><p>下面我们将着重介绍如何在模型中融入 Mel-Spectrum condition 。</p><p>由于采样点长度和 Mel-Spectrum 长度不匹配，我们需要想办法将长度对齐，完成这一目标有两种方法：一种是将 Mel-Spectrum 反卷积上采样到采样点长度，另一种是将 Mel-Spectrum 直接复制上采样到采样点长度，两种方案效果差异很小。我们希望模型尽量简洁，故而采用第二种方法，如图6所示。</p><p>方便起见，我们借用 Deep Voice1 （图5）来说明。经过复制上采样的 Mel-Spectrum condition，首先需要经过一个 1x1 卷积，使 Mel-Spectrum condition 维度与 WaveNet GAU 输入维度相同，然后分两部分累加送入 GAU 即可，注意，WaveNet 每层 GAU 都需要独立添加 Mel-Spectrum condition。</p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/deepvoice.jpg" alt="wavenet" style="zoom:67%;"></p><p>图5: Mel-Spectrum condition 计算方法</p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/image2018-1-12 11_43_38.png" alt="wavenet" style="zoom:80%;"></p><p>图6: Mel-Spectrum 时间分辨率对齐</p><p>WaveNet 有很多优点，训练快、效果好、网络结构清晰简洁。但 WaveNet 也引入了新问题：inference 性能差，在 CPU 平台通常需要数十秒时间合成一秒语音，这让商业化几乎不可能。</p><p>针对这一问题，DeepMind 推出了 WaveNet 加速方案 Parallel WaveNet，Parallel WaveNet 将 inference 速度提升上千倍。</p><h2 id="Transformer-TTS"><a href="#Transformer-TTS" class="headerlink" title="Transformer TTS"></a>Transformer TTS</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>该方法来自于<a href="https://arxiv.org/abs/1809.08895" target="_blank" rel="noopener">Neural Speech Synthesis with Transformer Network (2018)</a>。</p><p>虽然Tacotron2解决了一些在Tacotron中存在的问题，但是Tacotron2和Tacotron整体结构依然一样，二者都是一个自回归模型，也就是每一次的解码操作都需要先前的解码信息，导致模型难以进行并行计算训练和推理过程中的效率低下。其次，二者在编码上下文信息的时候，都使用了LSTM进行建模。理论上，LSTM可以建模长距离的上下文信息，但是实际应用上，LSTM对于建模较长距离的上下文信息能力并不强。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/DeepLearningApplications/语音合成/语音合成模型/v2-08d2694c3119102d90c87d46e4baa6b6_b.jpg" alt="img" style="zoom:70%;"></p><p>如果对Tacotron2和Transformer比较熟悉的话，可以从上图3中看出，其实Transformer TTS就是Tacotron2和Transformer的结合体。其中，一方面，Transformer TTS继承了Transformer Encoder，MHAttention，Decoder的整体架构；另一方面，Transformer TTS的Encoder Pre-net、Decoder Pre-net、Post-net、stop Linear皆来自于Tacotron2，所起的作用也都一致。换句话说，</p><blockquote><p>将Tacotron2: Encoder BiLSTM ——&gt;Transformer: Multi-head Attention（+positional encoding）;</p><p>Tacotron2: Decoder Location-sensitive Attention + LSTM ——&gt;Transformer: Multi-head Attention （+positional encoding）;<br>其余保持不变，就变成了Transformer TTS。</p></blockquote><p>也正是Transformer相对于LSTM的优势，使得Transformer TTS解决了Tacotron2中存在的训练速度低下和难以建立长依赖性模型的问题。</p><p>其中值得一提的是，Transformer TTS保留了原始Transformer中的scaled positional encoding信息。为什么非得保留这个呢？原因就是Multi-head Attention无法对序列的时序信息进行建模。可以用下列公式表示：</p><script type="math/tex; mode=display">\begin{array}{r}P E(p o s, 2 i)=\sin \left(\frac{\text { pos }}{10000 \frac{2 i}{d_{\text {model }}}}\right) \\P E(\text { pos }, 2 i+1)=\cos \left(\frac{\text { pos }}{10000^{\frac{2 i}{d_{\text {model }}}}}\right) \\x_{i}=\operatorname{prenet}\left(\text { phoneme } e_{i}\right)+\alpha P E(i)\end{array}</script><p>其中，$ \alpha $ 是可训练的权重，使得编码器和解码器预处理网络可以学习到输入音素级别对梅尔谱帧级别的尺度适应关系。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文作者结合Tacotron2和Transformer提出了Transformer TTS，在一定程度上解决了Tacotron2中存在的一些问题。但仍然存在一些问题：如1）在训练的时候可以并行计算，但是在推理的时候，模型依旧是自回归结构，运算无法并行化处理；2）相比于Tacotron2，位置编码导致模型无法合成任意长度的语音；3）Attention encoder-decoder依旧存在错误对齐的现象。</p><p>有关代码解读可以参考<a href="https://zhuanlan.zhihu.com/p/512240545" target="_blank" rel="noopener">声学模型（02）：Transformer based TTS</a></p><h2 id="Fastspeech"><a href="#Fastspeech" class="headerlink" title="Fastspeech"></a>Fastspeech</h2><p>该算法来自于<a href="https://arxiv.org/abs/1905.09263" target="_blank" rel="noopener"> FastSpeech: Fast, Robust and Controllable Text to Speech (2019)</a>。</p><h3 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h3><p>在先前基于神经网络的TTS系统中，mel-spectrogram是以自回归（auto-regressive）方式产生的。由于mel-spectrogram的长序列和自回归性质，这些系统依旧面临着几个问题：</p><ul><li>在推理阶段，mel-spectrogram生成的<strong>速度很慢（slow inference speed）</strong>。尽管Transformer TTS相比较基于RNN的模型显著加快了训练速度，但是这些模型在推理阶段都会基于先前生成的mel-spectrogram帧来生成当前时刻的mel-spectrogram帧，导致推理速度较慢。</li><li>合成语音通常<strong>不够稳定（not robust）</strong>。由于自回归生成中的错误传播和文本与语音之间存在的错误注意力对齐（传统语音合成系统的Alignment是隐式的导致的），生成的mel-spectrogram通常存在跳词和重复（skip and repeat）问题。</li><li>合成语音<strong>缺乏可控性（lack of controllability）</strong>，如语速和韵律方面的可控性（这里笔者的可控应该主要指的是生成的语速方面，因为在Prosody的层面已经有工作做到了很好的效果）。</li></ul><p>基于以上动机，作者提出了Fastspeech。作者描述到：与自回归TTS模型相比，FastSpeech在mel谱图生成上实现了270倍的加速，在最终语音合成上实现了38倍的加速，几乎消除了跳词和重复的问题，并且可以平滑地调整语音速度。</p><h3 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/DeepLearningApplications/语音合成/语音合成模型/v2-63c3336ca59567f5a2599a1a4c2036e9_b.jpg" alt="img"></p><p>如图1所示，Fastspeech的整体框架和Transformer的Encoder很像，可以简单的理解为是移除了Decoder部分的Transformer模块，以此实现了模型的并行训练和加快推理速度（采用了non auto-regressive的seq-to-seq模型（如上图(a)），不需要依赖上一个时间步的输入，可以让整个模型真正的并行化）。可以看出，Fastspeech主要由三部分构成：FFT Block，Length Regulator和Duration Predictor。</p><blockquote><p>从图1（a）中可以看出，Fastspeech的整体流程和先前的自回归模型还是有几分相似之处的。</p><p>先前的自回归模型流程是：Encoder+Attention（隐式alignment）+Decoder；</p><p>Fastspeech的流程是FFT Block+Length Regulator+FFT Block；其中第一个FFT Block可以看成是Encoder部分，第二个FFT Block可以看成是Decoder部分。明显不同的是Length Regulator，可以看成是一种显式的Attention alignment方式，至于为什么下文有介绍。</p></blockquote><ul><li><strong>Feed-Forward Block</strong></li></ul><p>从上图b可以看出，其实这个模块和Transformer中Multihead attention+Feed-forward结构很相似。稍微有点不同的是作者把原始Feed-forward中的全连接层换成了1D卷积层。为什么要这么做呢？作者描述到：其动机是，在语音任务中，相邻的隐藏状态在字符/音素和mel谱图序列中的关系更为密切。说白了就是作者认为在合成语音的时候局部范围的上下文信息更为重要，较远距离的上下文信息则不那么重要。</p><blockquote><p>The motivation is that the adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram sequence in speech tasks.</p></blockquote><p>其余的部分均和Transformer中的一致，包括positional encoding、multi-head attention、LayerNorm、residual connections。</p><ul><li><strong>Length Regulator</strong></li></ul><p>正上图a所示，第一个FFT Block模块可以简单理解为把因素序列转为一个隐状态，而第二FFT Block模块可以简单理解为把隐状态转换为mel谱图。这就意味着一个问题，要知道因素序列的长度是普遍远远短于mel谱图的长度，那么模型是怎么知道每一个因素应该到底对应多长时间的mel谱帧呢？</p><p>基于以上考虑，作者设计了长度调节器模块（如上图c所示），其作用也就显而易见了，主要是用于解决转换过程中音素和mel谱图序列之间的长度不匹配问题，并且还可以控制语音合成的速度（如何控制下文会有介绍）。</p><p>形式上，一个音素映射到mel谱图上帧的个数称为音素的持续时间。根据音素的持续时间d，长度调节器将音素序列的隐藏状态扩大d倍，同时确保隐藏状态的总长度等于mel谱图的长度。可以用公式表示：</p><script type="math/tex; mode=display">\mathcal{H}_{m e l}=\mathcal{L R}\left(\mathcal{H}_{p h o}, \mathcal{D}, \alpha\right)</script><p>其中 $ \mathcal{H}_{p h o}=\left[h_{1}, h_{2}, \ldots, h_{n}\right]， \mathcal{D}=\left[d_{1}, d_{2}, \ldots, d_{n}\right]， \mathrm{n} $ 表示音素序列的长度，$\quad \sum_{i=1}^{n} d_{i}=m$ ， $ \mathrm{m} $ 表示mel谱图的长度，而 $ \alpha $ 就是用来控制合成mel谱图长度的超参数, 以此来控制合成语音的语速。举个例子, 比如说音素序列 $ \mathcal{H}_{p h o}=\left[h_{1}, h_{2}, h_{3}, h_{4}\right] $ 对应的每个音素的持续时间为 $ \mathcal{D}=[2,2,3,1] $， 如果 $ \alpha=1 $，那么长度调节器模块就会将h1复制1次、h2复制1次、h3复制2次、 $ \mathrm{h} 4 $ 不复制，最终得到 $ \mathcal{H}_{m e l}=\left[h_{1}, h_{1}, h_{2}, h_{2}, h_{3}, h_{3}, h_{3}, h_{4}\right] $ 。如果 $ \alpha=0 $.5代表合成的语速为原先的两倍（变快）, 则每个音素对应的持续时间为 $ \mathcal{D}=[1,1,1.5,0.5] $，因为时间对应的是mel谱图的帧数，帧数不存在小数之说，所以在实际处理的时候会进行向上取整，也就是<br>$ \mathcal{D}=[1,1,2,1] $，因此最终得到的 $ \mathcal{H}_{m e l}=\left[h_{1}, h_{2}, h_{3}, h_{3}, h_{4}\right] $ 。如果 $ \alpha=2 $ 代表合成的语速为原先的0.5倍（变慢），原理和上述分析类似, 此处不再阐述。</p><ul><li><strong>Duration Predictor</strong></li></ul><p>那么问题就来了，模型应该如何确定每个音素的持续时间呢？为了解决这个问题，作者设计了一个持续时间预测器模块。如图1d所示，持续时间预测器包括一个具有ReLU激活函数的2层1D卷积网络，每层后面都有归一化和dropout层，还有一个额外的线性层来输出一个标量，这个标量就表示预测的音素对应的持续时间。</p><p>值得一提的是，训练后的长度预测器只用于TTS推理阶段。在训练阶段，直接使用从训练好的自回归teacher模型中提取的音素长度。</p><p>具体来说就是在训练阶段，首先用训练一个auto-regressive的TTS模型，这个时候我们不需要知道phoneme duration。</p><p>接下来我们用这个TTS模型来为每个训练数据对儿生成attention alignment（也就是说真实的音素持续时间是由已经训练好的Transformer TTS的multi-head attention提供）。因为multi-head attention，包含多种注意力排列，而不是所有的注意头都表现出对角线的特性（即attention weight分布在对角上）。所以，作者制定了一种方式：$ F=\frac{1}{S} \sum_{s=1}^{S} \max _{1 \leq t \leq T} a_{s, t} $，其中S和T分别代表真实的mel谱图和音素序列的长度、 $ a_{s, t} $ 表示 attention矩阵中第s行第t列的元素的数值，最终选择F最大的head用作attention alignment。</p><p>有了上面得到的alignment，我们用下面的式子计算$ \mathcal{D}=\left[d_{1}, d_{2}, \ldots, d_{n}\right] $：</p><script type="math/tex; mode=display">d_{i}=\sum_{s=1}^{S}\left[\arg \max _{t} a_{s, t}=i\right]</script><blockquote><p>That is, the duration of a phoneme is the number of mel-spectrograms attended to it according to the attention head selected in the above step.</p></blockquote><p>在训练过程中，Duration Predictor模块与Fastspeech一起做联合训练，其预测结果与目标做Loss。</p><blockquote><p>也就是在这个模块中，作者抛弃了传统Encoder+attention+Decoder模型中隐式的attention alignment方式，加入了显式的alignment标签。</p></blockquote><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>在两个小节中我们分析了FastSpeech主要解决了以下三个问题：</p><p>1）解决已有自回归模型推理过程中合成语音速度慢的问题；</p><p>2）取消了先前模型中编码器-解码器之间的隐式注意力机制，从而避免因为注意力对齐不准而带来的合成语句不稳定的问题；</p><p>3）在音素持续时间预测模块中引入了$\alpha$因子，使得合成语音的时长（语速)可控。</p><p>但是一些不足也很明显，如：</p><p>1）合成语音的质量（上限）会受到teacher模型的严重影响；</p><p>2）只能控制合成语音的速度，可控性依旧有限。</p><p>代码解读可以参考<a href="https://zhuanlan.zhihu.com/p/517028509" target="_blank" rel="noopener">声学模型（03）：Fastspeech</a>。</p><p>个人补充一下关于注意力对齐：注意力对齐应该是文本和音频的对齐。假设我们的输入时音素和对应的音频，由于对齐不准，可能导致其中一个音素包含了其它音素的音频或者缺失了一部分，导致跳词和重复的问题。</p><h2 id="Fastspeech2"><a href="#Fastspeech2" class="headerlink" title="Fastspeech2"></a>Fastspeech2</h2><h3 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h3><p>虽然FastSpeech作为一个non-autogressive TTS模型已经取得了比auto-regressive模型如Tacotron更快的生成速度和类似的语音质量，但是FastSpeech仍然存在一些缺点，比如</p><ol><li><p>使用一个auto-regressive的TTS模型作为teacher，训练模型非常耗费时间；</p></li><li><p>使用知识蒸馏的方式来训练模型会导致信息损失，从而对合成出的语音的音质造成影响。</p></li></ol><p>在<a href="https://arxiv.org/abs/2006.04558" target="_blank" rel="noopener">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a>文章中，作者针对这些问题进行了改进，作者首先摒弃了知识蒸馏的teacher-student训练，采用了直接在ground-truth上训练的方式。其次在模型中引入了更多的可以控制语音的输入，其中既包括我们在FastSpeech中提到的phoneme duration，也包括energy、pitch等新的量。作者将这个模型命名为FastSpeech2。作者在此基础之上提出了FastSpeech2s，这个模型可以直接从text生成语音而不是mel-spectrogram。实验结果证明FastSpeech2的训练速度比FastSpeech加快了3倍，FastSpeech2s有比其它模型更快的合成速度。在音质方面，FastSpeech2和2s都超过了之前auto-regressive模型。</p><h3 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型的整体架构如下图所示：</p><p><img src="/DeepLearningApplications/语音合成/语音合成模型/image-20220605010039989.png" alt="image-20220605010039989" style="zoom: 43%;"></p><p>整体上来说（上图(a)），FastSpeech2在encoder和decoder上采用了和FastSpeech类似的基于self-attention和1D卷积的结构。</p><p>不同的是，FastSpeech2使用了variance adaptor（上图(b)）用来引入更多的输入来控制合成出的语音，正如之前提到的，这里不仅有phoneme duration也有energy和pitch，我们看到这个adaptor的结构使得它可以引入任意多的额外输入。最后，作者没有使用之前的从attention matrix中推断phoneme duration的方法，而是使用了forced alignment得到的duration作为训练的ground truth，实验结果也证明这种方法得到的duration会更加精确。</p><p>这里可能一部分的读者不清楚forced alignment是什么。其实这是一种TTS中常用的技术，用来推断音素对应的音频，比如Montreal Forced Aligner (MFA)库。</p><h4 id="Variance-Adaptor"><a href="#Variance-Adaptor" class="headerlink" title="Variance Adaptor"></a>Variance Adaptor</h4><p>Variance Adaptor（VA）是给phoneme hidden seq加上变化信息（各种声学特征），对于TTS的one-to-many映射提供帮助。作者在这里加上了三种：duration，pitch和energy。此外像emotion、style、speaker等信息都可以加到VA上。</p><p>VA的设计如图（b）所示，GT的duration、pitch、energy一方面被用来在训练时作为condition预测mel谱，另一方面被用来训练声学特征预测器Duration Predictor（DP）、Pitch Predictor（PP）和Energy Predictor（EP）。</p><p>Duration Predictor用到了forced alignment抽出的phoneme duration作为训练目标。输入phoneme hidden seq，输出每个音素对应的预测帧数（为便于预测转换成对数域）。DP训练用的是MSE loss，GT 音素时长是通过<strong>Montreal Forced Alignment（MFA）</strong>工具从原音频中提取的。</p><p>Pitch Predictor需要语音的pitch信息作为训练目标，一般情况下会使用pitch contour（基频轮廓），不过这里作者认为pitch contour的variation很大，不好预测。因此作者使用了pitch spectrogram作为训练目标。作者首先使用continuous wavelet transform (连续小波变换，CWT) 获得pitch spectrogram，然后训练predictor去预测它。在合成语音的时候，作者使用inverse CWT (iCWT)，即CWT的逆运算来将pitch spectrogram转换称pitch contour。作者进一步根据pitch F0的大小把它们映射到对数域的256个值上 ，最后把值对应的pitch embedding加在phoneme hidden state上，以此为GT target计算MSE loss。</p><p>Energy Predictor：对于每一STFT帧计算其幅度的L2范数作为能量值，然后将energy均匀量化成256个可能值，最后将值对应的embedding加到hidden state上。这里训练的时候predictor会直接预测映射之前的energy，并计算MSE loss。</p><h4 id="FastSpeech2s"><a href="#FastSpeech2s" class="headerlink" title="FastSpeech2s"></a>FastSpeech2s</h4><p>作者希望实现text-to-waveform而不是text-to-mel-to-waveform的合成方式，因此扩展FastSpeech2提出了FastSpeech2s。在上一节的架构图的子图(a)中我们可以看到，FastSpeech2s直接从hidden state中生成waveform，而不使用mel-spectrogram decoder。</p><p>架构图的子图(d)给出了waveform decoder的架构，作者使用类似WaveNet的结构，其中包含了dilated卷积和gated activation。这里作者使用了WaveGAN中的对抗训练的方法来让模型隐式地学习到恢复phase information的方法。值得注意的是这里作者在训练FastSpeech2s的时候也同时训练FastSpeech2的mel-spectrogram decoder，作者认为这样可以从text中提取更多的信息。</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>本文介绍了FastSpeech的改进版FastSpeech2/2s，FastSpeech2改进了FastSpeech的训练方法，通过引入forced alignment以及pitch和energy信息提升了模型的训练速度和精度。FastSpeech2s进一步实现了text-to-waveform的训练方式，因此提升了合成速度。实验结果证明FastSpeech2的训练速度比FastSpeech快了3倍，另外FastSpeech2s由于不需要生成mel-spectrogram因此有更快的合成速度。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.dengbocong.cn/Paper-Reading/cea2357273fe/" target="_blank" rel="noopener">论文阅读笔记：Tacotron和Tacotron2</a><br><a href="https://www.jianshu.com/p/ba30cc13093a" target="_blank" rel="noopener">语音合成(三)：端到端的TTS深度学习模型tacotron</a><br><a href="https://blog.csdn.net/junbaba_/article/details/118357486" target="_blank" rel="noopener">Tacotron以及Tacotron2详解</a><br><a href="https://unisound.github.io/end-to-end_tts/" target="_blank" rel="noopener">端到端语音合成及其优化实践(上)</a><br><a href="https://www.jianshu.com/p/46888767dcef" target="_blank" rel="noopener">语音合成简介 Text-to-speech</a><br><a href="https://www.cnblogs.com/mengnan/p/9474111.html" target="_blank" rel="noopener">语音合成技术综述</a><br><a href="https://zhuanlan.zhihu.com/p/512240545" target="_blank" rel="noopener">声学模型（02）：Transformer based TTS</a><br><a href="https://zhuanlan.zhihu.com/p/517028509" target="_blank" rel="noopener">声学模型（03）：Fastspeech</a><br><a href="https://zhuanlan.zhihu.com/p/67325775" target="_blank" rel="noopener">FastSpeech阅读笔记</a><br><a href="https://zhuanlan.zhihu.com/p/363808377" target="_blank" rel="noopener">FastSpeech2——快速高质量语音合成</a><br><a href="https://zhuanlan.zhihu.com/p/371094738" target="_blank" rel="noopener">TTS paper阅读：FastSpeech 2</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇介绍几个经典的语音合成模型。&lt;/p&gt;
&lt;h2 id=&quot;什么是语音合成&quot;&gt;&lt;a href=&quot;#什么是语音合成&quot; class=&quot;headerlink&quot; title=&quot;什么是语音合成&quot;&gt;&lt;/a&gt;什么是语音合成&lt;/h2&gt;&lt;p&gt;语音合成是通过文字人工生成人类声音， 也可以说语音生成是给定一段文字去生成对应的人类读音。 这里声音是一个连续的模拟的信号。而合成过程是通过计算机， 数字信号去模拟。 这里就需要数字信号处理模拟信号信息，详细内容可参考 [1]。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DeepLearningApplications/语音合成/语音合成模型/webp&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;Fig. 1 an example of voice signal. &lt;/p&gt;
    
    </summary>
    
    
      <category term="DeepLearningApplications" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/"/>
    
      <category term="语音合成" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
    
      <category term="语音合成" scheme="https://www.zdaiot.com/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>语音基础知识</title>
    <link href="https://www.zdaiot.com/DeepLearningApplications/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>https://www.zdaiot.com/DeepLearningApplications/语音合成/语音基础知识/</id>
    <published>2022-06-04T07:22:03.000Z</published>
    <updated>2022-06-04T07:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>开新坑了，最近在攻击语音合成系统，现在学习一下语音基础知识。</p><h2 id="辅音和元音的区别"><a href="#辅音和元音的区别" class="headerlink" title="辅音和元音的区别"></a>辅音和元音的区别</h2><ul><li>辅音发音时，气流在通过咽头、口腔的过程中， 要受到某部位的阻碍；元音发音时，气流在咽头、 口腔不受阻碍。这是元音和辅音最主要的区别。</li><li>辅音发音时，发音器官成阻的部位特别紧张； 元音发音时发音器官各部位保持均衡的紧张状态。</li><li>辅音发音时，气流较强；元音发音时，气流较 弱。</li><li>辅音发音时，声带不一定振动，声音一般不响亮；元音发音时，声带振动，声音比辅音响亮。</li></ul><blockquote><p>一般只有元音（一些介于元音辅音中间分类不明的音暂不讨论）才会有共振峰，而元音的音质由声道的形状决定，而声道的形状又通过发音的动作来塑造（articulatory+movements）。</p></blockquote><h2 id="清音和浊音"><a href="#清音和浊音" class="headerlink" title="清音和浊音"></a>清音和浊音</h2><ul><li>清音：声带不振动</li><li>浊音：声带振动而发音</li><li>元音都是浊音、辅音有清音也有浊音。</li></ul><h2 id="波形、频谱和语谱（声谱）"><a href="#波形、频谱和语谱（声谱）" class="headerlink" title="波形、频谱和语谱（声谱）"></a>波形、频谱和语谱（声谱）</h2><p>以下内容主要来源于<a href="https://www.zhihu.com/question/27126800/answer/35376174" target="_blank" rel="noopener">不同元音辅音在声音频谱的表现是什么样子？ - 王赟 Maigo的回答 - 知乎</a>。</p><h3 id="波形"><a href="#波形" class="headerlink" title="波形"></a>波形</h3><p>声音最直接的表示方式是波形，英文叫waveform。另外两种表示方式（频谱和语谱图）下文再说。波形的横轴是时间（所以波形也叫声音的时域表示），纵轴的含义并不重要，可以理解成位移（声带或者耳机膜的位置）或者压强。</p><p>当横轴的分辨率不高的时候，语音的波形看起来就是像你贴的图中一样，呈现一个个的三角形。这些三角形的轮廓叫作波形的<strong>包络（envelope）</strong>。包络的大小代表了声音的响度。一般来说，每一个音节会对应着一个三角形，因为一般地每个音节含有一个元音，而元音比辅音听起来响亮。但例外也是有的，比如：1) 像/s/这样的音，持续时间比较长，也会形成一个三角形；2) 爆破音（尤其是送气爆破音，如/p/）可能会在瞬时聚集大量能量，在波形的包络上就体现为一个脉冲。</p><p>下面这张图中上方的子图，是读单词pass /pæs/的录音。它的横坐标已经被拉开了一些，但其实这个波形是由两个“三角形”组成的。0.05秒处那个小突起是爆破音/p/，0.05秒到0.3秒是元音/æ/，0.3到0.58秒是辅音/s/。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/69e0842536f5cb23b16e05e2cc8402f5_720w.jpg" data-rawwidth="675" data-rawheight="328" class="origin_image zh-lightbox-thumb" width="675" data-original="https://pica.zhimg.com/69e0842536f5cb23b16e05e2cc8402f5_r.jpg?source=1940ef5c"></p><p>如果你把横轴的分辨率调高，比如只观察0.02s秒甚至更短时间内的波形，你就可以看到波形的精细结构（fine structure），像上图的下面两个子图。波形的精细结构可能呈现两种情况：一种是有周期性的，比如左边那段波形（图中显示了两个周期多一点），这种波形一般是元音或者辅音中的鼻音、浊擦音以及/l/、/r/等；另一种是乱的，比如右边那段波形，这种波形一般是辅音中的清擦音。辅音中的爆破音，则往往表现为一小段静音加一个脉冲（如pass开头的/p/）。</p><h3 id="频谱"><a href="#频谱" class="headerlink" title="频谱"></a>频谱</h3><p>看完了声音的<strong>时域</strong>表示，我们再来看它的<strong>频域</strong>表示——<strong>频谱（spectrum）</strong>。它是由一小段波形做傅里叶变换（Fourier transform）之后取模得到的。注意，必须是一小段波形，太长了弄出来的东西（比如你贴的右边的图）就没意义了！这样的一小段波形（通常在0.02~0.05s这样的数量级）称为<strong>一帧（frame）</strong>。下面是读的pass的波形中，以0.17s和0.4s为中心截取0.04s波形经傅里叶变换得到的频谱。频谱的横轴是频率；录音的采样率用的是16000 Hz，频谱的频率范围也是0 ~ 16000 Hz。但由于0 ~ 8000 Hz和8000 ~ 16000 Hz的频谱是对称的，所以一般只画0 ~ 8000 Hz的部分。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/7301a5be8a003cdaa272a5eba362d43a_1440w.jpg" alt="img"></p><p>频谱跟波形一样，也有包络和精细结构。你把横轴压缩，看到的就是包络；把横轴拉开，看到的就是精细结构。我上面这两张图使得二者都能看到。</p><p>第一个频谱是元音/æ/的频谱，可以看到它的精细结构是有周期性的，每隔108 Hz出现一个峰。从这儿也可以看出来，语音不是一个单独的频率，而是由许多频率的简谐振动叠加而成的。第一个峰叫<strong>基音</strong>，其余的峰叫<strong>泛音</strong>。第一个峰的频率（也是相邻峰的间隔）叫作<strong>基频（fundamental frequency），也叫音高（pitch）</strong>，常记作$f_0$。有时说“一个音的频率”，就是特指基频。基频的倒数叫<strong>基音周期</strong>。你再看看上面元音/æ/的波形的周期，大约是0.009 s，跟基频108 Hz吻合。频谱上每个峰的高度是不一样的，这些峰的高度之比决定了<strong>音色（timbre）</strong>。不过对于语音来说，一般没有必要精确地描写每个峰的高度，而是用<strong>“共振峰”（formant）</strong>来描述音色。共振峰指的是包络的峰。在我这个图中，忽略精细结构，可以看到0~1000 Hz形成一个比较宽的峰，1800 Hz附近形成一个比较窄的峰。共振峰的频率一般用$f_1$、$f_2$等等来表示。上图中，$f_1$是多少很难精确地读出来，但$f_2  \approx 1800Hz$。当然，在2800 Hz、3800 Hz、5000 Hz处还有第三、四、五共振峰，但它们与第一、二共振峰相比就弱了许多。除了元音以外，辅音中的鼻音、浊擦音以及/l/、/r/等也具有这种频谱，可以讨论基频和共振峰频率（不过浊擦音一般不讨论共振峰频率）。</p><p>第二个频谱是辅音/s/的频谱。可以看出它的精细结构是没有周期性的，所以就无所谓基频。一般也不提这种频谱的共振峰。清擦音的频谱一般都是这样。</p><h3 id="语谱（声谱）"><a href="#语谱（声谱）" class="headerlink" title="语谱（声谱）"></a>语谱（声谱）</h3><p>我们最后来看一下声音的第三种表示方式——<strong>语谱图</strong>（<strong>spectrogram</strong>）。上面说过，频谱只能表示一小段声音。那么，如果我想观察一整段语音信号的频域特性，要怎么办呢？我们可以把一整段语音信号截成许多帧，把它们各自的频谱“竖”起来（即用纵轴表示频率），用颜色的深浅来代替频谱强度，再把所有帧的频谱横向并排起来（即用横轴表示时间），就得到了<strong>语谱图</strong>，它可以称为声音的<strong>时频域</strong>表示。下面我就偷懒，不用Matlab自己画语谱图，而用Cool Edit绘制上面“pass”的语谱图，如下：</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/fd6436356cdd4647bfabff165d24df1c_1440w.jpg" alt="img"></p><p>注意横轴是时间，纵轴是频率，颜色越亮代表强度越大。可以观察一下0.17s和0.4s处，是不是跟我上面画的频谱相似？然后再试着从这张语谱图上读出元音/æ/的第二共振峰频率。</p><p>语谱图的好处是可以直观地看出共振峰频率的变化。我上面读的“pass”中只有一个单元音，如果有双元音就会非常明显了。比如下面这张我读的“eye” /aɪ/，可以非常明显地看出在元音从/a/向/ɪ/过渡的阶段（0.2 ~ 0.25s），$f_1$在降低，而$f_2$在升高。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/5c20e251d868c7d535ab9c4e4e8a5a16_1440w.jpg" alt="img"></p><p>元音与共振峰的关系已经研究得比较透彻了，简单地说：</p><p>1) 开口度越大, $ f_{1} $ 越高;<br>2) 舌位越靠前, $ f_{2} $ 越高;<br>3) 不圆唇元音的 $ f_{3} $ 比圆唇元音高。例如, $ / \mathrm{a} / $ 是开、后、不圆唇元音, 所以 $ f_{1} $ 高, $ f_{2} $ 低, $ f_{3} $ 高；/y/（即汉语拼音的ü）是闭、前、圆 唇元音, 所以 $ f_{1} $ 低, $ f_{2} $ 高, $ f_{3} $ 低。也许大家见过下图那样的元音图Q (vowel chart) , 我把 $ f_{1} $ 和 $ f_{2} $ 的变化方向标 $ Q $ 上去。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/791ea703a54bac444deacdce9f039cb1_1440w.jpg" alt="img" style="zoom:75%;"></p><p>$f_3$最明显的体现其实是在英语的辅音/r/中，例如下面我读的erase /ɪ’reɪz/的语谱图，可以看到辅音/r/处（0.19s左右）$f_3$明显低，把$f_2$也压下去了。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/517c32cad639c6f66721060bf2f3aa9e_1440w.jpg" alt="img"></p><p>清擦音可以根据能量集中的频段来分辨。下面是我读的/f/, /θ/, /s/, /ʃ/的语谱图。浊擦音会在清擦音的基础上有周期性的精细结构。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/25ccd5ecb60a5dcd0acf414e67b12594_1440w.jpg" alt="img"></p><p>爆破音的爆破时间很短，在语谱图上一般较难分辨。</p><p>“两个音之间的音是什么样子”，就要分情况讨论了。</p><p>1) 如果是两个元音，那么可以在元音图上找到两个元音，取它们连线的中点。这对应着把$f_1$、$f_2$分别取平均。<br>2) 如果是两个清擦音，那么可以把它们的频谱取平均，这样听起来应该是个四不像（后来我做了实验，结果见这里：<a href="http://maigoakisame.github.io/fricative-mix/" target="_blank" rel="noopener">Mixture of Unvoiced Fricatives</a>）。<br>3) /t/和/ʃ/属于不同类型的辅音，很难定义它们“之间”是什么东西。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/DeepLearningApplications/语音合成/语音基础知识/6169076_1625140337.jpg" alt="img" style="zoom: 33%;"></p><h2 id="语音基本概念"><a href="#语音基本概念" class="headerlink" title="语音基本概念"></a>语音基本概念</h2><p>以下内容主要来源于<a href="https://zhuanlan.zhihu.com/p/510550742" target="_blank" rel="noopener">语音基础知识（附相关实现代码）</a>。在不理解的地方我会加上自己的注释。</p><p>声波通过空气传播，被麦克风接收，通过<strong>采样、量化、编码</strong>转换为离散的数字信号，即波形文件。<strong>音量、音高和音色是声音的基本属性。</strong></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-0b6dcfd913268a433bf3abaee8948cd1_b.jpg" alt></p><p><strong>1）采样</strong>：原始的语音信号是连续的模拟信号，需要对语音进行采样，转化为时间轴上离散的数据。 </p><p><strong>采样后</strong>，模拟信号被等间隔地取样，这时信号在时间上就不再连续了，但在幅度上还是连续的。经过采样处理之后，模拟信号变成了离散时间信号。</p><p><strong>采样频率</strong>是指一秒钟内对声音信号的采样次数，采样频率越高声音的还原就越真实越自然。</p><p>在当今的主流采集卡上，采样频率一般共分为 22.05KHz、44.1KHz、48KHz 三个等级，22.05KHz 只能达到 FM 广播的声音品质，44.1KHz 则是理论上的 CD 音质界限（人耳一般可以感觉到 20-20K Hz 的声音，根据香农采样定理，采样频率应该不小于最高频率的两倍，所以 40KHz 是能够将人耳听见的声音进行很好的还原的一个数值，于是 CD 公司把采样率定为 44.1KHz），48KHz 则更加精确一些。</p><p>对于高于 48KHz 的采样频率人耳已无法辨别出来了，所以在电脑上没有多少使用价值。</p><p><strong>2）量化</strong>：进行分级量化，将信号采样的幅度划分成几个区段，把落在某区段的采样到的样品值归成一类，并给出相应的量化值。根据量化间隔是否均匀划分，又分为均匀量化和非均匀量化。</p><p><strong>均匀量化</strong>的特点为 “大信号的信噪比大，小信号的信噪比小”。缺点为 “为了保证信噪比要求，编码位数必须足够大，但是这样导致了信道利用率低，如果减少编码位数又不能满足信噪比的要求”（根据信噪比公式，编码位数越大，信噪比越大，通信质量越好）。</p><p>通常对语音信号采用<strong>非均匀量化</strong>，基本方法是对大信号使用大的量化间隔，对小信号使用小的量化间隔。由于小信号时量化间隔变小，其相应的量化噪声功率也减小（根据量化噪声功率公式），从而使小信号时的量化信噪比增大，改善了小信号时的信噪比。</p><p><strong>量化后</strong>，信号不仅在时间上不再连续，在幅度上也不连续了。经过量化处理之后，离散时间信号变成了数字信号。</p><p><strong>3）编码</strong>：在量化之后信号已经变成了数字信号，需要将数字信号编码成二进制。<strong>“</strong>CD 质量<strong>”</strong> 的语音采用 44100 个样本每秒的采样率，每个样本 16 比特，这个 16 比特就是编码的位数。</p><p>采样，量化，编码的过程称为 A/D（从模拟信号到数字信号）转换，如上图 1 所示。</p><p>补充<strong>比特率</strong>的概念：比特率是指每秒传送的比特(bit)数。单位为 bps(Bit Per Second)，比特率越高，传送的数据越大，音质越好。以电话为例，每秒3000点取样，每个样本是7比特，那么电话的比特率是21000。而CD是每秒44100点取样，两个声道，每个取样是13位PCM编码，所以CD的比特率是$44100<em>2</em>13=1146600$，也就是说CD每秒的数据量大约是144KB，而一张CD的容量是74分等于4440秒，就是639360KB＝640MB。</p><h3 id="能量"><a href="#能量" class="headerlink" title="能量"></a>能量</h3><p><strong>音频的能量通常指的是时域上每帧的能量，幅度的平方。</strong>在简单的语音活动检测（Voice Activity Detection，VAD）中，直接利用能量特征：能量大的音频片段是语音，能量小的音频片段是非语音（包括噪音、静音段等）。这种 VAD 的局限性比较大，正确率也不高，对噪音非常敏感。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_file, sr=None, frame_len=<span class="number">512</span>, n_fft=None, win_step=<span class="number">2</span> / <span class="number">3</span>, window=<span class="string">"hamming"</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化</span></span><br><span class="line"><span class="string">        :param input_file: 输入音频文件</span></span><br><span class="line"><span class="string">        :param sr: 所输入音频文件的采样率，默认为None</span></span><br><span class="line"><span class="string">        :param frame_len: 帧长，默认512个采样点(32ms,16kHz),与窗长相同</span></span><br><span class="line"><span class="string">        :param n_fft: FFT窗口的长度，默认与窗长相同</span></span><br><span class="line"><span class="string">        :param win_step: 窗移，默认移动2/3，512*2/3=341个采样点(21ms,16kHz)</span></span><br><span class="line"><span class="string">        :param window: 窗类型，默认汉明窗</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.input_file = input_file</span><br><span class="line">        self.frame_len = frame_len  <span class="comment"># 帧长，单位采样点数</span></span><br><span class="line">        self.wave_data, self.sr = librosa.load(self.input_file, sr=sr)</span><br><span class="line">        self.window_len = frame_len  <span class="comment"># 窗长512</span></span><br><span class="line">        <span class="keyword">if</span> n_fft <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.fft_num = self.window_len  <span class="comment"># 设置NFFT点数与窗长相等</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fft_num = n_fft</span><br><span class="line">        self.win_step = win_step</span><br><span class="line">        self.hop_length = round(self.window_len * win_step)  <span class="comment"># 重叠部分采样点数设置为窗长的1/3（1/3~1/2）,即帧移(窗移)2/3</span></span><br><span class="line">        self.window = window</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">energy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        每帧内所有采样点的幅值平方和作为能量值</span></span><br><span class="line"><span class="string">        :return: 每帧能量值，np.ndarray[shape=(1，n_frames), dtype=float64]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mag_spec = np.abs(librosa.stft(self.wave_data, n_fft=self.fft_num, hop_length=self.hop_length,</span><br><span class="line">                                       win_length=self.frame_len, window=self.window))</span><br><span class="line">        pow_spec = np.square(mag_spec) <span class="comment"># [frequency, time (n_frames)]</span></span><br><span class="line">        energy = np.sum(pow_spec, axis=<span class="number">0</span>) <span class="comment"># [n_frames]</span></span><br><span class="line">        energy = np.where(energy == <span class="number">0</span>, np.finfo(np.float64).eps, energy)  <span class="comment"># 避免能量值为0，防止后续取log出错(eps是取非负的最小值), 即np.finfo(np.float64).eps = 2.220446049250313e-16</span></span><br><span class="line">        <span class="keyword">return</span> energy</span><br></pre></td></tr></table></figure><h3 id="短时能量"><a href="#短时能量" class="headerlink" title="短时能量"></a>短时能量</h3><p>短时能量体现的是信号在不同时刻的强弱程度。设第 n 帧语音信号的短时能量用$E_n$表示，则其计算公式为：</p><script type="math/tex; mode=display">E_n = \sum_{m=0}^{M-1}x_n^2(m)</script><p>上式中，M 为帧长，$x_n(m)$为该帧中的样本点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">short_time_energy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算语音短时能量：每一帧中所有语音信号的平方和</span></span><br><span class="line"><span class="string">        :return: 语音短时能量列表(值范围0-每帧归一化后能量平方和，这里帧长512，则最大值为512)，</span></span><br><span class="line"><span class="string">        np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=float64]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        energy = []  <span class="comment"># 语音短时能量列表</span></span><br><span class="line">        energy_sum_per_frame = <span class="number">0</span>  <span class="comment"># 每一帧短时能量累加和</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.wave_data)):  <span class="comment"># 遍历每一个采样点数据</span></span><br><span class="line">            energy_sum_per_frame += self.wave_data[i] ** <span class="number">2</span>  <span class="comment"># 求语音信号能量的平方和</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % self.frame_len == <span class="number">0</span>:  <span class="comment"># 一帧所有采样点遍历结束</span></span><br><span class="line">                energy.append(energy_sum_per_frame)  <span class="comment"># 加入短时能量列表</span></span><br><span class="line">                energy_sum_per_frame = <span class="number">0</span>  <span class="comment"># 清空和</span></span><br><span class="line">            <span class="keyword">elif</span> i == len(self.wave_data) - <span class="number">1</span>:  <span class="comment"># 不满一帧，最后一个采样点</span></span><br><span class="line">                energy.append(energy_sum_per_frame)  <span class="comment"># 将最后一帧短时能量加入列表</span></span><br><span class="line">        energy = np.array(energy)</span><br><span class="line">        energy = np.where(energy == <span class="number">0</span>, np.finfo(np.float64).eps, energy)  <span class="comment"># 避免能量值为0，防止后续取log出错(eps是取非负的最小值)</span></span><br><span class="line">        <span class="keyword">return</span> energy</span><br></pre></td></tr></table></figure><h3 id="声强和声强级（声压和声压级）"><a href="#声强和声强级（声压和声压级）" class="headerlink" title="声强和声强级（声压和声压级）"></a>声强和声强级（声压和声压级）</h3><p>单位时间内通过垂直于声波传播方向的单位面积的平均声能，称作声强，声强用 P 表示，单位为 “瓦 / 平米”。实验研究表明，人对声音的强弱感觉并不是与声强成正比，而是与其对数成正比，所以一般<strong>声强用声强级来表示</strong>：</p><script type="math/tex; mode=display">L = 10 \text{log}\left(\frac{P}{P'} \right)</script><p>其中，P 为声强， $P’=10e^{-12}$单位（$w/m^2$）称为基本声强，声强级的常用单位是分贝 (dB)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intensity</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算声音强度，用声压级表示：每帧语音在空气中的声压级Sound Pressure Level(SPL)，单位dB</span></span><br><span class="line"><span class="string">        公式：20*lg(P/Pref)，P为声压（Pa），Pref为参考压力(听力阈值压力)，一般为1.0*10-6 Pa</span></span><br><span class="line"><span class="string">        这里P认定为声音的幅值：求得每帧所有幅值平方和均值，除以Pref平方，再取10倍lg</span></span><br><span class="line"><span class="string">        :return: 每帧声压级，dB，np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=float64]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p0 = <span class="number">1.0e-6</span>  <span class="comment"># 听觉阈限压力auditory threshold pressure: 2.0*10-5 Pa</span></span><br><span class="line">        e = self.short_time_energy()</span><br><span class="line">        spl = <span class="number">10</span> * np.log10(<span class="number">1</span> / (np.power(p0, <span class="number">2</span>) * self.frame_len) * e)</span><br><span class="line">        <span class="keyword">return</span> spl</span><br></pre></td></tr></table></figure><h3 id="过零率"><a href="#过零率" class="headerlink" title="过零率"></a>过零率</h3><p>过零率体现的是信号过零点的次数，体现的是频率特性。</p><script type="math/tex; mode=display">Z_{n}=\sum_{n=0}^{N-1} \sum_{m=0}^{M-1}\left|\operatorname{sgn}\left(x_{n}(m)\right)-\operatorname{sgn}\left(x_{n}(m-1)\right)\right|</script><p>其中，N 表示帧数，M 表示每一帧中的样本点个数，sgn 为符号函数，即：</p><script type="math/tex; mode=display">\operatorname{sgn}=\left\{\begin{array}{c}1, x \geq 0 \\-1, x<0\end{array}\right.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_crossing_rate</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算语音短时过零率：单位时间(每帧)穿过横轴（过零）的次数</span></span><br><span class="line"><span class="string">        :return: 每帧过零率次数列表，np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=uint32]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        zcr = []  <span class="comment"># 语音短时过零率列表</span></span><br><span class="line">        counting_sum_per_frame = <span class="number">0</span>  <span class="comment"># 每一帧过零次数累加和，即过零率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.wave_data)):  <span class="comment"># 遍历每一个采样点数据</span></span><br><span class="line">            <span class="keyword">if</span> i % self.frame_len == <span class="number">0</span>:  <span class="comment"># 开头采样点无过零，因此每一帧的第一个采样点跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> self.wave_data[i] * self.wave_data[i - <span class="number">1</span>] &lt; <span class="number">0</span>:  <span class="comment"># 相邻两个采样点乘积小于0，则说明穿过横轴</span></span><br><span class="line">                counting_sum_per_frame += <span class="number">1</span>  <span class="comment"># 过零次数加一</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % self.frame_len == <span class="number">0</span>:  <span class="comment"># 一帧所有采样点遍历结束</span></span><br><span class="line">                zcr.append(counting_sum_per_frame)  <span class="comment"># 加入短时过零率列表</span></span><br><span class="line">                counting_sum_per_frame = <span class="number">0</span>  <span class="comment"># 清空和</span></span><br><span class="line">            <span class="keyword">elif</span> i == len(self.wave_data) - <span class="number">1</span>:  <span class="comment"># 不满一帧，最后一个采样点</span></span><br><span class="line">                zcr.append(counting_sum_per_frame)  <span class="comment"># 将最后一帧短时过零率加入列表</span></span><br><span class="line">        <span class="keyword">return</span> np.array(zcr, dtype=np.uint32)</span><br></pre></td></tr></table></figure><h3 id="基频和基音周期"><a href="#基频和基音周期" class="headerlink" title="基频和基音周期"></a>基频和基音周期</h3><p>基音周期反映了声门相邻两次开闭之间的时间间隔，基频（fundamental frequency， F0）则是基音周期的倒数，对应着声带振动的频率，代表声音的音高，声带振动越快，基频越高。如图 2 所示，蓝色箭头指向的就是基频的位置，决定音高。它是语音激励源的一个重要特征，比如可以通过基频区分性别。一般来说，成年男性基频在 100-250Hz 左右，成年女性基频在 150-350Hz 左右，女声的音高一般比男声稍高。 人类可感知声音的频率大致在 20-20000Hz 之间，人类对于基频的感知遵循对数律，也就是说，人们会感觉 100Hz 到 200Hz 的差距，与 200Hz 到 400Hz 的差距相同。因此，<strong>音高常常用基频的对数来表示。</strong></p><blockquote><p>这部分的详细介绍可以看前面的<code>波形、频谱和语谱（声谱）</code>小节。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-624ccef5c445fe5b2502d65de1ee38bd_r.jpg" alt></p><h3 id="音高"><a href="#音高" class="headerlink" title="音高"></a>音高</h3><p>音高（pitch）是由声音的基频决定的，音高和基频常常混用。可以这样认为，<strong>音高（pitch）是稀疏离散化的基频（F0）</strong>。由规律振动产生的声音一般都会有基频，比如语音中的元音和浊辅音；也有些声音没有基频，比如人类通过口腔挤压气流的清辅音。在汉语中，元音有 a/e/i/o/u，浊辅音有 y/w/v，其余音素比如 b/p/q/x 等均为清辅音，在发音时，可以通过触摸喉咙感受和判断发音所属音素的种类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pitch</span><span class="params">(self, ts_mag=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        获取每帧音高，即基频，这里应该包括基频和各次谐波，最小的为基频（一次谐波），其他的依次为二次、三次...谐波</span></span><br><span class="line"><span class="string">        各次谐波等于基频的对应倍数，因此基频也等于各次谐波除以对应的次数，精确些等于所有谐波之和除以谐波次数之和</span></span><br><span class="line"><span class="string">        :param ts_mag: 幅值倍乘因子阈值，&gt;0，大于np.average(np.nonzero(magnitudes)) * ts_mag则认为对应的音高有效,默认0.25</span></span><br><span class="line"><span class="string">        :return: 每帧基频及其对应峰的幅值(&gt;0)，</span></span><br><span class="line"><span class="string">                 np.ndarray[shape=(1 + n_fft/2，n_frames), dtype=float32]，（257，全部采样点数/(512*2/3)+1）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mag_spec = np.abs(librosa.stft(self.wave_data, n_fft=self.fft_num, hop_length=self.hop_length,</span><br><span class="line">                                       win_length=self.frame_len, window=self.window))</span><br><span class="line">        pitches, magnitudes = librosa.piptrack(S=mag_spec, sr=self.sr, threshold=<span class="number">1.0</span>, ref=np.mean,</span><br><span class="line">                                               fmin=<span class="number">50</span>, fmax=<span class="number">500</span>)  <span class="comment"># 人类正常说话基频最大可能范围50-500Hz</span></span><br><span class="line">        ts = np.average(magnitudes[np.nonzero(magnitudes)]) * ts_mag</span><br><span class="line">        pit_likely = pitches</span><br><span class="line">        mag_likely = magnitudes</span><br><span class="line">        pit_likely[magnitudes &lt; ts] = <span class="number">0</span></span><br><span class="line">        mag_likely[magnitudes &lt; ts] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> pit_likely, mag_likely</span><br><span class="line">pitches, mags = self.pitch()  <span class="comment"># 获取每帧基频</span></span><br><span class="line">f0_likely = []  <span class="comment"># 可能的基频F0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(pitches.shape[<span class="number">1</span>]):  <span class="comment"># 按列遍历非0最小值，作为每帧可能的F0</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        f0_likely.append(np.min(pitches[np.nonzero(pitches[:, i]), i]))</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        f0_likely.append(np.nan)  <span class="comment"># 当一列，即一帧全为0时，赋值最小值为nan</span></span><br><span class="line">f0_all = np.array(f0_likely)</span><br></pre></td></tr></table></figure><h3 id="共振峰"><a href="#共振峰" class="headerlink" title="共振峰"></a>共振峰</h3><p>声门处的准周期激励进入声道时会引起共振特性，产生一组共振频率，这一组共振频率称为共振峰频率或简称共振峰。共振峰包含在语音的频谱包络中，频谱极大值就是共振峰。<strong>频率最低的共振峰称为第一共振峰，对应的频率也称作基频，决定语音的 F0，其它的共振峰统称为谐波</strong>，如上图 2 所示，蓝色箭头指向频谱的第一共振峰，也就是基频的位置，决定音高；而绿框则是其它共振峰，统称为谐波。谐波是基频对应的整数次频率成分，由声带发声带动空气共振形成的，对应着声音三要素的音色。谐波的位置，相邻的距离共同形成了音色特征。谐波之间距离近听起来则偏厚粗，之间距离远听起来偏清澈。在男声变女声的时候，除了基频的移动，还需要调整谐波间的包络，距离等，否则将会丢失音色信息。</p><h3 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h3><p>为了有一个直观的图来解释上述的理论，可以把语音波形、短时能量、声强级、过零率、音高绘制在一张图上，如下图 3 所示：</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-4cb311bf47a663f26f2f78ff4b59cb78_r.jpg" style="zoom:67%;"></p><h2 id="语音信号的预处理操作"><a href="#语音信号的预处理操作" class="headerlink" title="语音信号的预处理操作"></a>语音信号的预处理操作</h2><p>以下内容主要来源于<a href="https://zhuanlan.zhihu.com/p/510550742" target="_blank" rel="noopener">语音基础知识（附相关实现代码）</a>。在不理解的地方我会加上自己的注释。</p><p>在进行语音特征（如 MFCC、频谱图、声谱图等）提取之前一般要进行语音信号的预处理操作，主要包括：预加重、分帧、加窗。</p><h3 id="预加重"><a href="#预加重" class="headerlink" title="预加重"></a>预加重</h3><p>语音经过说话人的口唇辐射发出，受到唇端辐射抑制，高频能量明显降低。一般来说，当语音信号的频率提高两倍时，其功率谱的幅度下降约 6dB，即语音信号的高频部分受到的抑制影响较大。比如像元音等一些因素的发音包含了较多的高频信号的成分，高频信号的丢失，可能会导致音素的共振峰并不明显，使得声学模型对这些音素的建模能力不强。预加重（pre-emphasis）是个一阶高通滤波器，可以提高信号高频部分的能量，给定时域输入信号$x[n]$，预加重之后信号为：</p><script type="math/tex; mode=display">x'[n]=x[n]-a\times x[n-1]</script><p>其中，a 是预加重系数，一般取 0.97 或 0.95。如下图 4 所示，元音音素 /aa/ 原始的频谱图（左）和经过预加重之后的频谱图（右）。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-8aaf99cbab0e57fb43597277da4f716f_r.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preemphasis</span><span class="params">(y, coef=<span class="number">0.97</span>, zi=None, return_zf=False)</span>:</span></span><br><span class="line">    <span class="string">"""Pre-emphasize an audio signal with a first-order auto-regressive filter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y[n] -&gt; y[n] - coef * y[n-1]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y : np.ndarray</span></span><br><span class="line"><span class="string">        Audio signal</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    coef : positive number</span></span><br><span class="line"><span class="string">        Pre-emphasis coefficient.  Typical values of ``coef`` are between 0 and 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        At the limit ``coef=0``, the signal is unchanged.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        At ``coef=1``, the result is the first-order difference of the signal.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The default (0.97) matches the pre-emphasis filter used in the HTK</span></span><br><span class="line"><span class="string">        implementation of MFCCs [#]_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. [#] http://htk.eng.cam.ac.uk/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    zi : number</span></span><br><span class="line"><span class="string">        Initial filter state.  When making successive calls to non-overlapping</span></span><br><span class="line"><span class="string">        frames, this can be set to the ``zf`` returned from the previous call.</span></span><br><span class="line"><span class="string">        (See example below.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        By default ``zi`` is initialized as ``2*y[0] - y[1]``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return_zf : boolean</span></span><br><span class="line"><span class="string">        If ``True``, return the final filter state.</span></span><br><span class="line"><span class="string">        If ``False``, only return the pre-emphasized signal.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    y_out : np.ndarray</span></span><br><span class="line"><span class="string">        pre-emphasized signal</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    zf : number</span></span><br><span class="line"><span class="string">        if ``return_zf=True``, the final filter state is also returned</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    b = np.asarray([<span class="number">1.0</span>, -coef], dtype=y.dtype)</span><br><span class="line">    a = np.asarray([<span class="number">1.0</span>], dtype=y.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> zi <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Initialize the filter to implement linear extrapolation</span></span><br><span class="line">        zi = <span class="number">2</span> * y[..., <span class="number">0</span>] - y[..., <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    zi = np.atleast_1d(zi)</span><br><span class="line"></span><br><span class="line">    y_out, z_f = scipy.signal.lfilter(b, a, y, zi=np.asarray(zi, dtype=y.dtype))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> return_zf:</span><br><span class="line">        <span class="keyword">return</span> y_out, z_f</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wave_data, self.sr = librosa.load(input_file, sr=sr)  <span class="comment"># 音频全部采样点的归一化数组形式数据</span></span><br><span class="line">wave_data = preemphasis(wave_data, coef=preemph)  <span class="comment"># 预加重，系数0.97</span></span><br></pre></td></tr></table></figure><h3 id="分帧"><a href="#分帧" class="headerlink" title="分帧"></a>分帧</h3><p>语音信号是非平稳信号，考虑到发浊音时声带有规律振动，即基音频率在短时范围内时相对固定的，因此可以认为语音信号具有短时平稳特性，一般认为 10ms~50ms 的语音信号片段是一个准稳态过程。_短时分析_采用分帧方式，一般每帧帧长为 20ms 或 50ms。假设语音采样率为 16kHz，帧长为 20ms，则一帧有 16000×0.02=320 个样本点。</p><p>相邻两帧之间的基音有可能发生变化，如两个音节之间，或者声母向韵母过渡。为确保声学特征参数的平滑性，一般采用重叠取帧的方式，即相邻帧之间存在重叠部分。一般来说，帧长和帧移的比例为 1:4 或 1:5。</p><blockquote><p>短时分析：虽然语音信号具有时变特性，但是在一个短时间范围内（一般认为在 10-30ms）其特性基本保持相对稳定，即语音具有短时平稳性。所以任何语音信号的分析和处理必须建立在 “短时” 的基础上，即进行“短时分析”。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">framesig</span><span class="params">(sig,frame_len,frame_step)</span>:</span></span><br><span class="line">    <span class="string">"""Frame a signal into overlapping frames.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sig: the audio signal to frame.</span></span><br><span class="line"><span class="string">    :param frame_len: length of each frame measured in samples.</span></span><br><span class="line"><span class="string">    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.</span></span><br><span class="line"><span class="string">    :returns: an array of frames. Size is NUMFRAMES by frame_len.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    slen = len(sig)</span><br><span class="line">    frame_len = int(round_half_up(frame_len))</span><br><span class="line">    frame_step = int(round_half_up(frame_step))</span><br><span class="line">    <span class="keyword">if</span> slen &lt;= frame_len:</span><br><span class="line">        numframes = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        numframes = <span class="number">1</span> + int(math.ceil((<span class="number">1.0</span>*slen - frame_len)/frame_step))</span><br><span class="line"></span><br><span class="line">    padlen = int((numframes<span class="number">-1</span>)*frame_step + frame_len)</span><br><span class="line"></span><br><span class="line">    zeros = numpy.zeros((padlen - slen,))</span><br><span class="line">    padsignal = numpy.concatenate((sig,zeros))</span><br><span class="line"></span><br><span class="line">    indices = numpy.tile(numpy.arange(<span class="number">0</span>,frame_len),(numframes,<span class="number">1</span>)) + numpy.tile(numpy.arange(<span class="number">0</span>,numframes*frame_step,frame_step),(frame_len,<span class="number">1</span>)).T</span><br><span class="line">    indices = numpy.array(indices,dtype=numpy.int32)</span><br><span class="line">    frames = padsignal[indices]</span><br><span class="line">    <span class="keyword">return</span> frames</span><br><span class="line"></span><br><span class="line">frames = framesig(sig=sig, frame_len=<span class="number">0.030</span> * sample_rate, <span class="comment"># 取帧长为30ms</span></span><br><span class="line">                                          frame_step=<span class="number">0.006</span> * sample_rate, <span class="comment"># 取帧移为6ms</span></span><br><span class="line">                                          )</span><br></pre></td></tr></table></figure><h3 id="加窗"><a href="#加窗" class="headerlink" title="加窗"></a>加窗</h3><p>分帧相当于对语音信号加矩形窗（用矩形窗其实就是不加窗），矩形窗在时域上对信号进行截断，在边界处存在多个旁瓣，会发生频谱泄露。为了减少频谱泄露，通常对分帧之后的信号进行其它形式的加窗操作。常用的窗函数有：汉明（Hamming）窗、汉宁（Hanning）窗和布莱克曼（Blackman）窗等。 <strong>加窗主要是为了使时域信号似乎更好地满足 FFT 处理的周期性要求，减少泄漏（加窗不能消除泄漏，只能减少， 如下图 5 所示）。</strong></p><blockquote><p>什么是频谱泄露？</p><p>音频处理中，经常需要利用傅里叶变换将时域信号转换到频域，而一次快速傅里叶变换（FFT）只能处理有限长的时域信号，但语音信号通常是长的，所以需要将原始语音截断成一帧一帧长度的数据块。这个过程叫信号截断，也叫分帧。分完帧后再对每帧做 FFT，得到对应的频域信号。FFT 是离散傅里叶变换（DFT）的快速计算方式，而<strong>做 DFT 有一个先验条件：分帧得到的数据块必须是整数周期的信号，也即是每次截断得到的信号要求是周期主值序列。</strong>但做分帧时，很难满足周期截断，因此就会导致频谱泄露。一句话，频谱泄露就是分析结果中，出现了本来没有的频率分量。比如说，50Hz 的纯正弦波，本来只有一种频率分量，分析结果却包含了与 50Hz 频率相近的其它频率分量。</p><p>非周期的无限长序列，任意截取一段有限长的序列，都不能代表实际信号，分析结果当然与实际信号不一致！也就是会造成频谱泄露。而周期的无限长序列，假设截取的是正好一个或整数个信号周期的序列，这个有限长序列就可以代表原无限长序列，如果分析的方法得当的话，分析结果应该与实际信号一致！因此也就不会造成频谱泄露。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/语音基础知识/v2-ab5db002d1d5126c1b5bf3e0a20b7af8_r.jpg" alt></p><p>汉明窗的窗函数为: $ W_{\mathrm{ham}}[n]=0.54-0.46 \cos \left(\frac{2 \pi n}{N}-1\right) $；汉宁窗的窗函数为: $ W_{h a n}[n]=0.5\left[1-\cos \left(\frac{2 \pi n}{N}-1\right)\right] $ ，其中$n$介于0到$ \mathrm{N}-1 $ 之间，$ \mathrm{N} $ 是窗的长度。</p><p>加窗就是用一定的窗函数$ w(n) $来乘$ s(n) $， 从而形成加窗语音信号$s_{w}(n)=\mathrm{s}(\mathrm{n}) * w(\mathrm{n}) $。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">framesig</span><span class="params">(sig,frame_len,frame_step,winfunc=lambda x:numpy.ones<span class="params">(<span class="params">(x,)</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Frame a signal into overlapping frames.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sig: the audio signal to frame.</span></span><br><span class="line"><span class="string">    :param frame_len: length of each frame measured in samples.</span></span><br><span class="line"><span class="string">    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.</span></span><br><span class="line"><span class="string">    :param winfunc: the analysis window to apply to each frame. By default no window is applied.</span></span><br><span class="line"><span class="string">    :returns: an array of frames. Size is NUMFRAMES by frame_len.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    slen = len(sig)</span><br><span class="line">    frame_len = int(round_half_up(frame_len))</span><br><span class="line">    frame_step = int(round_half_up(frame_step))</span><br><span class="line">    <span class="keyword">if</span> slen &lt;= frame_len:</span><br><span class="line">        numframes = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        numframes = <span class="number">1</span> + int(math.ceil((<span class="number">1.0</span>*slen - frame_len)/frame_step))</span><br><span class="line"></span><br><span class="line">    padlen = int((numframes<span class="number">-1</span>)*frame_step + frame_len)</span><br><span class="line"></span><br><span class="line">    zeros = numpy.zeros((padlen - slen,))</span><br><span class="line">    padsignal = numpy.concatenate((sig,zeros))</span><br><span class="line"></span><br><span class="line">    indices = numpy.tile(numpy.arange(<span class="number">0</span>,frame_len),(numframes,<span class="number">1</span>)) + numpy.tile(numpy.arange(<span class="number">0</span>,numframes*frame_step,frame_step),(frame_len,<span class="number">1</span>)).T</span><br><span class="line">    indices = numpy.array(indices,dtype=numpy.int32)</span><br><span class="line">    frames = padsignal[indices]</span><br><span class="line">    <span class="comment"># 加窗操作</span></span><br><span class="line">    win = numpy.tile(winfunc(frame_len),(numframes,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> frames*win</span><br><span class="line"></span><br><span class="line">frames = framesig(sig=sig, frame_len=<span class="number">0.030</span> * sample_rate, <span class="comment"># 取帧长为30ms</span></span><br><span class="line">                  frame_step=<span class="number">0.006</span> * sample_rate, <span class="comment"># 取帧移为6ms</span></span><br><span class="line">                  winfunc=np.hamming</span><br><span class="line">                  )</span><br></pre></td></tr></table></figure><h2 id="语音声学特征介绍"><a href="#语音声学特征介绍" class="headerlink" title="语音声学特征介绍"></a>语音声学特征介绍</h2><p>以下内容主要来源于<a href="https://www.cnblogs.com/liaohuiqiang/p/10159429.html" target="_blank" rel="noopener">论文笔记：语音情感识别（四）语音特征之声谱图，log梅尔谱，MFCC，deltas</a></p><p>声音信号本是一维的时域信号，直观上很难看出频率变化规律。傅里叶变换可把它变到频域上，虽然可看出信号的频率分布，但是丢失了时域信息，无法看出频率分布随时间的变化。为了解决这个问题，很多时频分析手段应运而生，如短时傅里叶，小波，Wigner分布等都是常用的<strong>时频域分析方法</strong>。</p><h3 id="原始信号"><a href="#原始信号" class="headerlink" title="原始信号"></a>原始信号</h3><p>从音频文件中读取出来的原始语音信号通常称为 raw waveform，是一个一维数组，长度是由音频长度和采样率决定，比如采样率 Fs 为 16KHz，表示一秒钟内采样 16000 个点，这个时候如果音频长度是 10 秒，那么 raw waveform 中就有 160000 个值，<strong>值的大小通常表示的是振幅。</strong></p><h3 id="（线性）声谱图"><a href="#（线性）声谱图" class="headerlink" title="（线性）声谱图"></a>（线性）声谱图</h3><p>（1）对原始信号进行分帧加窗后，可以得到很多帧，对每一帧做 FFT（快速傅里叶变换），傅里叶变换的作用是把时域信号转为频域信号，把每一帧 FFT 后的频域信号（频谱图）在时间上堆叠起来就可以得到声谱图，其直观理解可以形象地表示为以下几个图，图源见<a href="http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf" target="_blank" rel="noopener">CMU 语音课程 slides</a>。</p><p>（2）有些论文提到的 DCT（离散傅里叶变换）和 STFT（短时傅里叶变换）其实是差不多的东西。STFT 就是对一系列加窗数据做 FFT。而 DCT 跟 FFT 的关系就是：FFT 是实现 DCT 的一种快速算法。 </p><p>（3）FFT 有个参数 N，表示对多少个点做 FFT，如果一帧里面的点的个数小于 N 就会 zero-padding 到 N 的长度。对一帧信号做 FFT 后会得到 N 点的复数，这个点的模值就是该频率值下的幅度特性。每个点对应一个频率点，某一点 n（n 从 1 开始）表示的频率为$F_n = (n-1)*Fs/N$，第一个点（n=1，Fn 等于 0）表示直流信号，最后一个点 N 的下一个点（n=N+1，Fn=Fs 时，实际上这个点是不存在的）表示采样频率 Fs。</p><p>（4）FFT 后我们可以得到 N 个频点，频率间隔（也叫频率分辨率或）为 Fs / N，比如，采样频率为 16000，N 为 1600，那么 FFT 后就会得到 1600 个点，频率间隔为 10Hz，FFT 得到的 1600 个值的模可以表示 1600 个频点对应的振幅。因为 FFT 具有对称性，当 N 为偶数时取 N/2+1 个点，当 N 为奇数时，取 (N+1)/2 个点，比如 N 为 512 时最后会得到 257 个值。</p><p>（5）用 python_speech_feature 库时可以看到有三种声谱图，包括振幅谱，功率谱（有些资料称为能量谱，是一个意思，功率就是单位时间的能量），log 功率谱。振幅谱就是 fft 后取绝对值。功率谱就是在振幅谱的基础上平方然后除以 N。log 功率谱就是在功率谱的基础上取 10 倍 lg，然后减去最大值。得到声谱图矩阵后可以通过 matplotlib 来画图。</p><p>（6）常用的声谱图都是 STFT 得到的，另外也有用 CQT（constant-Q transform）得到的，为了区分，将它们分别称为 STFT 声谱图和 CQT 声谱图。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224530922-479480973.png" style="zoom: 67%;"> </p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224541923-1780314401.png" style="zoom:67%;"> </p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224554397-162344585.png" style="zoom:67%;"></p><h3 id="梅尔声谱图"><a href="#梅尔声谱图" class="headerlink" title="梅尔声谱图"></a>梅尔声谱图</h3><p>梅尔频谱的英文为Mel-spectrogram。</p><p>（1）人耳听到的声音高低和实际（Hz）频率不呈线性关系，用 Mel 频率更符合人耳的听觉特性（这正是用 Mel 声谱图的一个动机，由人耳听力系统启发），即在 1000Hz 以下呈线性分布，1000Hz 以上呈对数增长，Mel 频率与 Hz 频率的关系为$f_{mel} = 2595 \cdot lg(1+\frac{f}{700Hz})$，如下图所示，图源见<a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" target="_blank" rel="noopener">一个 MFCC 的介绍教程</a>。有另一种计算方式为$f_{mel} = 1125 \cdot ln(1+\frac{f}{700Hz})$。下面给出一个计算 Mel 声谱图的例子。另，python 中可以用 librosa 调包得到梅尔声谱图。</p><blockquote><p>通过实际的主观实验，科学家发现人耳对低频信号的区别更加敏感，而对高频信号的区别则不那么敏感。也就是说低频段上的两个频度和高频段上的两个频度，人们会更容易区分前者。因此我们就明白了，频域上相等距离的两对频度，对于人耳来说他们的距离不一定相等。那么，能不能调整频域的刻度，使得这个新的刻度上相等距离的两对频度，对于人耳来说也相等呢？答案是可以的，这就是梅尔刻度。</p><p>下图展示了梅尔频度-正常频度的对应关系，正如之前所说明的，低频段的部分，梅尔刻度和正常频度几乎呈线性关系，而在高频段，因为人耳的感知变弱，因此两者呈对数关系。</p></blockquote><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224622095-1486888859.png" style="zoom:67%;"></p><p>（2）假设现在用 10 个 Mel filterbank（一些论文会用 40 个，如果求 MFCC 一般是用 26 个然后在最后取前 13 个），为了获得 filterbanks 需要选择一个 lower 频率和 upper 频率，用 300 作为 lower，8000 作为 upper 是不错的选择。如果采样率是 8000Hz 那么 upper 频率应该限制为 4000。然后用公式把 lower 和 upper 转为 Mel 频率，我们使用上述第二个公式（ln 那条），可以得到 401.25Mel 和 2834.99Mel。</p><p>（3）因为用 10 个滤波器，所以需要 12 个点来划分出 10 个区间，在 401.25Mel 和 2834.99Mel 之间划分出 12 个点，m(i) = (401.25, 622.50, 843.75, 1065.00, 1286.25, 1507.50, 1728.74, 1949.99, 2171.24, 2392.49, 2613.74, 2834.99)。</p><p>（4）然后把这些点转回 Hz 频率，h(i) = (300, 517.33, 781.90, 1103.97, 1496.04, 1973.32, 2554.33, 3261.62, 4122.63, 5170.76, 6446.70, 8000)。</p><p>（5）把这些频率转为 fft bin，f(i) = floor( (N+1)*h(i)/Fs)，N 为 FFT 长度，默认为 512，Fs 为采样频率，默认为 16000Hz，则 f(i) = (9, 16, 25, 35, 47, 63, 81, 104, 132, 165, 206, 256)。这里 256 刚好对应 512 点 FFT 的 8000Hz。 </p><p>（6）然后创建滤波器，第一个滤波器从第一个点开始，在第二个点到达最高峰，第三个点跌回零。第二个滤波器从第二个点开始，在第三个点到达最大值，在第四个点跌回零。以此类推。滤波器的示意图如下图所示，图源见<a href="https://blog.csdn.net/xiaoding133/article/details/8106672" target="_blank" rel="noopener">csdn-MFCC 计算过程</a>。可以看到随着频率的增加，滤波器的宽度也增加。 </p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224736710-1682561520.png" alt></p><p>（7）接下来给出滤波器输出的计算公式，如下所示，其中 m 从 1 到 M，M 表示滤波器数量，这里是 10。k 表示点的编号，一个 fft 内 256 个点，k 从 1 到 256，表示了 fft 中的 256 个频点（k=0 表示直流信号，算进来就是 257 个频点，为了简单起见这里省略 k=0 的情况）。</p><script type="math/tex; mode=display">H_m(k) = \left\{\begin{matrix} \frac{k-f(m-1)}{f(m)-f(m-1)} & f(m-1) \leq k \leq f(m)\\ \frac{f(m+1)-k}{f(m+1)-f(m)} & f(m) \leq k \leq f(m+1) \\ 0 & others \\ \end{matrix}\right.</script><p>（8）最后还要乘上 fft 计算出来的能量谱，关于能量谱在前一节（线性）声谱图中已经讲过了。将滤波器的输出应用到能量谱后得到的就是梅尔谱，具体应用公式如下，其中 $|X(k)|^2$表示能量谱中第 k 个点的能量。以每个滤波器的频率范围内的输出作为权重，乘以能量谱中对应频率的对应能量，然后把这个滤波器范围内的能量加起来。举个例子，比如第一个滤波器负责的是 9 和 16 之间的那些点（在其它范围的点滤波器的输出为 0），那么只对这些点对应的频率对应的能量做加权和。</p><script type="math/tex; mode=display">MelSpec(m) = \sum_{k=f(m-1)}^{f(m+1)} H_m(k) * |X(k)|^2</script><p>（9）这样计算后，对于一帧会得到 M 个输出。经常会在论文中看到说 40 个梅尔滤波器输出，指的就是这个（实际上前面说的梅尔滤波器输出是权重 H，但是这里的意思应该是将滤波器输出应用到声谱后得到的结果，根据上下文可以加以区分）。然后在时间上堆叠多个 “40 个梅尔滤波器输出” 就得到了梅尔尺度的声谱（梅尔谱），如果再取个 log，就是 log 梅尔谱，log-Mels。 </p><p>（10）把滤波器范围内的能量加起来，可以解决一个问题，这个问题就是人耳是很难理解两个靠的很近的线性频率（就是和梅尔频率相对应的赫兹频率）之间不同。如果把一个频率区域的能量加起来，只关心在每个频率区域有多少能量，这样人耳就比较能区分，我们希望这种方式得到的（Mel）声谱图可以更加具有辨识度。最后取 log 的 motivation 也是源于人耳的听力系统，人对声音强度的感知也不是线性的，一般来说，要使声音的音量翻倍，我们需要投入 8 倍的能量，为了把能量进行压缩，所以取了 log，这样，当 x 的 log 要翻倍的话，就需要增加很多的 x。另外一个取 log 的原因是为了做倒谱分析得到 MFCC，具体细节见下面 MFCC 的介绍。</p><h3 id="MFCC"><a href="#MFCC" class="headerlink" title="MFCC"></a>MFCC</h3><p>（1）MFCC，梅尔频率的倒谱系数（Mel Frequency Cepstral Coefficents），是广泛应用于语音领域的特征，在这之前常用的是线性预测系数 Linear Prediction Coefficients（LPCs）和线性预测倒谱系数（LPCCs），特别是用在 HMM 上。 </p><p>（2）先说一下获得 MFCC 的步骤，首先分帧加窗，然后对每一帧做 FFT 后得到（单帧）能量谱（具体步骤见上面线性声谱图的介绍），对线性声谱图应用梅尔滤波器后然后取 log 得到 log 梅尔声谱图（具体步骤见上面梅尔声谱图的介绍），然后对 log 滤波能量（log 梅尔声谱）做 DCT，离散余弦变换（傅里叶变换的一种），然后保留第二个到第 13 个系数，得到的这 12 个系数就是 MFCC。 </p><p>（3）然后再大致说说 MFCC 的含义，下图第一个图（图源见参考资料 [1]）是语音的频谱图，峰值是语音的主要频率成分，这些峰值称为共振峰，共振峰携带了声音的辨识（相当于人的身份证）。把这些峰值平滑地连起来得到的曲线称为频谱包络，包络描述了携带声音辨识信息的共振峰，所以我们希望能够得到这个包络来作为语音特征。频谱由频谱包络和频谱细节组成，如下第二个图（图源见参考资料[1]）所示，其中 log X[k] 代表频谱（注意图中给出的例子是赫兹谱，这里只是举例子，实际我们做的时候通常都是用梅尔谱），log H[k]代表频谱包络，log E[k]代表频谱细节。我们要做的就是从频谱中分离得到包络，这个过程也称为倒谱分析，下面就说说倒谱分析是怎么做的。 </p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224852822-1848315135.png" style="zoom:67%;"></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224902529-1572460338.png" style="zoom:67%;"></p><p>（4）要做的其实就是对频谱做 FFT，在频谱上做 FFT 这个操作称为逆 FFT，需要注意的是我们是在频谱的 log 上做的，因为这样做 FFT 后的结果 x[k]可以分解成 h[k]和 e[k]的和。我们先看下图（图源见参考资料 [1]），对包络 log H[k] 做 IFFT 的结果，可以看成 “每秒 4 个周期的正弦波”，于是我们在伪频率轴上的 4Hz 上给一个峰值，记作 h[k]。对细节 log E[k] 做 IFFT 的结果，可以看成 “每秒 100 个周期的正弦波”，于是我们在伪频率轴上的 100Hz 上给一个峰值，记作 e[k]。对频谱 log X[k] 做 IFFT 后的结果记作 x[k]，这就是我们说的倒谱，它会等于 h[k]和 e[k]的叠加，如下第二个图所示。我们想要得到的就是包络对应的 h[k]，而 h[k]是 x[k]的低频部分，只需要对 x[k]取低频部分就可以得到了。 </p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224927630-2116898656.png" style="zoom:67%;"></p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221224944430-837617222.png" style="zoom:67%;"></p><p>（5）最后再总结一下得到 MFCC 的步骤，求线性声谱图，做梅尔滤波得到梅尔声谱图，求个 log 得到 log 梅尔谱，做倒谱分析也就是对 log X[k] 做 DCT 得到 x[k]，取低频部分就可以得到倒谱向量，通常会保留第 2 个到第 13 个系数，得到 12 个系数，这 12 个系数就是常用的 MFCC。图源见参考资料 [1]。 </p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/1160281-20181221225011597-1988899505.png" style="zoom:67%;"></p><h3 id="deltas，deltas-deltas"><a href="#deltas，deltas-deltas" class="headerlink" title="deltas，deltas-deltas"></a>deltas，deltas-deltas</h3><p>（1）deltas 和 deltas-deltas，看到很多人翻译成一阶差分和二阶差分，也被称为微分系数和加速度系数。使用它们的原因是，MFCC 只是描述了一帧语音上的能量谱包络，但是语音信号似乎有一些动态上的信息，也就是 MFCC 随着时间的改变而改变的轨迹。有证明说计算 MFCC 轨迹并把它们加到原始特征中可以提高语音识别的表现。</p><p>（2）以下是 deltas 的一个计算公式，其中 t 表示第几帧，N 通常取 2，c 指的就是 MFCC 中的某个系数。deltas-deltas 就是在 deltas 上再计算以此 deltas。</p><script type="math/tex; mode=display">d_t = \frac{\sum_{n=1}^{N} n(c_{t+n}-c_{t-n})}{2 \sum_{n=1}^{N} n^2}</script><p>（3）对 MFCC 中每个系数都做这样的计算，最后会得到 12 个一阶差分和 12 个二阶差分，我们通常在论文中看到的 “MFCC 以及它们的一阶差分和二阶差分” 指的就是这个。</p><p>（4）值得一提的是 deltas 和 deltas-deltas 也可以用在别的参数上来表述动态特性，有论文中是直接在 log Mels 上做一阶差分和二阶差分的，<a href="https://www.cnblogs.com/liaohuiqiang/p/10128835.html" target="_blank" rel="noopener">论文笔记：语音情感识别（二）声谱图 + CRNN</a> 中 3-D Convolutional Recurrent Neural Networks with Attention Model for Speech Emotion Recognition 这篇论文就是这么做的。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><strong>1.频谱</strong>：时域信号（一维）短时傅里叶变换后的频域信号（一维）。</p><p><strong>2.声谱图/语谱图</strong>：把一整段语音信号截成许多帧，把它们各自的频谱“竖”起来（即用纵轴表示频率），用颜色的深浅来代替频谱强度，再把所有帧的频谱横向并排起来（即用横轴表示时间），就得到了<strong>语谱图</strong>，它可以称为声音的<strong>时频域</strong>表示。</p><p><strong>3.倒谱</strong>：也叫做倒频谱，二次谱，对数功率谱等。对声谱图取对数后，再DFT变回时域，此时不是完全意义上的时域，应叫做倒谱域。</p><p><strong>4.MFCC</strong>：对线性声谱图应用mel滤波器后，取log，得到log梅尔声谱图，然后对log滤波能量（log梅尔声谱）做DCT离散余弦变换（傅里叶变换的一种），然后保留第2到第13个系数，得到的这12个系数就是MFCC。</p><p><img src="/DeepLearningApplications/语音合成/语音基础知识/watermark.png" alt="在这里插入图片描述"></p><p><strong>附加：</strong></p><p><strong>1.能量谱</strong>：也叫做能量密度谱。是原信号傅里叶变化的平方。用于描述时间序列的能量随频率的分布。</p><p><strong>2.功率谱</strong>：将频谱或时频谱（语谱）中的幅值进行平方，得到功率谱。</p><p><strong>3.功率谱密度</strong>：定义为单位频带内的吸纳后功率。其推导公式较为复杂，但维纳-辛欣定理证明了：一段信号的功率谱等于这段信号自相关函数的傅里叶变换。</p><p>注：信号分为确定和随机，确定信号又分为能量和功率，随机信号一定是功率信号。语音信号是随机信号。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[1] <a href="http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf" target="_blank" rel="noopener">CMU 语音课程 slides</a></p><p>[2] <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" target="_blank" rel="noopener">一个 MFCC 的介绍教程</a></p><p>[3] <a href="https://blog.csdn.net/xiaoding133/article/details/8106672" target="_blank" rel="noopener">csdn-MFCC 计算过程</a></p><p>[4] <a href="https://www.cnblogs.com/BaroC/p/4283380.html" target="_blank" rel="noopener">博客园 - MFCC 学习笔记</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/cnlinxi/book-text-to-speech" target="_blank" rel="noopener">cnlinxi/book-text-to-speech: A book about Text-to-Speech (TTS) in Chinese. (github.com)</a><br><a href="https://blog.csdn.net/qq_36002089/article/details/108378796" target="_blank" rel="noopener">声谱图，梅尔语谱，倒谱，梅尔倒谱系数</a><br><a href="https://www.cnblogs.com/liaohuiqiang/p/10159429.html" target="_blank" rel="noopener">论文笔记：语音情感识别（四）语音特征之声谱图，log梅尔谱，MFCC，deltas</a><br><a href="https://zhuanlan.zhihu.com/p/510550742" target="_blank" rel="noopener">语音基础知识（附相关实现代码）</a><br><a href="https://www.zhihu.com/question/27126800/answer/35376174" target="_blank" rel="noopener">不同元音辅音在声音频谱的表现是什么样子？ - 王赟 Maigo的回答 - 知乎</a><br><a href="https://audiosns.com/1571.html" target="_blank" rel="noopener">搬运工：波形、频谱和声谱的关系</a><br><a href="https://zhuanlan.zhihu.com/p/421460202" target="_blank" rel="noopener">语音合成基础(3)——关于梅尔频谱你想知道的都在这里</a><br><a href="https://zhuanlan.zhihu.com/p/99122527" target="_blank" rel="noopener">语音合成基础(1)——语音和TTS</a><br><a href="https://www.jianshu.com/p/2b83e68a055b" target="_blank" rel="noopener">《语音信号处理》整理</a><br><a href="https://www.cnblogs.com/guanghe/p/10020120.html" target="_blank" rel="noopener">MP3的采样率和比特率</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开新坑了，最近在攻击语音合成系统，现在学习一下语音基础知识。&lt;/p&gt;
&lt;h2 id=&quot;辅音和元音的区别&quot;&gt;&lt;a href=&quot;#辅音和元音的区别&quot; class=&quot;headerlink&quot; title=&quot;辅音和元音的区别&quot;&gt;&lt;/a&gt;辅音和元音的区别&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;辅音发音时，气流在通过咽头、口腔的过程中， 要受到某部位的阻碍；元音发音时，气流在咽头、 口腔不受阻碍。这是元音和辅音最主要的区别。&lt;/li&gt;
&lt;li&gt;辅音发音时，发音器官成阻的部位特别紧张； 元音发音时发音器官各部位保持均衡的紧张状态。&lt;/li&gt;
&lt;li&gt;辅音发音时，气流较强；元音发音时，气流较 弱。&lt;/li&gt;
&lt;li&gt;辅音发音时，声带不一定振动，声音一般不响亮；元音发音时，声带振动，声音比辅音响亮。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;一般只有元音（一些介于元音辅音中间分类不明的音暂不讨论）才会有共振峰，而元音的音质由声道的形状决定，而声道的形状又通过发音的动作来塑造（articulatory+movements）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;清音和浊音&quot;&gt;&lt;a href=&quot;#清音和浊音&quot; class=&quot;headerlink&quot; title=&quot;清音和浊音&quot;&gt;&lt;/a&gt;清音和浊音&lt;/h2&gt;
    
    </summary>
    
    
      <category term="DeepLearningApplications" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/"/>
    
      <category term="语音合成" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
    
      <category term="语音合成" scheme="https://www.zdaiot.com/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
      <category term="语音基础知识" scheme="https://www.zdaiot.com/tags/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>bert模型详解</title>
    <link href="https://www.zdaiot.com/DeepLearningApplications/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/bert%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/"/>
    <id>https://www.zdaiot.com/DeepLearningApplications/自然语言处理/bert模型详解/</id>
    <published>2022-05-02T10:07:03.000Z</published>
    <updated>2022-05-02T10:07:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近终于搞懂了transformer和原理和实现，可以看bert了。事先声明，本文的大部分内容来自<a href="http://fancyerii.github.io/2019/03/09/bert-theory/" target="_blank" rel="noopener">BERT模型详解</a>。大佬写的太好了，本来不想复制粘贴的，但是有部分内容看不明白，必须加点笔记。</p><h2 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h2><p>2018年深度学习在NLP领域取得了比较大的突破，最大的新闻当属Google的BERT模型横扫各大比赛的排行榜。作者认为，深度学习在NLP领域比较重点的三大突破为：Word Embedding、RNN/LSTM/GRU+Seq2Seq+Attention+Self-Attention机制和Contextual Word Embedding(Universal Sentence Embedding)。</p><p>Word Embedding解决了传统机器学习方法的特征稀疏问题，它通过把一个词映射到一个低维稠密的语义空间，从而使得相似的词可以共享上下文信息，从而提升泛化能力。而且通过无监督的训练可以获得高质量的词向量(比如Word2vec和Glove等方法)，从而把这些语义知识迁移到数据较少的具体任务上。但是Word Embedding学到的是一个词的所有语义，比如bank可以是”银行”也可以是”水边。如果一定要用一个固定的向量来编码其语义，那么我们只能把这两个词的语义都编码进去，但是实际一个句子中只有一个语义是合理的，这显然是有问题的。</p><p>这时我们可以通过RNN/LSTM/GRU来编码上下文的语义，这样它能学到如果周围是money，那么bank更可能是”银行”的语义。最原始的RNN由于梯度消失和梯度爆炸等问题很难训练，后来引入了LSTM和GRU等模型来解决这个问题。最早的RNN只能用于分类、回归和序列标注等任务，通过引入两个RNN构成的Seq2Seq模型可以解决序列的变换问题。比如机器翻译、摘要、问答和对话系统都可以使用这个模型。尤其机器翻译这个任务的训练数据比较大，使用深度学习的方法的效果已经超过传统的机器学习方法，而且模型结构更加简单。到了2017年，Google提出了Transformer模型，引入了Self-Attention。Self-Attention的初衷是为了用Attention替代LSTM，从而可以更好的并行(因为LSTM的时序依赖特效很难并行)，从而可以处理更大规模的语料。Transformer出来之后被广泛的用于以前被RNN/LSTM/GRU霸占的地盘，Google更是在Transformer的论文里使用”Attention is all you need”这样霸气的标题。现在Transformer已经成为Encoder/Decoder的霸主。</p><p>虽然RNN可以学到上下文的信息，但是这些上下文的语义是需要通过特定任务的标注数据使用来有监督的学习。很多任务的训练数据非常少并且获取成本很高，因此在实际任务中RNN很难学到复杂的语义关系。当然通过Multi-Task Learning，我们可以利用其它相关任务的数据。比如我们要做文本分类，我们可以利用机器翻译的训练数据，通过同时优化两个(多个)目标，让模型同时学到两个任务上的语义信息，因为这两个任务肯定是共享很多基础语义信息的，所以它的效果要比单个任务好。但即使这样，标注的数据量还是非常有限的。</p><p>因此2018年的研究热点就变成了怎么利用无监督的数据学习Contextual Word Embedding(也叫做Universal Sentence Embedding)，也就是通过无监督的方法，让模型能够学到一个词在不同上下文的不同语义表示方法。当然这个想法很早就有了，比如2015年的Skip Thought Vector，但是它只使用了BookCorpus，这只有一万多本书，七千多万个句子，因此效果并没有太明显的提升。</p><p>在BERT之前比较大的进展是ELMo、ULMFiT和OpenAI GPT。尤其是OpenAI GPT，它在BERT出现之前已经横扫过各大排行榜一次了，当然Google的BERT又横扫了一次。</p><p>UMLFiT比较复杂，而且效果也不是特别好，我们暂且不提。ELMo和OpenAI GPT的思想其实非常非常简单，就是用海量的无标注数据学习语言模型，在学习语言模型的过程中自然而然的就学到了上下文的语义关系。它们都是来学习一个语言模型，前者使用的是LSTM而后者使用Transformer，在进行下游任务处理的时候也有所不同，ELMo是把它当成特征。拿分类任务来说，输入一个句子，ELMo用LSTM把它扫一次，这样就可以得到每个词的表示，这个表示是考虑上下文的，因此”He deposited his money in this bank”和”His soldiers were arrayed along the river bank”中的两个bank的向量是不同的。下游任务用这些向量来做分类，它会增加一些网络层，但是ELMo语言模型的参数是固定的。而OpenAI GPT不同，它直接用特定任务来Fine-Tuning Transformer的参数。因为用特定任务的数据来调整Transformer的参数，这样它更可能学习到与这个任务特定的上下文语义关系，因此效果也更好。</p><p>而BERT和OpenAI GPT的方法类似，也是Fine-Tuning的思路，但是它解决了OpenAI GPT(包括ELMo)单向信息流的问题，同时它的模型和语料库也更大。依赖Google强大的计算能力和工程能力，BERT横扫了OpenAI GPT。成王败寇，很少还有人记得OpenAI GPT的贡献了。但是BERT的很多思路都是沿用OpenAI GPT的，要说BERT的学术贡献，最多是利用了Mask LM(这个模型在上世纪就存在了)和Predicting Next Sentence这个Multi-task Learning而已。</p><h2 id="Skip-Thought-Vector"><a href="#Skip-Thought-Vector" class="headerlink" title="Skip Thought Vector"></a>Skip Thought Vector</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>我们之前学习过word2vec，其中一种模型是Skip-Gram模型，根据中心词预测周围的(context)词，这样我们可以学到词向量。那怎么学习到句子向量呢？一种很自然想法就是用一个句子预测它周围的句子，这就是Skip Thought Vector的思路。它需要有连续语义相关性的句子，比如论文中使用的书籍。一本书由很多句子组成，前后的句子是有关联的。那么我们怎么用一个句子预测另一个句子呢？这可以使用Encoder-Decoder，类似于机器翻译。</p><p>比如一本书里有3个句子”I got back home”、”I could see the cat on the steps”和”This was strange”。我们想用中间的句子”I could see the cat on the steps.”来预测前后两个句子。如下图所示，输入是句子”I could see the cat on the steps.”，输出是两个句子”I got back home.”和”This was strange.”。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/skip-thought-1-16514865427645.png" alt="img"></p><p><em>图：Skip Thought Vector</em></p><p>我们首先用一个Encoder(比如LSTM或者GRU)把输入句子编码成一个向量。而右边是两个Decoder(我们任务前后是不对称的，因此用两个Decoder)。因为我们不需要预测(像机器翻译那样生成一个句子)，所以我们只考虑Decoder的训练。Decoder的输入是”&lt;eos&gt; I got back home”，而Decoder的输出是”I got back home &lt;eos&gt;”。</p><p>经过训练之后，我们就得到了一个Encoder(Decoder不需要了)。给定一个新的句子，我们可以把它编码成一个向量。这个向量可以用于下游(down stream)的任务，比如情感分类，语义相似度计算等等。</p><h3 id="训练数据集"><a href="#训练数据集" class="headerlink" title="训练数据集"></a>训练数据集</h3><p>和训练Word2Vec不同，Word2Vec只需要提供句子，而Skip Thought Vector需要文章(至少是段落)。论文使用的数据集是BookCorpus(<a href="http://yknzhu.wixsite.com/mbweb)，目前网站已经不提供下载了。BookCorpus的统计信息如下图所示，有一万多本书，七千多万个句子。" target="_blank" rel="noopener">http://yknzhu.wixsite.com/mbweb)，目前网站已经不提供下载了。BookCorpus的统计信息如下图所示，有一万多本书，七千多万个句子。</a></p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/skip-thought-2-16514865564837.png" alt="img"></p><p><em>图：BookCorpus统计信息</em></p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>接下来我们介绍一些论文中使用的模型，注意这是2015年的论文，过去好几年了，其实我们是可以使用更新的模型。但是基本的思想还是一样的。</p><p>Encoder是一个GRU。假设句子$s_i=w_i^1…w_i^N$，t时刻的隐状态是$h_i^t$认为编码了字符串$w_i^1…w_i^t$的语义，因此$h_i^N$可以看成对整个句子语义的编码。t时刻GRU的计算公式为：</p><script type="math/tex; mode=display">\begin{split}r^t & =\sigma(W_rx^t+U_rh^{t-1}) \\z^t & =\sigma(W_zx^t+U_zh^{t-1}) \\\bar{h}^t & =tanh(Wx^t+U(r^t \odot h^{t-1})) \\h^t & =(1-z^t) \odot h^{t-1} + z^t \odot \bar{h}^t\end{split}</script><p>这就是标准的GRU，其中$x^t$是$w_i^t$的Embedding向量，$r^t$是重置(reset)门，$z^t$是更新(update)门，$\odot$是element-wise的乘法。Decoder是一个神经网络语言模型。</p><script type="math/tex; mode=display">\begin{split}r^t & = \sigma(W_r^dx^{t-1} + U_r^dh^{t-1} + C_rh_i) \\z^t & = \sigma(W_z^dx^{t-1} + U_z^dh^{t-1} + C_zh_i) \\\bar{h}^t & =tanh(W^dx^{t-1} + U^d(r^t \odot h^{t-1}) +Ch_i) \\h^t & =(1-z^t) \odot h^{t-1} + z^t \odot \bar{h}^t\end{split}</script><p>和之前我们在机器翻译里介绍的稍微有一些区别。标准Encoder-Decoder里Decoder每个时刻的输入是$x^{t-1}$和$h^{t-1}$，Decoder的初始状态设置为Encoder的输出$h_i$。而这里Decodert时刻的输入除了$x^{t-1}$和$h^{t-1}$，还有Encoder的输出$h_i$。</p><p>计算出Decoder每个时刻的隐状态$h^t$之后，我们在用一个矩阵V把它投影到词的空间，输出的是预测每个词的概率分布。注意：预测前一个句子和后一个句子是两个GRU模型，它们的参数是不共享的，但是投影矩阵V是共享的。当然输入$w^t$到Embedding $x^t$的Embedding矩阵也是共享的。和Word2Vec对比的话，V是输出向量(矩阵)而这个Embedding(这里没有起名字)是输入向量(矩阵)。</p><h3 id="词汇扩展"><a href="#词汇扩展" class="headerlink" title="词汇扩展"></a>词汇扩展</h3><p>这篇论文还有一个比较重要的方法就是词汇扩展。因为BookCorpus相对于训练Word2Vec等的语料来说还是太小，很多的词都根本没有在这个语料中出现，因此直接使用的话效果肯定不好。</p><p>本文使用了词汇扩展的办法。具体来说我们可以先用海量的语料训练一个Word2Vec，这样可以把一个词映射到一个语义空间，我们把这个向量叫作$\mathcal{V}_{w2v}$。而我们之前训练的得到的输入向量也是把一个词映射到另外一个语义空间，我们记作<script type="math/tex">\mathcal{V}_{rnn}</script>。</p><p>我们假设它们之间存在一个线性变换<script type="math/tex">f: \mathcal{V}_{w2v} \rightarrow \mathcal{V}_{rnn}</script>。这个线性变换的参数是矩阵W，使得<script type="math/tex">v_{rnn}=Wv_{w2v}</script>。那怎么求这个变换矩阵W呢？因为两个训练语料会有公共的词(通常训练word2vec的语料比skip vector大得多，从而词也多得多)。因此我们可以用这些公共的词来寻找W。寻找的依据是：遍历所有可能的W，使得$Wv_{w2v}$和$v_{rnn}$尽量接近。用数学语言描述就是：</p><script type="math/tex; mode=display">W^* = \underset{W}{argmin} \sum_{w \in both set} |Wv_{w2v}(w)-v_{rnn}(w)|^2</script><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>首先训练了单向的GRU，向量的维度是2400，我们把它叫作uni-skip向量。此外还训练了bi-skip向量，它是这样得到的：首先训练1200维的uni-skip，然后句子倒过来，比如原来是”aa bb”、”cc dd”和”ee ff”，我们是用”cc dd”来预测”aa bb”以及”ee ff”，现在反过来变成”ff ee”、”dd cc”和”bb aa”。这样也可以训练一个模型，当然也就得到一个encoder(两个decoder不需要了)，给定一个句子我们把它倒过来然后也编码成1200为的向量，最后把这个两个1200维的向量拼接成2400维的向量。</p><p>模型训练完成之后还需要进行词汇扩展。通过BookCorpus学习到了20,000个词，而word2vec共选择了930,911词，通过它们共同的词学习出变换矩阵W，从而使得我们的Skip Thought Vector可以处理930,911个词。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>为了验证效果，本文把Sentence Embedding作为下游任务的输入特征，任务包括分类(情感分类)，SNI(RTE)等。前者的输入是一个句子，而后者的输入是两个句子。</p><h4 id="Semantic-relatedness任务"><a href="#Semantic-relatedness任务" class="headerlink" title="Semantic relatedness任务"></a>Semantic relatedness任务</h4><p>这里使用了SICK(SemEval 2014 Task 1，给定两个句子，输出它们的语义相关性1-5五个分类)和Microsoft Paraphrase Corpus(给定两个句子，判断它们是否一个意思/两分类)。</p><p>它们的输入是两个句子，输出是分类数。对于输入的两个句子，我们用Skip Thought Vector把它们编码成两个向量u和v，然后计算$u \cdot v$与$\vert u-v \vert $，然后把它们拼接起来，最后接一个logistic regression层(全连接加softmax)。</p><p>使用这么简单的分类模型的原因是想看看Sentence Embedding是否能够学习到复杂的非线性的语义关系。使用结果如下图所示。可以看到效果还是非常不错的，和(当时)最好的结果差别不大，而那些结果都是使用非常复杂的模型得到结果，而这里只使用了简单的逻辑回归模型。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/skip-thought-3-16514865823989.png" alt="img" style="zoom:80%;"></p><p><em>图：Semantic relatedness的效果</em> </p><h4 id="COCO图像检索任务"><a href="#COCO图像检索任务" class="headerlink" title="COCO图像检索任务"></a>COCO图像检索任务</h4><p>这个任务的输入是一幅图片和一个句子，模型输出的是它们的相关性(句子是否描述了图片的内容)。句子我们可以用Skip Thought Vector编码成一个向量；而图片也可以用预训练的CNN编码成一个向量。模型细节这里不再赘述了，最终的结果如下图所示。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/skip-thought-4-165148662203611.png" alt="img" style="zoom:80%;"></p><p><em>图：Image Retrieval的效果</em> </p><h4 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h4><p>这里比较了5个分类任务： 电影评论情感分类(MR), 商品评论情感分类(CR) , 主观/客观分类(SUBJ), 意见分类(MPQA)和TREC问题类型分类。结果如下图所示。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/skip-thought-5-165148663978213.png" alt="img" style="zoom:80%;"></p><p><em>图：分类任务的效果</em></p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>ELMo是Embeddings from Language Models的缩写，意思就是语言模型得到的(句子)Embedding。另外Elmo是美国儿童教育电视节目芝麻街(Sesame Street)里的小怪兽的名字。原始论文是<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>，这个标题是很合适的，也就是用深度的Transformer模型来学习上下文相关的词表示。</p><p>这篇论文的想法其实非常非常简单，但是取得了非常好的效果。它的思路是用深度的双向RNN(LSTM)在大量未标注数据上训练语言模型，如下图所示。然后在实际的任务中，对于输入的句子，我们使用这个语言模型来对它处理，得到输出的向量，因此这可以看成是一种特征提取。但是和普通的Word2Vec或者GloVe的pretraining不同，ELMo得到的Embedding是有上下文的。比如我们使用Word2Vec也可以得到词”bank”的Embedding，我们可以认为这个Embedding包含了bank的语义。但是bank有很多意思，可以是银行也可以是水边，使用普通的Word2Vec作为Pretraining的Embedding，只能同时把这两种语义都编码进向量里，然后靠后面的模型比如RNN来根据上下文选择合适的语义——比如上下文有money，那么它更可能是银行；而如果上下文是river，那么更可能是水边的意思。但是RNN要学到这种上下文的关系，需要这个任务有大量相关的标注数据，这在很多时候是没有的。而ELMo的特征提取可以看成是上下文相关的，如果输入句子有money，那么它就(或者我们期望)应该能知道bank更可能的语义，从而帮我们选择更加合适的编码。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/elmo-3-165148666130915.png" alt="img"></p><p><em>图：RNN语言模型</em></p><h3 id="无监督的预训练"><a href="#无监督的预训练" class="headerlink" title="无监督的预训练"></a>无监督的预训练</h3><p>给定一个长度为N的句子，假设为$t_1,t_2,…,t_N$，语言模型会计算给定$t_1,…,t_{k-1}$的条件下出现$t_k$的概率：</p><script type="math/tex; mode=display">p(t_1,...,t_N)=\prod_{i=1}^{k}p(t_k|t_1,...,t_{k-1})</script><p>传统的N-gram语言模型不能考虑很长的历史，因此现在的主流是使用多层双向的RNN(LSTM/GRU)来实现语言模型。在每个时刻k，RNN的第j层会输出一个隐状态<script type="math/tex">\overrightarrow{h}_{kj}^{LM}</script>，其中$j=1,2,…,L$，L是RNN的层数。最上层是<script type="math/tex">\overrightarrow{h}_{kL}^{LM}</script>，对它进行softmax之后就可以预测输出词的概率。类似的，我们可以用一个反向的RNN来计算概率：</p><script type="math/tex; mode=display">p(t_1,...,t_N)=\prod_{i=1}^{k}p(t_k|t_{k+1},...,t_N)</script><p>通过这个RNN，我们可以得到$\overleftarrow{h}_{kj}^{LM}$。我们把这两个方向的RNN合并起来就得到Bi-LSTM。我们优化的损失函数是两个LSTM的交叉熵加起来是最小的：</p><script type="math/tex; mode=display">Loss=\sum_{k=1}^{N}(logp(t_k|t_1,...,t_{k-1};\Theta_x,\overrightarrow{\Theta}_{LSTM},\Theta_s) + logp(t_k|t_{k+1},...,t_N;\Theta_x,\overleftarrow{\Theta}_{LSTM},\Theta_s)</script><p>这两个LSTM有各自的参数<script type="math/tex">\overrightarrow{\Theta}_{LSTM}</script>和<script type="math/tex">\overleftarrow{\Theta}_{LSTM}</script>，但是word embedding参数$\Theta_x$和softmax参数$\Theta_s$是共享的。</p><h3 id="应用ELMo"><a href="#应用ELMo" class="headerlink" title="应用ELMo"></a>应用ELMo</h3><p>ELMo会根据不同的任务，把上面得到的双向的LSTM的不同层的隐状态组合起来。对于输入的词$t_k$，我们可以得到2L+1个向量，分别是<script type="math/tex">\{x_k^{LM}, \overrightarrow{h}_{kj}^{LM}, \overleftarrow{h}_{kj}^{LM}, j=1,2,...,L\}</script>，我们把它记作<script type="math/tex">R_k=\{h_{kj}^{LM}, j=0,1,...,L\}</script>。其中<script type="math/tex">h_{k0}^{LM}</script>是词的Embedding，它与上下文无关，而其它的<script type="math/tex">h_{kj}^{LM}=[\overrightarrow{h}_{kj}^{LM}; \overleftarrow{h}_{kj}^{LM}], j>0</script>是把双向的LSTM的输出拼接起来的，它们与上下文相关的。为了用于下游(downstream)的特定任务，我们会把不同层的隐状态组合起来，组合的参数是根据特定任务学习出来的，公式如下：</p><script type="math/tex; mode=display">ELMo_k^{task}=E(R_k;\Theta_{task})=\gamma^{task}\sum_{j=0}^{L}s_j^{task}h_{kj}^{LM}</script><p>这里的$\gamma^{task}$是一个缩放因子，而$s_j^{task}$用于把不同层的输出加权组合出来。在实际的任务中，RNN的参数$h_{kj}^{LM}$都是固定的，可以调的参数只是$\gamma^{task}$和$s_j^{task}$。当然这里ELMo只是一个特征提取，实际任务会再加上一些其它的网络结构，那么那些参数也是一起调整的。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>下图是ELMo在SQuAD、SNLI等常见任务上的效果，相对于Baseline系统都有不小的提高。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/elmo-1-165148667717217.png" alt="img" style="zoom:80%;"></p><p><em>图：ELMo的效果</em> </p><h2 id="OpenAI-GPT"><a href="#OpenAI-GPT" class="headerlink" title="OpenAI GPT"></a>OpenAI GPT</h2><p>OpenAI GPT是来自OpenAI的论文<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a>，BERT借鉴了很多它的方法。</p><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>和前面的ELMo不同，GPT得到的语言模型的参数不是固定的，它会根据特定的任务进行调整(通常是微调)，这样得到的句子表示能更好的适配特定任务。它的思想其实也很简单，使用Transformer来学习一个语言模型，对句子进行无监督的Embedding，然后根据具体任务对Transformer的参数进行微调。</p><h3 id="无监督的Pretraining"><a href="#无监督的Pretraining" class="headerlink" title="无监督的Pretraining"></a>无监督的Pretraining</h3><p>之前我们介绍的Transformer模型是用来做机器翻译的，它有一个Encoder和一个Decoder。这里使用的是Encoder，只不过Encoder的输出不是给Decoder使用，而是直接用它来预测下一个词，如下图所示。但是直接用Self-Attention来训练语言模型是有问题的，因为在k时刻$p(t_k \vert t_1,..,t_{k-1})$，也就是计算$t_k$的时候只能利用它之前的词(或者逆向的语言模型只能用它之后的词)。但是Transformer的Self-Attention是可以利用整个句子的信息的，这显然不行，因为你让它根据”it is a”来预测后面的词，而且还告诉它整个句子是”it is a good day”，它就可能”作弊”，直接把下一个词输出了，这样loss是零。</p><p>因此这里要借鉴Decoder的Mask技巧，通过Mask让它在编码$t_k$的时候只能利用k之前(包括k本身)的信息。具体来说，给定一个未标注的语料库$\mathcal{U}=\\{u_1,…,u_n\\}$，我们训练一个语言模型，对参数进行最大(对数)似然估计：</p><script type="math/tex; mode=display">L_1(\mathcal{U})=\sum_i logP(u_i|u_1,...,u_{k-1})</script><p>我们这里使用多层的Transformer来实现语言模型，具体为：</p><script type="math/tex; mode=display">\begin{split}h_0 & =UW_e+W_p \\h_l & = transformer\_block(h_{l-1}) \\P(u) & = softmax(h_n W_e^T)\end{split}</script><p>这里的$W_e$是词的Embedding Matrix，$W_p$是位置Embedding Matrix。注意这里的位置编码没有使用前面Transformer的固定编码方式，而是采用类似词的Embedding Matrix，让它自己根据任务学习出合适的位置编码。</p><h3 id="监督的Fine-Tuning"><a href="#监督的Fine-Tuning" class="headerlink" title="监督的Fine-Tuning"></a>监督的Fine-Tuning</h3><p>无监督的Pretraining之后，我们还需要针对特定任务进行Fine-Tuning。我们先假设监督数据集合$\mathcal{C}$的输入x是一个词序列(后面会讲到怎么处理相似度计算或者问答这种输入有两个序列的问题)$x^1,…,x^m$，输出是一个分类的标签y，比如情感分类(Sentiment Classification)任务就是满足上述的条件。</p><p>我们把$x^1,…,x^m$输入Transformer模型，得到最上层的最后一个时刻的输出$h_l^m$，然后我们再加一个softmax层(参数为$W_y$)进行分类，最后用交叉熵损失函数计算损失，从而根据标准数据调整Transformer的参数以及softmax的参数$W_y$。这等价于最大似然估计：</p><script type="math/tex; mode=display">L_2(\mathcal{C})=\sum{x,y}logP(y|x^1,...,x^m)</script><p>正常我们应该调整参数使得$L_2$最大，但是为了提高训练速度和模型的泛化能力，我们使用Multi-task Learning，同时让它最大似然$L_1$和$L_2$：</p><script type="math/tex; mode=display">L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda \times L_1(\mathcal{C})</script><p>注意，这里使用的$L_1$还是之前的语言模型的损失(似然)，但是使用的数据不是前面无监督的数据$\mathcal{U}$，而是使用简单的数据$\mathcal{C}$，而且只使用其中的x，而不需要标签y。</p><h3 id="其它任务"><a href="#其它任务" class="headerlink" title="其它任务"></a>其它任务</h3><p>前面讲了，我们能够处理的任务要求输入是一个序列，而输出是一个分类标签。对于有些任务，比如情感分类，这是没有问题的，但是对于相似度计算或者问答，输入是两个序列。为了能够使用GPT，我们需要一些特殊的技巧把两个输入序列变成一个输入序列。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/gpt-1-165148669057419.png" alt="img" style="zoom:80%;"></p><p><em>图：处理其它任务</em> </p><p>如图上图所示，对于输入是一个序列的任务，我们在序列前后增加两个特殊token——“start”和”extract”，分别表示开始和结束；而如果输入是两个序列，那么在它们中间增加一个特殊的token “delim”。比如Entailment，输入是Premise和Hypothesis，输出是3个分类标签中的一个。</p><p>如果是相似度计算，因为对称性，我们把它们交换顺序，然后输入两个Transformer。如果是多选题，比如给定一个问题和N个答案，那么我们可以把问题和N个答案分别输入N个Transformer。</p><h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><p>下图是部分实验结果，相对于之前的baseline对很多任务都有提高。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/gpt-2-165148670241221.png" alt="img" style="zoom:80%;"></p><p><em>图：OpenAI GPT的部分实验结果</em> </p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="ELMo和OpenAI-GPT的问题"><a href="#ELMo和OpenAI-GPT的问题" class="headerlink" title="ELMo和OpenAI GPT的问题"></a>ELMo和OpenAI GPT的问题</h3><p>ELMo和GPT最大的问题就是传统的语言模型是单向的——我们是根据之前的历史来预测当前词。但是我们不能利用后面的信息。比如句子”The animal didn’t cross the street because it was too tired”。我们在编码it的语义的时候需要同时利用前后的信息，因为在这个句子中，it可能指代animal也可能指代street。根据tired，我们推断它指代的是animal，因为street是不能tired。但是如果把tired改成wide，那么it就是指代street了。传统的语言模型，不管是RNN还是Transformer，它都只能利用单方向的信息。比如前向的RNN，在编码it的时候它看到了animal和street，但是它还没有看到tired，因此它不能确定it到底指代什么。如果是后向的RNN，在编码的时候它看到了tired，但是它还根本没看到animal，因此它也不能知道指代的是animal。Transformer的Self-Attention理论上是可以同时attend to到这两个词的，但是根据前面的介绍，由于我们需要用Transformer来学习语言模型，因此必须用Mask来让它看不到未来的信息，所以它也不能解决这个问题的。</p><p>注意：即使ELMo训练了双向的两个RNN，但是一个RNN只能看一个方向，因此也是无法”同时”利用前后两个方向的信息的。也许有的读者会问，我的RNN有很多层，比如第一层的正向RNN在编码it的时候编码了animal和street的语义，反向RNN编码了tired的语义，然后第二层的RNN就能同时看到这两个语义，然后判断出it指代animal。理论上是有这种可能，但是实际上很难。举个反例，理论上一个三层(一个隐层)的全连接网络能够拟合任何函数，那我们还需要更多层词的全连接网络或者CNN、RNN干什么呢？如果数据不是足够足够多，如果不对网络结构做任何约束，那么它有很多中拟合的方法，其中很多是过拟合的。但是通过对网络结构的约束，比如CNN的局部特效，RNN的时序特效，多层网络的层次结构，对它进行了很多约束，从而使得它能够更好的收敛到最佳的参数。我们研究不同的网络结构(包括resnet、dropout、batchnorm等等)都是为了对网络增加额外的(先验的)约束。</p><h3 id="BERT简介"><a href="#BERT简介" class="headerlink" title="BERT简介"></a>BERT简介</h3><p>BERT来自Google的论文<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Pre-training of Deep Bidirectional Transformers for Language Understanding</a>，BERT是”Bidirectional Encoder Representations from Transformers”的首字母缩写。如下图所示，BERT能够同时利用前后两个方向的信息，而ELMo和GPT只能使用单个方向的。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/bert-1-165148671588823.png" alt="img"></p><p><em>图：BERT vs ELMo and GPT</em></p><p>BERT仍然使用的是Transformer模型，那它是怎么解决语言模型只能利用一个方向的信息的问题呢？答案是它的pretraining训练的不是普通的语言模型，而是Mask语言模型。在介绍Mask语言模型之前我们先介绍BERT的输入表示。</p><h3 id="输入表示"><a href="#输入表示" class="headerlink" title="输入表示"></a>输入表示</h3><p>BERT的输入表示如图下图所示。比如输入的是两个句子”my dog is cute”，”he likes playing”。后面会解释为什么需要两个句子。这里采用类似GPT的两个句子的表示方法，首先会在第一个句子的开头增加一个特殊的Token [CLS]，在cute的后面增加一个[SEP]表示第一个句子结束，在##ing后面也会增加一个[SEP]。注意这里的分词会把”playing”分成”play”和”##ing”两个Token，这种把词分成更细粒度的Word Piece的方法在前面的机器翻译部分介绍过了，这是一种解决未登录词的常见办法，后面的代码部分也会简单介绍。接着对每个Token进行3个Embedding：词的Embedding；位置的Embedding和Segment的Embedding。词的Embedding大家都很熟悉了，而位置的Embedding和词类似，把一个位置(比如2)映射成一个低维稠密的向量。而Segment只有两个，要么是属于第一个句子(segment)要么属于第二个句子，不管那个句子，它都对应一个Embedding向量。同一个句子的Segment Embedding是共享的，这样它能够学习到属于不同Segment的信息。对于情感分类这样的任务，只有一个句子，因此Segment id总是0；而对于Entailment任务，输入是两个句子，因此Segment是0或者1。</p><p>BERT模型要求有一个固定的Sequence的长度，比如128。如果不够就在后面padding，否则就截取掉多余的Token，从而保证输入是一个固定长度的Token序列，后面的代码会详细的介绍。第一个Token总是特殊的[CLS]，它本身没有任何语义，因此它会(必须)编码整个句子(其它词)的语义。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/bert-2-165148673188125.png" alt="img" style="zoom:80%;"></p><p><em>图：BERT的输入表示</em></p><p>segment embeddings示意图：</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/1135245-20190227235737504-479706108.png" alt="img" style="zoom:80%;"></p><h3 id="Mask-LM"><a href="#Mask-LM" class="headerlink" title="Mask LM"></a>Mask LM</h3><p>为了解决只能利用单向信息的问题，BERT使用的是Mask语言模型而不是普通的语言模型。Mask语言模型有点类似与完形填空——给定一个句子，把其中某个词遮挡起来，让人猜测可能的词。这里会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。</p><p>但是这有一个问题：在Pretraining Mask LM时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。因此BERT中，如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行：</p><ul><li>80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK]</li><li>10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple</li><li>10%的概率替换成它本身，比如my dog is hairy → my dog is hairy</li></ul><p>这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。</p><h3 id="预测句子关系"><a href="#预测句子关系" class="headerlink" title="预测句子关系"></a>预测句子关系</h3><p>在有些任务中，比如问答，前后两个句子有一定的关联关系，我们希望BERT Pretraining的模型能够学习到这种关系。因此BERT还增加了一个新的任务——预测两个句子是否有关联关系。这是一种Multi-Task Learing。BERT要求的Pretraining的数据是一个一个的”文章”，比如它使用了BookCorpus和维基百科的数据，BookCorpus是很多本书，每本书的前后句子是有关联关系的；而维基百科的文章的前后句子也是有关系的。对于这个任务，BERT会以50%的概率抽取有关联的句子(注意这里的句子实际只是联系的Token序列，不是语言学意义上的句子)，另外以50%的概率随机抽取两个无关的句子，然后让BERT模型来判断这两个句子是否相关。比如下面的两个相关的句子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</span><br></pre></td></tr></table></figure><p>下面是两个不相关的句子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</span><br></pre></td></tr></table></figure></p><h3 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h3><p>BERT的Fine-Tuning如下图所示，共分为4类任务。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/v2-a8de0f4b233bd5d267737689db32388e_r.jpg" alt="preview" style="zoom: 50%;"></p><p><em>图：BERT的Fine-Tuning</em></p><p>对于普通的分类任务，输入是一个序列，如图中右上所示，所有的Token都是属于同一个Segment(Id=0)，我们用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，用分类的数据来进行Fine-Tuning。</p><p>对于相似度计算等输入为两个序列的任务，过程如图左上所示。两个序列的Token对应不同的Segment(Id=0/1)。我们也是用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，然后用分类数据进行Fine-Tuning。</p><p>第三类任务是序列标注，比如命名实体识别，输入是一个句子(Token序列)，除了[CLS]和[SEP]的每个时刻都会有输出的Tag，比如B-PER表示人名的开始，本章的序列标注部分已经介绍过怎么把NER变成序列标注的问题了，这里不再赘述。然后用输出的Tag来进行Fine-Tuning，过程如图右下所示。</p><p>第四类是问答类问题，比如SQuAD v1.1数据集，输入是一个问题和一段很长的包含答案的文字(Paragraph)，输出在这段文字里找到问题的答案。</p><p>比如输入的问题是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Where do water droplets collide with ice crystals to form precipitation?</span><br></pre></td></tr></table></figure></p><p>包含答案的文字是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. ...</span><br></pre></td></tr></table></figure></p><p>正确答案是”within a cloud”。</p><p>我们怎么用BERT处理这样的问题呢？我们首先把问题和Paragraph表示成一个长的序列，中间用[SEP]分开，问题对应一个Segment(id=0)，包含答案的文字对于另一个Segment(id=1)。这里有一个假设，那就是答案是Paragraph里的一段连续的文字(Span)。BERT把寻找答案的问题转化成寻找这个Span的开始下标和结束下标的问题。</p><p>如<a href="#bert-3">上图</a>的左下所示。对于Paragraph的第i个Token，BERT的最后一层把它编码成$T_i$，然后我们用一个向量S(这是模型的参数，需要根据训练数据调整)和它相乘(内积)计算它是开始位置的得分，因为Paragraph的每一个Token(当然WordPiece的中间，比如##ing是不可能是开始的)都有可能是开始可能，我们用softmax把它变成概率，然后选择概率最大的作为答案的开始：</p><script type="math/tex; mode=display">P_i=\frac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j} }</script><p>类似的有一个向量T，用于计算答案结束的位置。</p><h3 id="实验结果-2"><a href="#实验结果-2" class="headerlink" title="实验结果"></a>实验结果</h3><p>在GLUE评测平台上的结果如下图所示，我们可以发现BERT比之前最好的OpenAI GPT还提高了很多。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/bert-4-165148679408228.png" alt="img" style="zoom:80%;"></p><p><em>图：BERT在GLUE上的结果</em> </p><p>在SQuAD数据集上，BERT之前最好的结果F1=89.3%，而7个BERT的ensembling能达到93.2%的F1得分。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/bert-5-165148680783330.png" alt="img" style="zoom:80%;"></p><p><em>图：BERT在SQuAD上的结果</em> </p><p>在CoNLL-2003命名实体识别任务上，之前最好的结果是ELMo+Bi-LSTM-CRF(本书前面介绍过Bi-LSTM-CRF)，F1是92.2，而BERT没有使用CRF，也没有使用Bi-LSTM，只是一个Softmax就可以达到92.8的F1得分，如果加上CRF可能还会有一些提高(这是我的猜测，论文并没有尝试)。</p><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>BERT的效果比好的原因是什么呢？从算法上说，它只有两点改动：Mask LM和预测句子关系的Multi-Task Learning。为了知道每个改动的贡献，文章做了如下的<a href="https://zhidao.baidu.com/question/1865645172350926907.html" target="_blank" rel="noopener">对照(Ablation)实验</a>。</p><p>如下图所示，$BERT_{BASE}$是小参数的一个BERT参考模型；No NSP是没有预测句子关系(只有Mask LM)的BERT模型；LTR &amp; No NSP基本等同于OpenAI GPT，它是基于Transoformer的从左到右的普通语言模型；而最后一行+BiLSTM是指在Fine-Tuning OpenAI GPT的时候多加一个双向LSTM层(通常的Fine-Tuning都是只有一个线性层)。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/bert-7-165148682078032.png" alt="img" style="zoom:80%;"></p><p><em>图：不同模型的比较</em> </p><p>从上图可以看出，BERT比双向的OpenAI GPT好不少。</p><p>另外文章也对比了不同的参数的效果，如下图所示。</p><p><img src="/DeepLearningApplications/自然语言处理/bert模型详解/bert-8-165148683133934.png" alt="img" style="zoom:80%;"></p><p><em>图：模型参数的比较</em> </p><p>可以看出，模型的参数越多，效果也更好。</p><p>但是和OpenAI GPT相比，还有一点很重要的区别就是训练数据。OpenAI GPT使用的是BooksCorpus语料，总的词数800M；而BERT还增加了wiki语料，其词数是2,500M，所以BERT训练数据的总词数是3,300M。因此BERT的训练数据是OpenAI GPT的4倍多，这是非常重要的一点。我谨慎的怀疑BERT效果好的很大原因是数据量造成的，这从模型参数比较实验可以看出，参数越多效果越好，但是如果训练数据不够，参数再多也是没有用的。文章并没有给出和较大BERT模型等价参数的OpenAI GPT模型的效果，不知是忽略了还是有意为之？</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/d0main/p/10447853.html#segment-embeddings" target="_blank" rel="noopener">【译】为什么BERT有3个嵌入层，它们都是如何实现的</a><br><a href="http://fancyerii.github.io/2019/03/09/bert-theory/" target="_blank" rel="noopener">BERT模型详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近终于搞懂了transformer和原理和实现，可以看bert了。事先声明，本文的大部分内容来自&lt;a href=&quot;http://fancyerii.github.io/2019/03/09/bert-theory/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BERT模型详解&lt;/a&gt;。大佬写的太好了，本来不想复制粘贴的，但是有部分内容看不明白，必须加点笔记。&lt;/p&gt;
&lt;h2 id=&quot;背景简介&quot;&gt;&lt;a href=&quot;#背景简介&quot; class=&quot;headerlink&quot; title=&quot;背景简介&quot;&gt;&lt;/a&gt;背景简介&lt;/h2&gt;&lt;p&gt;2018年深度学习在NLP领域取得了比较大的突破，最大的新闻当属Google的BERT模型横扫各大比赛的排行榜。作者认为，深度学习在NLP领域比较重点的三大突破为：Word Embedding、RNN/LSTM/GRU+Seq2Seq+Attention+Self-Attention机制和Contextual Word Embedding(Universal Sentence Embedding)。&lt;/p&gt;
&lt;p&gt;Word Embedding解决了传统机器学习方法的特征稀疏问题，它通过把一个词映射到一个低维稠密的语义空间，从而使得相似的词可以共享上下文信息，从而提升泛化能力。而且通过无监督的训练可以获得高质量的词向量(比如Word2vec和Glove等方法)，从而把这些语义知识迁移到数据较少的具体任务上。但是Word Embedding学到的是一个词的所有语义，比如bank可以是”银行”也可以是”水边。如果一定要用一个固定的向量来编码其语义，那么我们只能把这两个词的语义都编码进去，但是实际一个句子中只有一个语义是合理的，这显然是有问题的。&lt;/p&gt;
&lt;p&gt;这时我们可以通过RNN/LSTM/GRU来编码上下文的语义，这样它能学到如果周围是money，那么bank更可能是”银行”的语义。最原始的RNN由于梯度消失和梯度爆炸等问题很难训练，后来引入了LSTM和GRU等模型来解决这个问题。最早的RNN只能用于分类、回归和序列标注等任务，通过引入两个RNN构成的Seq2Seq模型可以解决序列的变换问题。比如机器翻译、摘要、问答和对话系统都可以使用这个模型。尤其机器翻译这个任务的训练数据比较大，使用深度学习的方法的效果已经超过传统的机器学习方法，而且模型结构更加简单。到了2017年，Google提出了Transformer模型，引入了Self-Attention。Self-Attention的初衷是为了用Attention替代LSTM，从而可以更好的并行(因为LSTM的时序依赖特效很难并行)，从而可以处理更大规模的语料。Transformer出来之后被广泛的用于以前被RNN/LSTM/GRU霸占的地盘，Google更是在Transformer的论文里使用”Attention is all you need”这样霸气的标题。现在Transformer已经成为Encoder/Decoder的霸主。&lt;/p&gt;
    
    </summary>
    
    
      <category term="DeepLearningApplications" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/"/>
    
      <category term="自然语言处理" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NLP" scheme="https://www.zdaiot.com/tags/NLP/"/>
    
      <category term="bert" scheme="https://www.zdaiot.com/tags/bert/"/>
    
      <category term="transformer" scheme="https://www.zdaiot.com/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer实战</title>
    <link href="https://www.zdaiot.com/DeepLearningApplications/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer%E5%AE%9E%E6%88%98/"/>
    <id>https://www.zdaiot.com/DeepLearningApplications/自然语言处理/Transformer实战/</id>
    <published>2022-04-28T12:18:03.000Z</published>
    <updated>2022-04-28T12:18:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们详细的介绍了Transformer的原理，但是有的细节还是一头雾水，所以我们接下来介绍一下Transformer的实现，主要参考了文章<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a>，<a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">github地址</a>。</p><blockquote><p>本文的代码部分来自于github，而图来源于<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a>。</p></blockquote><h2 id="Prelims"><a href="#Prelims" class="headerlink" title="Prelims"></a>Prelims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> exists</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> log_softmax, pad</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt</span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">import</span> torchtext.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> GPUtil</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set to False to skip notebook execution (e.g. for debugging)</span></span><br><span class="line">RUN_EXAMPLES = <span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some convenience helper functions used throughout the notebook</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_interactive_notebook</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> __name__ == <span class="string">"__main__"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_example</span><span class="params">(fn, args=[])</span>:</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span> <span class="keyword">and</span> RUN_EXAMPLES:</span><br><span class="line">        <span class="keyword">return</span> fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute_example</span><span class="params">(fn, args=[])</span>:</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span> <span class="keyword">and</span> RUN_EXAMPLES:</span><br><span class="line">        fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DummyOptimizer</span><span class="params">(torch.optim.Optimizer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.param_groups = [&#123;<span class="string">"lr"</span>: <span class="number">0</span>&#125;]</span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span><span class="params">(self, set_to_none=False)</span>:</span></span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DummyScheduler</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="literal">None</span></span><br></pre></td></tr></table></figure><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>大多数的neural sequence transduction模型都使用了encoder-decoder结构，encoder结构将一个用符号（symbols）表示的输入系列$(x_1, …, x_n)$，表示成为连续表征$\mathbf{z} = (z_1, …, z_n)$。给出$\mathbf{z}$，decoder生成输出序列$(y_1,…,y_m)$，并且一次生成一个元素。在每一步，模型都是自回归的（auto-regressive），在生成下一步时，使用先前生成的符号作为附加输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many</span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>Transformer总体结构如下，encoder和decoder结构都是堆叠self-attention and point-wise, fully connected layers。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_14_0.png" alt="png"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder由$N=6$个一模一样的层（EncoderLayer）组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>我们在each of the two sub-layers使用残差连接，并且后接layer normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><p>因此，每一个sub-layer的输出是$\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$。我们还添加了Dropout层。为了facilitate这些残差连接，模型中所有sub-layer和embedding layers的输出维度均是$d_{\text{model}}=512$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>每一个layer含有两个sub-layers，第一个是multi-head self-attention mechanism，第二个是simple, position-wise fully connected feed-forward network。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder同样由$N=6$个一模一样的层（encoder layer）组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>在每个encoder layer除了两个 sub-layers 外，还插入了第三个sub-layer，它在encoder stack的输出上执行multi-head attention。与encoder相同，我们在each of the two sub-layers使用残差连接，并且后接layer normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>我们还修改了decoder中的self-attention sub-layer，以防止它利用到后续位置的信息。This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position  $i$  can depend only on the known outputs at positions less than $i$ .</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=<span class="number">1</span>).type(</span><br><span class="line">        torch.uint8</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_mask</span><span class="params">()</span>:</span></span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"Subsequent Mask"</span>: subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>][x, y].flatten(),</span><br><span class="line">                    <span class="string">"Window"</span>: y,</span><br><span class="line">                    <span class="string">"Masking"</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">20</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">20</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .properties(height=<span class="number">250</span>, width=<span class="number">250</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">"Window:O"</span>),</span><br><span class="line">            alt.Y(<span class="string">"Masking:O"</span>),</span><br><span class="line">            alt.Color(<span class="string">"Subsequent Mask:Q"</span>, scale=alt.Scale(scheme=<span class="string">"viridis"</span>)),</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_mask)</span><br></pre></td></tr></table></figure><p>下图展示了each tgt word（row），被允许看到的信息（column）。单词在训练过程中被遮挡，使模型关注预测下一个words。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_31_0.png" alt="png"></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>attention函数可以被描述为 mapping a query and a set of key-value pairs to an output，其中query, keys, values, and output都是向量。output是values的加权求和，其中每个value的权重是通过query with the corresponding key的compatibility function计算得到。</p><p>我们将这种特别的attention称为“Scaled Dot-Product Attention”。它的输入由$d_k$维度的queries、keys，$d_v$维度的values组成。 We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_33_0.png" alt="png"></p><p>实际上我们会同时在一系列的的queries上计算attention 函数，对应的会有一系列的keys $K$、values $V$。attention函数的输出为：</p><script type="math/tex; mode=display">\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = scores.softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>最常用的两个attention函数是additive attention和dot-product (multiplicative) attention。后者除了没有缩放$\frac{1}{\sqrt{d_k}}$，其余与我们的相同。而Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  虽然两者在理论复杂性上相似，但dot-product attention在实践中要快得多，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>虽然对于较小的$d_k$值，这两种机制的性能相似，但对于较大的$d_k$值，additive attention优于dot product attention。我们怀疑较大的$d_k$值，dot product的幅度会增大，从而将Softmax函数推入其梯度极小的区域。(To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance$d_k$.). 为了抵消这种影响，我们使用$\frac{1}{\sqrt{d_k}}$对dot products进行缩放。</p><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_38_0.png" alt="png"></p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 当仅有一个 attention head，平均化抑制了这一点。</p><script type="math/tex; mode=display">\mathrm{MultiHead}(Q, K, V) =    \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)</script><p>Where the projections are parameter matrices $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$。在本工作中，我们使用了$h=8$个平行attention layers, or heads。对于其中每一个，我们使用了$d_k=d_v=d_{\text{model}}/h=64$。由于each head的维度降低了，所以总的计算量与full dimensionality的single-head attention相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            lin(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> lin, x <span class="keyword">in</span> zip(self.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, self.attn = attention(</span><br><span class="line">            query, key, value, mask=mask, dropout=self.dropout</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        x = (</span><br><span class="line">            x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">del</span> query</span><br><span class="line">        <span class="keyword">del</span> key</span><br><span class="line">        <span class="keyword">del</span> value</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><p>Transformer以三种不同的方式使用了multi-head attention。</p><ol><li>在“encoder-decoder attention” layers中，queries来自于之前的decoder layer， memory keys and values 来自encoder的输出。This allows every position in the decoder to attend over all positions in the input sequence.  这模仿了sequence-to-sequence模型中典型的encoder-decoder attention机制。</li><li>encoder中的self-attention layers. 这里的self-attention layers中，所有的 keys, values and queries均来自于上一层的输出。Each position in the encoder can attend to all positions in the previous layer of the encoder.</li><li>decoder中的self-attention layers.  self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position（up to and including：直到并包括）。我们需要防止信息在decoder中向左流动，以保持自回归（auto-regressive）特性。我们在scaled dot-product attention中实现了这一点，通过屏蔽Softmax输入中对应于非法连接的所有值（设置为$-\infty$）。</li></ol><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>除了attention sub-layers，encoder and decoder中的每一层都包含一个fully connected feed-forward network（完全连接的前馈网络），该网络分别且相同地应用于每个位置。它由两个线性变换组成，中间有一个ReLU激活。</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2</script><p>虽然在不同位置上都是线性变换，但它们在不同的层之间使用不同的参数。另一种描述方式是将其描述为核大小为1的两个卷积。input和output的维度为$d_{\text{model}}=512$，内层的维度为$d_{ff}=2048$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(self.w_1(x).relu()))</span><br></pre></td></tr></table></figure><h3 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h3><p>与其它的sequence transduction models相似，we use learned embeddings to convert the input tokens and output tokens to vectors of dimension  $d_{\text{model}}$。我们还使用常用的 linear transformation and softmax function 将 decoder output转换为 predicted next-token probabilities. 在我们的模型中，我们在two embedding layers 和pre-softmax linear transformation共享相同的权重矩阵。在embedding layers，我们multiply those weights by $\sqrt{d_{\text{model}}}$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>由于我们的模型不包含recurrence和卷积，为了使模型利用序列的顺序，我们必须注入一些关于tokens in the sequence的相对或绝对位置的信息。为此，我们将“positional encodings”添加到 encoder 和 decoder 堆栈底部的input embeddings中。 positional encodings具有与embeddings相同的维度$d_{\text{model}}$，因此这两个模型可以求和。positional encodings有许多选择，学习的和固定的。</p><p>在这项工作中，我们使用不同频率的正弦和余弦函数。其中$pos$表示单词在句子中的位置，$2i$ 表示偶数的维度，$2i+1$ 表示奇数维度。也就是说，位置编码的每个维度对应于一个正弦。波长形成从2π到10000⋅2π的几何级数。我们选择这个函数是因为我们假设它将允许模型更容易学习相对位置，因为对于任何固定的偏移量$k$，$PE_{pos+k}$可以表示为$PE_{pos}$的线性函数。</p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}}) \\PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})</script><p>初次之外，我们还将dropout应用于the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. 这里dropout的比例为$P_{drop}=0.1$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model)</span><br><span class="line">        )</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">"pe"</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:, : x.size(<span class="number">1</span>)].requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p>Below the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_positional</span><span class="params">()</span>:</span></span><br><span class="line">    pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">    y = pe.forward(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">    data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"embedding"</span>: y[<span class="number">0</span>, :, dim],</span><br><span class="line">                    <span class="string">"dimension"</span>: dim,</span><br><span class="line">                    <span class="string">"position"</span>: list(range(<span class="number">100</span>)),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> dim <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">800</span>)</span><br><span class="line">        .encode(x=<span class="string">"position"</span>, y=<span class="string">"embedding"</span>, color=<span class="string">"dimension:N"</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_positional)</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_49_0.png" alt="png"></p><p>我们还试验了使用学习的positional embeddings，发现两个版本产生的结果几乎相同。我们选择正弦版本是因为它可能允许模型推广到比训练期间遇到的序列长度更长的序列长度。</p><h3 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was important from their code.</span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/v2-680089c6c10bec7cb81b99640536de16_b.jpg" alt="img" style="zoom: 80%;"></p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在这里，我们执行一个forward，以生成模型的预测。我们尝试使用我们的transformer来记忆输入。正如您将看到的，由于模型尚未经过训练，因此输出是随机生成的。在下一个教程中，我们将构建训练函数，并尝试训练我们的模型记住从1到10的数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference_test</span><span class="params">()</span>:</span></span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.eval()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Example Untrained Model Prediction:"</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_tests</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(run_tests)</span><br></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>接下来我们来介绍训练流程，在此之前我们先介绍train a standard encoder decoder model所需的工具。首先我们定义一个batch object保存用于训练的 src and target sentences、masks。</p><h3 id="Batches-and-Masking"><a href="#Batches-and-Masking" class="headerlink" title="Batches and Masking"></a>Batches and Masking</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"""Object for holding a batch of data with mask during training."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, tgt=None, pad=<span class="number">2</span>)</span>:</span>  <span class="comment"># 2 = &lt;blank&gt; for IWST</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        <span class="keyword">if</span> tgt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.tgt = tgt[:, :<span class="number">-1</span>]</span><br><span class="line">            self.tgt_y = tgt[:, <span class="number">1</span>:]</span><br><span class="line">            self.tgt_mask = self.make_std_mask(self.tgt, pad)</span><br><span class="line">            self.ntokens = (self.tgt_y != pad).data.sum()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(</span><br><span class="line">            tgt_mask.data</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure><h3 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h3><p>接下来我们创建一个通用的训练和打分函数，来跟踪损失。我们传入一个损失函数，它还会执行参数更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainState</span>:</span></span><br><span class="line">    <span class="string">"""Track number of steps, examples, and tokens processed"""</span></span><br><span class="line"></span><br><span class="line">    step: int = <span class="number">0</span>  <span class="comment"># Steps in the current epoch</span></span><br><span class="line">    accum_step: int = <span class="number">0</span>  <span class="comment"># Number of gradient accumulation steps</span></span><br><span class="line">    samples: int = <span class="number">0</span>  <span class="comment"># total # of examples used</span></span><br><span class="line">    tokens: int = <span class="number">0</span>  <span class="comment"># total # of tokens processed</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    data_iter,</span></span></span><br><span class="line"><span class="function"><span class="params">    model,</span></span></span><br><span class="line"><span class="function"><span class="params">    loss_compute,</span></span></span><br><span class="line"><span class="function"><span class="params">    optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">    scheduler,</span></span></span><br><span class="line"><span class="function"><span class="params">    mode=<span class="string">"train"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    accum_iter=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    train_state=TrainState<span class="params">()</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">"""Train a single epoch"""</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    n_accum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(</span><br><span class="line">            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span><br><span class="line">        )</span><br><span class="line">        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        <span class="comment"># loss_node = loss_node / accum_iter</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"train"</span> <span class="keyword">or</span> mode == <span class="string">"train+log"</span>:</span><br><span class="line">            loss_node.backward()</span><br><span class="line">            train_state.step += <span class="number">1</span></span><br><span class="line">            train_state.samples += batch.src.shape[<span class="number">0</span>]</span><br><span class="line">            train_state.tokens += batch.ntokens</span><br><span class="line">            <span class="keyword">if</span> i % accum_iter == <span class="number">0</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">                n_accum += <span class="number">1</span></span><br><span class="line">                train_state.accum_step += <span class="number">1</span></span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">40</span> == <span class="number">1</span> <span class="keyword">and</span> (mode == <span class="string">"train"</span> <span class="keyword">or</span> mode == <span class="string">"train+log"</span>):</span><br><span class="line">            lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">"lr"</span>]</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            print(</span><br><span class="line">                (</span><br><span class="line">                    <span class="string">"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f "</span></span><br><span class="line">                    + <span class="string">"| Tokens / Sec: %7.1f | Learning Rate: %6.1e"</span></span><br><span class="line">                )</span><br><span class="line">                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)</span><br><span class="line">            )</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">        <span class="keyword">del</span> loss</span><br><span class="line">        <span class="keyword">del</span> loss_node</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens, train_state</span><br></pre></td></tr></table></figure><h3 id="Training-Data-and-Batching"><a href="#Training-Data-and-Batching" class="headerlink" title="Training Data and Batching"></a>Training Data and Batching</h3><p>我们在标准的WMT 2014英语-德语数据集上进行了训练，该数据集由大约450万个句子对组成。Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.  对于英语-法语，我们使用了更大的2014年WMT英语-法语数据集，包括3600万个句子和split tokens into a 32000 word-piece vocabulary. </p><p>Sentence pairs were batched together by approximate sequence length. 每个训练批次包含一组句子对，其中包含大约25000个source tokens和25000个target tokens。</p><h3 id="Hardware-and-Schedule"><a href="#Hardware-and-Schedule" class="headerlink" title="Hardware and Schedule"></a>Hardware and Schedule</h3><p>我们在一台配备8个NVIDIA P100图形处理器的机器上训练了我们的模型。对于使用本文中描述的超参数的基本模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总共100,000步或12小时的培训。对于我们的大型模型，step time是1.0秒。这些大模型接受了300,000步(3.5天)的训练。</p><h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><p>我们使用adam作为优化器，$\beta_1=0.9$, $\beta_2=0.98$ and $\epsilon=10^{-9}$，在训练中学习率也是变化的，变化方式是：</p><script type="math/tex; mode=display">lrate = d_{\text{model}}^{-0.5} \cdot \min({step\_num}^{-0.5},  {step\_num} \cdot {warmup\_steps}^{-1.5})</script><p>这对应于在前面$warmup_steps$线性增加学习率，此后按与步数的平方根倒数成比例递减。这里$warmup_steps=4000$。</p><blockquote><p>注意：这部分非常重要。 需要使用这种模型设置进行训练。</p><p>该模型曲线的示例，用于不同的模型大小和优化超参数。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def rate(step, model_size, factor, warmup):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    we have to default the step to 1 for LambdaLR function</span><br><span class="line">    to avoid zero raising to negative power.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if step == 0:</span><br><span class="line">        step = 1</span><br><span class="line">    return factor * (</span><br><span class="line">        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_learning_schedule</span><span class="params">()</span>:</span></span><br><span class="line">    opts = [</span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 1</span></span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>],  <span class="comment"># example 2</span></span><br><span class="line">        [<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 3</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    dummy_model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    learning_rates = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we have 3 examples in opts list.</span></span><br><span class="line">    <span class="keyword">for</span> idx, example <span class="keyword">in</span> enumerate(opts):</span><br><span class="line">        <span class="comment"># run 20000 epoch for each example</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            dummy_model.parameters(), lr=<span class="number">1</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">        )</span><br><span class="line">        lr_scheduler = LambdaLR(</span><br><span class="line">            optimizer=optimizer, lr_lambda=<span class="keyword">lambda</span> step: rate(step, *example)</span><br><span class="line">        )</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="comment"># take 20K dummy training steps, save the learning rate at each step</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">            tmp.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">"lr"</span>])</span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        learning_rates.append(tmp)</span><br><span class="line"></span><br><span class="line">    learning_rates = torch.tensor(learning_rates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Enable altair to handle more than 5000 rows</span></span><br><span class="line">    alt.data_transformers.disable_max_rows()</span><br><span class="line"></span><br><span class="line">    opts_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"Learning Rate"</span>: learning_rates[warmup_idx, :],</span><br><span class="line">                    <span class="string">"model_size:warmup"</span>: [<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>][</span><br><span class="line">                        warmup_idx</span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">"step"</span>: range(<span class="number">20000</span>),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> warmup_idx <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(opts_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">600</span>)</span><br><span class="line">        .encode(x=<span class="string">"step"</span>, y=<span class="string">"Learning Rate"</span>, color=<span class="string">"model_size:warmup:N"</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">example_learning_schedule()</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_69_0.png" alt="png"></p><h3 id="Regularization（Label-Smoothing）"><a href="#Regularization（Label-Smoothing）" class="headerlink" title="Regularization（Label Smoothing）"></a>Regularization（Label Smoothing）</h3><p>在训练过程中我们使用了label smoothing，$\epsilon_{ls}=0.1$。虽然模型学会了更多的不确定，但提高了准确性和BLEU的分数。</p><p>我们使用KL div loss实现label smoothing. Instead of using a one-hot target distribution, we create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">"sum"</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure><p>Here we can see an example of how the mass is distributed to the words based on confidence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_label_smoothing</span><span class="params">()</span>:</span></span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">    predict = torch.FloatTensor(</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    crit(x=predict.log(), target=torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>]))</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"target distribution"</span>: crit.true_dist[x, y].flatten(),</span><br><span class="line">                    <span class="string">"columns"</span>: y,</span><br><span class="line">                    <span class="string">"rows"</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">5</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">5</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect(color=<span class="string">"Blue"</span>, opacity=<span class="number">1</span>)</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">"columns:O"</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Y(<span class="string">"rows:O"</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Color(</span><br><span class="line">                <span class="string">"target distribution:Q"</span>, scale=alt.Scale(scheme=<span class="string">"viridis"</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_label_smoothing)</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_74_0.png" alt="png"></p><p>Label smoothing actually starts to penalize the model if it gets very confident about a given choice.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x, crit)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d]])</span><br><span class="line">    <span class="keyword">return</span> crit(predict.log(), torch.LongTensor([<span class="number">1</span>])).data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">penalization_visualization</span><span class="params">()</span>:</span></span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">    loss_data = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"Loss"</span>: [loss(x, crit) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)],</span><br><span class="line">            <span class="string">"Steps"</span>: list(range(<span class="number">99</span>)),</span><br><span class="line">        &#125;</span><br><span class="line">    ).astype(<span class="string">"float"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(loss_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">350</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            x=<span class="string">"Steps"</span>,</span><br><span class="line">            y=<span class="string">"Loss"</span>,</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(penalization_visualization)</span><br></pre></td></tr></table></figure><p><img src="/DeepLearningApplications/自然语言处理/Transformer实战/the-annotated-transformer_76_0.png" alt="png"></p><h2 id="A-First-Example"><a href="#A-First-Example" class="headerlink" title="A First Example"></a>A First Example</h2><p>我们可以从尝试一项简单的抄写任务开始。给定一组来自较小词汇表的随机输入symbols，目标是生成相同的symbols。</p><h3 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch_size, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="Loss-Computation"><a href="#Loss-Computation" class="headerlink" title="Loss Computation"></a>Loss Computation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        sloss = (</span><br><span class="line">            self.criterion(</span><br><span class="line">                x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), y.contiguous().view(<span class="number">-1</span>)</span><br><span class="line">            )</span><br><span class="line">            / norm</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br></pre></td></tr></table></figure><h3 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h3><p>为简单起见，此代码使用Greedy Decoding来预测翻译。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_simple_model</span><span class="params">()</span>:</span></span><br><span class="line">    V = <span class="number">11</span></span><br><span class="line">    criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">    model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, model_size=model.src_embed[<span class="number">0</span>].d_model, factor=<span class="number">1.0</span>, warmup=<span class="number">400</span></span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">80</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">20</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">"train"</span>,</span><br><span class="line">        )</span><br><span class="line">        model.eval()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">5</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">"eval"</span>,</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">    max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">execute_example(example_simple_model)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">3.023465</span> Tokens per Sec: <span class="number">403.074173</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.920030</span> Tokens per Sec: <span class="number">641.689380</span></span><br><span class="line"><span class="number">1.9274832487106324</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.940011</span> Tokens per Sec: <span class="number">432.003378</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.699767</span> Tokens per Sec: <span class="number">641.979665</span></span><br><span class="line"><span class="number">1.657595729827881</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.860276</span> Tokens per Sec: <span class="number">433.320240</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.546011</span> Tokens per Sec: <span class="number">640.537198</span></span><br><span class="line"><span class="number">1.4888023376464843</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.682198</span> Tokens per Sec: <span class="number">432.092305</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.313169</span> Tokens per Sec: <span class="number">639.441857</span></span><br><span class="line"><span class="number">1.3485562801361084</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.278768</span> Tokens per Sec: <span class="number">433.568756</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.062384</span> Tokens per Sec: <span class="number">642.542067</span></span><br><span class="line"><span class="number">0.9853351473808288</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.269471</span> Tokens per Sec: <span class="number">433.388727</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.590709</span> Tokens per Sec: <span class="number">642.862135</span></span><br><span class="line"><span class="number">0.5686767101287842</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.997076</span> Tokens per Sec: <span class="number">433.009746</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.343118</span> Tokens per Sec: <span class="number">642.288427</span></span><br><span class="line"><span class="number">0.34273059368133546</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.459483</span> Tokens per Sec: <span class="number">434.594030</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.290385</span> Tokens per Sec: <span class="number">642.519464</span></span><br><span class="line"><span class="number">0.2612409472465515</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.031042</span> Tokens per Sec: <span class="number">434.557008</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.437069</span> Tokens per Sec: <span class="number">643.630322</span></span><br><span class="line"><span class="number">0.4323212027549744</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.617165</span> Tokens per Sec: <span class="number">436.652626</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.258793</span> Tokens per Sec: <span class="number">644.372296</span></span><br><span class="line"><span class="number">0.27331129014492034</span></span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure><h2 id="A-Real-World-Example"><a href="#A-Real-World-Example" class="headerlink" title="A Real World Example"></a>A Real World Example</h2><p>现在，我们考虑一个使用IWSLT德语-英语翻译任务的真实世界示例。这项任务比本文考虑的WMT任务小得多，但它能说明整个流程。我们还展示了如何使用多GPU处理来实现真正的速度。</p><h3 id="Data-Loading"><a href="#Data-Loading" class="headerlink" title="Data Loading"></a>Data Loading</h3><p>We will load the dataset using torchtext and spacy for tokenization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load spacy tokenizer models, download them if they haven't been</span></span><br><span class="line"><span class="comment"># downloaded already</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_tokenizers</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_de = spacy.load(<span class="string">"de_core_news_sm"</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">"python -m spacy download de_core_news_sm"</span>)</span><br><span class="line">        spacy_de = spacy.load(<span class="string">"de_core_news_sm"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_en = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">"python -m spacy download en_core_web_sm"</span>)</span><br><span class="line">        spacy_en = spacy.load(<span class="string">"en_core_web_sm"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> spacy_de, spacy_en</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(text, tokenizer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> tokenizer.tokenizer(text)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yield_tokens</span><span class="params">(data_iter, tokenizer, index)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> from_to_tuple <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(from_to_tuple[index])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocabulary</span><span class="params">(spacy_de, spacy_en)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Building German Vocabulary ..."</span>)</span><br><span class="line">    train, val, test = datasets.IWSLT2016(language_pair=(<span class="string">"de"</span>, <span class="string">"en"</span>))</span><br><span class="line">    vocab_src = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_de, index=<span class="number">0</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">"&lt;s&gt;"</span>, <span class="string">"&lt;/s&gt;"</span>, <span class="string">"&lt;blank&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Building English Vocabulary ..."</span>)</span><br><span class="line">    train, val, test = datasets.IWSLT2016(language_pair=(<span class="string">"de"</span>, <span class="string">"en"</span>))</span><br><span class="line">    vocab_tgt = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_en, index=<span class="number">1</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">"&lt;s&gt;"</span>, <span class="string">"&lt;/s&gt;"</span>, <span class="string">"&lt;blank&gt;"</span>, <span class="string">"&lt;unk&gt;"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    vocab_src.set_default_index(vocab_src[<span class="string">"&lt;unk&gt;"</span>])</span><br><span class="line">    vocab_tgt.set_default_index(vocab_tgt[<span class="string">"&lt;unk&gt;"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_vocab</span><span class="params">(spacy_de, spacy_en)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(<span class="string">"vocab.pt"</span>):</span><br><span class="line">        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)</span><br><span class="line">        torch.save((vocab_src, vocab_tgt), <span class="string">"vocab.pt"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab_src, vocab_tgt = torch.load(<span class="string">"vocab.pt"</span>)</span><br><span class="line">    print(<span class="string">"Finished.\nVocabulary sizes:"</span>)</span><br><span class="line">    print(len(vocab_src))</span><br><span class="line">    print(len(vocab_tgt))</span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    <span class="comment"># global variables used later in the script</span></span><br><span class="line">    spacy_de, spacy_en = show_example(load_tokenizers)</span><br><span class="line">    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])</span><br></pre></td></tr></table></figure><h3 id="Iterators"><a href="#Iterators" class="headerlink" title="Iterators"></a>Iterators</h3><p>Batching对训练速度很重要。我们希望非常均匀的划分批次（with absolutely minimal padding）。要做到这一点，我们必须修改一下默认的torchtext batching。这段代码修改了它们的默认批处理，以确保我们搜索足够多的句子来找到紧凑的批处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_batch</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    batch,</span></span></span><br><span class="line"><span class="function"><span class="params">    src_pipeline,</span></span></span><br><span class="line"><span class="function"><span class="params">    tgt_pipeline,</span></span></span><br><span class="line"><span class="function"><span class="params">    src_vocab,</span></span></span><br><span class="line"><span class="function"><span class="params">    tgt_vocab,</span></span></span><br><span class="line"><span class="function"><span class="params">    device,</span></span></span><br><span class="line"><span class="function"><span class="params">    max_padding=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_id=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    bs_id = torch.tensor([<span class="number">0</span>], device=device)  <span class="comment"># &lt;s&gt; token id</span></span><br><span class="line">    eos_id = torch.tensor([<span class="number">1</span>], device=device)  <span class="comment"># &lt;/s&gt; token id</span></span><br><span class="line">    src_list, tgt_list = [], []</span><br><span class="line">    <span class="keyword">for</span> (_src, _tgt) <span class="keyword">in</span> batch:</span><br><span class="line">        processed_src = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    src_vocab(src_pipeline(_src)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        processed_tgt = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    tgt_vocab(tgt_pipeline(_tgt)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        src_list.append(</span><br><span class="line">            <span class="comment"># warning - overwrites values for negative values of padding - len</span></span><br><span class="line">            pad(</span><br><span class="line">                processed_src,</span><br><span class="line">                (</span><br><span class="line">                    <span class="number">0</span>,</span><br><span class="line">                    max_padding - len(processed_src),</span><br><span class="line">                ),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        tgt_list.append(</span><br><span class="line">            pad(</span><br><span class="line">                processed_tgt,</span><br><span class="line">                (<span class="number">0</span>, max_padding - len(processed_tgt)),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    src = torch.stack(src_list)</span><br><span class="line">    tgt = torch.stack(tgt_list)</span><br><span class="line">    <span class="keyword">return</span> (src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataloaders</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    device,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_src,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_tgt,</span></span></span><br><span class="line"><span class="function"><span class="params">    spacy_de,</span></span></span><br><span class="line"><span class="function"><span class="params">    spacy_en,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size=<span class="number">12000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    max_padding=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    is_distributed=True,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="comment"># def create_dataloaders(batch_size=12000):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> collate_batch(</span><br><span class="line">            batch,</span><br><span class="line">            tokenize_de,</span><br><span class="line">            tokenize_en,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            device,</span><br><span class="line">            max_padding=max_padding,</span><br><span class="line">            pad_id=vocab_src.get_stoi()[<span class="string">"&lt;blank&gt;"</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = datasets.IWSLT2016(</span><br><span class="line">        language_pair=(<span class="string">"de"</span>, <span class="string">"en"</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_iter_map = to_map_style_dataset(</span><br><span class="line">        train_iter</span><br><span class="line">    )  <span class="comment"># DistributedSampler needs a dataset len()</span></span><br><span class="line">    train_sampler = (</span><br><span class="line">        DistributedSampler(train_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line">    valid_iter_map = to_map_style_dataset(valid_iter)</span><br><span class="line">    valid_sampler = (</span><br><span class="line">        DistributedSampler(valid_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(</span><br><span class="line">        train_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    valid_dataloader = DataLoader(</span><br><span class="line">        valid_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(valid_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=valid_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br></pre></td></tr></table></figure><h2 id="Training-the-System"><a href="#Training-the-System" class="headerlink" title="Training the System"></a>Training the System</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_worker</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    gpu, ngpus_per_node, vocab_src, vocab_tgt, spacy_de, spacy_en, config</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    print(<span class="string">f"Train worker process using GPU: <span class="subst">&#123;gpu&#125;</span> for training"</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line">    is_main_process = gpu == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    pad_idx = vocab_tgt[<span class="string">"&lt;blank&gt;"</span>]</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    model = make_model(len(vocab_src), len(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda(gpu)</span><br><span class="line">    module = model</span><br><span class="line"></span><br><span class="line">    dist.init_process_group(</span><br><span class="line">        <span class="string">"nccl"</span>, init_method=<span class="string">"env://"</span>, rank=gpu, world_size=ngpus_per_node</span><br><span class="line">    )</span><br><span class="line">    model = DDP(model, device_ids=[gpu])</span><br><span class="line">    module = model.module</span><br><span class="line"></span><br><span class="line">    criterion = LabelSmoothing(</span><br><span class="line">        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=<span class="number">0.1</span></span><br><span class="line">    )</span><br><span class="line">    criterion.cuda(gpu)</span><br><span class="line"></span><br><span class="line">    train_dataloader, valid_dataloader = create_dataloaders(</span><br><span class="line">        gpu,</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=config[<span class="string">"batch_size"</span>] // ngpus_per_node,</span><br><span class="line">        max_padding=config[<span class="string">"max_padding"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">"base_lr"</span>], betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, d_model, factor=<span class="number">1</span>, warmup=config[<span class="string">"warmup"</span>]</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line">    train_state = TrainState()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(config[<span class="string">"num_epochs"</span>]):</span><br><span class="line"></span><br><span class="line">        train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">        valid_dataloader.sampler.set_epoch(epoch)</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        print(<span class="string">f"[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Training ===="</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        _, train_state = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> train_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">"train+log"</span>,</span><br><span class="line">            accum_iter=config[<span class="string">"accum_iter"</span>],</span><br><span class="line">            train_state=train_state,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        GPUtil.showUtilization()</span><br><span class="line">        <span class="keyword">if</span> is_main_process:</span><br><span class="line">            file_path = <span class="string">"%s%.2d.pt"</span> % (config[<span class="string">"file_prefix"</span>], epoch)</span><br><span class="line">            torch.save(module.state_dict(), file_path)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">f"[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Validation ===="</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        model.eval()</span><br><span class="line">        sloss = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> valid_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">"eval"</span>,</span><br><span class="line">        )</span><br><span class="line">        print(sloss)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_main_process:</span><br><span class="line">        file_path = <span class="string">"%sfinal.pt"</span> % config[<span class="string">"file_prefix"</span>]</span><br><span class="line">        torch.save(module.state_dict(), file_path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> the_annotated_transformer <span class="keyword">import</span> train_worker</span><br><span class="line"></span><br><span class="line">    ngpus = torch.cuda.device_count()</span><br><span class="line">    os.environ[<span class="string">"MASTER_ADDR"</span>] = <span class="string">"localhost"</span></span><br><span class="line">    os.environ[<span class="string">"MASTER_PORT"</span>] = <span class="string">"12356"</span></span><br><span class="line">    print(<span class="string">f"Number of GPUs detected: <span class="subst">&#123;ngpus&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">"Spawning training processes ..."</span>)</span><br><span class="line">    mp.spawn(</span><br><span class="line">        train_worker,</span><br><span class="line">        nprocs=ngpus,</span><br><span class="line">        args=(</span><br><span class="line">            ngpus,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            spacy_de,</span><br><span class="line">            spacy_en,</span><br><span class="line">            config,</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_trained_model</span><span class="params">(create_model)</span>:</span></span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">"batch_size"</span>: <span class="number">150</span>,</span><br><span class="line">        <span class="string">"num_epochs"</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">"accum_iter"</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">"base_lr"</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">"max_padding"</span>: <span class="number">72</span>,</span><br><span class="line">        <span class="string">"warmup"</span>: <span class="number">3000</span>,</span><br><span class="line">        <span class="string">"file_prefix"</span>: <span class="string">"iwslt_model_"</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> create_model:</span><br><span class="line">        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span><br><span class="line"></span><br><span class="line">    model = make_model(len(vocab_src), len(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">"iwslt_model_final.pt"</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    model = load_trained_model(create_model=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>一旦经过训练，我们就可以对模型进行decode，以产生一组翻译。在这里，我们只需翻译验证集中的第一句话。这个数据集非常小，因此使用greedy search的翻译相当准确。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Translation:&lt;unk&gt; &lt;unk&gt; . In my language , that means , thank you very much . </span><br><span class="line">Gold:&lt;unk&gt; &lt;unk&gt; . It means in my language , thank you very much .</span><br></pre></td></tr></table></figure><h2 id="Additional-Components-BPE-Search-Averaging"><a href="#Additional-Components-BPE-Search-Averaging" class="headerlink" title="Additional Components: BPE, Search, Averaging"></a>Additional Components: BPE, Search, Averaging</h2><p>上述代码主要介绍了Transformer自身的实现，还有四个函数我们没有实现。</p><h3 id="BPE-Word-piece"><a href="#BPE-Word-piece" class="headerlink" title="BPE/ Word-piece"></a>BPE/ Word-piece</h3><p>我们使用了subword units库对数据进行预处理。它将把训练数据转换成如下所示的形式：▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</p><h3 id="Shared-Embeddings"><a href="#Shared-Embeddings" class="headerlink" title="Shared Embeddings"></a>Shared Embeddings</h3><p>当使用共享vocabulary的BPE时，我们可以在source / target / generator之间共享权重向量。详细信息可以阅读<a href="https://arxiv.org/abs/1608.05859" target="_blank" rel="noopener">cite</a>。要将此想法实现到模型中，只需执行以下操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure><h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>详情可以看<a href="https://github.com/OpenNMT/OpenNMT-py/blob/onmt/translate/Beam.py" target="_blank" rel="noopener">OpenNMT-py</a>。</p><h3 id="Model-Averaging"><a href="#Model-Averaging" class="headerlink" title="Model Averaging"></a>Model Averaging</h3><p>文章中平均了最后k个checkpoints来达到集成效果。如果我们有一堆checkpoint，我们可以在事后做这件事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">(model, models)</span>:</span></span><br><span class="line">    <span class="string">"Average models into model"</span></span><br><span class="line">    <span class="keyword">for</span> ps <span class="keyword">in</span> zip(*[m.params() <span class="keyword">for</span> m <span class="keyword">in</span> [model] + models]):</span><br><span class="line">        ps[<span class="number">0</span>].copy_(torch.sum(*ps[<span class="number">1</span>:]) / len(ps[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>在WMT 2014英德翻译任务中，big transformer模型(表2中的Transformer (big) )的表现超过了已有的最好的模型(包括集成)，BLEU的表现高上了2.0%，创造了最好的BLEU 28.4分的新纪录。表3的底部列出了该模型参数配置。在8个P100 GPU上进行了3.5天的训练。甚至我们的base model模型也超过了已有的所有模型和集成模型，而训练成本只是任何已有模型的一小部分。</p><p>在2014年WMT英法翻译任务中，我们的大模型达到了BLEU的41.0分，超过了之前已有的所有单一模型，培训成本不到以前最好模型的四分之一。用于英法翻译的Transformer (big) 的dropout 系数等于0.1，而不是0.3。</p><blockquote><p>我们在这里编写的代码是base model的一个版本。完整版本可以看 <a href="http://opennmt.net/Models-py/" target="_blank" rel="noopener">(Example Models)</a>。</p><p>使用上以小节的附加扩展，OpenNMT-py 在EN-DE WMT数据集上达到了26.9的BLEU。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load data and model for output checks</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_outputs</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    valid_dataloader,</span></span></span><br><span class="line"><span class="function"><span class="params">    model,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_src,</span></span></span><br><span class="line"><span class="function"><span class="params">    vocab_tgt,</span></span></span><br><span class="line"><span class="function"><span class="params">    n_examples=<span class="number">15</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    pad_idx=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    eos_string=<span class="string">"&lt;/s&gt;"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    results = [()] * n_examples</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(n_examples):</span><br><span class="line">        print(<span class="string">"\nExample %d ========\n"</span> % idx)</span><br><span class="line">        b = next(iter(valid_dataloader))</span><br><span class="line">        rb = Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx)</span><br><span class="line">        greedy_decode(model, rb.src, rb.src_mask, <span class="number">64</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        src_tokens = [</span><br><span class="line">            vocab_src.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.src[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line">        tgt_tokens = [</span><br><span class="line">            vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.tgt[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        print(</span><br><span class="line">            <span class="string">"Source Text (Input)        : "</span></span><br><span class="line">            + <span class="string">" "</span>.join(src_tokens).replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        )</span><br><span class="line">        print(</span><br><span class="line">            <span class="string">"Target Text (Ground Truth) : "</span></span><br><span class="line">            + <span class="string">" "</span>.join(tgt_tokens).replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        )</span><br><span class="line">        model_out = greedy_decode(model, rb.src, rb.src_mask, <span class="number">72</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        model_txt = (</span><br><span class="line">            <span class="string">" "</span>.join(</span><br><span class="line">                [vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> model_out <span class="keyword">if</span> x != pad_idx]</span><br><span class="line">            ).split(eos_string, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            + eos_string</span><br><span class="line">        )</span><br><span class="line">        print(<span class="string">"Model Output               : "</span> + model_txt.replace(<span class="string">"\n"</span>, <span class="string">""</span>))</span><br><span class="line">        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_model_example</span><span class="params">(n_examples=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> vocab_src, vocab_tgt, spacy_de, spacy_en</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Preparing Data ..."</span>)</span><br><span class="line">    _, valid_dataloader = create_dataloaders(</span><br><span class="line">        torch.device(<span class="string">"cpu"</span>),</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        is_distributed=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Loading Trained Model ..."</span>)</span><br><span class="line"></span><br><span class="line">    model = make_model(len(vocab_src), len(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(</span><br><span class="line">        torch.load(<span class="string">"iwslt_model_final.pt"</span>, map_location=torch.device(<span class="string">"cpu"</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Checking Model Outputs:"</span>)</span><br><span class="line">    example_data = check_outputs(</span><br><span class="line">        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model, example_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">execute_example(run_model_example)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Translation:&lt;s&gt; ▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</span><br></pre></td></tr></table></figure><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>即使使用greedy decoder，翻译看起来也很好。我们可以进一步把它形象化，看看注意力的每一层都在发生什么。</p><h3 id="Encoder-Self-Attention"><a href="#Encoder-Self-Attention" class="headerlink" title="Encoder Self Attention"></a>Encoder Self Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mtx2df</span><span class="params">(m, max_row, max_col, row_tokens, col_tokens)</span>:</span></span><br><span class="line">    <span class="string">"convert a dense matrix to a data frame with row and column indices"</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                float(m[r, c]),</span><br><span class="line">                <span class="string">"%.3d %s"</span></span><br><span class="line">                % (r, row_tokens[r] <span class="keyword">if</span> len(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">                <span class="string">"%.3d %s"</span></span><br><span class="line">                % (c, col_tokens[c] <span class="keyword">if</span> len(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">"&lt;blank&gt;"</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> range(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        <span class="comment"># if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],</span></span><br><span class="line">        columns=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attn_map</span><span class="params">(attn, layer, head, row_tokens, col_tokens, max_dim=<span class="number">30</span>)</span>:</span></span><br><span class="line">    df = mtx2df(</span><br><span class="line">        attn[<span class="number">0</span>, head].data,</span><br><span class="line">        max_dim,</span><br><span class="line">        max_dim,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">"col_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">"row_token"</span>, axis=alt.Axis(title=<span class="string">""</span>)),</span><br><span class="line">            color=<span class="string">"value"</span>,</span><br><span class="line">            tooltip=[<span class="string">"row"</span>, <span class="string">"column"</span>, <span class="string">"value"</span>, <span class="string">"row_token"</span>, <span class="string">"col_token"</span>],</span><br><span class="line">        )</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encoder</span><span class="params">(model, layer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model.encoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_decoder_self</span><span class="params">(model, layer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_decoder_src</span><span class="params">(model, layer)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].src_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_layer</span><span class="params">(model, layer, getter_fn, ntokens, row_tokens, col_tokens)</span>:</span></span><br><span class="line">    <span class="comment"># ntokens = last_example[0].ntokens</span></span><br><span class="line">    attn = getter_fn(model, layer)</span><br><span class="line">    n_heads = attn.shape[<span class="number">1</span>]</span><br><span class="line">    charts = [</span><br><span class="line">        attn_map(</span><br><span class="line">            attn,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            h,</span><br><span class="line">            row_tokens=row_tokens,</span><br><span class="line">            col_tokens=col_tokens,</span><br><span class="line">            max_dim=ntokens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_heads)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">assert</span> n_heads == <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(</span><br><span class="line">        charts[<span class="number">0</span>]</span><br><span class="line">        | charts[<span class="number">1</span>]</span><br><span class="line">        | charts[<span class="number">2</span>]</span><br><span class="line">        | charts[<span class="number">3</span>]</span><br><span class="line">        | charts[<span class="number">4</span>]</span><br><span class="line">        | charts[<span class="number">5</span>]</span><br><span class="line">        | charts[<span class="number">6</span>]</span><br><span class="line">        | charts[<span class="number">7</span>]</span><br><span class="line">        <span class="comment"># layer + 1 due to 0-indexing</span></span><br><span class="line">    ).properties(title=<span class="string">"Layer %d"</span> % (layer + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="Encoder-Self-Attention-1"><a href="#Encoder-Self-Attention-1" class="headerlink" title="Encoder Self Attention"></a>Encoder Self Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_encoder_self</span><span class="params">()</span>:</span></span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[</span><br><span class="line">        len(example_data) - <span class="number">1</span></span><br><span class="line">    ]  <span class="comment"># batch object for the final example</span></span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model, layer, get_encoder, len(example[<span class="number">1</span>]), example[<span class="number">1</span>], example[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_encoder_self)</span><br></pre></td></tr></table></figure><h3 id="Decoder-Self-Attention"><a href="#Decoder-Self-Attention" class="headerlink" title="Decoder Self Attention"></a>Decoder Self Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_decoder_self</span><span class="params">()</span>:</span></span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[len(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_self,</span><br><span class="line">            len(example[<span class="number">1</span>]),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_self)</span><br></pre></td></tr></table></figure><h3 id="Decoder-Src-Attention"><a href="#Decoder-Src-Attention" class="headerlink" title="Decoder Src Attention"></a>Decoder Src Attention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_decoder_src</span><span class="params">()</span>:</span></span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[len(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_src,</span><br><span class="line">            max(len(example[<span class="number">1</span>]), len(example[<span class="number">2</span>])),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">2</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_src)</span><br></pre></td></tr></table></figure><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><h3 id="Transformer-mask"><a href="#Transformer-mask" class="headerlink" title="Transformer mask"></a>Transformer mask</h3><p>在《<a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">the annotated transformer</a>》中有多个mask，这里总结一下。</p><p>整个模型中使用到的mask主要就是source mask和target mask，其各自的作用如下所示：</p><ol><li>source mask：</li></ol><ul><li><p>source长短不一而无法形成batch，因此引入了pad。将source mask传入到encoder中，让attention在计算$\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})$时，pad位置的值不起作用。</p></li><li><p>同时这个mask还需要传入每个decoderLayer第二个multi-head attention模块中，就是防止来自encoder的key和来自decoder的query在计算多头注意力的时候算了target中的词和source中pad的权重</p></li></ul><ol><li>target mask：需要分training和testing进行讨论</li></ol><ul><li><strong>训练</strong>时，用于防止target的ground truth长短不一引入pad造成的误差，以及<strong>避免在自回归时看到正在预测的字和以后字的ground truth</strong></li><li><strong>测试</strong>时，逻辑上decoder不需要target mask，但出于编程方便的考虑引入mask，假装用于防止看到后面的ground truth，target mask的最后两维的shape和目前生成出来的序列长度相同，但实际上每次都会有一些重复运算在里面，比如目前在预测第10个词时，第1-9个词还需要重新算一遍。核心原因是：模型在写的时候主要考虑的是训练，执行一次attention函数翻译完一个batch的所有句子，而测试时必须是单个或多个句子word by word进行计算</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">annotated-transformer</a><br><a href="https://zhuanlan.zhihu.com/p/504408727" target="_blank" rel="noopener">the annotated transformer中的关于mask的问题 - lumino的文章 - 知乎</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面我们详细的介绍了Transformer的原理，但是有的细节还是一头雾水，所以我们接下来介绍一下Transformer的实现，主要参考了文章&lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Annotated Transformer&lt;/a&gt;，&lt;a href=&quot;https://github.com/harvardnlp/annotated-transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;github地址&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文的代码部分来自于github，而图来源于&lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Annotated Transformer&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Prelims&quot;&gt;&lt;a href=&quot;#Prelims&quot; class=&quot;headerlink&quot; title=&quot;Prelims&quot;&gt;&lt;/a&gt;Prelims&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; os.path &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; exists&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.nn.functional &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; log_softmax, pad&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; math&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; copy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; time&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.optim.lr_scheduler &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; LambdaLR&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; altair &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; alt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torchtext.data.functional &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; to_map_style_dataset&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.utils.data &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; DataLoader&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torchtext.vocab &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; build_vocab_from_iterator&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torchtext.datasets &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; datasets&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; spacy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; GPUtil&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.utils.data.distributed &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; DistributedSampler&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.distributed &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; dist&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.multiprocessing &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; mp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.nn.parallel &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; DistributedDataParallel &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; DDP&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Set to False to skip notebook execution (e.g. for debugging)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RUN_EXAMPLES = &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Some convenience helper functions used throughout the notebook&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;is_interactive_notebook&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; __name__ == &lt;span class=&quot;string&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;show_example&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(fn, args=[])&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;string&quot;&gt;&quot;__main__&quot;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; RUN_EXAMPLES:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; fn(*args)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;execute_example&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(fn, args=[])&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;string&quot;&gt;&quot;__main__&quot;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; RUN_EXAMPLES:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fn(*args)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;DummyOptimizer&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(torch.optim.Optimizer)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.param_groups = [&amp;#123;&lt;span class=&quot;string&quot;&gt;&quot;lr&quot;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&amp;#125;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, set_to_none=False)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;DummyScheduler&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="DeepLearningApplications" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/"/>
    
      <category term="自然语言处理" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NLP" scheme="https://www.zdaiot.com/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.zdaiot.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>fairseq翻译任务解读</title>
    <link href="https://www.zdaiot.com/DeepLearningApplications/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/fairseq%E7%BF%BB%E8%AF%91%E4%BB%BB%E5%8A%A1%E8%A7%A3%E8%AF%BB/"/>
    <id>https://www.zdaiot.com/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/</id>
    <published>2022-04-27T02:42:03.000Z</published>
    <updated>2022-04-27T02:42:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近需要用到<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">fairseq</a>框架中的翻译任务，这里记录一下。</p><h2 id="从实战开始"><a href="#从实战开始" class="headerlink" title="从实战开始"></a>从实战开始</h2><p>首先下载翻译模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p model</span><br><span class="line"><span class="built_in">cd</span> model</span><br><span class="line">wget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2</span><br><span class="line"></span><br><span class="line">bunzip2 wmt16.en-de.joined-dict.transformer.tar.bz2</span><br><span class="line">tar -xvf wmt16.en-de.joined-dict.transformer.tar</span><br></pre></td></tr></table></figure><p>解压后的文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./model</span><br><span class="line">├── wmt16.en-de.joined-dict.transformer</span><br><span class="line">│   ├── bpecodes</span><br><span class="line">│   ├── dict.de.txt</span><br><span class="line">│   ├── dict.en.txt</span><br><span class="line">│   └── model.pt</span><br></pre></td></tr></table></figure><p>然后调用翻译模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fairseq.models.transformer <span class="keyword">import</span> TransformerModel</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_fairseq_tm</span><span class="params">(path, device)</span>:</span></span><br><span class="line">    <span class="comment"># data_name_or_path和bpe_codes可以省略</span></span><br><span class="line">    model = TransformerModel.from_pretrained(</span><br><span class="line">        path,</span><br><span class="line">        checkpoint_file=<span class="string">'model.pt'</span>,</span><br><span class="line">        data_name_or_path=<span class="string">'.'</span>,</span><br><span class="line">        bpe=<span class="string">'subword_nmt'</span>,</span><br><span class="line">        bpe_codes=path+<span class="string">"/bpecode"</span></span><br><span class="line">    )</span><br><span class="line">    model.cuda(device=device)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">bt_model = load_fairseq_tm(<span class="string">'./model/wmt16.en-de.joined-dict.transformer'</span>, <span class="number">0</span>)</span><br><span class="line">output = bt_model.translate(<span class="string">'Hello world!'</span>)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><p>得到的结果是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hallo Welt !</span><br></pre></td></tr></table></figure><p>那么，这个过程都干了哪些事呢？我们对此进行了详细的分析。</p><h2 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h2><p>在分析之前，我们先介绍一下BPE算法。以下内容来源于<a href="http://txshi-mt.com/2019/02/28/NMT-Tutorial-3e2-subword/" target="_blank" rel="noopener">NMT Tutorial 3扩展e第2部分. Subword</a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>按照布隆菲尔德的理论，词被认为是人类语言中能自行独立存在的最小单位，是“最小自由形式”。因此，对西方语言做NLP时，以词为基石是一个很自然的想法。</p><p>但是将某个语言的词穷举出来是不太现实的。首先，名词、动词、形容词、副词这四种属于开放词类，总会有新的词加入进来。其次，网络用语会创造出更多新词，或者为某个词给出不规则的变形。最后，以德语为代表的语言通常会将几个基本词组合起来，形成一个复合词，例如Abwasserbehandlungsanlage “污水处理厂”可以被细分为Abwasser、behandlungs和Anlage三个部分。</p><p>即便是存在某个语言能获得其完整词表，词表的数量也会非常庞大，使得模型复杂度很高，训练起来很难。对于以德语、西班牙语、俄语为代表的<strong>屈折语</strong>，也会存在类似的问题（例如西班牙语动词可能有80种变化）。</p><p>因此，在机器翻译等任务中，从训练语料构造词表时，通常会过滤掉出现频率很低的单词，并将这些单词统一标记为<strong>UNK（Unknown）</strong>。根据Zipf定律，这种做法能筛掉很多不常见词，简化模型结构，而且可以起到部分防止过拟合的作用。此外，模型上线做推断时，也有很大概率会遇到在训练语料里没见过的词，这些词也会被标为UNK。所有不在词表里被标记为UNK的词，通常被称作<strong>集外词</strong>（Out Of Vocabulary，OOV）或者<strong>未登录词</strong>。</p><p>对未登录词的处理是机器翻译领域里一个十分重要的问题。<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">sennrich2016</a>认为，对于某些未登录词的翻译可能是”透明“的，包括</p><ul><li>命名实体，例如人名、地名等。对于这些词，如果目标语言和源语言的字母体系相同，可能可以直接抄写；如果不同，需要做些转写。例如将英语的Barack Obama转写成俄语的Барак Обама</li><li>借词，可以通过字母级别的翻译做到，例如将claustrophobia翻译成德语的Klaustrophobie和俄语的Клаустрофобия</li><li>词素复杂的词，例如通过组合或者屈折变化得到的词，可以将其进一步拆分为词素，通过分别翻译各个词素的得到结果。例如将英语的solar system翻译成德语的Sonnensystem或者匈牙利语的Naprendszer</li></ul><p>因此，将词拆分为更细粒度的subword，可以有助于处理OOV问题。另外传统tokenization方法不利于模型学习词缀之间的关系。E.g. 模型学到的“old”, “older”, and “oldest”之间的关系无法泛化到“smart”, “smarter”, and “smartest”。</p><p>由此，<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">sennrich2016</a>文章还同时指出使用一种称为<strong>“比特对编码”（Byte Pair Encoding——BPE）</strong>的算法可以将词拆分为更细粒度的subword。但是BPE对单词的划分是纯基于统计的，得到的subword所蕴含的词素，或者说形态学信息，并不明显。除此BPE之外，Morfessor是一种基于形态学的分词器，它使用的是无监督学习的方法，能达到不错的准确率。最后，2016年FAIR提出的一种基于subword的词嵌入表示方法fastText。但是本文只关注BPE算法，其余可以参考文章<a href="http://txshi-mt.com/2019/02/28/NMT-Tutorial-3e2-subword/" target="_blank" rel="noopener">NMT Tutorial 3扩展e第2部分. Subword</a>。</p><p>除去subword方法以外，还可以将词拆成字符，为每个字符训练一个字符向量。这种方法很直观，也很有效，不过无需太费笔墨来描述。关于字符向量的优秀工作，可以参考<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00051/43387/Enriching-Word-Vectors-with-Subword-Information" target="_blank" rel="noopener">Bojanowski2017</a>的“相关工作”部分。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>BPE算法[gage1994]的本质实际上是一种数据压缩算法。<strong>数据压缩的一般做法都是将常见比特串替换为更短的表示方法</strong>，而BPE也不例外。更具体地说，BPE是找出最常出现的相邻字节对，将其替换成一个在原始数据里没有出现的字节，一直循环下去，直到找不到最常出现的字节对或者所有字节都用光了为止。后期使用时需要一个替换表来重建原始数据。例如，对”lwlwlwlwrr”使用BPE算法，会先把lw替换为a，得到”aaaarr”，然后把”aa”替换为”b”，得到”bbrr”。此时所有相邻字节对”bb”、”br”、”rr”的出现次数相等，迭代结束，输出替换表{“b” -&gt; “aa”, “a” -&gt; “lw”}。</p><ul><li>优点：可以有效地平衡词汇表大小和步数(编码句子所需的token数量)。</li><li>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li></ul><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ol><li>准备足够大的训练语料</li><li>确定期望的subword词表大小</li><li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li><li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li><li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li></ol><p>停止符”&lt;/w&gt;”的意义在于表示subword是词后缀。举例来说：”st”字词不加”&lt;/w&gt;”可以出现在词首如”st ar”，加了”&lt;/w&gt;”表明改字词位于词尾，如”wide st&lt;/w&gt;”，二者意义截然不同。</p><p>每次合并后词表可能出现3种变化：</p><ul><li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li><li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li><li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li></ul><p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p><p><strong>例子</strong></p><p>输入：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”。输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w es t &lt;/w&gt;': 6, 'w i d es t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”。输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est &lt;/w&gt;': 6, 'w i d est &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 3, 以此类推，最高频连续字节对为”est”和”&lt;/w&gt;” 输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>……</p><p>Iter n, 继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。</p><p>BPE算法的核心学习过程可以写做如下Python代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab)</span>:</span></span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols)<span class="number">-1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair, v_in)</span>:</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line">vocab = &#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w e s t &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d e s t &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print output</span></span><br><span class="line"><span class="comment"># ('e', 's')</span></span><br><span class="line"><span class="comment"># ('es', 't')</span></span><br><span class="line"><span class="comment"># ('est', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('l', 'o')</span></span><br><span class="line"><span class="comment"># ('lo', 'w')</span></span><br><span class="line"><span class="comment"># ('n', 'e')</span></span><br><span class="line"><span class="comment"># ('ne', 'w')</span></span><br><span class="line"><span class="comment"># ('new', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('w', 'i')</span></span><br><span class="line"><span class="comment"># ('wi', 'd')</span></span><br><span class="line"><span class="comment"># ('wid', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', 'e')</span></span><br><span class="line"><span class="comment"># ('lowe', 'r')</span></span><br><span class="line"><span class="comment"># ('lower', '&lt;/w&gt;')</span></span><br></pre></td></tr></table></figure><h3 id="编码和解码"><a href="#编码和解码" class="headerlink" title="编码和解码"></a>编码和解码</h3><h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4><p>在之前的算法中，我们已经得到了subword的词表（即常说的<code>code</code>文件），且该词表已经按照频率从高到低进行排序了。那么我们就可以对单词进行编码（下文的<code>subword-nmt</code>小节中，利用得到的<code>code.file</code>对<code>./en.txt</code>进行编码得到<code>result1.txt</code>就利用了当前要介绍的编码过程）。</p><p>以单词“where”为例，首先按照字符拆分开，然后查找<code>code</code>文件，逐对合并，优先合并频率靠前的字符对。<code>85 319 9 15</code> 表示在该字符对在<code>code</code>文件中的频率排名。</p><blockquote><p>根据我自己的实验，<code>e&lt;/w&gt;</code>可以直接合并，所以这里的频率排名直接是1，即使<code>code</code>文件中无<code>e &lt;/w&gt;</code>。</p></blockquote><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/2019051614192910.jpg" alt="img" style="zoom: 67%;"></p><p> 如果仍然有子字符串没被替换但所有token都已迭代完毕，则有两种做法，一种是将剩余的子词替换为特殊token，如<unk>。另外一种比较常用，由于未登录词通常会被这种方法拆成若干个subword，因此通常会向不在原来词表的subword后面写明一个分隔符，通常是@@。例如，假如要编码的词是said</unk></p><ol><li>若这个词的子词<code>s a</code>在词表中，但是<code>sa i</code>和<code>i d&lt;/w&gt;</code>不在词表里，<code>encode</code>只能得到<code>(&#39;sa&#39;, &#39;i&#39;, &#39;d&#39;)</code>，那么输出会是<code>sa@@ i@@ d</code>。</li><li>若子词<code>s a</code>和<code>i d</code>在词表中，但是<code>sa i</code>和<code>i d&lt;/w&gt;</code>不在词表里，那么输出仍然是<code>sa@@ i@@ d</code>。</li><li>若子词<code>s a</code>和<code>i d&lt;/w&gt;</code>在词表中，但是<code>sa id&lt;/w&gt;</code>不在词表里，那么输出是<code>sa@@ id</code>。</li><li>若子词<code>s a</code>，<code>i d&lt;/w&gt;</code>和<code>sa id&lt;/w&gt;</code>在词表中，那么输出是<code>said</code>。</li><li>若仅有<code>sa id&lt;/w&gt;</code>在词表中，那么输出是<code>s@@ a@@ i@@ d</code>。</li><li>若仅有<code>i d&lt;/w&gt;</code>和<code>sa id&lt;/w&gt;</code>在词表中，那么输出是<code>s@@ a@@ id</code>。</li></ol><p>编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。</p><h4 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h4><p>将所有的tokens拼在一起，如果有<code>@@</code>符号则去除（下文的<code>后处理</code>小节中<code>self.remove_bpe</code>函数，就利用了当前小节要介绍的解码过程）。</p><p>例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 编码序列</span><br><span class="line">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 解码序列</span><br><span class="line">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure><h3 id="subword-nmt"><a href="#subword-nmt" class="headerlink" title="subword-nmt"></a>subword-nmt</h3><p>安装subword-nmt</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install subword-nmt</span><br></pre></td></tr></table></figure><h4 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h4><p>先准备一个语料库。例如：链接：<a href="https://pan.baidu.com/s/1BAWDeAw5QYXS7xCrLIBIAw，提取码：kfy9" target="_blank" rel="noopener">https://pan.baidu.com/s/1BAWDeAw5QYXS7xCrLIBIAw，提取码：kfy9</a></p><p><strong>生成codevocabulary和：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt learn-joint-bpe-and-vocab -i ./en.txt -o ./code.file --write-vocabulary voc.txt</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li>-i后面的参数是输入文件名</li><li>-o后面是输出的code文件文件名</li><li>—write-vocabulary后面是输出字典的文件名</li></ul><p>其他参数说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">usage: subword-nmt learn-joint-bpe-<span class="keyword">and</span>-vocab [-h] --input PATH [PATH ...]</span><br><span class="line">                                             --output PATH [--symbols SYMBOLS]</span><br><span class="line">                                             [--separator STR]</span><br><span class="line">                                             --write-vocabulary PATH</span><br><span class="line">                                             [PATH ...] [--min-frequency FREQ]</span><br><span class="line">                                             [--total-symbols] [--verbose]</span><br><span class="line"></span><br><span class="line">learn BPE-based word segmentation</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message <span class="keyword">and</span> exit</span><br><span class="line">  --input PATH [PATH ...], -i PATH [PATH ...]</span><br><span class="line">                        Input texts (multiple allowed).</span><br><span class="line">  --output PATH, -o PATH</span><br><span class="line">                        Output file <span class="keyword">for</span> BPE codes.</span><br><span class="line">  --symbols SYMBOLS, -s SYMBOLS</span><br><span class="line">                        Create this many new symbols (each representing a</span><br><span class="line">                        character n-gram) (default: <span class="number">10000</span>))</span><br><span class="line">  --separator STR       Separator between non-final subword units (default:</span><br><span class="line">                        <span class="string">'@@'</span>))</span><br><span class="line">  --write-vocabulary PATH [PATH ...]</span><br><span class="line">                        Write to these vocabulary files after applying BPE.</span><br><span class="line">                        One per input text. Used <span class="keyword">for</span> filtering <span class="keyword">in</span> apply_bpe.py</span><br><span class="line">  --min-frequency FREQ  Stop <span class="keyword">if</span> no symbol pair has frequency &gt;= FREQ (default:</span><br><span class="line">                        <span class="number">2</span>))</span><br><span class="line">  --total-symbols, -t   subtract number of characters <span class="keyword">from</span> the symbols to be</span><br><span class="line">                        generated (so that <span class="string">'--symbols'</span> becomes an estimate <span class="keyword">for</span></span><br><span class="line">                        the total number of symbols needed to encode text).</span><br><span class="line">  --verbose, -v         verbose mode.</span><br></pre></td></tr></table></figure><p>我们可以看一下生成的code.file和voc.txt。</p><p>code.file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#version: 0.2</span><br><span class="line">t h</span><br><span class="line">i n</span><br><span class="line">th e&lt;/w&gt;</span><br><span class="line">a n</span><br><span class="line">r e</span><br><span class="line">t i</span><br><span class="line">e n</span><br><span class="line">o n</span><br><span class="line">an d&lt;/w&gt;</span><br><span class="line">e r</span><br><span class="line">···</span><br></pre></td></tr></table></figure><p>voc.txt部分内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">···</span><br><span class="line">ary 14</span><br><span class="line">apart 14</span><br><span class="line">conscientiously 14</span><br><span class="line">flight 14</span><br><span class="line">association 14</span><br><span class="line">represent 14</span><br><span class="line">th 14</span><br><span class="line">activity 14</span><br><span class="line">standard 14</span><br><span class="line">call 14</span><br><span class="line">jia 14</span><br><span class="line">solid 14</span><br><span class="line">seven 14</span><br><span class="line">···</span><br></pre></td></tr></table></figure><p>这里需要注意的是，<code>code.file</code>文件一共有10001行，而<code>voc.txt</code>文件一共有8760行，并且<code>voc.txt</code>含有一部分带有<code>@@</code>的行。</p><p>那么这里的<code>code.file</code>和<code>voc.txt</code>有什么关系呢？我们继续进行探索。</p><p>安装完subword-nmt之后，我们可以在终端输入<code>subword-nmt -h</code>，得到内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">(base) PS E:\Working\learn_bpe&gt; subword-nmt -h</span><br><span class="line">usage: subword-nmt [-h] &#123;learn-bpe,apply-bpe,get-vocab,learn-joint-bpe-and-vocab&#125; ...</span><br><span class="line"></span><br><span class="line">subword-nmt: unsupervised word segmentation <span class="keyword">for</span> neural machine translation and text generation</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  &#123;learn-bpe,apply-bpe,get-vocab,learn-joint-bpe-and-vocab&#125;</span><br><span class="line">                        <span class="built_in">command</span> to run. Run one of the commands with <span class="string">'-h'</span> <span class="keyword">for</span> more info.</span><br><span class="line"></span><br><span class="line">                        apply-bpe: apply given BPE operations to input text.</span><br><span class="line">                        learn-joint-bpe-and-vocab: executes recommended workflow <span class="keyword">for</span> joint BPE.</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br></pre></td></tr></table></figure><p>也就是说，<code>subword-nmt</code>有<code>learn-bpe,apply-bpe,get-vocab,learn-joint-bpe-and-vocab</code>方法，继续输入<code>subword-nmt learn-bpe -h</code>可以查看子函数的用法。</p><p>详细的探索这几个函数的用法之后，可以发现如下结论。</p><ol><li><code>learn-joint-bpe-and-vocab</code>其实是三条指令的合体。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt learn-joint-bpe-and-vocab -i ./en.txt -o ./code.file --write-vocabulary voc.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述指令等价于</span></span><br><span class="line">subword-nmt learn-bpe -i ./en.txt  -o ./code.file</span><br><span class="line">subword-nmt apply-bpe -i ./en.txt -c ./code.file -o result1.txt</span><br><span class="line">subword-nmt get-vocab -i ./result1.txt -o voc.txt</span><br></pre></td></tr></table></figure><ol><li><p><code>get-vocab</code>函数会对文件中出现的单词以及对应的频率进行统计，得到<code>voc.txt</code>文件，该过程不需要<code>code.file</code>文件。</p></li><li><p><code>code.file</code>和<code>voc.txt</code>关系是：首先利用<code>learn-bpe</code>从<code>en.txt</code>文件中学习bpe分词规则，然后利用该规则对<code>en.txt</code>编码，统计编码之后文件的词语和词频得到<code>voc.txt</code>文件。所以两者并不是意义对应的关系，而且哪个文件行数更多也不一定。</p></li></ol><p><strong>使用bpe编码</strong></p><p>在使用learn-bpe功能得到code后，可以使用apply-bpe来对语料进行编码。值得注意的是，这里解码时并不需要用到<code>voc.txt</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subword-nmt apply-bpe -i ./en.test.txt -c ./code.file -o result.txt</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>-i</code> 后面是输入的待解码文件名</li><li><code>-c</code> 后面跟着learn-bpe步骤得到的code文件</li><li><code>-o</code> 结果输出文件</li></ul><p>我们可以查看结果，就会自动根据bpe生成的code文件对语料进行分割。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">beijing , 1 mar ( xinhua ) -- tian feng@@ shan , former heilongjiang governor who is 5@@ 9 years old , was appointed minister of land and resources today .</span><br><span class="line">tian feng@@ shan , who was born in zhao@@ yuan county , heilongjiang province , took part in work since july 196@@ 1 and joined the cpc in march 1970 .</span><br><span class="line">this should be a natural process set off by economic development ; the &quot; third tier construction &quot; of the 1960s involving fac@@ tory relocation was something entirely different .</span><br><span class="line">we must also realize however that from the angle of changing the pattern of resource allocation , we have not yet made the big breakthrough in reform .</span><br><span class="line">with regard to joining the world trade organization , one recent reaction has been blind optim@@ ism and the belief that china will profit whatever it does .</span><br><span class="line">since these areas where objective conditions are not particularly good can achieve this , other areas where conditions are better can naturally do the same .</span><br><span class="line">the objective trend of globalization is calling for international cooperation on a global scale , and a global cooperation has far exceeded the scope of the economy .</span><br></pre></td></tr></table></figure><p><strong>解码</strong></p><p>那么我们的文件怎么恢复到bpe编码之前的结果呢？</p><p>只需要执行下面指令即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sed -r <span class="string">'s/(@@ )|(@@ ?$)//g'</span> result.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将解码结果保存到文件中</span></span><br><span class="line">sed -r <span class="string">'s/(@@ )|(@@ ?$)//g'</span> result.txt &gt; restore.txt</span><br></pre></td></tr></table></figure><p>我们恢复之后的结果是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">beijing , 1 mar ( xinhua ) -- tian fengshan , former heilongjiang governor who is 59 years old , was appointed minister of land and resources today .</span><br><span class="line">tian fengshan , who was born in zhaoyuan county , heilongjiang province , took part in work since july 1961 and joined the cpc in march 1970 .</span><br><span class="line">this should be a natural process set off by economic development ; the &quot; third tier construction &quot; of the 1960s involving factory relocation was something entirely different .</span><br><span class="line">we must also realize however that from the angle of changing the pattern of resource allocation , we have not yet made the big breakthrough in reform .</span><br><span class="line">with regard to joining the world trade organization , one recent reaction has been blind optimism and the belief that china will profit whatever it does .</span><br><span class="line">since these areas where objective conditions are not particularly good can achieve this , other areas where conditions are better can naturally do the same .</span><br><span class="line">the objective trend of globalization is calling for international cooperation on a global scale , and a global cooperation has far exceeded the scope of the economy .</span><br></pre></td></tr></table></figure><h4 id="Python接口"><a href="#Python接口" class="headerlink" title="Python接口"></a>Python接口</h4><p>可以用命令<code>pip install subword-nmt</code>安装包<code>subword-nmt</code>以后，可以使用如下代码得到BPE的分词结果，以及将BPE的分词方法用到测试语料上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> subword_nmt <span class="keyword">import</span> apply_bpe, learn_bpe</span><br><span class="line"><span class="comment"># 得到分词结果，写到../data/toy_bpe.txt这个文件中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../data/toy_vocab.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> in_file, \</span><br><span class="line">        open(<span class="string">'../data/toy_bpe.txt'</span>, <span class="string">'w+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> out_file:</span><br><span class="line">    <span class="comment"># 得到分词结果，写到../data/toy_bpe.txt这个文件中</span></span><br><span class="line">    <span class="comment"># 1500是最后BPE词表大小，is_dict说明输入文件是个词表文件，格式为"&lt;单词&gt; &lt;次数&gt;"</span></span><br><span class="line">    learn_bpe.learn_bpe(in_file, out_file, <span class="number">1500</span>, verbose=<span class="literal">True</span>, is_dict=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取../data/toy_bpe.txt分词结果，并作用于../data/bpe_test_raw.txt中的文本，最后写到../data/bpe_test_processed.txt文件中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../data/bpe_test_raw.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> in_file, \</span><br><span class="line">        open(<span class="string">'../data/bpe_test_processed.txt'</span>, <span class="string">'w+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> out_file, \</span><br><span class="line">        open(<span class="string">'../data/toy_bpe.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> code_file:</span><br><span class="line">    <span class="comment"># 构造BPE词表</span></span><br><span class="line">    bpe = apply_bpe.BPE(code_file)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> in_file:</span><br><span class="line">        <span class="comment"># 使用BPE分词</span></span><br><span class="line">        out_file.write(bpe.process_line(line))</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>subword可以平衡词汇量和对未知词的覆盖。 极端的情况下，我们只能使用26个token（即字符）来表示所有英语单词。一般情况，建议使用16k或32k子词足以取得良好的效果，Facebook <a href="https://github.com/pytorch/fairseq/tree/main/examples/roberta" target="_blank" rel="noopener">RoBERTa</a>甚至建立的多达50k的词表。</p><h2 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h2><p>补充完毕BPE算法的原理之后，我们开始对该源码进行分析。首先来看模型加载部分。</p><p>模型加载的核心函数为<code>fairseq/hub_utils.py: from_pretrained</code>函数。在该函数的会调用<code>checkpoint_utils.load_model_ensemble_and_task</code>函数。该函数不仅加载了模型权重，而且会初始化task。我们重点关注这个初始化过程。初始化该task时，默认会初始化为<code>translation</code>任务。</p><p>然后跳入函数<code>fairseq/tasks/translation.py</code>中，可以看到在<code>setup_task</code>函数中，会读取<code>model/wmt16.en-de.joined-dict.transformer/dict.en.txt</code>和<code>model/wmt16.en-de.joined-dict.transformer/dict.de.txt</code>文件，然后放到<code>fairseq.tasks.translation.TranslationTask</code>的<code>src_dict</code>和<code>tgt_dict</code>中。另外值得注意的是<code>fairseq.data.dictionary.Dictionary</code>的实例，在实例化该类的时候，会在最前面加上<code>bos=&quot;&lt;s&gt;&quot;, pad=&quot;&lt;pad&gt;&quot;, eos=&quot;&lt;/s&gt;&quot;, unk=&quot;&lt;unk&gt;&quot;</code>，因此虽然这两个txt文件中都有32764行（两个文件内容一模一样），最终都会有32768行，与翻译模型的输出维度一致。</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427221019923.png" alt="image-20220427221019923" style="zoom:50%;"></p><p>加载模型并初始化task之后，<code>from_pretrained</code>函数接着实例化了<code>hub_utils.GeneratorHubInterface</code>。我们接着看该类在实例化的时候会做些什么。</p><p>从下图可以看到该初始化函数依次做了：从task中创建<code>src_dict</code>和<code>tgt_dict</code>属性（与<code>fairseq.tasks.translation.TranslationTask</code>的<code>src_dict</code>和<code>tgt_dict</code>一致），然后加载了<code>align_dict、tokenizer、bpe</code>。</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427223531357.png" alt="image-20220427223531357" style="zoom:50%;"></p><p>我们这里重点关注一下<code>bpe</code>的初始化过程，单步调试进入到<code>fairseq/registry.py</code>文件后，可以发现fairseq支持的所有bpe有：<code>dict_keys([&#39;bytes&#39;, &#39;gpt2&#39;, &#39;hf_byte_bpe&#39;, &#39;bert&#39;, &#39;characters&#39;, &#39;fastbpe&#39;, &#39;byte_bpe&#39;, &#39;sentencepiece&#39;, &#39;subword_nmt&#39;])</code>。我们这里在初始化模型时传入了<code>bpe=&#39;subword_nmt</code>参数，所以我们重点关注一下<code>subword_nmt</code>的初始化方式。</p><p>该初始化过程的详细过程在<code>fairseq/data/encoders/subword_nmt_bpe.py</code>文件的<code>SubwordNMTBPE</code>类的<code>__init__</code>函数中。从下图中可以看出，该函数会读取<code>args.bpe_codes</code>对应的文件，也就是<code>&#39;./model/wmt16.en-de.joined-dict.transformer/bpecodes&#39;</code>文件，用于实例化<code>subword_nmt.apply_bpe.BPE</code>得到对应<code>self.bpe</code>，同时<code>SubwordNMTBPE</code>类还有对应的<code>encode</code>和<code>decode</code>函数。</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427233419637.png" alt="image-20220427233419637" style="zoom:50%;"></p><p>介绍完BPE的初始化，我们接着回到<code>hub_utils.GeneratorHubInterface</code>类中，此时的<code>self.bpe</code>的类型为<code>fairseq.data.encoders.subword_nmt_bpe.SubwordNMTBPE</code>。</p><p>由此，翻译模型的模型加载部分已经介绍完了。</p><h2 id="前向推理"><a href="#前向推理" class="headerlink" title="前向推理"></a>前向推理</h2><p>从上面的调用关系来看，翻译模型进行推理的函数是<code>translate</code>。我们调试进入该函数，发现该函数位于<code>/root/anaconda3/lib/python3.8/site-packages/fairseq/hub_utils.py</code>文件中。核心代码如下：</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427105444788.png" alt="image-20220427105444788" style="zoom:50%;"></p><p>可以看到，翻译时需要经过三个关键步骤：<code>encode</code>、<code>generate</code>和<code>decode</code>。这里我们先关注预处理和后处理步骤。关键代码如下：</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220427110136773.png" alt="image-20220427110136773" style="zoom:50%;"></p><p>可以看出来预处理主要流程为<code>分词-&gt;BPE-&gt;binarize</code>，后处理的主要步骤是<code>string-&gt;去除bpe-&gt;去分词</code>。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>我们接着来看预处理过程。在使用该模型的时候，并没有用到分词，而是直接使用了BPE的方式，所以我们跳过<code>self.tokenize</code>函数，首先来看<code>self.apply_bpe</code>函数。该函数会调用<code>SubwordNMTBPE.encode of &lt;fairseq.data.encoders.subword_nmt_bpe.SubwordNMTBPE&gt;</code>，我们这里先不管这个函数干了啥，先介绍它的输入输出。其输入为：<code>&#39;Hello world!&#39;</code>，输出为<code>&#39;H@@ ello world@@ !&#39;</code>。</p><p>接着我们来看<code>self.binarize</code>，它的输入是<code>&#39;H@@ ello world@@ !&#39;</code>，输出是<code>tensor([  190,  7016, 29382,    88,     2])</code>。该函数会调用<code>Dictionary.encode_line of &lt;fairseq.data.dictionary.Dictionary&gt;</code>。查询前面的<code>src_dict</code>，将字符串映射到唯一ID上（ID简单理解为<code>model/wmt16.en-de.joined-dict.transformer/dict.en.txt</code>中的行数+4）。</p><h2 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h2><p>介绍完预处理流程，我们来看下网络结构，网络结构如下（因为该网络结构很长，所以只摘出来关键部分）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">GeneratorHubInterface(</span><br><span class="line">  (models): ModuleList(</span><br><span class="line">    (<span class="number">0</span>): TransformerModel(</span><br><span class="line">      (encoder): TransformerEncoder(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (embed_tokens): Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">        (embed_positions): SinusoidalPositionalEmbedding()</span><br><span class="line">        (layers): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): TransformerEncoderLayer(</span><br><span class="line">            (self_attn): MultiheadAttention(</span><br><span class="line">              (dropout_module): FairseqDropout()</span><br><span class="line">              (k_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (v_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (q_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (out_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">            (self_attn_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (dropout_module): FairseqDropout()</span><br><span class="line">            (activation_dropout_module): FairseqDropout()</span><br><span class="line">            (fc1): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (fc2): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (final_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          <span class="comment"># 省略(1~5)TransformerEncoderLayer</span></span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (decoder): TransformerDecoder(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (embed_tokens): Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">        (embed_positions): SinusoidalPositionalEmbedding()</span><br><span class="line">        (layers): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): TransformerDecoderLayer(</span><br><span class="line">            (dropout_module): FairseqDropout()</span><br><span class="line">            (self_attn): MultiheadAttention(</span><br><span class="line">              (dropout_module): FairseqDropout()</span><br><span class="line">              (k_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (v_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (q_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (out_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">            (activation_dropout_module): FairseqDropout()</span><br><span class="line">            (self_attn_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (encoder_attn): MultiheadAttention(</span><br><span class="line">              (dropout_module): FairseqDropout()</span><br><span class="line">              (k_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (v_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (q_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">              (out_proj): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">            (encoder_attn_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (fc1): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (fc2): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1024</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (final_layer_norm): LayerNorm((<span class="number">1024</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          <span class="comment"># 省略(1~5)TransformerDecoderLayer</span></span><br><span class="line">        )</span><br><span class="line">        (output_projection): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">32768</span>, bias=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>总结概括一下该结构，如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TransformerEncoder(</span><br><span class="line">    FairseqDropout(), </span><br><span class="line">    Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>), </span><br><span class="line">    SinusoidalPositionalEmbedding(), </span><br><span class="line">    <span class="number">6</span>个TransformerEncoderLayer</span><br><span class="line">)</span><br><span class="line">TransformerDecoder(</span><br><span class="line">    FairseqDropout(), </span><br><span class="line">    Embedding(<span class="number">32768</span>, <span class="number">1024</span>, padding_idx=<span class="number">1</span>), </span><br><span class="line">    SinusoidalPositionalEmbedding(), </span><br><span class="line">    <span class="number">6</span>个TransformerDecoderLayer, </span><br><span class="line">    Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">32768</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>也就是说，在该模型中，使用了<a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener">torch.nn.Embedding</a>层对输入进行了Embedding并学习。</p><p>接着我们看下模型推理部分——<code>generate</code>函数。</p><p>该函数首先会调用<code>FairseqTask.build_generator of &lt;fairseq.tasks.translation.TranslationTask&gt;</code>函数，并传入<code>gen_args</code>参数（该参数中包含了<code>beam</code>）。在该函数会执行<code>search_strategy = search.BeamSearch(self.target_dictionary)</code>函数实例化BeamSearch（使用到了<code>model/wmt16.en-de.joined-dict.transformer/dict.de.txt</code>），并与模型一块放到<code>SequenceGenerator</code>中进行实例化，而实际进行推理时也是调用的<code>SequenceGenerator.generate of SequenceGenerator</code>，同时进行模型推理+BeamSearch过程。</p><p>具体细节我们先不关注，先说下输入输出。其输入为</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220428003141180.png" alt="image-20220428003141180" style="zoom:50%;"></p><p>经过推理之后，输出结果为（下面5个结果的tokens是不一样的，这里显示不出来）：</p><p><img src="/DeepLearningApplications/自然语言处理/fairseq翻译任务解读/image-20220428003252483.png" alt="image-20220428003252483" style="zoom:50%;"></p><h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>最后，我们来看下后处理流程。后处理的对应的代码是<code>[self.decode(hypos[0][&quot;tokens&quot;]) for hypos in batched_hypos]</code>。也就是将<code>tensor([12006,   165,   488,    88,     2], device=&#39;cuda:0&#39;)</code>输入到<code>self.decode</code>函数中。该函数的主要流程是<code>string-&gt;去除bpe-&gt;去分词</code>。</p><p>我们先来看<code>self.string</code>函数，该函数与<code>self.binarize</code>函数相反，它会调用<code>Dictionary.string of &lt;fairseq.data.dictionary.Dictionary&gt;</code>，查询前面的<code>tgt_dict</code>，将ID映射回字符串（ID简单理解为<code>model/wmt16.en-de.joined-dict.transformer/dict.de.txt</code>中的行数+4）。它的输入为<code>tensor([12006,   165,   488,    88,     2], device=&#39;cuda:0&#39;)</code>，输出为<code>&#39;Hall@@ o Welt !&#39;</code>。</p><p>接着来看<code>self.remove_bpe</code>函数，它与<code>self.apply_bpe</code>函数作用相反，该函数会调用<code>SubwordNMTBPE.decode of &lt;fairseq.data.encoders.subword_nmt_bpe.SubwordNMTBPE&gt;</code>，我们这里先不管这个函数干了啥，先介绍它的输入输出。其输入为：<code>&#39;Hall@@ o Welt !&#39;</code>，输出为<code>&#39;Hallo Welt !&#39;</code>。</p><p>同样的，最后，该过程并没有调用<code>self.detokenize</code>，这里先不管。</p><h2 id="训练数据准备"><a href="#训练数据准备" class="headerlink" title="训练数据准备"></a>训练数据准备</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>下文主要来源于<a href="https://www.cnblogs.com/mmxy/p/14076930.html" target="_blank" rel="noopener">WMT14 en-de翻译数据集预处理步骤</a></p><p>fairseq提供了一份wmt14英德数翻译据集的<a href="https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-wmt14en2de.sh" target="_blank" rel="noopener">预处理脚本</a>，简单结合其代码分析一下其处理步骤：</p><p>1、下载mosesdecoder。mosesdecoder的使用文档在<a href="http://www.statmt.org/moses/?n=Moses.Baseline" target="_blank" rel="noopener">这里</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Moses github repository (for tokenization scripts)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/moses-smt/mosesdecoder.git</span><br></pre></td></tr></table></figure><p>2、下载subword nmt。这个开源库是用于构造bpecodes及其字典的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Subword NMT repository (for BPE pre-processing)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/rsennrich/subword-nmt.git</span><br></pre></td></tr></table></figure><p>3、</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">SCRIPTS</span>=mosesdecoder/scripts      <span class="comment"># 定义SCRIPTS变量，指向mosesdecoder的脚本文件夹</span></span><br><span class="line"><span class="attr">TOKENIZER</span>=<span class="variable">$SCRIPTS</span>/tokenizer/tokenizer.perl      <span class="comment"># 定义TOKENIZER变量，指向mosesdecoder的tokenizer.perl, 用来分词</span></span><br><span class="line"><span class="attr">CLEAN</span>=<span class="variable">$SCRIPTS</span>/training/clean-corpus-n.perl      <span class="comment"># 定义CLEAN变量，指向mosesdecoder的clean-corpus-n.perl，clean的主要作用是保留指定长度的数据</span></span><br><span class="line"><span class="attr">NORM_PUNC</span>=<span class="variable">$SCRIPTS</span>/tokenizer/normalize-punctuation.perl      <span class="comment"># 定义NORM_PUNC变量，指向normalize-punctuation.perl,用来将标点符号规范化</span></span><br><span class="line"><span class="attr">REM_NON_PRINT_CHAR</span>=<span class="variable">$SCRIPTS</span>/tokenizer/remove-non-printing-char.perl      <span class="comment"># 定义REM_NON_PRINT_CHAR变量，指向remove-non-printing-char.perl,去除语料中的非打印字符 </span></span><br><span class="line"><span class="attr">BPEROOT</span>=subword-nmt/subword_nmt      <span class="comment"># 定义BPEROOT变量，指向subword_nmt根目录。</span></span><br><span class="line"><span class="attr">BPE_TOKENS</span>=<span class="number">40000</span>      <span class="comment"># 指定BPE TOKENS的数量为40000</span></span><br></pre></td></tr></table></figure><p>4、</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定语料来源，其中包括了训练、验证、测试语料</span></span><br><span class="line">URLS=(</span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/dev.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt14/test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 指定文件名，和上面URLS对应</span></span><br><span class="line">FILES=(</span><br><span class="line">    <span class="string">"training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"dev.tgz"</span></span><br><span class="line">    <span class="string">"test-full.tgz"</span>      <span class="comment"># 只要test-full是测试集，上面四个都是训练+验证集。</span></span><br><span class="line">)</span><br><span class="line">CORPORA=(</span><br><span class="line">    <span class="string">"training/europarl-v7.de-en"</span></span><br><span class="line">    <span class="string">"commoncrawl.de-en"</span></span><br><span class="line">    <span class="string">"training/news-commentary-v12.de-en"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>5、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1705.03122</span></span><br><span class="line"><span class="comment"># 如果指定参数--icml17，就将语料2替换成wmt14的语料，而不是使用wmt17的语料，这是为了和ConvS2S论文保持一致</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> == <span class="string">"--icml17"</span> ]; <span class="keyword">then</span></span><br><span class="line">    URLS[2]=<span class="string">"http://statmt.org/wmt14/training-parallel-nc-v9.tgz"</span></span><br><span class="line">    FILES[2]=<span class="string">"training-parallel-nc-v9.tgz"</span></span><br><span class="line">    CORPORA[2]=<span class="string">"training/news-commentary-v9.de-en"</span></span><br><span class="line">    OUTDIR=wmt14_en_de      <span class="comment"># 指定输出文件夹名</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    OUTDIR=wmt17_en_de</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>6、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">src=en      <span class="comment"># 源语言为英文</span></span><br><span class="line">tgt=de      <span class="comment"># 目标语言是德语</span></span><br><span class="line">lang=en-de      <span class="comment"># 语言对为英德</span></span><br><span class="line">prep=<span class="variable">$OUTDIR</span>      <span class="comment"># 文件夹前缀为$OUTDIR</span></span><br><span class="line">tmp=<span class="variable">$prep</span>/tmp      <span class="comment"># 文件夹$OUTDIR内有一个tmp文件夹</span></span><br><span class="line">orig=orig      <span class="comment"># orig=orig</span></span><br><span class="line">dev=dev/newstest2013      <span class="comment"># 开发集使用newstest2013</span></span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$orig</span> <span class="variable">$tmp</span> <span class="variable">$prep</span>      <span class="comment"># 递归创建上面定义的文件夹，包括orig文件夹，$OUTDIR/tmp文件夹，$OUTDIR文件夹</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$orig</span>      <span class="comment"># 切换到orig文件夹中</span></span><br></pre></td></tr></table></figure><p>7、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$&#123;#URLS[@]&#125;</span>;++i)); <span class="keyword">do</span>      <span class="comment"># 迭代每一个URLS</span></span><br><span class="line">    file=<span class="variable">$&#123;FILES[i]&#125;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span> already exists, skipping download"</span>      <span class="comment"># 如果文件之前已经下载下来了，就跳过</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        url=<span class="variable">$&#123;URLS[i]&#125;</span>      </span><br><span class="line">        wget <span class="string">"<span class="variable">$url</span>"</span>      <span class="comment"># 否则下载</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span>      </span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> successfully  downloaded."</span>       <span class="comment"># 下载完文件存在表示下载成功</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> not successfully downloaded."</span>  <span class="comment"># 查无此人，下载失败</span></span><br><span class="line">            <span class="built_in">exit</span> -1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tgz"</span> ]; <span class="keyword">then</span>      <span class="comment"># 对于.tgz格式的文件，用zxvf命令解压</span></span><br><span class="line">            tar zxvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">elif</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tar"</span> ]; <span class="keyword">then</span>      <span class="comment"># 对于.tar格式的文件，用xvf命令解压</span></span><br><span class="line">            tar xvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure><p>执行完毕之后，<code>$OUTDIR</code>文件夹存放的内容有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br></pre></td></tr></table></figure><p>8、<strong>重点来了</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing train data..."</span>      <span class="comment"># 预处理训练语料</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    rm <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span>      <span class="comment"># 如果存在，先移除</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;CORPORA[@]&#125;</span>"</span>; <span class="keyword">do</span>      </span><br><span class="line">        cat <span class="variable">$orig</span>/<span class="variable">$f</span>.<span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$NORM_PUNC</span> <span class="variable">$l</span> | \      <span class="comment"># 先标准化符号</span></span><br><span class="line">            perl <span class="variable">$REM_NON_PRINT_CHAR</span> | \      <span class="comment"># 移除非打印字符</span></span><br><span class="line">            perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt;&gt; <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span>  <span class="comment"># 分词</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing test data..."</span>      <span class="comment"># 预处理测试语料</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$l</span>"</span> == <span class="string">"<span class="variable">$src</span>"</span> ]; <span class="keyword">then</span>      </span><br><span class="line">        t=<span class="string">"src"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        t=<span class="string">"ref"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    grep <span class="string">'&lt;seg id'</span> <span class="variable">$orig</span>/<span class="built_in">test</span>-full/newstest2014-deen-<span class="variable">$t</span>.<span class="variable">$l</span>.sgm | \      <span class="comment">#这一块操作没看懂</span></span><br><span class="line">        sed -e <span class="string">'s/&lt;seg id="[0-9]*"&gt;\s*//g'</span> | \      </span><br><span class="line">        sed -e <span class="string">'s/\s*&lt;\/seg&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">"s/\’/\'/g"</span> | \</span><br><span class="line">    perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/<span class="built_in">test</span>.<span class="variable">$l</span>      <span class="comment"># 分词</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── orig  <span class="comment"># 原始数据集的tgz文件+解压之后的结果</span></span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── <span class="built_in">test</span>-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br><span class="line">    ├── train.tags.en-de.tok.de</span><br><span class="line">    └── train.tags.en-de.tok.en</span><br></pre></td></tr></table></figure><p>预处理完毕之后，<code>test.en</code>的其中一条语句为<code>They are not even 100 metres apart : On Tuesday , the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights .</code>。可以看出来，标点符号已经和字母分开了。</p><p>9、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"splitting train and valid..."</span>      <span class="comment"># 划分训练集和验证集</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 == 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/valid.<span class="variable">$l</span>      <span class="comment"># 从训练集中，每100个句子抽1个句子作为验证集</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 != 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/train.<span class="variable">$l</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── orig  <span class="comment"># 原始数据集的tgz文件+解压之后的结果</span></span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── <span class="built_in">test</span>-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br><span class="line">    ├── train.de</span><br><span class="line">    ├── train.en</span><br><span class="line">    ├── train.tags.en-de.tok.de</span><br><span class="line">    ├── train.tags.en-de.tok.en</span><br><span class="line">    ├── valid.de</span><br><span class="line">    └── valid.en</span><br></pre></td></tr></table></figure><p>10、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">TRAIN=<span class="variable">$tmp</span>/train.de-en      <span class="comment"># 训练语料（包含src和tgt)</span></span><br><span class="line">BPE_CODE=<span class="variable">$prep</span>/code      <span class="comment"># BPECODE文件</span></span><br><span class="line">rm -f <span class="variable">$TRAIN</span>      <span class="comment"># train.de-en如果存在就删掉</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span>      </span><br><span class="line">    cat <span class="variable">$tmp</span>/train.<span class="variable">$l</span> &gt;&gt; <span class="variable">$TRAIN</span>  <span class="comment"># 其实就是简单地将src语料和tgt语料按顺序放到一个文件中，方便后面联合学习bpe</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"learn_bpe.py on <span class="variable">$&#123;TRAIN&#125;</span>..."</span>      <span class="comment"># 学习BPE</span></span><br><span class="line">python <span class="variable">$BPEROOT</span>/learn_bpe.py -s <span class="variable">$BPE_TOKENS</span> &lt; <span class="variable">$TRAIN</span> &gt; <span class="variable">$BPE_CODE</span>       <span class="comment"># 这里是将源语言和目标语言的语料联合起来学BPE的，因为我们用的是train.de-en</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> train.<span class="variable">$L</span> valid.<span class="variable">$L</span> <span class="built_in">test</span>.<span class="variable">$L</span>; <span class="keyword">do</span>      <span class="comment"># 用学到的bpecode应用到三份语料中（训练语料，验证语料，测试语料）</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"apply_bpe.py to <span class="variable">$&#123;f&#125;</span>..."</span></span><br><span class="line">        python <span class="variable">$BPEROOT</span>/apply_bpe.py -c <span class="variable">$BPE_CODE</span> &lt; <span class="variable">$tmp</span>/<span class="variable">$f</span> &gt; <span class="variable">$tmp</span>/bpe.<span class="variable">$f</span>      <span class="comment"># 输出到tmp中对应的文件，以bpe.作为前缀</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>在执行<code>learn_bpe.py</code>的时候，刚开始的速度特别慢，但是速度会越来越快，最终得到<code>code</code>文件。</p><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── code</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">    ├── bpe.test.de</span><br><span class="line">    ├── bpe.test.en</span><br><span class="line">    ├── bpe.train.de</span><br><span class="line">    ├── bpe.train.en</span><br><span class="line">    ├── bpe.valid.de</span><br><span class="line">    ├── bpe.valid.en</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br><span class="line">    ├── train.de</span><br><span class="line">    ├── train.de-en</span><br><span class="line">    ├── train.en</span><br><span class="line">    ├── train.tags.en-de.tok.de</span><br><span class="line">    ├── train.tags.en-de.tok.en</span><br><span class="line">    ├── valid.de</span><br><span class="line">    └── valid.en</span><br></pre></td></tr></table></figure><p>11、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250      <span class="comment"># 按照长度对训练语料和验证语料进行clean，只保留前250个token（cutoff 1-250），并将结果输出到output文件夹中</span></span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br></pre></td></tr></table></figure><p>中间输出结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhaodali@ubuntua:wmt14$ perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250</span><br><span class="line">clean-corpus.perl: processing /mnt/private_zhaodali_cq/datasets/security/wmt14/tmp/bpe.train.en &amp; .de to /mnt/private_zhaodali_cq/datasets/security/wmt14/train, cutoff 1-250, ratio 1.5</span><br><span class="line">..........(100000)..........(200000)..........(300000)..........(400000)..........(500000)..........(600000)..........(700000)..........(800000)..........(900000)..........(1000000)..........(1100000)..........(1200000)..........(1300000)..........(1400000)..........(1500000)..........(1600000)..........(1700000)..........(1800000)..........(1900000)..........(2000000)..........(2100000)..........(2200000)..........(2300000)..........(2400000)..........(2500000)..........(2600000)..........(2700000)..........(2800000)..........(2900000)..........(3000000)..........(3100000)..........(3200000)..........(3300000)..........(3400000)..........(3500000)..........(3600000)..........(3700000)..........(3800000)..........(3900000)..........(4000000)..........(4100000)..........(4200000)..........(4300000)..........(4400000)..........(4500000)....</span><br><span class="line">Input sentences: 4544200  Output sentences:  3961179</span><br><span class="line">zhaodali@ubuntua:wmt14$ perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br><span class="line">clean-corpus.perl: processing /mnt/private_zhaodali_cq/datasets/security/wmt14/tmp/bpe.valid.en &amp; .de to /mnt/private_zhaodali_cq/datasets/security/wmt14/valid, cutoff 1-250, ratio 1.5</span><br><span class="line">....</span><br><span class="line">Input sentences: 45901  Output sentences:  40058</span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── code</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">│   ├── bpe.test.de</span><br><span class="line">│   ├── bpe.test.en</span><br><span class="line">│   ├── bpe.train.de</span><br><span class="line">│   ├── bpe.train.en</span><br><span class="line">│   ├── bpe.valid.de</span><br><span class="line">│   ├── bpe.valid.en</span><br><span class="line">│   ├── test.de</span><br><span class="line">│   ├── test.en</span><br><span class="line">│   ├── train.de</span><br><span class="line">│   ├── train.de-en</span><br><span class="line">│   ├── train.en</span><br><span class="line">│   ├── train.tags.en-de.tok.de</span><br><span class="line">│   ├── train.tags.en-de.tok.en</span><br><span class="line">│   ├── valid.de</span><br><span class="line">│   └── valid.en</span><br><span class="line">├── train.de</span><br><span class="line">├── train.en</span><br><span class="line">├── valid.de</span><br><span class="line">└── valid.en</span><br></pre></td></tr></table></figure><p>12、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cp <span class="variable">$tmp</span>/bpe.test.<span class="variable">$L</span> <span class="variable">$prep</span>/<span class="built_in">test</span>.<span class="variable">$L</span>      <span class="comment"># 对于test语料，不进行clean，直接放到output文件夹。</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>执行完毕之后，得到的文件结构是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">./wmt17_en_de</span><br><span class="line">├── code</span><br><span class="line">├── orig  # 原始数据集的tgz文件+解压之后的结果</span><br><span class="line">│   ├── dev</span><br><span class="line">│   ├── test-full</span><br><span class="line">│   └── training</span><br><span class="line">└── tmp</span><br><span class="line">│   ├── bpe.test.de  # bpe之后的结果</span><br><span class="line">│   ├── bpe.test.en</span><br><span class="line">│   ├── bpe.train.de</span><br><span class="line">│   ├── bpe.train.en</span><br><span class="line">│   ├── bpe.valid.de</span><br><span class="line">│   ├── bpe.valid.en</span><br><span class="line">│   ├── test.de</span><br><span class="line">│   ├── test.en</span><br><span class="line">│   ├── train.de  # 训练集与验证划分之后的结果</span><br><span class="line">│   ├── train.de-en</span><br><span class="line">│   ├── train.en</span><br><span class="line">│   ├── train.tags.en-de.tok.de  # 训练集与验证集</span><br><span class="line">│   ├── train.tags.en-de.tok.en</span><br><span class="line">│   ├── valid.de</span><br><span class="line">│   └── valid.en</span><br><span class="line">├── train.de  # clean之后的结果</span><br><span class="line">├── train.en</span><br><span class="line">├── valid.de</span><br><span class="line">└── valid.en</span><br><span class="line">├── test.de</span><br><span class="line">├── test.en</span><br></pre></td></tr></table></figure><h3 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h3><p>执行完上述指令之后，我们需要继续将数据Binarize。并且统计词频，得到vocabulary文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../</span><br><span class="line">TEXT=./wmt17_en_de  <span class="comment"># 放置前面小节文件的根目录</span></span><br><span class="line">fairseq-preprocess \</span><br><span class="line">    --<span class="built_in">source</span>-lang en --target-lang de \</span><br><span class="line">    --trainpref <span class="variable">$TEXT</span>/train --validpref <span class="variable">$TEXT</span>/valid --testpref <span class="variable">$TEXT</span>/<span class="built_in">test</span> \</span><br><span class="line">    --destdir data-bin/wmt17_en_de --thresholdtgt 0 --thresholdsrc 0 \</span><br><span class="line">    --workers 20</span><br></pre></td></tr></table></figure><p>中间输出为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2022-04-29 19:38:39 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion=<span class="string">'cross_entropy'</span>, dataset_impl=<span class="string">'mmap'</span>, destdir=<span class="string">'data-bin/wmt17_en_de'</span>, dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler=<span class="string">'fixed'</span>, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path=<span class="string">'/tmp/plasma'</span>, profile=False, quantization_config_path=None, reset_logging=False, scoring=<span class="string">'bleu'</span>, seed=1, simul_type=None, source_lang=<span class="string">'en'</span>, srcdict=None, suppress_crashes=False, target_lang=<span class="string">'de'</span>, task=<span class="string">'translation'</span>, tensorboard_logdir=None, testpref=<span class="string">'wmt14//test'</span>, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=<span class="string">'wmt14//train'</span>, use_plasma_view=False, user_dir=None, validpref=<span class="string">'wmt14//valid'</span>, wandb_project=None, workers=20)</span><br><span class="line">2022-04-29 19:38:57 | INFO | fairseq_cli.preprocess | [en] Dictionary: 40360 types</span><br><span class="line">2022-04-29 19:39:39 | INFO | fairseq_cli.preprocess | [en] wmt14//train.en: 3961179 sents, 116600288 tokens, 0.0% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:39:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 40360 types</span><br><span class="line">2022-04-29 19:39:42 | INFO | fairseq_cli.preprocess | [en] wmt14//valid.en: 40058 sents, 1180285 tokens, 0.00322% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:39:42 | INFO | fairseq_cli.preprocess | [en] Dictionary: 40360 types</span><br><span class="line">2022-04-29 19:39:44 | INFO | fairseq_cli.preprocess | [en] wmt14//test.en: 3003 sents, 81185 tokens, 0.00246% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:39:44 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42720 types</span><br><span class="line">2022-04-29 19:40:26 | INFO | fairseq_cli.preprocess | [de] wmt14//train.de: 3961179 sents, 119369232 tokens, 0.0% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:40:26 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42720 types</span><br><span class="line">2022-04-29 19:40:31 | INFO | fairseq_cli.preprocess | [de] wmt14//valid.de: 40058 sents, 1209744 tokens, 0.00116% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:40:31 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42720 types</span><br><span class="line">2022-04-29 19:40:32 | INFO | fairseq_cli.preprocess | [de] wmt14//test.de: 3003 sents, 84629 tokens, 0.907% replaced by &lt;unk&gt;</span><br><span class="line">2022-04-29 19:40:32 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt17_en_de</span><br></pre></td></tr></table></figure><p>执行完毕之后，可以得到的文件结构如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data-bin</span><br><span class="line">│   ├── preprocess.log</span><br><span class="line">│   └── wmt17_en_de</span><br><span class="line">│       ├── dict.de.txt  <span class="comment"># vocabulary文件</span></span><br><span class="line">│       ├── dict.en.txt</span><br><span class="line">│       ├── preprocess.log</span><br><span class="line">│       ├── test.en-de.de.bin</span><br><span class="line">│       ├── test.en-de.de.idx</span><br><span class="line">│       ├── test.en-de.en.bin</span><br><span class="line">│       ├── test.en-de.en.idx</span><br><span class="line">│       ├── train.en-de.de.bin</span><br><span class="line">│       ├── train.en-de.de.idx</span><br><span class="line">│       ├── train.en-de.en.bin</span><br><span class="line">│       ├── train.en-de.en.idx</span><br><span class="line">│       ├── valid.en-de.de.bin</span><br><span class="line">│       ├── valid.en-de.de.idx</span><br><span class="line">│       ├── valid.en-de.en.bin</span><br><span class="line">│       └── valid.en-de.en.idx</span><br><span class="line">└── wmt17_en_de</span><br><span class="line">    ├── code</span><br><span class="line">    ├── orig  <span class="comment"># 原始数据集的tgz文件+解压之后的结果</span></span><br><span class="line">    │   ├── dev</span><br><span class="line">    │   ├── <span class="built_in">test</span>-full</span><br><span class="line">    │   ├── training</span><br><span class="line">    ├── tmp</span><br><span class="line">    │   ├── bpe.test.de</span><br><span class="line">    │   ├── bpe.test.en</span><br><span class="line">    │   ├── bpe.train.de</span><br><span class="line">    │   ├── bpe.train.en</span><br><span class="line">    │   ├── bpe.valid.de</span><br><span class="line">    │   ├── bpe.valid.en</span><br><span class="line">    │   ├── test.de</span><br><span class="line">    │   ├── test.en</span><br><span class="line">    │   ├── train.de</span><br><span class="line">    │   ├── train.de-en</span><br><span class="line">    │   ├── train.en</span><br><span class="line">    │   ├── train.tags.en-de.tok.de</span><br><span class="line">    │   ├── train.tags.en-de.tok.en</span><br><span class="line">    │   ├── valid.de</span><br><span class="line">    │   └── valid.en</span><br><span class="line">    ├── train.de</span><br><span class="line">    ├── train.en</span><br><span class="line">    ├── valid.de</span><br><span class="line">    └── valid.en</span><br><span class="line">    ├── test.de</span><br><span class="line">    ├── test.en</span><br></pre></td></tr></table></figure><p>得到的<code>data-bin</code>文件夹就是我们处理完之后的结果，可以直接用来训练和测试。因为它其中的文件已经使用bpe编码了，所以不需要<code>code</code>文件，但是仍然需要<code>dict.de.txt</code>和<code>dict.en.txt</code>用于字符与ID之间的转换。</p><p>若直接测试句子的话，仍然需要<code>code</code>文件对该句子进行编码，然后需要<code>dict.de.txt</code>和<code>dict.en.txt</code>用于字符与ID之间的转换。</p><p>另外需要注意的是，因为<code>joined_dictionary=False</code>，所以<code>dict.de.txt</code>与<code>dict.en.txt</code>文件内容是不一样的。</p><blockquote><p>joined_dictionary：源端和目标端使用同一个词表，对于相似语言（如英语和西班牙语）来说，有很多的单词是相同的，使用同一个词表可以降低词表和参数的总规模。</p></blockquote><p>所以<a href="https://github.com/pytorch/fairseq/tree/main/examples/translation#iwslt14-german-to-english-transformer" target="_blank" rel="noopener">官方教程</a>在训练时用的<code>--share-decoder-input-output-embed</code>参数。而我看另外一个<code>dict.de.txt</code>与<code>dict.en.txt</code>文件内容一致的，训练时用了<code>--share-all-embeddings</code>参数。</p><blockquote><p>可以看<a href="https://github.com/pytorch/fairseq/issues/2537#issuecomment-683989567" target="_blank" rel="noopener">这里</a>： when you specify —share-all-embeddings then the embedding matrices for encoder input, decoder input and decoder output are all shared. when you specify —share-decoder-input-output-embed, then the matrices for decoder input and output are shared, but encoder has its own embeddings. </p></blockquote><p>补充一下，当<code>--share-decoder-input-output-embed</code>时，实际对应的代码如下（<code>fairseq/models/transformer/transformer_decoder.py</code>文件中的<code>build_output_projection</code>函数）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> self.share_input_output_embed:</span><br><span class="line">    self.output_projection = nn.Linear(</span><br><span class="line">        self.embed_tokens.weight.shape[<span class="number">1</span>],</span><br><span class="line">        self.embed_tokens.weight.shape[<span class="number">0</span>],</span><br><span class="line">        bias=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    self.output_projection.weight = self.embed_tokens.weight  <span class="comment"># torch.Size([37056, 512])</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://txshi-mt.com/2019/02/28/NMT-Tutorial-3e2-subword/" target="_blank" rel="noopener">NMT Tutorial 3扩展e第2部分. Subword</a><br><a href="https://zhuanlan.zhihu.com/p/86965595" target="_blank" rel="noopener">深入理解NLP Subword算法：BPE、WordPiece、ULM</a><br><a href="https://blog.csdn.net/weixin_38937984/article/details/103995209" target="_blank" rel="noopener">moses(mosesdecoder)数据预处理&amp;BPE分词&amp;moses用法总结</a><br><a href="https://blog.csdn.net/jmh1996/article/details/89286898" target="_blank" rel="noopener">机器翻译 bpe——bytes-pair-encoding以及开源项目subword-nmt快速入门</a><br><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" target="_blank" rel="noopener">Byte Pair Encoding</a><br><a href="https://www.dengbocong.cn/Deep-Learning/38fa61dd1a7b/" target="_blank" rel="noopener">有必要了解的Subword算法模型</a><br><a href="http://www.noobyard.com/article/p-kcottpoh-uv.html" target="_blank" rel="noopener">bpe分词算法的原理</a><br><a href="https://juejin.cn/post/7088322473640329230" target="_blank" rel="noopener">BPE 算法原理及使用指南【深入浅出】</a><br><a href="https://wmathor.com/index.php/archives/1517/" target="_blank" rel="noopener">BPE 算法详解</a><br><a href="https://www.cnblogs.com/mmxy/p/14076930.html" target="_blank" rel="noopener">WMT14 en-de翻译数据集预处理步骤</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近需要用到&lt;a href=&quot;https://github.com/pytorch/fairseq&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fairseq&lt;/a&gt;框架中的翻译任务，这里记录一下。&lt;/p&gt;
&lt;h2 id=&quot;从实战开始&quot;&gt;&lt;a href=&quot;#从实战开始&quot; class=&quot;headerlink&quot; title=&quot;从实战开始&quot;&gt;&lt;/a&gt;从实战开始&lt;/h2&gt;&lt;p&gt;首先下载翻译模型：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mkdir -p model&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; model&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bunzip2 wmt16.en-de.joined-dict.transformer.tar.bz2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tar -xvf wmt16.en-de.joined-dict.transformer.tar&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;解压后的文件如下：&lt;/p&gt;
    
    </summary>
    
    
      <category term="DeepLearningApplications" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/"/>
    
      <category term="自然语言处理" scheme="https://www.zdaiot.com/categories/DeepLearningApplications/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NLP" scheme="https://www.zdaiot.com/tags/NLP/"/>
    
      <category term="fairseq" scheme="https://www.zdaiot.com/tags/fairseq/"/>
    
      <category term="translation" scheme="https://www.zdaiot.com/tags/translation/"/>
    
  </entry>
  
</feed>
